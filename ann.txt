 Percent |	Source code & Disassembly of vmlinux for cycles (1479384 samples, percent: local period)
--------------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010708a20 <arm_smmu_cmdq_issue_cmdlist>:
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      *   insert their own list of commands then all of the commands from one
         :                      *   CPU will appear before any of the commands from the other CPU.
         :                      */
         :                      static int arm_smmu_cmdq_issue_cmdlist(struct arm_smmu_device *smmu,
         :                      u64 *cmds, int n, bool sync)
         :                      {
    0.00 :   ffff800010708a20:       stp     x29, x30, [sp, #-496]!
    0.00 :   ffff800010708a24:       mov     x29, sp
    0.00 :   ffff800010708a28:       add     x5, x29, #0xcf
    0.02 :   ffff800010708a2c:       stp     x25, x26, [sp, #64]
    0.00 :   ffff800010708a30:       and     x26, x5, #0xffffffffffffffc0
    0.01 :   ffff800010708a34:       stp     x23, x24, [sp, #48]
    0.01 :   ffff800010708a38:       stp     x27, x28, [sp, #80]
         :                      u64 cmd_sync[CMDQ_ENT_DWORDS];
         :                      u32 prod;
         :                      unsigned long flags;
         :                      bool owner;
         :                      struct arm_smmu_cmdq *cmdq = &smmu->cmdq;
         :                      struct arm_smmu_ll_queue llq = {
    0.00 :   ffff800010708a3c:       add     x23, x26, #0x80
         :                      .max_n_shift = cmdq->q.llq.max_n_shift,
    0.00 :   ffff800010708a40:       add     x27, x0, #0x40
         :                      {
    0.01 :   ffff800010708a44:       stp     x19, x20, [sp, #16]
    0.01 :   ffff800010708a48:       stp     x21, x22, [sp, #32]
    0.00 :   ffff800010708a4c:       mov     x28, x0
         :                      struct arm_smmu_ll_queue llq = {
    0.01 :   ffff800010708a50:       stp     xzr, xzr, [x23, #16]
         :                      {
    0.00 :   ffff800010708a54:       adrp    x0, ffff800011899000 <page_wait_table+0x1500>
         :                      struct arm_smmu_ll_queue llq = {
    0.01 :   ffff800010708a58:       stp     xzr, xzr, [x23, #32]
         :                      {
    0.00 :   ffff800010708a5c:       add     x0, x0, #0x8c8
         :                      struct arm_smmu_ll_queue llq = {
    0.01 :   ffff800010708a60:       stp     xzr, xzr, [x23, #48]
    0.01 :   ffff800010708a64:       stp     xzr, xzr, [x23, #64]
         :                      .max_n_shift = cmdq->q.llq.max_n_shift,
    0.01 :   ffff800010708a68:       ldr     w19, [x27, #64]
         :                      {
    0.08 :   ffff800010708a6c:       str     x1, [x29, #96]
    0.00 :   ffff800010708a70:       and     w1, w3, #0xff
         :                      }, head = llq;
    0.00 :   ffff800010708a74:       str     w19, [x26, #192]
         :                      {
    0.00 :   ffff800010708a78:       str     w1, [x29, #108]
    0.00 :   ffff800010708a7c:       ldr     x1, [x0]
    0.01 :   ffff800010708a80:       str     x1, [x29, #488]
    0.00 :   ffff800010708a84:       mov     x1, #0x0                        // #0
    0.00 :   ffff800010708a88:       str     w2, [x29, #104]
         :                      }, head = llq;
    0.00 :   ffff800010708a8c:       mov     x1, x23
         :                      struct arm_smmu_ll_queue llq = {
    0.00 :   ffff800010708a90:       stp     xzr, xzr, [x26, #128]
         :                      }, head = llq;
    0.00 :   ffff800010708a94:       mov     x2, #0x80                       // #128
         :                      struct arm_smmu_ll_queue llq = {
    0.00 :   ffff800010708a98:       stp     xzr, xzr, [x23, #80]
         :                      }, head = llq;
    0.00 :   ffff800010708a9c:       mov     x0, x26
         :                      struct arm_smmu_ll_queue llq = {
    0.01 :   ffff800010708aa0:       stp     xzr, xzr, [x23, #96]
    0.00 :   ffff800010708aa4:       stp     xzr, xzr, [x23, #112]
         :                      }, head = llq;
    0.00 :   ffff800010708aa8:       bl      ffff800010c92540 <__memcpy>
         :                      arch_local_save_flags():
         :                      */
         :                      static inline unsigned long arch_local_save_flags(void)
         :                      {
         :                      unsigned long flags;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010708aac:       mrs     x8, daif
    0.00 :   ffff800010708ab0:       mov     x21, x8
         :                      arch_irqs_disabled_flags():
         :
         :                      static inline int arch_irqs_disabled_flags(unsigned long flags)
         :                      {
         :                      int res;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010708ab4:       and     w0, w8, #0x80
         :                      arch_local_irq_save():
         :
         :                      /*
         :                      * There are too many states with IRQs disabled, just keep the current
         :                      * state if interrupts are already disabled/masked.
         :                      */
         :                      if (!arch_irqs_disabled_flags(flags))
    0.00 :   ffff800010708ab8:       cbnz    w0, ffff800010708ac4 <arm_smmu_cmdq_issue_cmdlist+0xa4>
         :                      arch_local_irq_disable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010708abc:       mov     x0, #0x60                       // #96
    0.03 :   ffff800010708ac0:       msr     daifset, #0x2
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010708ac4:       ldr     x0, [x28, #64]
         :                      queue_has_space():
         :                      prod = Q_IDX(q, q->prod);
    0.00 :   ffff800010708ac8:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010708acc:       ldr     w2, [x29, #104]
    0.00 :   ffff800010708ad0:       lsl     w19, w1, w19
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      int ret = 0;
         :
         :                      /* 1. Allocate some space in the queue */
         :                      local_irq_save(flags);
         :                      llq.val = READ_ONCE(cmdq->q.llq.val);
    0.00 :   ffff800010708ad4:       str     x0, [x26, #128]
         :                      u64 old;
         :
         :                      while (!queue_has_space(&llq, n + sync)) {
         :                      local_irq_restore(flags);
         :                      if (arm_smmu_cmdq_poll_until_not_full(smmu, &llq))
         :                      dev_err_ratelimited(smmu->dev, "CMDQ timeout\n");
    0.00 :   ffff800010708ad8:       adrp    x1, ffff800010eac000 <arm_smmu_of_match+0x98>
    0.00 :   ffff800010708adc:       ldr     w0, [x29, #108]
         :                      queue_has_space():
         :                      prod = Q_IDX(q, q->prod);
    0.00 :   ffff800010708ae0:       sub     w22, w19, #0x1
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      dev_err_ratelimited(smmu->dev, "CMDQ timeout\n");
    0.00 :   ffff800010708ae4:       add     x1, x1, #0x708
         :                      queue_inc_prod_n():
         :                      u32 prod = (Q_WRP(q, q->prod) | Q_IDX(q, q->prod)) + n;
    0.00 :   ffff800010708ae8:       orr     w20, w22, w19
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      dev_err_ratelimited(smmu->dev, "CMDQ timeout\n");
    0.00 :   ffff800010708aec:       add     x24, x1, #0x20
    0.00 :   ffff800010708af0:       add     w25, w0, w2
    0.00 :   ffff800010708af4:       nop
         :                      while (!queue_has_space(&llq, n + sync)) {
    0.00 :   ffff800010708af8:       ldp     w1, w4, [x26, #128]
         :                      queue_has_space():
         :                      cons = Q_IDX(q, q->cons);
    0.00 :   ffff800010708afc:       and     w0, w4, w22
         :                      prod = Q_IDX(q, q->prod);
    0.00 :   ffff800010708b00:       and     w3, w1, w22
         :                      space = (1 << q->max_n_shift) - (prod - cons);
    0.00 :   ffff800010708b04:       add     w2, w0, w19
         :                      if (Q_WRP(q, q->prod) == Q_WRP(q, q->cons))
    0.00 :   ffff800010708b08:       eor     w9, w4, w1
         :                      space = (1 << q->max_n_shift) - (prod - cons);
    0.00 :   ffff800010708b0c:       tst     w9, w19
    0.00 :   ffff800010708b10:       sub     w2, w2, w3
    0.00 :   ffff800010708b14:       sub     w0, w0, w3
    0.00 :   ffff800010708b18:       csel    w0, w0, w2, ne  // ne = any
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      while (!queue_has_space(&llq, n + sync)) {
    0.00 :   ffff800010708b1c:       cmp     w0, w25
    0.00 :   ffff800010708b20:       b.cc    ffff800010708ca4 <arm_smmu_cmdq_issue_cmdlist+0x284>  // b.lo, b.ul, b.last
         :                      queue_inc_prod_n():
         :                      u32 prod = (Q_WRP(q, q->prod) | Q_IDX(q, q->prod)) + n;
    0.00 :   ffff800010708b24:       and     w3, w1, w20
         :                      return Q_OVF(q->prod) | Q_WRP(q, prod) | Q_IDX(q, prod);
    0.00 :   ffff800010708b28:       and     w0, w1, #0x80000000
         :                      u32 prod = (Q_WRP(q, q->prod) | Q_IDX(q, q->prod)) + n;
    0.00 :   ffff800010708b2c:       add     w3, w3, w25
         :                      arm_smmu_cmdq_issue_cmdlist():
         :
         :                      head.cons = llq.cons;
         :                      head.prod = queue_inc_prod_n(&llq, n + sync) |
         :                      CMDQ_PROD_OWNED_FLAG;
         :
         :                      old = cmpxchg_relaxed(&cmdq->q.llq.val, llq.val, head.val);
    0.00 :   ffff800010708b30:       ldr     x1, [x23]
         :                      queue_inc_prod_n():
         :                      return Q_OVF(q->prod) | Q_WRP(q, prod) | Q_IDX(q, prod);
    0.00 :   ffff800010708b34:       and     w3, w3, w20
    0.00 :   ffff800010708b38:       orr     w3, w3, w0
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      head.prod = queue_inc_prod_n(&llq, n + sync) |
    0.00 :   ffff800010708b3c:       orr     w0, w3, #0x80000000
         :                      head.cons = llq.cons;
    0.00 :   ffff800010708b40:       stp     w0, w4, [x26]
         :                      old = cmpxchg_relaxed(&cmdq->q.llq.val, llq.val, head.val);
    0.00 :   ffff800010708b44:       ldr     x2, [x26]
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010708b48:       b       ffff800010708b74 <arm_smmu_cmdq_issue_cmdlist+0x154>
    0.00 :   ffff800010708b4c:       b       ffff800010708b74 <arm_smmu_cmdq_issue_cmdlist+0x154>
         :                      __lse__cmpxchg_case_64():
         :                      }
         :
         :                      __CMPXCHG_CASE(w, b,     ,  8,   )
         :                      __CMPXCHG_CASE(w, h,     , 16,   )
         :                      __CMPXCHG_CASE(w,  ,     , 32,   )
         :                      __CMPXCHG_CASE(x,  ,     , 64,   )
    0.00 :   ffff800010708b50:       mov     x0, x27
    0.00 :   ffff800010708b54:       mov     x4, x1
    0.00 :   ffff800010708b58:       cas     x4, x2, [x27]
    0.00 :   ffff800010708b5c:       mov     x0, x4
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      if (old == llq.val)
    0.00 :   ffff800010708b60:       ldr     x1, [x23]
    0.00 :   ffff800010708b64:       cmp     x1, x0
    0.00 :   ffff800010708b68:       b.eq    ffff800010708b84 <arm_smmu_cmdq_issue_cmdlist+0x164>  // b.none
         :                      break;
         :
         :                      llq.val = old;
    0.00 :   ffff800010708b6c:       str     x0, [x23]
         :                      do {
    0.00 :   ffff800010708b70:       b       ffff800010708af8 <arm_smmu_cmdq_issue_cmdlist+0xd8>
         :                      __ll_sc__cmpxchg_case_64():
         :                      * constraint for 32 bit operations.
         :                      */
         :                      __CMPXCHG_CASE(w, b,     ,  8,        ,  ,  ,         , K)
         :                      __CMPXCHG_CASE(w, h,     , 16,        ,  ,  ,         , K)
         :                      __CMPXCHG_CASE(w,  ,     , 32,        ,  ,  ,         , K)
         :                      __CMPXCHG_CASE( ,  ,     , 64,        ,  ,  ,         , L)
    0.00 :   ffff800010708b74:       b       ffff80001070b738 <arm_smmu_device_probe+0x11c8>
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      if (old == llq.val)
    0.00 :   ffff800010708b78:       ldr     x1, [x23]
    0.00 :   ffff800010708b7c:       cmp     x1, x0
    0.00 :   ffff800010708b80:       b.ne    ffff800010708b6c <arm_smmu_cmdq_issue_cmdlist+0x14c>  // b.any
         :                      } while (1);
         :                      owner = !(llq.prod & CMDQ_PROD_OWNED_FLAG);
    0.00 :   ffff800010708b84:       ldr     w25, [x23]
         :                      head.prod &= ~CMDQ_PROD_OWNED_FLAG;
    0.00 :   ffff800010708b88:       and     w0, w3, #0x7fffffff
    0.00 :   ffff800010708b8c:       str     w0, [x26]
         :                      llq.prod &= ~CMDQ_PROD_OWNED_FLAG;
    0.00 :   ffff800010708b90:       and     w12, w25, #0x7fffffff
         :                      arm_smmu_cmdq_write_entries():
         :                      for (i = 0; i < n; ++i) {
    0.00 :   ffff800010708b94:       ldr     w0, [x29, #104]
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      llq.prod &= ~CMDQ_PROD_OWNED_FLAG;
    0.00 :   ffff800010708b98:       str     w12, [x23]
         :                      arm_smmu_cmdq_write_entries():
         :                      .max_n_shift    = cmdq->q.llq.max_n_shift,
    0.00 :   ffff800010708b9c:       ldr     w2, [x27, #64]
         :                      for (i = 0; i < n; ++i) {
    0.00 :   ffff800010708ba0:       cmp     w0, #0x0
    0.00 :   ffff800010708ba4:       b.le    ffff800010708c14 <arm_smmu_cmdq_issue_cmdlist+0x1f4>
         :                      queue_inc_prod_n():
         :                      u32 prod = (Q_WRP(q, q->prod) | Q_IDX(q, q->prod)) + n;
    0.00 :   ffff800010708ba8:       mov     w13, #0x1                       // #1
    0.00 :   ffff800010708bac:       sub     w10, w0, #0x1
    0.00 :   ffff800010708bb0:       lsl     w0, w13, w2
    0.00 :   ffff800010708bb4:       sub     w11, w0, #0x1
    0.00 :   ffff800010708bb8:       orr     w11, w11, w0
    0.00 :   ffff800010708bbc:       ldr     x24, [x29, #96]
    0.00 :   ffff800010708bc0:       and     w1, w12, w11
    0.00 :   ffff800010708bc4:       add     w10, w10, w1
    0.00 :   ffff800010708bc8:       b       ffff800010708bd4 <arm_smmu_cmdq_issue_cmdlist+0x1b4>
    0.00 :   ffff800010708bcc:       ldr     w2, [x27, #64]
    0.00 :   ffff800010708bd0:       add     w1, w1, #0x1
         :                      arm_smmu_cmdq_write_entries():
         :                      queue_write(Q_ENT(&cmdq->q, prod), cmd, CMDQ_ENT_DWORDS);
    0.00 :   ffff800010708bd4:       ldr     x3, [x27, #160]
    0.00 :   ffff800010708bd8:       lsl     w0, w13, w2
    0.00 :   ffff800010708bdc:       sub     w0, w0, #0x1
    0.00 :   ffff800010708be0:       ldr     x4, [x27, #136]
    0.00 :   ffff800010708be4:       and     w0, w0, w11
         :                      queue_write():
         :                      *dst++ = cpu_to_le64(*src++);
    0.00 :   ffff800010708be8:       ldr     x9, [x24]
         :                      arm_smmu_cmdq_write_entries():
         :                      queue_write(Q_ENT(&cmdq->q, prod), cmd, CMDQ_ENT_DWORDS);
    0.00 :   ffff800010708bec:       lsl     x2, x3, #3
    0.00 :   ffff800010708bf0:       and     w0, w0, w1
    0.00 :   ffff800010708bf4:       add     x24, x24, #0x10
         :                      for (i = 0; i < n; ++i) {
    0.00 :   ffff800010708bf8:       cmp     w1, w10
         :                      queue_write(Q_ENT(&cmdq->q, prod), cmd, CMDQ_ENT_DWORDS);
    0.00 :   ffff800010708bfc:       mul     x0, x0, x2
    0.00 :   ffff800010708c00:       add     x2, x4, x0
         :                      queue_write():
         :                      *dst++ = cpu_to_le64(*src++);
    0.00 :   ffff800010708c04:       str     x9, [x4, x0]
    0.00 :   ffff800010708c08:       ldur    x0, [x24, #-8]
    0.00 :   ffff800010708c0c:       str     x0, [x2, #8]
         :                      arm_smmu_cmdq_write_entries():
         :                      for (i = 0; i < n; ++i) {
    0.00 :   ffff800010708c10:       b.ne    ffff800010708bcc <arm_smmu_cmdq_issue_cmdlist+0x1ac>  // b.any
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      /*
         :                      * 2. Write our commands into the queue
         :                      * Dependency ordering from the cmpxchg() loop above.
         :                      */
         :                      arm_smmu_cmdq_write_entries(cmdq, cmds, llq.prod, n);
         :                      if (sync) {
    0.00 :   ffff800010708c14:       ldr     w0, [x29, #108]
    0.00 :   ffff800010708c18:       cbnz    w0, ffff800010708f8c <arm_smmu_cmdq_issue_cmdlist+0x56c>
         :                      */
         :                      arm_smmu_cmdq_shared_lock(cmdq);
         :                      }
         :
         :                      /* 3. Mark our slots as valid, ensuring commands are visible first */
         :                      dma_wmb();
    0.00 :   ffff800010708c1c:       dmb     oshst
         :                      arm_smmu_cmdq_set_valid_map():
         :                      __arm_smmu_cmdq_poll_set_valid_map(cmdq, sprod, eprod, true);
    0.00 :   ffff800010708c20:       ldr     w3, [x26]
    0.00 :   ffff800010708c24:       add     x5, x28, #0x100
    0.00 :   ffff800010708c28:       ldr     w0, [x28, #128]
    0.00 :   ffff800010708c2c:       mov     w4, #0x1                        // #1
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      arm_smmu_cmdq_set_valid_map(cmdq, llq.prod, head.prod);
    0.00 :   ffff800010708c30:       ldr     w24, [x26, #128]
         :                      arm_smmu_cmdq_set_valid_map():
         :                      __arm_smmu_cmdq_poll_set_valid_map(cmdq, sprod, eprod, true);
    0.00 :   ffff800010708c34:       mov     x1, x5
    0.00 :   ffff800010708c38:       str     x5, [x29, #96]
    0.00 :   ffff800010708c3c:       mov     w2, w24
    0.00 :   ffff800010708c40:       bl      ffff8000107080c8 <__arm_smmu_cmdq_poll_set_valid_map.isra.32>
         :                      arm_smmu_cmdq_issue_cmdlist():
         :
         :                      /* 4. If we are the owner, take control of the SMMU hardware */
         :                      if (owner) {
    0.00 :   ffff800010708c44:       tbnz    w25, #31, ffff800010708df8 <arm_smmu_cmdq_issue_cmdlist+0x3d8>
         :                      __read_once_size():
    0.00 :   ffff800010708c48:       ldr     w0, [x28, #264]
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      /* a. Wait for previous owner to finish */
         :                      atomic_cond_read_relaxed(&cmdq->owner_prod, VAL == llq.prod);
    0.00 :   ffff800010708c4c:       add     x1, x28, #0x108
    0.00 :   ffff800010708c50:       ldr     x5, [x29, #96]
    0.00 :   ffff800010708c54:       cmp     w24, w0
    0.00 :   ffff800010708c58:       b.eq    ffff800010708c8c <arm_smmu_cmdq_issue_cmdlist+0x26c>  // b.none
    0.00 :   ffff800010708c5c:       nop
    0.00 :   ffff800010708c60:       sxtw    x0, w0
         :                      __cmpwait_case_32():
         :                      : [val] "r" (val));                                             \
         :                      }
         :
         :                      __CMPWAIT_CASE(w, b, 8);
         :                      __CMPWAIT_CASE(w, h, 16);
         :                      __CMPWAIT_CASE(w,  , 32);
    0.00 :   ffff800010708c64:       sevl
    0.00 :   ffff800010708c68:       wfe
    0.00 :   ffff800010708c6c:       ldxr    w2, [x1]
    0.00 :   ffff800010708c70:       eor     w2, w2, w0
    0.00 :   ffff800010708c74:       cbnz    w2, ffff800010708c7c <arm_smmu_cmdq_issue_cmdlist+0x25c>
    0.00 :   ffff800010708c78:       wfe
         :                      __read_once_size():
    0.00 :   ffff800010708c7c:       ldr     w0, [x28, #264]
         :                      arm_smmu_cmdq_issue_cmdlist():
    0.00 :   ffff800010708c80:       ldr     w2, [x23]
    0.00 :   ffff800010708c84:       cmp     w2, w0
    0.00 :   ffff800010708c88:       b.ne    ffff800010708c60 <arm_smmu_cmdq_issue_cmdlist+0x240>  // b.any
         :                      arch_static_branch_jump():
    0.00 :   ffff800010708c8c:       b       ffff800010708dc0 <arm_smmu_cmdq_issue_cmdlist+0x3a0>
    0.00 :   ffff800010708c90:       b       ffff800010708dc0 <arm_smmu_cmdq_issue_cmdlist+0x3a0>
         :                      __lse_atomic_fetch_andnot_relaxed():
         :                      ATOMIC_FETCH_OPS(andnot, ldclr)
    0.00 :   ffff800010708c94:       mov     w24, #0x80000000                // #-2147483648
    0.00 :   ffff800010708c98:       add     x0, x28, #0x40
    0.00 :   ffff800010708c9c:       ldclr   w24, w24, [x0]
    0.00 :   ffff800010708ca0:       b       ffff800010708dcc <arm_smmu_cmdq_issue_cmdlist+0x3ac>
         :                      arch_local_irq_restore():
         :                      /*
         :                      * restore saved IRQ state
         :                      */
         :                      static inline void arch_local_irq_restore(unsigned long flags)
         :                      {
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010708ca4:       msr     daif, x21
         :                      arch_local_save_flags():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010708ca8:       mrs     x4, daif
         :                      arch_irqs_disabled_flags():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010708cac:       and     w0, w4, #0x80
         :                      arch_local_irq_save():
         :                      if (!arch_irqs_disabled_flags(flags))
    0.00 :   ffff800010708cb0:       cbnz    w0, ffff800010708cbc <arm_smmu_cmdq_issue_cmdlist+0x29c>
         :                      arch_local_irq_disable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010708cb4:       mov     x0, #0x60                       // #96
    0.00 :   ffff800010708cb8:       msr     daifset, #0x2
         :                      atomic_cmpxchg_relaxed():
         :                      #if defined(arch_atomic_cmpxchg_relaxed)
         :                      static inline int
         :                      atomic_cmpxchg_relaxed(atomic_t *v, int old, int new)
         :                      {
         :                      kasan_check_write(v, sizeof(*v));
         :                      return arch_atomic_cmpxchg_relaxed(v, old, new);
    0.00 :   ffff800010708cbc:       add     x3, x28, #0x10c
         :                      arch_static_branch_jump():
    0.00 :   ffff800010708cc0:       b       ffff800010708d24 <arm_smmu_cmdq_issue_cmdlist+0x304>
    0.00 :   ffff800010708cc4:       b       ffff800010708d24 <arm_smmu_cmdq_issue_cmdlist+0x304>
         :                      __lse__cmpxchg_case_32():
         :                      __CMPXCHG_CASE(w,  ,     , 32,   )
    0.00 :   ffff800010708cc8:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010708ccc:       mov     x0, x3
    0.00 :   ffff800010708cd0:       mov     w2, #0x80000000                 // #-2147483648
    0.00 :   ffff800010708cd4:       mov     w5, w1
    0.00 :   ffff800010708cd8:       cas     w5, w2, [x3]
    0.00 :   ffff800010708cdc:       mov     w0, w5
         :                      arm_smmu_cmdq_poll_until_not_full():
         :                      if (arm_smmu_cmdq_exclusive_trylock_irqsave(cmdq, flags)) {
    0.00 :   ffff800010708ce0:       cbnz    w0, ffff800010708d38 <arm_smmu_cmdq_issue_cmdlist+0x318>
         :                      __raw_readl():
         :
         :                      #define __raw_readl __raw_readl
         :                      static inline u32 __raw_readl(const volatile void __iomem *addr)
         :                      {
         :                      u32 val;
         :                      asm volatile(ALTERNATIVE("ldr %w0, [%1]",
    0.00 :   ffff800010708ce4:       ldr     x1, [x27, #176]
    0.00 :   ffff800010708ce8:       ldr     w1, [x1]
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010708cec:       str     w1, [x28, #68]
         :                      atomic_set_release():
         :
         :                      #ifndef atomic_set_release
         :                      static inline void
         :                      atomic_set_release(atomic_t *v, int i)
         :                      {
         :                      smp_store_release(&(v)->counter, i);
    0.00 :   ffff800010708cf0:       add     x1, x28, #0x10c
    0.00 :   ffff800010708cf4:       stlr    w0, [x1]
         :                      arch_local_irq_restore():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010708cf8:       msr     daif, x4
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010708cfc:       ldr     x0, [x28, #64]
         :                      arm_smmu_cmdq_poll_until_not_full():
         :                      llq->val = READ_ONCE(cmdq->q.llq.val);
    0.00 :   ffff800010708d00:       str     x0, [x23]
    0.00 :   ffff800010708d04:       nop
         :                      arch_local_save_flags():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010708d08:       mrs     x8, daif
    0.00 :   ffff800010708d0c:       mov     x21, x8
         :                      arch_irqs_disabled_flags():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010708d10:       and     w0, w8, #0x80
         :                      arch_local_irq_save():
         :                      if (!arch_irqs_disabled_flags(flags))
    0.00 :   ffff800010708d14:       cbnz    w0, ffff800010708af8 <arm_smmu_cmdq_issue_cmdlist+0xd8>
         :                      arch_local_irq_disable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010708d18:       mov     x0, #0x60                       // #96
    0.00 :   ffff800010708d1c:       msr     daifset, #0x2
    0.00 :   ffff800010708d20:       b       ffff800010708af8 <arm_smmu_cmdq_issue_cmdlist+0xd8>
         :                      __ll_sc__cmpxchg_case_32():
         :                      __CMPXCHG_CASE(w,  ,     , 32,        ,  ,  ,         , K)
    0.00 :   ffff800010708d24:       mov     x1, #0x0                        // #0
    0.00 :   ffff800010708d28:       mov     w2, #0x80000000                 // #-2147483648
    0.00 :   ffff800010708d2c:       add     x5, x28, #0x10c
    0.00 :   ffff800010708d30:       b       ffff80001070b754 <arm_smmu_device_probe+0x11e4>
         :                      arm_smmu_cmdq_poll_until_not_full():
         :                      if (arm_smmu_cmdq_exclusive_trylock_irqsave(cmdq, flags)) {
    0.00 :   ffff800010708d34:       cbz     w0, ffff800010708ce4 <arm_smmu_cmdq_issue_cmdlist+0x2c4>
         :                      arch_local_irq_restore():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010708d38:       msr     daif, x4
         :                      queue_poll_init():
         :                      qp->wfe = !!(smmu->features & ARM_SMMU_FEAT_SEV);
    0.00 :   ffff800010708d3c:       ldr     w0, [x28, #16]
         :                      qp->delay = 1;
    0.00 :   ffff800010708d40:       mov     x1, #0x1                        // #1
    0.00 :   ffff800010708d44:       str     x1, [x29, #120]
         :                      qp->wfe = !!(smmu->features & ARM_SMMU_FEAT_SEV);
    0.00 :   ffff800010708d48:       ubfx    x0, x0, #6, #1
    0.00 :   ffff800010708d4c:       strb    w0, [x29, #128]
         :                      qp->timeout = ktime_add_us(ktime_get(), ARM_SMMU_POLL_TIMEOUT_US);
    0.00 :   ffff800010708d50:       bl      ffff80001016ad10 <ktime_get>
         :                      ktime_add_us():
         :                      return ktime_to_ms(ktime_sub(later, earlier));
         :                      }
         :
         :                      static inline ktime_t ktime_add_us(const ktime_t kt, const u64 usec)
         :                      {
         :                      return ktime_add_ns(kt, usec * NSEC_PER_USEC);
    0.00 :   ffff800010708d54:       mov     x1, #0xca00                     // #51712
    0.00 :   ffff800010708d58:       movk    x1, #0x3b9a, lsl #16
    0.00 :   ffff800010708d5c:       add     x0, x0, x1
         :                      queue_poll_init():
    0.00 :   ffff800010708d60:       str     x0, [x29, #112]
    0.00 :   ffff800010708d64:       nop
         :                      __read_once_size():
    0.00 :   ffff800010708d68:       ldr     x0, [x28, #64]
         :                      arm_smmu_cmdq_poll_until_not_full():
         :                      llq->val = READ_ONCE(smmu->cmdq.q.llq.val);
    0.00 :   ffff800010708d6c:       str     x0, [x23]
         :                      queue_full():
         :                      return Q_IDX(q, q->prod) == Q_IDX(q, q->cons) &&
    0.00 :   ffff800010708d70:       lsr     x1, x0, #32
    0.00 :   ffff800010708d74:       eor     w0, w0, w1
    0.00 :   ffff800010708d78:       tst     w22, w0
    0.00 :   ffff800010708d7c:       b.ne    ffff800010708d08 <arm_smmu_cmdq_issue_cmdlist+0x2e8>  // b.any
    0.00 :   ffff800010708d80:       tst     w19, w0
    0.00 :   ffff800010708d84:       b.eq    ffff800010708d08 <arm_smmu_cmdq_issue_cmdlist+0x2e8>  // b.none
         :                      arm_smmu_cmdq_poll_until_not_full():
         :                      ret = queue_poll(&qp);
    0.00 :   ffff800010708d88:       add     x0, x29, #0x70
    0.00 :   ffff800010708d8c:       bl      ffff800010707f10 <queue_poll>
         :                      } while (!ret);
    0.00 :   ffff800010708d90:       cbz     w0, ffff800010708d68 <arm_smmu_cmdq_issue_cmdlist+0x348>
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      dev_err_ratelimited(smmu->dev, "CMDQ timeout\n");
    0.00 :   ffff800010708d94:       adrp    x0, ffff800011a0f000 <input_pool+0x18>
    0.00 :   ffff800010708d98:       add     x0, x0, #0xf40
    0.00 :   ffff800010708d9c:       mov     x1, x24
    0.00 :   ffff800010708da0:       add     x0, x0, #0x138
    0.00 :   ffff800010708da4:       bl      ffff800010c9eac8 <___ratelimit>
    0.00 :   ffff800010708da8:       cbz     w0, ffff800010708d08 <arm_smmu_cmdq_issue_cmdlist+0x2e8>
    0.00 :   ffff800010708dac:       ldr     x0, [x28]
    0.00 :   ffff800010708db0:       adrp    x1, ffff800011229000 <kallsyms_token_index+0xaf330>
    0.00 :   ffff800010708db4:       add     x1, x1, #0x10
    0.00 :   ffff800010708db8:       bl      ffff800010718e00 <_dev_err>
    0.00 :   ffff800010708dbc:       b       ffff800010708d08 <arm_smmu_cmdq_issue_cmdlist+0x2e8>
         :                      __ll_sc_atomic_fetch_andnot_relaxed():
         :                      ATOMIC_OPS(andnot, bic, )
    0.00 :   ffff800010708dc0:       mov     w0, #0x80000000                 // #-2147483648
    0.00 :   ffff800010708dc4:       add     x3, x28, #0x40
    0.00 :   ffff800010708dc8:       b       ffff80001070b770 <arm_smmu_device_probe+0x1200>
         :                      arm_smmu_cmdq_poll_valid_map():
         :                      __arm_smmu_cmdq_poll_set_valid_map(cmdq, sprod, eprod, false);
    0.00 :   ffff800010708dcc:       ldr     w0, [x28, #128]
         :                      arm_smmu_cmdq_issue_cmdlist():
         :
         :                      /* b. Stop gathering work by clearing the owned flag */
         :                      prod = atomic_fetch_andnot_relaxed(CMDQ_PROD_OWNED_FLAG,
         :                      &cmdq->q.llq.atomic.prod);
         :                      prod &= ~CMDQ_PROD_OWNED_FLAG;
    0.00 :   ffff800010708dd0:       and     w24, w24, #0x7fffffff
         :                      arm_smmu_cmdq_poll_valid_map():
         :                      __arm_smmu_cmdq_poll_set_valid_map(cmdq, sprod, eprod, false);
    0.00 :   ffff800010708dd4:       ldr     w2, [x26, #128]
    0.00 :   ffff800010708dd8:       mov     w4, #0x0                        // #0
    0.00 :   ffff800010708ddc:       mov     w3, w24
    0.00 :   ffff800010708de0:       mov     x1, x5
    0.00 :   ffff800010708de4:       bl      ffff8000107080c8 <__arm_smmu_cmdq_poll_set_valid_map.isra.32>
         :                      __raw_writel():
         :                      asm volatile("str %w0, [%1]" : : "rZ" (val), "r" (addr));
    0.00 :   ffff800010708de8:       ldr     x0, [x27, #168]
    0.00 :   ffff800010708dec:       str     w24, [x0]
         :                      atomic_set_release():
    0.00 :   ffff800010708df0:       add     x0, x28, #0x108
    0.00 :   ffff800010708df4:       stlr    w24, [x0]
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      */
         :                      atomic_set_release(&cmdq->owner_prod, prod);
         :                      }
         :
         :                      /* 5. If we are inserting a CMD_SYNC, we must wait for it to complete */
         :                      if (sync) {
    0.00 :   ffff800010708df8:       ldr     w0, [x29, #108]
         :                      int ret = 0;
    0.00 :   ffff800010708dfc:       mov     w24, #0x0                       // #0
         :                      if (sync) {
    0.00 :   ffff800010708e00:       cbnz    w0, ffff800010708e40 <arm_smmu_cmdq_issue_cmdlist+0x420>
         :                      arch_local_irq_restore():
    0.00 :   ffff800010708e04:       msr     daif, x21
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      }
         :                      }
         :
         :                      local_irq_restore(flags);
         :                      return ret;
         :                      }
   99.55 :   ffff800010708e08:       adrp    x0, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010708e0c:       add     x21, x0, #0x8c8
    0.02 :   ffff800010708e10:       ldr     x2, [x29, #488]
    0.11 :   ffff800010708e14:       ldr     x1, [x21]
    0.00 :   ffff800010708e18:       eor     x1, x2, x1
    0.00 :   ffff800010708e1c:       mov     w0, w24
    0.00 :   ffff800010708e20:       cbnz    x1, ffff800010709194 <arm_smmu_cmdq_issue_cmdlist+0x774>
    0.00 :   ffff800010708e24:       ldp     x19, x20, [sp, #16]
    0.02 :   ffff800010708e28:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010708e2c:       ldp     x23, x24, [sp, #48]
    0.01 :   ffff800010708e30:       ldp     x25, x26, [sp, #64]
    0.00 :   ffff800010708e34:       ldp     x27, x28, [sp, #80]
    0.01 :   ffff800010708e38:       ldp     x29, x30, [sp], #496
    0.00 :   ffff800010708e3c:       ret
         :                      llq.prod = queue_inc_prod_n(&llq, n);
    0.00 :   ffff800010708e40:       ldr     w2, [x26, #128]
         :                      queue_inc_prod_n():
         :                      u32 prod = (Q_WRP(q, q->prod) | Q_IDX(q, q->prod)) + n;
    0.00 :   ffff800010708e44:       ldr     w3, [x29, #104]
    0.00 :   ffff800010708e48:       and     w0, w2, w20
         :                      arm_smmu_cmdq_poll_until_sync():
         :                      if (smmu->features & ARM_SMMU_FEAT_MSI &&
    0.00 :   ffff800010708e4c:       ldr     w1, [x28, #16]
         :                      queue_inc_prod_n():
         :                      u32 prod = (Q_WRP(q, q->prod) | Q_IDX(q, q->prod)) + n;
    0.00 :   ffff800010708e50:       add     w3, w0, w3
         :                      return Q_OVF(q->prod) | Q_WRP(q, prod) | Q_IDX(q, prod);
    0.00 :   ffff800010708e54:       and     w2, w2, #0x80000000
    0.00 :   ffff800010708e58:       and     w3, w3, w20
         :                      arm_smmu_cmdq_poll_until_sync():
         :                      if (smmu->features & ARM_SMMU_FEAT_MSI &&
    0.00 :   ffff800010708e5c:       and     w0, w1, #0x180
         :                      queue_inc_prod_n():
         :                      return Q_OVF(q->prod) | Q_WRP(q, prod) | Q_IDX(q, prod);
    0.00 :   ffff800010708e60:       orr     w25, w3, w2
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      llq.prod = queue_inc_prod_n(&llq, n);
    0.00 :   ffff800010708e64:       str     w25, [x26, #128]
    0.00 :   ffff800010708e68:       ubfx    x1, x1, #6, #1
         :                      arm_smmu_cmdq_poll_until_sync():
         :                      if (smmu->features & ARM_SMMU_FEAT_MSI &&
    0.00 :   ffff800010708e6c:       cmp     w0, #0x180
    0.00 :   ffff800010708e70:       b.eq    ffff8000107090b4 <arm_smmu_cmdq_issue_cmdlist+0x694>  // b.none
         :                      queue_poll_init():
         :                      qp->delay = 1;
    0.00 :   ffff800010708e74:       mov     x0, #0x1                        // #1
    0.00 :   ffff800010708e78:       str     x0, [x29, #120]
         :                      qp->wfe = !!(smmu->features & ARM_SMMU_FEAT_SEV);
    0.00 :   ffff800010708e7c:       strb    w1, [x29, #128]
    0.00 :   ffff800010708e80:       and     w20, w22, w25
         :                      qp->timeout = ktime_add_us(ktime_get(), ARM_SMMU_POLL_TIMEOUT_US);
    0.00 :   ffff800010708e84:       bl      ffff80001016ad10 <ktime_get>
         :                      __read_once_size():
    0.00 :   ffff800010708e88:       ldr     x1, [x28, #64]
         :                      ktime_add_us():
    0.00 :   ffff800010708e8c:       mov     x2, #0xca00                     // #51712
         :                      __arm_smmu_cmdq_poll_until_consumed():
         :                      llq->val = READ_ONCE(smmu->cmdq.q.llq.val);
    0.00 :   ffff800010708e90:       str     x1, [x26, #128]
         :                      ktime_add_us():
    0.00 :   ffff800010708e94:       movk    x2, #0x3b9a, lsl #16
    0.00 :   ffff800010708e98:       add     x0, x0, x2
         :                      queue_poll_init():
         :                      qp->timeout = ktime_add_us(ktime_get(), ARM_SMMU_POLL_TIMEOUT_US);
    0.00 :   ffff800010708e9c:       str     x0, [x29, #112]
    0.00 :   ffff800010708ea0:       lsr     x1, x1, #32
    0.00 :   ffff800010708ea4:       nop
         :                      queue_consumed():
         :                      return ((Q_WRP(q, q->cons) == Q_WRP(q, prod)) &&
    0.00 :   ffff800010708ea8:       eor     w0, w25, w1
    0.00 :   ffff800010708eac:       and     w1, w22, w1
         :                      (Q_IDX(q, q->cons) > Q_IDX(q, prod))) ||
    0.00 :   ffff800010708eb0:       tst     w0, w19
    0.00 :   ffff800010708eb4:       b.eq    ffff800010708ef4 <arm_smmu_cmdq_issue_cmdlist+0x4d4>  // b.none
         :                      ((Q_WRP(q, q->cons) != Q_WRP(q, prod)) &&
    0.00 :   ffff800010708eb8:       cmp     w1, w20
    0.00 :   ffff800010708ebc:       b.hi    ffff800010708efc <arm_smmu_cmdq_issue_cmdlist+0x4dc>  // b.pmore
         :                      __arm_smmu_cmdq_poll_until_consumed():
    0.00 :   ffff800010708ec0:       mov     w24, #0x0                       // #0
         :                      __read_once_size():
    0.00 :   ffff800010708ec4:       ldr     w1, [x28, #268]
    0.00 :   ffff800010708ec8:       add     x0, x28, #0x10c
         :                      arm_smmu_cmdq_shared_tryunlock():
         :                      if (atomic_read(&cmdq->lock) == 1)
    0.00 :   ffff800010708ecc:       cmp     w1, #0x1
    0.00 :   ffff800010708ed0:       b.eq    ffff80001070914c <arm_smmu_cmdq_issue_cmdlist+0x72c>  // b.none
         :                      arch_static_branch_jump():
    0.00 :   ffff800010708ed4:       b       ffff800010708f7c <arm_smmu_cmdq_issue_cmdlist+0x55c>
    0.00 :   ffff800010708ed8:       b       ffff800010708f7c <arm_smmu_cmdq_issue_cmdlist+0x55c>
         :                      __lse_atomic_sub_return_release():
         :                      ATOMIC_OP_SUB_RETURN(_release,  l, "memory")
    0.00 :   ffff800010708edc:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010708ee0:       add     x3, x28, #0x10c
    0.00 :   ffff800010708ee4:       neg     w1, w1
    0.00 :   ffff800010708ee8:       ldaddl  w1, w2, [x3]
    0.00 :   ffff800010708eec:       add     w1, w1, w2
    0.00 :   ffff800010708ef0:       b       ffff800010708e04 <arm_smmu_cmdq_issue_cmdlist+0x3e4>
         :                      queue_consumed():
         :                      return ((Q_WRP(q, q->cons) == Q_WRP(q, prod)) &&
    0.00 :   ffff800010708ef4:       cmp     w1, w20
    0.00 :   ffff800010708ef8:       b.hi    ffff800010708ec0 <arm_smmu_cmdq_issue_cmdlist+0x4a0>  // b.pmore
         :                      __arm_smmu_cmdq_poll_until_consumed():
         :                      ret = queue_poll(&qp);
    0.00 :   ffff800010708efc:       add     x0, x29, #0x70
    0.00 :   ffff800010708f00:       bl      ffff800010707f10 <queue_poll>
         :                      __raw_readl():
         :                      asm volatile(ALTERNATIVE("ldr %w0, [%1]",
    0.00 :   ffff800010708f04:       ldr     x1, [x27, #176]
         :                      __arm_smmu_cmdq_poll_until_consumed():
    0.00 :   ffff800010708f08:       mov     w24, w0
         :                      __raw_readl():
    0.00 :   ffff800010708f0c:       ldr     w1, [x1]
         :                      __arm_smmu_cmdq_poll_until_consumed():
         :                      llq->cons = readl(cmdq->q.cons_reg);
    0.00 :   ffff800010708f10:       dmb     oshld
    0.00 :   ffff800010708f14:       mov     w0, w1
    0.00 :   ffff800010708f18:       eor     x0, x0, x0
    0.00 :   ffff800010708f1c:       cbnz    x0, ffff800010708f1c <arm_smmu_cmdq_issue_cmdlist+0x4fc>
    0.00 :   ffff800010708f20:       str     w1, [x23, #4]
         :                      } while (!ret);
    0.00 :   ffff800010708f24:       cbz     w24, ffff800010708ea8 <arm_smmu_cmdq_issue_cmdlist+0x488>
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      dev_err_ratelimited(smmu->dev,
    0.00 :   ffff800010708f28:       adrp    x1, ffff800010eac000 <arm_smmu_of_match+0x98>
    0.00 :   ffff800010708f2c:       adrp    x0, ffff800011a0f000 <input_pool+0x18>
    0.00 :   ffff800010708f30:       add     x1, x1, #0x708
    0.00 :   ffff800010708f34:       add     x0, x0, #0xf40
    0.00 :   ffff800010708f38:       add     x1, x1, #0x20
    0.00 :   ffff800010708f3c:       add     x0, x0, #0x110
    0.00 :   ffff800010708f40:       bl      ffff800010c9eac8 <___ratelimit>
    0.00 :   ffff800010708f44:       cbz     w0, ffff800010708ec4 <arm_smmu_cmdq_issue_cmdlist+0x4a4>
    0.00 :   ffff800010708f48:       ldr     x7, [x28]
    0.00 :   ffff800010708f4c:       ldr     w2, [x26, #128]
    0.00 :   ffff800010708f50:       ldr     x0, [x27, #168]
    0.00 :   ffff800010708f54:       bl      ffff800010707540 <__raw_readl>
    0.00 :   ffff800010708f58:       mov     w3, w0
    0.00 :   ffff800010708f5c:       ldr     x0, [x27, #176]
    0.00 :   ffff800010708f60:       bl      ffff800010707540 <__raw_readl>
    0.00 :   ffff800010708f64:       mov     w4, w0
    0.00 :   ffff800010708f68:       adrp    x1, ffff800011229000 <kallsyms_token_index+0xaf330>
    0.00 :   ffff800010708f6c:       mov     x0, x7
    0.00 :   ffff800010708f70:       add     x1, x1, #0x20
    0.00 :   ffff800010708f74:       bl      ffff800010718e00 <_dev_err>
    0.00 :   ffff800010708f78:       b       ffff800010708ec4 <arm_smmu_cmdq_issue_cmdlist+0x4a4>
         :                      __ll_sc_atomic_sub_return_release():
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff800010708f7c:       mov     w0, #0x1                        // #1
    0.00 :   ffff800010708f80:       add     x3, x28, #0x10c
    0.00 :   ffff800010708f84:       b       ffff80001070b788 <arm_smmu_device_probe+0x1218>
    0.00 :   ffff800010708f88:       b       ffff800010708e04 <arm_smmu_cmdq_issue_cmdlist+0x3e4>
         :                      arm_smmu_cmdq_build_sync_cmd():
         :                      struct arm_smmu_cmdq_ent ent = {
    0.00 :   ffff800010708f8c:       mov     w1, #0x46                       // #70
    0.00 :   ffff800010708f90:       stp     xzr, xzr, [x29, #112]
    0.00 :   ffff800010708f94:       strb    w1, [x29, #112]
         :                      queue_inc_prod_n():
         :                      u32 prod = (Q_WRP(q, q->prod) | Q_IDX(q, q->prod)) + n;
    0.00 :   ffff800010708f98:       and     w9, w12, w20
         :                      arm_smmu_cmdq_build_sync_cmd():
         :                      if (smmu->features & ARM_SMMU_FEAT_MSI &&
    0.00 :   ffff800010708f9c:       ldr     w0, [x28, #16]
         :                      queue_inc_prod_n():
         :                      u32 prod = (Q_WRP(q, q->prod) | Q_IDX(q, q->prod)) + n;
    0.00 :   ffff800010708fa0:       ldr     w1, [x29, #104]
         :                      arm_smmu_cmdq_build_sync_cmd():
         :                      struct arm_smmu_cmdq_ent ent = {
    0.00 :   ffff800010708fa4:       stp     xzr, xzr, [x29, #128]
         :                      if (smmu->features & ARM_SMMU_FEAT_MSI &&
    0.00 :   ffff800010708fa8:       and     w0, w0, #0x180
         :                      queue_inc_prod_n():
         :                      u32 prod = (Q_WRP(q, q->prod) | Q_IDX(q, q->prod)) + n;
    0.00 :   ffff800010708fac:       add     w9, w9, w1
         :                      arm_smmu_cmdq_build_sync_cmd():
         :                      if (smmu->features & ARM_SMMU_FEAT_MSI &&
    0.00 :   ffff800010708fb0:       cmp     w0, #0x180
         :                      queue_inc_prod_n():
         :                      return Q_OVF(q->prod) | Q_WRP(q, prod) | Q_IDX(q, prod);
    0.00 :   ffff800010708fb4:       and     w9, w9, w20
         :                      arm_smmu_cmdq_build_sync_cmd():
         :                      if (smmu->features & ARM_SMMU_FEAT_MSI &&
    0.00 :   ffff800010708fb8:       b.ne    ffff800010708fe4 <arm_smmu_cmdq_issue_cmdlist+0x5c4>  // b.any
         :                      ent.sync.msiaddr = q->base_dma + Q_IDX(&q->llq, prod) *
    0.00 :   ffff800010708fbc:       ldr     w3, [x27, #64]
    0.00 :   ffff800010708fc0:       mov     w0, #0x1                        // #1
         :                      q->ent_dwords * 8;
    0.00 :   ffff800010708fc4:       ldr     x1, [x27, #160]
         :                      ent.sync.msiaddr = q->base_dma + Q_IDX(&q->llq, prod) *
    0.00 :   ffff800010708fc8:       ldr     x2, [x27, #144]
    0.00 :   ffff800010708fcc:       lsl     w0, w0, w3
    0.00 :   ffff800010708fd0:       sub     w0, w0, #0x1
    0.00 :   ffff800010708fd4:       and     w0, w0, w9
         :                      q->ent_dwords * 8;
    0.00 :   ffff800010708fd8:       lsl     x1, x1, #3
         :                      ent.sync.msiaddr = q->base_dma + Q_IDX(&q->llq, prod) *
    0.00 :   ffff800010708fdc:       madd    x0, x0, x1, x2
    0.00 :   ffff800010708fe0:       str     x0, [x29, #120]
         :                      arm_smmu_cmdq_build_cmd(cmd, &ent);
    0.00 :   ffff800010708fe4:       add     x1, x29, #0x70
    0.00 :   ffff800010708fe8:       add     x0, x29, #0x1d8
    0.00 :   ffff800010708fec:       bl      ffff8000107079c0 <arm_smmu_cmdq_build_cmd>
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      queue_write(Q_ENT(&cmdq->q, prod), cmd_sync, CMDQ_ENT_DWORDS);
    0.00 :   ffff800010708ff0:       ldr     w4, [x27, #64]
    0.00 :   ffff800010708ff4:       mov     w0, #0x1                        // #1
    0.00 :   ffff800010708ff8:       ldr     x1, [x27, #160]
    0.00 :   ffff800010708ffc:       ldr     x2, [x27, #136]
    0.00 :   ffff800010709000:       lsl     w0, w0, w4
    0.00 :   ffff800010709004:       sub     w0, w0, #0x1
    0.00 :   ffff800010709008:       lsl     x1, x1, #3
    0.00 :   ffff80001070900c:       and     w0, w0, w9
         :                      queue_write():
         :                      *dst++ = cpu_to_le64(*src++);
    0.00 :   ffff800010709010:       ldr     x3, [x29, #472]
         :                      arm_smmu_cmdq_shared_lock():
         :                      if (atomic_fetch_inc_relaxed(&cmdq->lock) >= 0)
    0.00 :   ffff800010709014:       add     x4, x28, #0x10c
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      queue_write(Q_ENT(&cmdq->q, prod), cmd_sync, CMDQ_ENT_DWORDS);
    0.00 :   ffff800010709018:       mul     x0, x0, x1
    0.00 :   ffff80001070901c:       add     x1, x2, x0
         :                      queue_write():
         :                      *dst++ = cpu_to_le64(*src++);
    0.00 :   ffff800010709020:       str     x3, [x2, x0]
    0.00 :   ffff800010709024:       ldr     x0, [x29, #480]
    0.00 :   ffff800010709028:       str     x0, [x1, #8]
         :                      arch_static_branch_jump():
    0.00 :   ffff80001070902c:       b       ffff80001070909c <arm_smmu_cmdq_issue_cmdlist+0x67c>
    0.00 :   ffff800010709030:       b       ffff80001070909c <arm_smmu_cmdq_issue_cmdlist+0x67c>
         :                      __lse_atomic_fetch_add_relaxed():
         :                      ATOMIC_FETCH_OPS(add, ldadd)
    0.00 :   ffff800010709034:       mov     w0, #0x1                        // #1
    0.00 :   ffff800010709038:       ldadd   w0, w0, [x4]
         :                      arm_smmu_cmdq_shared_lock():
         :                      if (atomic_fetch_inc_relaxed(&cmdq->lock) >= 0)
    0.00 :   ffff80001070903c:       tbz     w0, #31, ffff800010708c1c <arm_smmu_cmdq_issue_cmdlist+0x1fc>
         :                      __read_once_size():
    0.00 :   ffff800010709040:       ldr     w3, [x28, #268]
         :                      arm_smmu_cmdq_shared_lock():
         :                      val = atomic_cond_read_relaxed(&cmdq->lock, VAL >= 0);
    0.00 :   ffff800010709044:       tbz     w3, #31, ffff80001070906c <arm_smmu_cmdq_issue_cmdlist+0x64c>
    0.00 :   ffff800010709048:       sxtw    x3, w3
         :                      __cmpwait_case_32():
    0.00 :   ffff80001070904c:       sevl
    0.00 :   ffff800010709050:       wfe
    0.00 :   ffff800010709054:       ldxr    w0, [x4]
    0.00 :   ffff800010709058:       eor     w0, w0, w3
    0.00 :   ffff80001070905c:       cbnz    w0, ffff800010709064 <arm_smmu_cmdq_issue_cmdlist+0x644>
    0.00 :   ffff800010709060:       wfe
         :                      __read_once_size():
    0.00 :   ffff800010709064:       ldr     w3, [x28, #268]
         :                      arm_smmu_cmdq_shared_lock():
    0.00 :   ffff800010709068:       tbnz    w3, #31, ffff800010709048 <arm_smmu_cmdq_issue_cmdlist+0x628>
         :                      atomic_cmpxchg_relaxed():
    0.00 :   ffff80001070906c:       sxtw    x1, w3
         :                      arm_smmu_cmdq_shared_lock():
         :                      } while (atomic_cmpxchg_relaxed(&cmdq->lock, val, val + 1) != val);
    0.00 :   ffff800010709070:       add     w2, w3, #0x1
         :                      arch_static_branch_jump():
    0.00 :   ffff800010709074:       b       ffff8000107090ac <arm_smmu_cmdq_issue_cmdlist+0x68c>
    0.00 :   ffff800010709078:       b       ffff8000107090ac <arm_smmu_cmdq_issue_cmdlist+0x68c>
         :                      __lse__cmpxchg_case_32():
         :                      __CMPXCHG_CASE(w,  ,     , 32,   )
    0.00 :   ffff80001070907c:       mov     x0, x4
    0.00 :   ffff800010709080:       mov     w1, w3
    0.00 :   ffff800010709084:       mov     w5, w1
    0.00 :   ffff800010709088:       cas     w5, w2, [x4]
    0.00 :   ffff80001070908c:       mov     w0, w5
         :                      arm_smmu_cmdq_shared_lock():
    0.00 :   ffff800010709090:       cmp     w3, w0
    0.00 :   ffff800010709094:       b.ne    ffff800010709064 <arm_smmu_cmdq_issue_cmdlist+0x644>  // b.any
    0.00 :   ffff800010709098:       b       ffff800010708c1c <arm_smmu_cmdq_issue_cmdlist+0x1fc>
         :                      __ll_sc_atomic_fetch_add_relaxed():
         :                      ATOMIC_OPS(add, add, I)
    0.00 :   ffff80001070909c:       add     x3, x28, #0x10c
    0.00 :   ffff8000107090a0:       b       ffff80001070b7a0 <arm_smmu_device_probe+0x1230>
         :                      arm_smmu_cmdq_shared_lock():
         :                      if (atomic_fetch_inc_relaxed(&cmdq->lock) >= 0)
    0.00 :   ffff8000107090a4:       tbz     w0, #31, ffff800010708c1c <arm_smmu_cmdq_issue_cmdlist+0x1fc>
    0.00 :   ffff8000107090a8:       b       ffff800010709040 <arm_smmu_cmdq_issue_cmdlist+0x620>
         :                      __ll_sc__cmpxchg_case_32():
         :                      __CMPXCHG_CASE(w,  ,     , 32,        ,  ,  ,         , K)
    0.00 :   ffff8000107090ac:       b       ffff80001070b7b8 <arm_smmu_device_probe+0x1248>
    0.00 :   ffff8000107090b0:       b       ffff800010709090 <arm_smmu_cmdq_issue_cmdlist+0x670>
         :                      __arm_smmu_cmdq_poll_until_msi():
         :                      u32 *cmd = (u32 *)(Q_ENT(&cmdq->q, llq->prod));
    0.00 :   ffff8000107090b4:       ldr     w2, [x27, #64]
    0.00 :   ffff8000107090b8:       mov     w19, #0x1                       // #1
    0.00 :   ffff8000107090bc:       ldr     x0, [x27, #160]
         :                      queue_poll_init():
         :                      qp->delay = 1;
    0.00 :   ffff8000107090c0:       mov     x3, #0x1                        // #1
         :                      __arm_smmu_cmdq_poll_until_msi():
         :                      u32 *cmd = (u32 *)(Q_ENT(&cmdq->q, llq->prod));
    0.00 :   ffff8000107090c4:       ldr     x24, [x27, #136]
    0.00 :   ffff8000107090c8:       lsl     w19, w19, w2
    0.00 :   ffff8000107090cc:       sub     w19, w19, #0x1
    0.00 :   ffff8000107090d0:       lsl     x0, x0, #3
    0.00 :   ffff8000107090d4:       and     w19, w19, w25
         :                      queue_poll_init():
         :                      qp->delay = 1;
    0.00 :   ffff8000107090d8:       str     x3, [x29, #120]
         :                      qp->wfe = !!(smmu->features & ARM_SMMU_FEAT_SEV);
    0.00 :   ffff8000107090dc:       strb    w1, [x29, #128]
         :                      __arm_smmu_cmdq_poll_until_msi():
         :                      u32 *cmd = (u32 *)(Q_ENT(&cmdq->q, llq->prod));
    0.00 :   ffff8000107090e0:       mul     x19, x19, x0
         :                      queue_poll_init():
         :                      qp->timeout = ktime_add_us(ktime_get(), ARM_SMMU_POLL_TIMEOUT_US);
    0.00 :   ffff8000107090e4:       bl      ffff80001016ad10 <ktime_get>
         :                      ktime_add_us():
    0.00 :   ffff8000107090e8:       mov     x1, #0xca00                     // #51712
         :                      __arm_smmu_cmdq_poll_until_msi():
         :                      qp.wfe = false;
    0.00 :   ffff8000107090ec:       strb    wzr, [x29, #128]
         :                      ktime_add_us():
    0.00 :   ffff8000107090f0:       movk    x1, #0x3b9a, lsl #16
    0.00 :   ffff8000107090f4:       add     x0, x0, x1
         :                      queue_poll_init():
         :                      qp->timeout = ktime_add_us(ktime_get(), ARM_SMMU_POLL_TIMEOUT_US);
    0.00 :   ffff8000107090f8:       str     x0, [x29, #112]
         :                      __arm_smmu_cmdq_poll_until_msi():
         :                      u32 *cmd = (u32 *)(Q_ENT(&cmdq->q, llq->prod));
    0.00 :   ffff8000107090fc:       add     x22, x24, x19
         :                      __read_once_size():
    0.00 :   ffff800010709100:       ldr     w19, [x24, x19]
         :                      __arm_smmu_cmdq_poll_until_msi():
         :                      smp_cond_load_relaxed(cmd, !VAL || (ret = queue_poll(&qp)));
    0.00 :   ffff800010709104:       cbnz    w19, ffff800010709130 <arm_smmu_cmdq_issue_cmdlist+0x710>
    0.00 :   ffff800010709108:       b       ffff800010709174 <arm_smmu_cmdq_issue_cmdlist+0x754>
    0.00 :   ffff80001070910c:       mov     w19, w19
         :                      __cmpwait_case_32():
    0.00 :   ffff800010709110:       sevl
    0.00 :   ffff800010709114:       wfe
    0.00 :   ffff800010709118:       ldxr    w0, [x22]
    0.00 :   ffff80001070911c:       eor     w0, w0, w19
    0.00 :   ffff800010709120:       cbnz    w0, ffff800010709128 <arm_smmu_cmdq_issue_cmdlist+0x708>
    0.00 :   ffff800010709124:       wfe
         :                      __read_once_size():
    0.00 :   ffff800010709128:       ldr     w19, [x22]
         :                      __arm_smmu_cmdq_poll_until_msi():
    0.00 :   ffff80001070912c:       cbz     w19, ffff800010709170 <arm_smmu_cmdq_issue_cmdlist+0x750>
    0.00 :   ffff800010709130:       add     x0, x29, #0x70
    0.00 :   ffff800010709134:       bl      ffff800010707f10 <queue_poll>
    0.00 :   ffff800010709138:       mov     w24, w0
    0.00 :   ffff80001070913c:       cbz     w0, ffff80001070910c <arm_smmu_cmdq_issue_cmdlist+0x6ec>
         :                      llq->cons = ret ? llq->prod : queue_inc_prod_n(llq, 1);
    0.00 :   ffff800010709140:       ldr     w0, [x26, #128]
    0.00 :   ffff800010709144:       str     w0, [x23, #4]
    0.00 :   ffff800010709148:       b       ffff800010708f28 <arm_smmu_cmdq_issue_cmdlist+0x508>
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      WRITE_ONCE(cmdq->q.llq.cons, llq.cons);
    0.00 :   ffff80001070914c:       ldr     w1, [x23, #4]
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010709150:       str     w1, [x28, #68]
         :                      arch_static_branch_jump():
    0.00 :   ffff800010709154:       b       ffff800010708f7c <arm_smmu_cmdq_issue_cmdlist+0x55c>
    0.00 :   ffff800010709158:       b       ffff800010708f7c <arm_smmu_cmdq_issue_cmdlist+0x55c>
         :                      __lse_atomic_sub_return_release():
         :                      ATOMIC_OP_SUB_RETURN(_release,  l, "memory")
    0.00 :   ffff80001070915c:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010709160:       neg     w1, w1
    0.00 :   ffff800010709164:       ldaddl  w1, w2, [x0]
    0.00 :   ffff800010709168:       add     w1, w1, w2
    0.00 :   ffff80001070916c:       b       ffff800010708e04 <arm_smmu_cmdq_issue_cmdlist+0x3e4>
    0.00 :   ffff800010709170:       ldr     w25, [x26, #128]
         :                      queue_inc_prod_n():
         :                      u32 prod = (Q_WRP(q, q->prod) | Q_IDX(q, q->prod)) + n;
    0.00 :   ffff800010709174:       and     w0, w20, w25
         :                      return Q_OVF(q->prod) | Q_WRP(q, prod) | Q_IDX(q, prod);
    0.00 :   ffff800010709178:       and     w22, w25, #0x80000000
         :                      u32 prod = (Q_WRP(q, q->prod) | Q_IDX(q, q->prod)) + n;
    0.00 :   ffff80001070917c:       add     w0, w0, #0x1
         :                      __arm_smmu_cmdq_poll_until_msi():
    0.00 :   ffff800010709180:       mov     w24, #0x0                       // #0
         :                      queue_inc_prod_n():
         :                      return Q_OVF(q->prod) | Q_WRP(q, prod) | Q_IDX(q, prod);
    0.00 :   ffff800010709184:       and     w20, w0, w20
    0.00 :   ffff800010709188:       orr     w22, w20, w22
         :                      __arm_smmu_cmdq_poll_until_msi():
         :                      llq->cons = ret ? llq->prod : queue_inc_prod_n(llq, 1);
    0.00 :   ffff80001070918c:       str     w22, [x23, #4]
    0.00 :   ffff800010709190:       b       ffff800010708ec4 <arm_smmu_cmdq_issue_cmdlist+0x4a4>
         :                      arm_smmu_cmdq_issue_cmdlist():
         :                      }
    0.00 :   ffff800010709194:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (262870 samples, percent: local period)
-------------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>:
         :                      _raw_spin_unlock_irqrestore():
         :                      EXPORT_SYMBOL(_raw_spin_unlock);
         :                      #endif
         :
         :                      #ifndef CONFIG_INLINE_SPIN_UNLOCK_IRQRESTORE
         :                      void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)
         :                      {
    0.00 :   ffff800010cb2bb0:       stp     x29, x30, [sp, #-16]!
         :                      queued_spin_unlock():
         :                      static __always_inline void queued_spin_unlock(struct qspinlock *lock)
         :                      {
         :                      /*
         :                      * unlock() needs release semantics:
         :                      */
         :                      smp_store_release(&lock->locked, 0);
    0.00 :   ffff800010cb2bb4:       mov     w2, #0x0                        // #0
         :                      _raw_spin_unlock_irqrestore():
    0.00 :   ffff800010cb2bb8:       mov     x29, sp
         :                      queued_spin_unlock():
    0.00 :   ffff800010cb2bbc:       stlrb   w2, [x0]
         :                      arch_local_irq_restore():
         :                      /*
         :                      * restore saved IRQ state
         :                      */
         :                      static inline void arch_local_irq_restore(unsigned long flags)
         :                      {
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010cb2bc0:       msr     daif, x1
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
   95.25 :   ffff800010cb2bc4:       mrs     x1, sp_el0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.16 :   ffff800010cb2bc8:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800010cb2bcc:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    3.39 :   ffff800010cb2bd0:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800010cb2bd4:       cbz     x0, ffff800010cb2be0 <_raw_spin_unlock_irqrestore+0x30>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.01 :   ffff800010cb2bd8:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff800010cb2bdc:       cbnz    x0, ffff800010cb2be4 <_raw_spin_unlock_irqrestore+0x34>
         :                      __raw_spin_unlock_irqrestore():
         :                      unsigned long flags)
         :                      {
         :                      spin_release(&lock->dep_map, _RET_IP_);
         :                      do_raw_spin_unlock(lock);
         :                      local_irq_restore(flags);
         :                      preempt_enable();
    0.00 :   ffff800010cb2be0:       bl      ffff800010cad640 <preempt_schedule>
         :                      _raw_spin_unlock_irqrestore():
         :                      __raw_spin_unlock_irqrestore(lock, flags);
         :                      }
    1.19 :   ffff800010cb2be4:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010cb2be8:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (70741 samples, percent: local period)
------------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000107094d8 <arm_smmu_atc_inv_domain.constprop.42>:
         :                      arm_smmu_atc_inv_domain():
         :                      }
         :
         :                      return arm_smmu_cmdq_issue_sync(master->smmu);
         :                      }
         :
         :                      static int arm_smmu_atc_inv_domain(struct arm_smmu_domain *smmu_domain,
    0.22 :   ffff8000107094d8:       stp     x29, x30, [sp, #-112]!
    0.00 :   ffff8000107094dc:       mov     x29, sp
    0.34 :   ffff8000107094e0:       str     x20, [sp, #24]
    0.00 :   ffff8000107094e4:       adrp    x20, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000107094e8:       add     x3, x20, #0x8c8
    0.21 :   ffff8000107094ec:       ldr     x4, [x3]
    0.23 :   ffff8000107094f0:       str     x4, [x29, #104]
    0.00 :   ffff8000107094f4:       mov     x4, #0x0                        // #0
         :                      int ret = 0;
         :                      unsigned long flags;
         :                      struct arm_smmu_cmdq_ent cmd;
         :                      struct arm_smmu_master *master;
         :
         :                      if (!(smmu_domain->smmu->features & ARM_SMMU_FEAT_ATS))
    0.18 :   ffff8000107094f8:       ldr     x3, [x0]
    1.14 :   ffff8000107094fc:       ldr     w3, [x3, #16]
    0.04 :   ffff800010709500:       tbz     w3, #5, ffff8000107095d8 <arm_smmu_atc_inv_domain.constprop.42+0x100>
         :                      *       atomic_read(&nr_ats_masters);   pci_enable_ats() // writel()
         :                      *
         :                      * Ensures that we always see the incremented 'nr_ats_masters' count if
         :                      * ATS was enabled at the PCI device before completion of the TLBI.
         :                      */
         :                      smp_mb();
   95.00 :   ffff800010709504:       dmb     ish
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    2.40 :   ffff800010709508:       ldr     w3, [x0, #52]
         :                      arm_smmu_atc_inv_domain():
         :                      if (!atomic_read(&smmu_domain->nr_ats_masters))
    0.00 :   ffff80001070950c:       cbz     w3, ffff8000107095d8 <arm_smmu_atc_inv_domain.constprop.42+0x100>
         :                      arm_smmu_atc_inv_to_cmd():
         :                      *cmd = (struct arm_smmu_cmdq_ent) {
    0.00 :   ffff800010709510:       mov     w3, #0x40                       // #64
    0.00 :   ffff800010709514:       strb    w3, [x29, #72]
    0.00 :   ffff800010709518:       add     x3, x29, #0x4a
    0.00 :   ffff80001070951c:       str     x19, [x29, #16]
    0.00 :   ffff800010709520:       stp     x21, x22, [x29, #32]
    0.00 :   ffff800010709524:       str     x24, [x29, #56]
    0.00 :   ffff800010709528:       stp     xzr, xzr, [x3]
    0.00 :   ffff80001070952c:       strb    wzr, [x29, #73]
    0.00 :   ffff800010709530:       stur    xzr, [x29, #90]
    0.00 :   ffff800010709534:       stur    wzr, [x29, #98]
    0.00 :   ffff800010709538:       strh    wzr, [x29, #102]
         :                      if (!size) {
    0.00 :   ffff80001070953c:       cbnz    x2, ffff8000107095fc <arm_smmu_atc_inv_domain.constprop.42+0x124>
         :                      cmd->atc.size = ATC_INV_SIZE_ALL;
    0.00 :   ffff800010709540:       mov     w1, #0x34                       // #52
    0.00 :   ffff800010709544:       strb    w1, [x29, #96]
         :                      arm_smmu_atc_inv_domain():
         :                      return 0;
         :
         :                      arm_smmu_atc_inv_to_cmd(ssid, iova, size, &cmd);
         :
         :                      spin_lock_irqsave(&smmu_domain->devices_lock, flags);
         :                      list_for_each_entry(master, &smmu_domain->devices, domain_head)
    0.00 :   ffff800010709548:       mov     x22, x0
         :                      spinlock_check():
         :                      * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
         :                      */
         :
         :                      static __always_inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
         :                      {
         :                      return &lock->rlock;
    0.00 :   ffff80001070954c:       add     x21, x0, #0xc8
         :                      arm_smmu_atc_inv_domain():
         :                      spin_lock_irqsave(&smmu_domain->devices_lock, flags);
    0.00 :   ffff800010709550:       mov     x0, x21
    0.00 :   ffff800010709554:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
    0.00 :   ffff800010709558:       mov     x24, x0
         :                      list_for_each_entry(master, &smmu_domain->devices, domain_head)
    0.00 :   ffff80001070955c:       ldr     x0, [x22, #184]!
    0.00 :   ffff800010709560:       sub     x19, x0, #0x18
    0.00 :   ffff800010709564:       cmp     x22, x0
    0.00 :   ffff800010709568:       b.eq    ffff80001070964c <arm_smmu_atc_inv_domain.constprop.42+0x174>  // b.none
    0.00 :   ffff80001070956c:       str     x23, [x29, #48]
         :                      int ret = 0;
    0.00 :   ffff800010709570:       mov     w23, #0x0                       // #0
    0.00 :   ffff800010709574:       b       ffff800010709588 <arm_smmu_atc_inv_domain.constprop.42+0xb0>
         :                      list_for_each_entry(master, &smmu_domain->devices, domain_head)
    0.00 :   ffff800010709578:       ldr     x1, [x19, #24]
    0.00 :   ffff80001070957c:       sub     x19, x1, #0x18
    0.00 :   ffff800010709580:       cmp     x22, x1
    0.00 :   ffff800010709584:       b.eq    ffff8000107095b0 <arm_smmu_atc_inv_domain.constprop.42+0xd8>  // b.none
         :                      arm_smmu_atc_inv_master():
         :                      if (!master->ats_enabled)
    0.00 :   ffff800010709588:       ldrb    w0, [x19, #52]
    0.00 :   ffff80001070958c:       cbz     w0, ffff800010709578 <arm_smmu_atc_inv_domain.constprop.42+0xa0>
    0.00 :   ffff800010709590:       add     x1, x29, #0x48
    0.00 :   ffff800010709594:       mov     x0, x19
    0.00 :   ffff800010709598:       bl      ffff800010709460 <arm_smmu_atc_inv_master.part.34>
    0.00 :   ffff80001070959c:       orr     w23, w23, w0
         :                      arm_smmu_atc_inv_domain():
         :                      list_for_each_entry(master, &smmu_domain->devices, domain_head)
    0.00 :   ffff8000107095a0:       ldr     x1, [x19, #24]
    0.00 :   ffff8000107095a4:       sub     x19, x1, #0x18
    0.00 :   ffff8000107095a8:       cmp     x22, x1
    0.00 :   ffff8000107095ac:       b.ne    ffff800010709588 <arm_smmu_atc_inv_domain.constprop.42+0xb0>  // b.any
         :                      spin_unlock_irqrestore():
         :                      raw_spin_unlock_irq(&lock->rlock);
         :                      }
         :
         :                      static __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
         :                      {
         :                      raw_spin_unlock_irqrestore(&lock->rlock, flags);
    0.00 :   ffff8000107095b0:       mov     x1, x24
    0.00 :   ffff8000107095b4:       mov     x0, x21
    0.00 :   ffff8000107095b8:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      arm_smmu_atc_inv_domain():
         :                      ret |= arm_smmu_atc_inv_master(master, &cmd);
         :                      spin_unlock_irqrestore(&smmu_domain->devices_lock, flags);
         :
         :                      return ret ? -ETIMEDOUT : 0;
    0.00 :   ffff8000107095bc:       cbnz    w23, ffff80001070965c <arm_smmu_atc_inv_domain.constprop.42+0x184>
    0.00 :   ffff8000107095c0:       ldr     x23, [x29, #48]
    0.00 :   ffff8000107095c4:       mov     w0, #0x0                        // #0
    0.00 :   ffff8000107095c8:       ldr     x19, [x29, #16]
    0.00 :   ffff8000107095cc:       ldp     x21, x22, [x29, #32]
    0.00 :   ffff8000107095d0:       ldr     x24, [x29, #56]
    0.00 :   ffff8000107095d4:       b       ffff8000107095dc <arm_smmu_atc_inv_domain.constprop.42+0x104>
         :                      return 0;
    0.02 :   ffff8000107095d8:       mov     w0, #0x0                        // #0
         :                      }
    0.00 :   ffff8000107095dc:       add     x20, x20, #0x8c8
    0.00 :   ffff8000107095e0:       ldr     x2, [x29, #104]
    0.00 :   ffff8000107095e4:       ldr     x1, [x20]
    0.00 :   ffff8000107095e8:       eor     x1, x2, x1
    0.00 :   ffff8000107095ec:       cbnz    x1, ffff800010709670 <arm_smmu_atc_inv_domain.constprop.42+0x198>
    0.21 :   ffff8000107095f0:       ldr     x20, [sp, #24]
    0.00 :   ffff8000107095f4:       ldp     x29, x30, [sp], #112
    0.00 :   ffff8000107095f8:       ret
         :                      arm_smmu_atc_inv_to_cmd():
         :                      page_end        = (iova + size - 1) >> inval_grain_shift;
    0.00 :   ffff8000107095fc:       sub     x2, x2, #0x1
         :                      page_start      = iova >> inval_grain_shift;
    0.00 :   ffff800010709600:       lsr     x3, x1, #12
         :                      page_end        = (iova + size - 1) >> inval_grain_shift;
    0.00 :   ffff800010709604:       add     x1, x2, x1
         :                      fls64():
         :                      return fls(x);
         :                      }
         :                      #elif BITS_PER_LONG == 64
         :                      static __always_inline int fls64(__u64 x)
         :                      {
         :                      if (x == 0)
    0.00 :   ffff800010709608:       mov     w2, #0x0                        // #0
         :                      arm_smmu_atc_inv_to_cmd():
    0.00 :   ffff80001070960c:       lsr     x1, x1, #12
         :                      fls64():
    0.00 :   ffff800010709610:       cmp     x3, x1
    0.00 :   ffff800010709614:       b.eq    ffff80001070963c <arm_smmu_atc_inv_domain.constprop.42+0x164>  // b.none
         :                      arm_smmu_atc_inv_to_cmd():
         :                      log2_span       = fls_long(page_start ^ page_end);
    0.00 :   ffff800010709618:       eor     x1, x3, x1
         :                      __fls():
         :                      *
         :                      * Undefined if no set bit exists, so code should check against 0 first.
         :                      */
         :                      static __always_inline unsigned long __fls(unsigned long word)
         :                      {
         :                      return (sizeof(word) * 8) - 1 - __builtin_clzl(word);
    0.00 :   ffff80001070961c:       mov     x2, #0x3f                       // #63
    0.00 :   ffff800010709620:       clz     x1, x1
    0.00 :   ffff800010709624:       mov     x4, #0xffffffffffffffff         // #-1
    0.00 :   ffff800010709628:       sub     x1, x2, x1
         :                      fls64():
         :                      return 0;
         :                      return __fls(x) + 1;
    0.00 :   ffff80001070962c:       add     w1, w1, #0x1
    0.00 :   ffff800010709630:       and     w2, w1, #0xff
    0.00 :   ffff800010709634:       lsl     x1, x4, x1
    0.00 :   ffff800010709638:       and     x1, x1, x3
         :                      arm_smmu_atc_inv_to_cmd():
         :                      cmd->atc.addr   = page_start << inval_grain_shift;
    0.00 :   ffff80001070963c:       lsl     x1, x1, #12
    0.00 :   ffff800010709640:       str     x1, [x29, #88]
         :                      cmd->atc.size   = log2_span;
    0.00 :   ffff800010709644:       strb    w2, [x29, #96]
    0.00 :   ffff800010709648:       b       ffff800010709548 <arm_smmu_atc_inv_domain.constprop.42+0x70>
         :                      spin_unlock_irqrestore():
    0.00 :   ffff80001070964c:       mov     x1, x24
    0.00 :   ffff800010709650:       mov     x0, x21
    0.00 :   ffff800010709654:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
    0.00 :   ffff800010709658:       b       ffff8000107095c4 <arm_smmu_atc_inv_domain.constprop.42+0xec>
         :                      arm_smmu_atc_inv_domain():
         :                      return ret ? -ETIMEDOUT : 0;
    0.00 :   ffff80001070965c:       mov     w0, #0xffffff92                 // #-110
    0.00 :   ffff800010709660:       ldr     x19, [x29, #16]
    0.00 :   ffff800010709664:       ldp     x21, x22, [x29, #32]
    0.00 :   ffff800010709668:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff80001070966c:       b       ffff8000107095dc <arm_smmu_atc_inv_domain.constprop.42+0x104>
    0.00 :   ffff800010709670:       str     x19, [x29, #16]
    0.00 :   ffff800010709674:       stp     x21, x22, [x29, #32]
    0.00 :   ffff800010709678:       stp     x23, x24, [x29, #48]
         :                      }
    0.00 :   ffff80001070967c:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (49764 samples, percent: local period)
------------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010700aa0 <__arm_lpae_unmap>:
         :                      __arm_lpae_unmap():
         :
         :                      static size_t __arm_lpae_unmap(struct arm_lpae_io_pgtable *data,
         :                      struct iommu_iotlb_gather *gather,
         :                      unsigned long iova, size_t size, int lvl,
         :                      arm_lpae_iopte *ptep)
         :                      {
    0.90 :   ffff800010700aa0:       stp     x29, x30, [sp, #-144]!
         :                      arm_lpae_iopte pte;
         :                      struct io_pgtable *iop = &data->iop;
         :
         :                      /* Something went horribly wrong and we ran out of page table */
         :                      if (WARN_ON(lvl == ARM_LPAE_MAX_LEVELS))
    0.01 :   ffff800010700aa4:       cmp     w4, #0x4
         :                      {
    0.01 :   ffff800010700aa8:       mov     x29, sp
         :                      if (WARN_ON(lvl == ARM_LPAE_MAX_LEVELS))
    0.00 :   ffff800010700aac:       b.eq    ffff800010700e1c <__arm_lpae_unmap+0x37c>  // b.none
    0.15 :   ffff800010700ab0:       str     x24, [x29, #56]
         :                      return 0;
         :
         :                      ptep += ARM_LPAE_LVL_IDX(iova, lvl, data);
    0.03 :   ffff800010700ab4:       mov     w7, #0x4                        // #4
    0.10 :   ffff800010700ab8:       str     x28, [x29, #88]
    0.00 :   ffff800010700abc:       sub     w7, w7, w4
    0.21 :   ffff800010700ac0:       ldp     w6, w28, [x0, #124]
    0.14 :   ffff800010700ac4:       mov     w9, w28
    0.09 :   ffff800010700ac8:       cmp     w6, w4
    0.01 :   ffff800010700acc:       mul     w7, w7, w28
    1.37 :   ffff800010700ad0:       add     w7, w7, #0x3
    0.01 :   ffff800010700ad4:       lsr     x8, x2, x7
    0.01 :   ffff800010700ad8:       b.ne    ffff800010700ae0 <__arm_lpae_unmap+0x40>  // b.any
    0.06 :   ffff800010700adc:       ldr     w9, [x0, #120]
    0.13 :   ffff800010700ae0:       mov     w6, #0x1                        // #1
    0.03 :   ffff800010700ae4:       lsl     w6, w6, w9
    0.00 :   ffff800010700ae8:       sub     w6, w6, #0x1
    0.18 :   ffff800010700aec:       sxtw    x6, w6
    0.12 :   ffff800010700af0:       and     x6, x6, x8
    0.06 :   ffff800010700af4:       add     x8, x5, x6, lsl #3
    0.33 :   ffff800010700af8:       str     x8, [x29, #112]
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.58 :   ffff800010700afc:       ldr     x24, [x5, x6, lsl #3]
         :                      __arm_lpae_unmap():
         :                      pte = READ_ONCE(*ptep);
         :                      if (WARN_ON(!pte))
    0.38 :   ffff800010700b00:       cbz     x24, ffff800010700e2c <__arm_lpae_unmap+0x38c>
   83.36 :   ffff800010700b04:       stp     x19, x20, [x29, #16]
    0.01 :   ffff800010700b08:       cmp     w4, #0x3
    1.21 :   ffff800010700b0c:       stp     x21, x22, [x29, #32]
    0.00 :   ffff800010700b10:       mov     x19, x0
    0.01 :   ffff800010700b14:       stp     x25, x26, [x29, #64]
         :                      return 0;
         :
         :                      /* If the size matches this level, we're in the right place */
         :                      if (size == ARM_LPAE_BLOCK_SIZE(lvl, data)) {
    0.00 :   ffff800010700b18:       mov     x0, #0x1                        // #1
    0.01 :   ffff800010700b1c:       str     x1, [x29, #120]
    0.00 :   ffff800010700b20:       cset    w26, eq  // eq = none
    0.02 :   ffff800010700b24:       lsl     x7, x0, x7
    0.00 :   ffff800010700b28:       mov     x21, x3
    0.05 :   ffff800010700b2c:       mov     x22, x2
    0.12 :   ffff800010700b30:       mov     w20, w4
    0.03 :   ffff800010700b34:       and     x25, x24, #0x3
    0.10 :   ffff800010700b38:       cmp     x7, x3
    0.92 :   ffff800010700b3c:       b.eq    ffff800010700d14 <__arm_lpae_unmap+0x274>  // b.none
         :                      iopte_leaf():
         :                      if (lvl == (ARM_LPAE_MAX_LEVELS - 1) && fmt != ARM_MALI_LPAE)
    0.00 :   ffff800010700b40:       ldr     w0, [x19]
    0.00 :   ffff800010700b44:       cmp     w26, #0x0
    0.00 :   ffff800010700b48:       ccmp    w0, #0x5, #0x4, ne  // ne = any
    0.00 :   ffff800010700b4c:       b.ne    ffff800010700ca0 <__arm_lpae_unmap+0x200>  // b.any
         :                      return iopte_type(pte, lvl) == ARM_LPAE_PTE_TYPE_BLOCK;
    0.02 :   ffff800010700b50:       cmp     x25, #0x1
    0.02 :   ffff800010700b54:       mov     x25, #0x8                       // #8
    0.00 :   ffff800010700b58:       lsl     x1, x25, x28
    0.06 :   ffff800010700b5c:       str     x1, [x29, #128]
    0.16 :   ffff800010700b60:       cset    w0, eq  // eq = none
    0.00 :   ffff800010700b64:       add     w20, w20, #0x1
         :                      __arm_lpae_unmap():
         :                      } else {
         :                      io_pgtable_tlb_add_page(iop, gather, iova, size);
         :                      }
         :
         :                      return size;
         :                      } else if (iopte_leaf(pte, lvl, iop->fmt)) {
    0.07 :   ffff800010700b68:       cbz     w0, ffff800010700cbc <__arm_lpae_unmap+0x21c>
         :                      arm_lpae_split_blk_unmap():
         :                      if (WARN_ON(lvl == ARM_LPAE_MAX_LEVELS))
    0.00 :   ffff800010700b6c:       cmp     w20, #0x4
    0.00 :   ffff800010700b70:       b.eq    ffff800010700ef8 <__arm_lpae_unmap+0x458>  // b.none
    0.00 :   ffff800010700b74:       str     x27, [x29, #80]
         :                      tablep = __arm_lpae_alloc_pages(tablesz, GFP_ATOMIC, cfg);
    0.00 :   ffff800010700b78:       add     x2, x19, #0x28
    0.00 :   ffff800010700b7c:       ldr     x0, [x29, #128]
    0.00 :   ffff800010700b80:       mov     w1, #0xa20                      // #2592
    0.00 :   ffff800010700b84:       ldr     x3, [x19, #56]
    0.00 :   ffff800010700b88:       bl      ffff8000107000c8 <__arm_lpae_alloc_pages.isra.22>
    0.00 :   ffff800010700b8c:       mov     x27, x0
         :                      if (!tablep)
    0.00 :   ffff800010700b90:       cbz     x0, ffff800010700c80 <__arm_lpae_unmap+0x1e0>
         :                      size_t split_sz = ARM_LPAE_BLOCK_SIZE(lvl, data);
    0.00 :   ffff800010700b94:       mov     w0, #0x4                        // #4
    0.00 :   ffff800010700b98:       sub     w0, w0, w20
    0.00 :   ffff800010700b9c:       str     x23, [x29, #48]
    0.00 :   ffff800010700ba0:       mov     x5, #0x1                        // #1
         :                      int i, unmap_idx = -1;
    0.00 :   ffff800010700ba4:       mov     w26, #0xffffffff                // #-1
         :                      size_t split_sz = ARM_LPAE_BLOCK_SIZE(lvl, data);
    0.00 :   ffff800010700ba8:       mul     w28, w28, w0
    0.00 :   ffff800010700bac:       ldr     w2, [x19, #128]
    0.00 :   ffff800010700bb0:       add     w28, w28, #0x3
    0.00 :   ffff800010700bb4:       lsl     x25, x5, x28
         :                      if (size == split_sz)
    0.00 :   ffff800010700bb8:       cmp     x21, x25
    0.00 :   ffff800010700bbc:       b.eq    ffff800010700e90 <__arm_lpae_unmap+0x3f0>  // b.none
         :                      iopte_to_paddr():
         :                      u64 paddr = pte & ARM_LPAE_PTE_ADDR_MASK;
    0.00 :   ffff800010700bc0:       and     x28, x24, #0xfffffffff000
         :                      if (ARM_LPAE_GRANULE(data) < SZ_64K)
    0.00 :   ffff800010700bc4:       mov     x1, #0x8                        // #8
    0.00 :   ffff800010700bc8:       lsl     x1, x1, x2
    0.00 :   ffff800010700bcc:       mov     x2, #0xffff                     // #65535
         :                      return (paddr | (paddr << (48 - 12))) & (ARM_LPAE_PTE_ADDR_MASK << 4);
    0.00 :   ffff800010700bd0:       orr     x0, x28, x28, lsl #36
    0.00 :   ffff800010700bd4:       cmp     x1, x2
    0.00 :   ffff800010700bd8:       and     x0, x0, #0xfffffffff0000
         :                      arm_lpae_split_blk_unmap():
         :                      pte = iopte_prot(blk_pte);
    0.00 :   ffff800010700bdc:       and     x1, x24, #0x7ffffffffffffc
         :                      iopte_to_paddr():
         :                      return (paddr | (paddr << (48 - 12))) & (ARM_LPAE_PTE_ADDR_MASK << 4);
    0.00 :   ffff800010700be0:       csel    x28, x0, x28, hi  // hi = pmore
         :                      arm_lpae_split_blk_unmap():
         :                      pte = iopte_prot(blk_pte);
    0.00 :   ffff800010700be4:       and     x0, x1, #0xffe0000000000fff
    0.00 :   ffff800010700be8:       str     x0, [x29, #104]
         :                      for (i = 0; i < tablesz / sizeof(pte); i++, blk_paddr += split_sz) {
    0.00 :   ffff800010700bec:       ldr     x0, [x29, #128]
    0.00 :   ffff800010700bf0:       lsr     x9, x0, #3
    0.00 :   ffff800010700bf4:       cbz     x9, ffff800010700c3c <__arm_lpae_unmap+0x19c>
    0.00 :   ffff800010700bf8:       mov     x4, #0x0                        // #0
    0.00 :   ffff800010700bfc:       mov     w23, #0x0                       // #0
         :                      if (i == unmap_idx)
    0.00 :   ffff800010700c00:       cmp     w26, w23
         :                      for (i = 0; i < tablesz / sizeof(pte); i++, blk_paddr += split_sz) {
    0.00 :   ffff800010700c04:       add     w23, w23, #0x1
         :                      if (i == unmap_idx)
    0.00 :   ffff800010700c08:       b.eq    ffff800010700c2c <__arm_lpae_unmap+0x18c>  // b.none
         :                      __arm_lpae_init_pte(data, blk_paddr, pte, lvl, &tablep[i]);
    0.00 :   ffff800010700c0c:       ldr     x2, [x29, #104]
    0.00 :   ffff800010700c10:       add     x4, x27, x4, lsl #3
    0.00 :   ffff800010700c14:       str     x9, [x29, #136]
    0.00 :   ffff800010700c18:       mov     x1, x28
    0.00 :   ffff800010700c1c:       mov     w3, w20
    0.00 :   ffff800010700c20:       mov     x0, x19
    0.00 :   ffff800010700c24:       bl      ffff8000106fff80 <__arm_lpae_init_pte>
    0.00 :   ffff800010700c28:       ldr     x9, [x29, #136]
         :                      for (i = 0; i < tablesz / sizeof(pte); i++, blk_paddr += split_sz) {
    0.00 :   ffff800010700c2c:       sxtw    x4, w23
    0.00 :   ffff800010700c30:       add     x28, x28, x25
    0.00 :   ffff800010700c34:       cmp     x4, x9
    0.00 :   ffff800010700c38:       b.cc    ffff800010700c00 <__arm_lpae_unmap+0x160>  // b.lo, b.ul, b.last
         :                      pte = arm_lpae_install_table(tablep, ptep, blk_pte, cfg);
    0.00 :   ffff800010700c3c:       ldr     x1, [x29, #112]
    0.00 :   ffff800010700c40:       add     x3, x19, #0x10
    0.00 :   ffff800010700c44:       mov     x2, x24
    0.00 :   ffff800010700c48:       mov     x0, x27
    0.00 :   ffff800010700c4c:       bl      ffff8000106ffff0 <arm_lpae_install_table>
    0.00 :   ffff800010700c50:       mov     x23, x0
         :                      if (pte != blk_pte) {
    0.00 :   ffff800010700c54:       cmp     x24, x0
    0.00 :   ffff800010700c58:       b.eq    ffff800010700dd8 <__arm_lpae_unmap+0x338>  // b.none
         :                      __arm_lpae_free_pages(tablep, tablesz, cfg);
    0.00 :   ffff800010700c5c:       ldrb    w2, [x19, #40]
    0.00 :   ffff800010700c60:       mov     x0, x27
    0.00 :   ffff800010700c64:       ldr     x1, [x29, #128]
    0.00 :   ffff800010700c68:       add     x3, x19, #0x38
    0.00 :   ffff800010700c6c:       bl      ffff8000107008a0 <__arm_lpae_free_pages.isra.20>
         :                      if (iopte_type(pte, lvl - 1) != ARM_LPAE_PTE_TYPE_TABLE)
    0.00 :   ffff800010700c70:       and     x0, x23, #0x3
    0.00 :   ffff800010700c74:       cmp     x0, #0x3
    0.00 :   ffff800010700c78:       b.eq    ffff800010700e40 <__arm_lpae_unmap+0x3a0>  // b.none
    0.00 :   ffff800010700c7c:       ldr     x23, [x29, #48]
    0.00 :   ffff800010700c80:       ldp     x19, x20, [x29, #16]
         :                      return 0;
    0.00 :   ffff800010700c84:       mov     x0, #0x0                        // #0
    0.00 :   ffff800010700c88:       ldp     x21, x22, [x29, #32]
    0.00 :   ffff800010700c8c:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff800010700c90:       ldp     x26, x27, [x29, #72]
    0.00 :   ffff800010700c94:       ldr     x28, [x29, #88]
         :                      __arm_lpae_unmap():
         :                      }
         :
         :                      /* Keep on walkin' */
         :                      ptep = iopte_deref(pte, data);
         :                      return __arm_lpae_unmap(data, gather, iova, size, lvl + 1, ptep);
         :                      }
    0.00 :   ffff800010700c98:       ldp     x29, x30, [sp], #144
    0.00 :   ffff800010700c9c:       ret
         :                      iopte_leaf():
         :                      return iopte_type(pte, lvl) == ARM_LPAE_PTE_TYPE_PAGE;
    0.00 :   ffff800010700ca0:       cmp     x25, #0x3
    0.00 :   ffff800010700ca4:       mov     x25, #0x8                       // #8
    0.00 :   ffff800010700ca8:       lsl     x1, x25, x28
    0.00 :   ffff800010700cac:       str     x1, [x29, #128]
    0.00 :   ffff800010700cb0:       cset    w0, eq  // eq = none
    0.00 :   ffff800010700cb4:       add     w20, w20, #0x1
         :                      __arm_lpae_unmap():
         :                      } else if (iopte_leaf(pte, lvl, iop->fmt)) {
    0.00 :   ffff800010700cb8:       cbnz    w0, ffff800010700b6c <__arm_lpae_unmap+0xcc>
         :                      ptep = iopte_deref(pte, data);
    0.64 :   ffff800010700cbc:       adrp    x1, ffff8000112ae000 <cpu_ops+0x248>
         :                      iopte_to_paddr():
         :                      u64 paddr = pte & ARM_LPAE_PTE_ADDR_MASK;
    0.04 :   ffff800010700cc0:       and     x24, x24, #0xfffffffff000
         :                      if (ARM_LPAE_GRANULE(data) < SZ_64K)
    0.00 :   ffff800010700cc4:       mov     x0, #0xffff                     // #65535
         :                      __arm_lpae_unmap():
         :                      return __arm_lpae_unmap(data, gather, iova, size, lvl + 1, ptep);
    0.00 :   ffff800010700cc8:       mov     w4, w20
         :                      ptep = iopte_deref(pte, data);
    1.16 :   ffff800010700ccc:       ldr     x5, [x1, #1872]
         :                      return __arm_lpae_unmap(data, gather, iova, size, lvl + 1, ptep);
    0.00 :   ffff800010700cd0:       mov     x3, x21
         :                      iopte_to_paddr():
         :                      return (paddr | (paddr << (48 - 12))) & (ARM_LPAE_PTE_ADDR_MASK << 4);
    0.31 :   ffff800010700cd4:       ldp     x1, x2, [x29, #120]
    0.00 :   ffff800010700cd8:       cmp     x2, x0
    0.00 :   ffff800010700cdc:       orr     x0, x24, x24, lsl #36
    0.00 :   ffff800010700ce0:       and     x0, x0, #0xfffffffff0000
         :                      __arm_lpae_unmap():
         :                      return __arm_lpae_unmap(data, gather, iova, size, lvl + 1, ptep);
    0.11 :   ffff800010700ce4:       mov     x2, x22
         :                      iopte_to_paddr():
         :                      return (paddr | (paddr << (48 - 12))) & (ARM_LPAE_PTE_ADDR_MASK << 4);
    0.00 :   ffff800010700ce8:       csel    x24, x0, x24, hi  // hi = pmore
         :                      __arm_lpae_unmap():
         :                      return __arm_lpae_unmap(data, gather, iova, size, lvl + 1, ptep);
    0.00 :   ffff800010700cec:       mov     x0, x19
    0.00 :   ffff800010700cf0:       sub     x5, x24, x5
    0.01 :   ffff800010700cf4:       bl      ffff800010700aa0 <__arm_lpae_unmap>
    0.18 :   ffff800010700cf8:       ldp     x19, x20, [x29, #16]
    1.35 :   ffff800010700cfc:       ldp     x21, x22, [x29, #32]
    0.28 :   ffff800010700d00:       ldp     x24, x25, [x29, #56]
    3.23 :   ffff800010700d04:       ldr     x26, [x29, #72]
    0.04 :   ffff800010700d08:       ldr     x28, [x29, #88]
         :                      }
    0.43 :   ffff800010700d0c:       ldp     x29, x30, [sp], #144
    0.00 :   ffff800010700d10:       ret
         :                      __arm_lpae_set_pte():
         :                      *ptep = pte;
    0.00 :   ffff800010700d14:       str     xzr, [x5, x6, lsl #3]
         :                      if (!cfg->coherent_walk)
    0.06 :   ffff800010700d18:       ldrb    w0, [x19, #40]
    0.00 :   ffff800010700d1c:       cbz     w0, ffff800010700dc8 <__arm_lpae_unmap+0x328>
         :                      iopte_leaf():
         :                      if (lvl == (ARM_LPAE_MAX_LEVELS - 1) && fmt != ARM_MALI_LPAE)
    0.06 :   ffff800010700d20:       ldr     w0, [x19]
    0.00 :   ffff800010700d24:       cmp     w26, #0x0
    0.01 :   ffff800010700d28:       ccmp    w0, #0x5, #0x4, ne  // ne = any
    0.01 :   ffff800010700d2c:       b.eq    ffff800010700dbc <__arm_lpae_unmap+0x31c>  // b.none
         :                      return iopte_type(pte, lvl) == ARM_LPAE_PTE_TYPE_PAGE;
    0.23 :   ffff800010700d30:       cmp     x25, #0x3
    0.00 :   ffff800010700d34:       cset    w0, eq  // eq = none
         :                      __arm_lpae_unmap():
         :                      if (!iopte_leaf(pte, lvl, iop->fmt)) {
    0.00 :   ffff800010700d38:       cbnz    w0, ffff800010700ec8 <__arm_lpae_unmap+0x428>
    0.00 :   ffff800010700d3c:       str     x23, [x29, #48]
         :                      io_pgtable_tlb_flush_walk(iop, iova, size,
    0.00 :   ffff800010700d40:       mov     x23, #0x8                       // #8
         :                      io_pgtable_tlb_flush_walk():
         :
         :                      static inline void
         :                      io_pgtable_tlb_flush_walk(struct io_pgtable *iop, unsigned long iova,
         :                      size_t size, size_t granule)
         :                      {
         :                      iop->cfg.tlb->tlb_flush_walk(iova, size, granule, iop->cookie);
    0.00 :   ffff800010700d44:       mov     x1, x21
    0.00 :   ffff800010700d48:       mov     x0, x22
    0.00 :   ffff800010700d4c:       ldr     x4, [x19, #48]
         :                      iopte_to_paddr():
         :                      u64 paddr = pte & ARM_LPAE_PTE_ADDR_MASK;
    0.00 :   ffff800010700d50:       and     x24, x24, #0xfffffffff000
         :                      io_pgtable_tlb_flush_walk():
    0.00 :   ffff800010700d54:       ldr     x3, [x19, #8]
         :                      __arm_lpae_unmap():
         :                      io_pgtable_tlb_flush_walk(iop, iova, size,
    0.00 :   ffff800010700d58:       ldr     w2, [x19, #128]
         :                      io_pgtable_tlb_flush_walk():
    0.00 :   ffff800010700d5c:       ldr     x4, [x4, #8]
    0.00 :   ffff800010700d60:       lsl     x2, x23, x2
    0.00 :   ffff800010700d64:       blr     x4
         :                      iopte_to_paddr():
         :                      if (ARM_LPAE_GRANULE(data) < SZ_64K)
    0.00 :   ffff800010700d68:       ldr     w4, [x19, #128]
         :                      __arm_lpae_unmap():
         :                      ptep = iopte_deref(pte, data);
    0.00 :   ffff800010700d6c:       adrp    x2, ffff8000112ae000 <cpu_ops+0x248>
         :                      iopte_to_paddr():
         :                      if (ARM_LPAE_GRANULE(data) < SZ_64K)
    0.00 :   ffff800010700d70:       mov     x3, #0xffff                     // #65535
         :                      return (paddr | (paddr << (48 - 12))) & (ARM_LPAE_PTE_ADDR_MASK << 4);
    0.00 :   ffff800010700d74:       orr     x0, x24, x24, lsl #36
    0.00 :   ffff800010700d78:       and     x0, x0, #0xfffffffff0000
         :                      __arm_lpae_unmap():
         :                      ptep = iopte_deref(pte, data);
    0.00 :   ffff800010700d7c:       ldr     x2, [x2, #1872]
         :                      iopte_to_paddr():
         :                      if (ARM_LPAE_GRANULE(data) < SZ_64K)
    0.00 :   ffff800010700d80:       lsl     x23, x23, x4
         :                      return (paddr | (paddr << (48 - 12))) & (ARM_LPAE_PTE_ADDR_MASK << 4);
    0.00 :   ffff800010700d84:       cmp     x23, x3
    0.00 :   ffff800010700d88:       csel    x24, x0, x24, hi  // hi = pmore
         :                      __arm_lpae_unmap():
         :                      __arm_lpae_free_pgtable(data, lvl + 1, ptep);
    0.00 :   ffff800010700d8c:       add     w1, w20, #0x1
    0.00 :   ffff800010700d90:       mov     x0, x19
    0.00 :   ffff800010700d94:       sub     x2, x24, x2
    0.00 :   ffff800010700d98:       bl      ffff800010700960 <__arm_lpae_free_pgtable>
    0.00 :   ffff800010700d9c:       ldr     x28, [x29, #88]
    0.00 :   ffff800010700da0:       mov     x0, x21
    0.00 :   ffff800010700da4:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff800010700da8:       ldp     x21, x22, [x29, #32]
    0.00 :   ffff800010700dac:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff800010700db0:       ldp     x25, x26, [x29, #64]
         :                      }
    0.00 :   ffff800010700db4:       ldp     x29, x30, [sp], #144
    0.00 :   ffff800010700db8:       ret
         :                      iopte_leaf():
         :                      return iopte_type(pte, lvl) == ARM_LPAE_PTE_TYPE_BLOCK;
    0.00 :   ffff800010700dbc:       cmp     x25, #0x1
    0.00 :   ffff800010700dc0:       cset    w0, eq  // eq = none
    0.00 :   ffff800010700dc4:       b       ffff800010700d38 <__arm_lpae_unmap+0x298>
         :                      __arm_lpae_set_pte():
         :                      __arm_lpae_sync_pte(ptep, cfg);
    0.00 :   ffff800010700dc8:       ldr     x1, [x19, #56]
    0.00 :   ffff800010700dcc:       ldr     x0, [x29, #112]
    0.00 :   ffff800010700dd0:       bl      ffff8000106ffef8 <__arm_lpae_sync_pte.isra.21>
    0.00 :   ffff800010700dd4:       b       ffff800010700d20 <__arm_lpae_unmap+0x280>
         :                      arm_lpae_split_blk_unmap():
         :                      } else if (unmap_idx >= 0) {
    0.00 :   ffff800010700dd8:       tbnz    w26, #31, ffff800010700e70 <__arm_lpae_unmap+0x3d0>
         :                      io_pgtable_tlb_add_page():
         :                      static inline void
         :                      io_pgtable_tlb_add_page(struct io_pgtable *iop,
         :                      struct iommu_iotlb_gather * gather, unsigned long iova,
         :                      size_t granule)
         :                      {
         :                      if (iop->cfg.tlb->tlb_add_page)
    0.00 :   ffff800010700ddc:       ldr     x0, [x19, #48]
         :                      arm_lpae_split_blk_unmap():
         :                      io_pgtable_tlb_add_page(&data->iop, gather, iova, size);
    0.00 :   ffff800010700de0:       ldr     x3, [x19, #8]
         :                      io_pgtable_tlb_add_page():
    0.00 :   ffff800010700de4:       ldr     x4, [x0, #24]
    0.00 :   ffff800010700de8:       cbz     x4, ffff800010700dfc <__arm_lpae_unmap+0x35c>
         :                      iop->cfg.tlb->tlb_add_page(gather, iova, granule, iop->cookie);
    0.00 :   ffff800010700dec:       ldr     x0, [x29, #120]
    0.00 :   ffff800010700df0:       mov     x2, x21
    0.00 :   ffff800010700df4:       mov     x1, x22
    0.00 :   ffff800010700df8:       blr     x4
         :                      if (iop->cfg.tlb->tlb_add_page)
    0.00 :   ffff800010700dfc:       mov     x0, x21
    0.00 :   ffff800010700e00:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff800010700e04:       ldp     x21, x22, [x29, #32]
    0.00 :   ffff800010700e08:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff800010700e0c:       ldp     x25, x26, [x29, #64]
    0.00 :   ffff800010700e10:       ldp     x27, x28, [x29, #80]
         :                      __arm_lpae_unmap():
         :                      }
    0.00 :   ffff800010700e14:       ldp     x29, x30, [sp], #144
    0.00 :   ffff800010700e18:       ret
         :                      if (WARN_ON(lvl == ARM_LPAE_MAX_LEVELS))
    0.00 :   ffff800010700e1c:       brk     #0x800
         :                      return 0;
    0.00 :   ffff800010700e20:       mov     x0, #0x0                        // #0
         :                      }
    0.00 :   ffff800010700e24:       ldp     x29, x30, [sp], #144
    0.00 :   ffff800010700e28:       ret
         :                      if (WARN_ON(!pte))
    0.00 :   ffff800010700e2c:       brk     #0x800
         :                      return 0;
    0.00 :   ffff800010700e30:       mov     x0, #0x0                        // #0
    0.00 :   ffff800010700e34:       ldr     x24, [x29, #56]
    0.00 :   ffff800010700e38:       ldr     x28, [x29, #88]
    0.00 :   ffff800010700e3c:       b       ffff800010700d0c <__arm_lpae_unmap+0x26c>
         :                      iopte_to_paddr():
         :                      u64 paddr = pte & ARM_LPAE_PTE_ADDR_MASK;
    0.00 :   ffff800010700e40:       and     x23, x23, #0xfffffffff000
         :                      if (ARM_LPAE_GRANULE(data) < SZ_64K)
    0.00 :   ffff800010700e44:       ldr     w4, [x19, #128]
         :                      arm_lpae_split_blk_unmap():
         :                      tablep = iopte_deref(pte, data);
    0.00 :   ffff800010700e48:       adrp    x2, ffff8000112ae000 <cpu_ops+0x248>
         :                      iopte_to_paddr():
         :                      if (ARM_LPAE_GRANULE(data) < SZ_64K)
    0.00 :   ffff800010700e4c:       mov     x1, #0x8                        // #8
         :                      return (paddr | (paddr << (48 - 12))) & (ARM_LPAE_PTE_ADDR_MASK << 4);
    0.00 :   ffff800010700e50:       orr     x0, x23, x23, lsl #36
         :                      if (ARM_LPAE_GRANULE(data) < SZ_64K)
    0.00 :   ffff800010700e54:       mov     x3, #0xffff                     // #65535
         :                      return (paddr | (paddr << (48 - 12))) & (ARM_LPAE_PTE_ADDR_MASK << 4);
    0.00 :   ffff800010700e58:       and     x0, x0, #0xfffffffff0000
         :                      arm_lpae_split_blk_unmap():
         :                      tablep = iopte_deref(pte, data);
    0.00 :   ffff800010700e5c:       ldr     x27, [x2, #1872]
         :                      iopte_to_paddr():
         :                      if (ARM_LPAE_GRANULE(data) < SZ_64K)
    0.00 :   ffff800010700e60:       lsl     x1, x1, x4
         :                      return (paddr | (paddr << (48 - 12))) & (ARM_LPAE_PTE_ADDR_MASK << 4);
    0.00 :   ffff800010700e64:       cmp     x1, x3
    0.00 :   ffff800010700e68:       csel    x23, x0, x23, hi  // hi = pmore
         :                      arm_lpae_split_blk_unmap():
         :                      tablep = iopte_deref(pte, data);
    0.00 :   ffff800010700e6c:       sub     x27, x23, x27
         :                      return __arm_lpae_unmap(data, gather, iova, size, lvl, tablep);
    0.00 :   ffff800010700e70:       ldr     x1, [x29, #120]
    0.00 :   ffff800010700e74:       mov     x5, x27
    0.00 :   ffff800010700e78:       mov     w4, w20
    0.00 :   ffff800010700e7c:       mov     x3, x21
    0.00 :   ffff800010700e80:       mov     x2, x22
    0.00 :   ffff800010700e84:       mov     x0, x19
    0.00 :   ffff800010700e88:       bl      ffff800010700aa0 <__arm_lpae_unmap>
    0.00 :   ffff800010700e8c:       b       ffff800010700e00 <__arm_lpae_unmap+0x360>
         :                      unmap_idx = ARM_LPAE_LVL_IDX(iova, lvl, data);
    0.00 :   ffff800010700e90:       mul     w0, w0, w2
    0.00 :   ffff800010700e94:       ldr     w3, [x19, #124]
    0.00 :   ffff800010700e98:       mov     w1, w2
    0.00 :   ffff800010700e9c:       add     w0, w0, #0x3
    0.00 :   ffff800010700ea0:       cmp     w3, w20
    0.00 :   ffff800010700ea4:       lsr     x0, x22, x0
    0.00 :   ffff800010700ea8:       b.eq    ffff800010700ec0 <__arm_lpae_unmap+0x420>  // b.none
    0.00 :   ffff800010700eac:       mov     w26, #0x1                       // #1
    0.00 :   ffff800010700eb0:       lsl     w26, w26, w1
    0.00 :   ffff800010700eb4:       sub     w26, w26, #0x1
    0.00 :   ffff800010700eb8:       and     w26, w26, w0
    0.00 :   ffff800010700ebc:       b       ffff800010700bc0 <__arm_lpae_unmap+0x120>
    0.00 :   ffff800010700ec0:       ldr     w1, [x19, #120]
    0.00 :   ffff800010700ec4:       b       ffff800010700eac <__arm_lpae_unmap+0x40c>
         :                      __arm_lpae_unmap():
         :                      } else if (iop->cfg.quirks & IO_PGTABLE_QUIRK_NON_STRICT) {
    0.00 :   ffff800010700ec8:       ldr     x0, [x19, #16]
    0.00 :   ffff800010700ecc:       tbnz    w0, #4, ffff800010700f04 <__arm_lpae_unmap+0x464>
         :                      io_pgtable_tlb_add_page():
    0.45 :   ffff800010700ed0:       ldr     x0, [x19, #48]
         :                      __arm_lpae_unmap():
         :                      io_pgtable_tlb_add_page(iop, gather, iova, size);
    0.00 :   ffff800010700ed4:       ldr     x3, [x19, #8]
         :                      io_pgtable_tlb_add_page():
    0.00 :   ffff800010700ed8:       ldr     x4, [x0, #24]
    0.00 :   ffff800010700edc:       cbz     x4, ffff800010700ef0 <__arm_lpae_unmap+0x450>
         :                      iop->cfg.tlb->tlb_add_page(gather, iova, granule, iop->cookie);
    0.11 :   ffff800010700ee0:       ldr     x0, [x29, #120]
    0.00 :   ffff800010700ee4:       mov     x2, x21
    0.00 :   ffff800010700ee8:       mov     x1, x22
    0.00 :   ffff800010700eec:       blr     x4
         :                      if (iop->cfg.tlb->tlb_add_page)
    0.23 :   ffff800010700ef0:       mov     x0, x21
    0.00 :   ffff800010700ef4:       b       ffff800010700cf8 <__arm_lpae_unmap+0x258>
         :                      arm_lpae_split_blk_unmap():
         :                      if (WARN_ON(lvl == ARM_LPAE_MAX_LEVELS))
    0.00 :   ffff800010700ef8:       brk     #0x800
         :                      return 0;
    0.00 :   ffff800010700efc:       mov     x0, #0x0                        // #0
    0.00 :   ffff800010700f00:       b       ffff800010700cf8 <__arm_lpae_unmap+0x258>
         :                      __arm_lpae_unmap():
         :                      smp_wmb();
    0.00 :   ffff800010700f04:       dmb     ishst
         :                      io_pgtable_tlb_add_page():
    0.00 :   ffff800010700f08:       mov     x0, x21
    0.00 :   ffff800010700f0c:       b       ffff800010700cf8 <__arm_lpae_unmap+0x258>
 Percent |	Source code & Disassembly of vmlinux for cycles (38523 samples, percent: local period)
------------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001027bbc0 <fput_many>:
         :                      fput_many():
         :                      EXPORT_SYMBOL_GPL(flush_delayed_fput);
         :
         :                      static DECLARE_DELAYED_WORK(delayed_fput_work, delayed_fput);
         :
         :                      void fput_many(struct file *file, unsigned int refs)
         :                      {
    0.02 :   ffff80001027bbc0:       stp     x29, x30, [sp, #-32]!
         :                      if (atomic_long_sub_and_test(refs, &file->f_count)) {
    0.01 :   ffff80001027bbc4:       mov     w1, w1
         :                      {
    0.00 :   ffff80001027bbc8:       mov     x29, sp
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff80001027bbcc:       b       ffff80001027bc4c <fput_many+0x8c>
    0.10 :   ffff80001027bbd0:       b       ffff80001027bc4c <fput_many+0x8c>
    0.31 :   ffff80001027bbd4:       add     x2, x0, #0x38
         :                      __lse_atomic64_sub_return():
         :                      }
         :
         :                      ATOMIC64_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC64_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff80001027bbd8:       neg     x1, x1
    0.02 :   ffff80001027bbdc:       ldaddal x1, x3, [x2]
   99.54 :   ffff80001027bbe0:       add     x1, x1, x3
         :                      fput_many():
         :                      if (atomic_long_sub_and_test(refs, &file->f_count)) {
    0.00 :   ffff80001027bbe4:       cbnz    x1, ffff80001027bc44 <fput_many+0x84>
    0.00 :   ffff80001027bbe8:       str     x19, [x29, #16]
    0.00 :   ffff80001027bbec:       mov     x19, x0
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80001027bbf0:       mrs     x0, sp_el0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001027bbf4:       ldr     w1, [x0, #16]
         :                      fput_many():
         :                      struct task_struct *task = current;
         :
         :                      if (likely(!in_interrupt() && !(task->flags & PF_KTHREAD))) {
    0.00 :   ffff80001027bbf8:       tst     w1, #0x1fff00
    0.00 :   ffff80001027bbfc:       b.ne    ffff80001027bc24 <fput_many+0x64>  // b.any
    0.00 :   ffff80001027bc00:       ldr     w1, [x0, #44]
    0.00 :   ffff80001027bc04:       tbnz    w1, #21, ffff80001027bc24 <fput_many+0x64>
         :                      init_task_work():
         :                      typedef void (*task_work_func_t)(struct callback_head *);
         :
         :                      static inline void
         :                      init_task_work(struct callback_head *twork, task_work_func_t func)
         :                      {
         :                      twork->func = func;
    0.00 :   ffff80001027bc08:       adrp    x1, ffff80001027b000 <__arm64_sys_copy_file_range+0xf0>
    0.00 :   ffff80001027bc0c:       add     x1, x1, #0x7d8
    0.00 :   ffff80001027bc10:       str     x1, [x19, #8]
         :                      fput_many():
         :                      init_task_work(&file->f_u.fu_rcuhead, ____fput);
         :                      if (!task_work_add(task, &file->f_u.fu_rcuhead, true))
    0.00 :   ffff80001027bc14:       mov     w2, #0x1                        // #1
    0.00 :   ffff80001027bc18:       mov     x1, x19
    0.00 :   ffff80001027bc1c:       bl      ffff800010108e28 <task_work_add>
    0.00 :   ffff80001027bc20:       cbz     w0, ffff80001027bc40 <fput_many+0x80>
         :                      llist_add():
         :                      *
         :                      * Returns true if the list was empty prior to adding this entry.
         :                      */
         :                      static inline bool llist_add(struct llist_node *new, struct llist_head *head)
         :                      {
         :                      return llist_add_batch(new, new, head);
    0.00 :   ffff80001027bc24:       adrp    x2, ffff800011ab1000 <mm_slots_hash+0x1ac0>
    0.00 :   ffff80001027bc28:       mov     x1, x19
    0.00 :   ffff80001027bc2c:       add     x2, x2, #0xae0
    0.00 :   ffff80001027bc30:       mov     x0, x19
    0.00 :   ffff80001027bc34:       bl      ffff800010487fa0 <llist_add_batch>
         :                      fput_many():
         :                      * task_work_add() will fail.  Fall through to delayed
         :                      * fput to avoid leaking *file.
         :                      */
         :                      }
         :
         :                      if (llist_add(&file->f_u.fu_llist, &delayed_fput_list))
    0.00 :   ffff80001027bc38:       tst     w0, #0xff
    0.00 :   ffff80001027bc3c:       b.ne    ffff80001027bc5c <fput_many+0x9c>  // b.any
    0.00 :   ffff80001027bc40:       ldr     x19, [x29, #16]
         :                      schedule_delayed_work(&delayed_fput_work, 1);
         :                      }
         :                      }
    0.02 :   ffff80001027bc44:       ldp     x29, x30, [sp], #32
    0.00 :   ffff80001027bc48:       ret
         :                      __ll_sc_atomic64_sub_return():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(add, add, I)
         :                      ATOMIC64_OPS(sub, sub, J)
    0.00 :   ffff80001027bc4c:       add     x4, x0, #0x38
    0.00 :   ffff80001027bc50:       b       ffff80001027bd70 <__fput_sync+0xd0>
    0.00 :   ffff80001027bc54:       mov     x1, x2
    0.00 :   ffff80001027bc58:       b       ffff80001027bbe4 <fput_many+0x24>
         :                      schedule_delayed_work():
         :                      * workqueue.
         :                      */
         :                      static inline bool schedule_delayed_work(struct delayed_work *dwork,
         :                      unsigned long delay)
         :                      {
         :                      return queue_delayed_work(system_wq, dwork, delay);
    0.00 :   ffff80001027bc5c:       adrp    x0, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      queue_delayed_work():
         :                      return queue_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);
    0.00 :   ffff80001027bc60:       adrp    x2, ffff8000118c0000 <mem_cgroup_legacy_files+0x1280>
    0.00 :   ffff80001027bc64:       add     x2, x2, #0x4b8
    0.00 :   ffff80001027bc68:       mov     x3, #0x1                        // #1
    0.00 :   ffff80001027bc6c:       ldr     x1, [x0, #432]
    0.00 :   ffff80001027bc70:       add     x2, x2, #0x18
    0.00 :   ffff80001027bc74:       mov     w0, #0x100                      // #256
    0.00 :   ffff80001027bc78:       bl      ffff800010104848 <queue_delayed_work_on>
    0.00 :   ffff80001027bc7c:       ldr     x19, [x29, #16]
         :                      fput_many():
    0.00 :   ffff80001027bc80:       ldp     x29, x30, [sp], #32
    0.00 :   ffff80001027bc84:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (36311 samples, percent: local period)
------------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000107bd600 <nvme_irq>:
         :                      nvme_irq():
         :                      nvme_ring_cq_doorbell(nvmeq);
         :                      return found;
         :                      }
         :
         :                      static irqreturn_t nvme_irq(int irq, void *data)
         :                      {
    0.02 :   ffff8000107bd600:       stp     x29, x30, [sp, #-80]!
    0.00 :   ffff8000107bd604:       mov     x29, sp
    0.25 :   ffff8000107bd608:       stp     x19, x20, [sp, #16]
         :
         :                      /*
         :                      * The rmb/wmb pair ensures we see all updates from a previous run of
         :                      * the irq handler, even if that was on another CPU.
         :                      */
         :                      rmb();
    0.08 :   ffff8000107bd60c:       dsb     ld
         :                      nvme_process_cq():
         :                      *start = nvmeq->cq_head;
    7.27 :   ffff8000107bd610:       ldrh    w19, [x1, #120]
    1.52 :   ffff8000107bd614:       ldrb    w3, [x1, #124]
         :                      while (nvme_cqe_pending(nvmeq)) {
    0.45 :   ffff8000107bd618:       ldr     x5, [x1, #72]
         :                      *start = nvmeq->cq_head;
    0.00 :   ffff8000107bd61c:       mov     w20, w19
    0.00 :   ffff8000107bd620:       and     w4, w3, #0xffff
         :                      nvme_cqe_pending():
         :                      return (le16_to_cpu(nvmeq->cqes[nvmeq->cq_head].status) & 1) ==
    0.00 :   ffff8000107bd624:       ubfiz   x0, x20, #4, #16
         :                      nvme_update_cq_head():
         :                      nvmeq->cq_head++;
    0.02 :   ffff8000107bd628:       add     w2, w20, #0x1
         :                      nvme_cqe_pending():
         :                      return (le16_to_cpu(nvmeq->cqes[nvmeq->cq_head].status) & 1) ==
    0.00 :   ffff8000107bd62c:       add     x0, x5, x0
    0.00 :   ffff8000107bd630:       ldrh    w0, [x0, #14]
         :                      nvme_process_cq():
         :                      while (nvme_cqe_pending(nvmeq)) {
    0.00 :   ffff8000107bd634:       and     w0, w0, #0x1
    0.03 :   ffff8000107bd638:       cmp     w0, w4
    0.00 :   ffff8000107bd63c:       b.ne    ffff8000107bd674 <nvme_irq+0x74>  // b.any
         :                      nvme_update_cq_head():
         :                      if (nvmeq->cq_head == nvmeq->q_depth - 1) {
   16.97 :   ffff8000107bd640:       ldrh    w0, [x1, #112]
    0.00 :   ffff8000107bd644:       sub     w0, w0, #0x1
    0.04 :   ffff8000107bd648:       cmp     w20, w0
         :                      nvmeq->cq_head++;
    0.00 :   ffff8000107bd64c:       and     w20, w2, #0xffff
         :                      if (nvmeq->cq_head == nvmeq->q_depth - 1) {
    0.70 :   ffff8000107bd650:       b.eq    ffff8000107bd790 <nvme_irq+0x190>  // b.none
         :                      nvme_cqe_pending():
         :                      return (le16_to_cpu(nvmeq->cqes[nvmeq->cq_head].status) & 1) ==
    0.01 :   ffff8000107bd654:       ubfiz   x0, x20, #4, #16
         :                      nvme_update_cq_head():
         :                      nvmeq->cq_head++;
    0.12 :   ffff8000107bd658:       strh    w20, [x1, #120]
         :                      nvme_cqe_pending():
         :                      return (le16_to_cpu(nvmeq->cqes[nvmeq->cq_head].status) & 1) ==
    0.00 :   ffff8000107bd65c:       add     x0, x5, x0
         :                      nvme_update_cq_head():
         :                      nvmeq->cq_head++;
    0.00 :   ffff8000107bd660:       add     w2, w20, #0x1
         :                      nvme_cqe_pending():
         :                      return (le16_to_cpu(nvmeq->cqes[nvmeq->cq_head].status) & 1) ==
    0.06 :   ffff8000107bd664:       ldrh    w0, [x0, #14]
         :                      nvme_process_cq():
         :                      while (nvme_cqe_pending(nvmeq)) {
    3.74 :   ffff8000107bd668:       and     w0, w0, #0x1
    3.45 :   ffff8000107bd66c:       cmp     w0, w4
    0.31 :   ffff8000107bd670:       b.eq    ffff8000107bd640 <nvme_irq+0x40>  // b.none
         :                      if (*start != *end)
    1.19 :   ffff8000107bd674:       cmp     w19, w20
    0.00 :   ffff8000107bd678:       b.eq    ffff8000107bd6cc <nvme_irq+0xcc>  // b.none
         :                      nvme_ring_cq_doorbell():
         :                      if (nvme_dbbuf_update_and_check_event(head, nvmeq->dbbuf_cq_db,
    0.11 :   ffff8000107bd67c:       ldr     x2, [x1, #144]
         :                      nvme_dbbuf_update_and_check_event():
         :                      if (dbbuf_db) {
    0.17 :   ffff8000107bd680:       cbz     x2, ffff8000107bd6b4 <nvme_irq+0xb4>
         :                      nvme_ring_cq_doorbell():
         :                      nvmeq->dbbuf_cq_ei))
    0.00 :   ffff8000107bd684:       ldr     x3, [x1, #160]
         :                      nvme_dbbuf_update_and_check_event():
         :                      wmb();
    0.00 :   ffff8000107bd688:       dsb     st
         :                      old_value = *dbbuf_db;
    0.00 :   ffff8000107bd68c:       ldrh    w0, [x2]
         :                      *dbbuf_db = value;
    0.00 :   ffff8000107bd690:       str     w20, [x2]
         :                      mb();
    0.00 :   ffff8000107bd694:       dsb     sy
         :                      if (!nvme_dbbuf_need_event(*dbbuf_ei, value, old_value))
    0.00 :   ffff8000107bd698:       ldr     w2, [x3]
         :                      nvme_dbbuf_need_event():
         :                      return (u16)(new_idx - event_idx - 1) < (u16)(new_idx - old);
    0.00 :   ffff8000107bd69c:       sub     w0, w20, w0
         :                      nvme_dbbuf_update_and_check_event():
         :                      if (!nvme_dbbuf_need_event(*dbbuf_ei, value, old_value))
    0.00 :   ffff8000107bd6a0:       and     w0, w0, #0xffff
         :                      nvme_dbbuf_need_event():
         :                      return (u16)(new_idx - event_idx - 1) < (u16)(new_idx - old);
    0.00 :   ffff8000107bd6a4:       mvn     w2, w2
    0.00 :   ffff8000107bd6a8:       add     w2, w20, w2
         :                      nvme_dbbuf_update_and_check_event():
         :                      if (!nvme_dbbuf_need_event(*dbbuf_ei, value, old_value))
    0.00 :   ffff8000107bd6ac:       cmp     w0, w2, uxth
    0.00 :   ffff8000107bd6b0:       b.ls    ffff8000107bd6cc <nvme_irq+0xcc>  // b.plast
         :                      nvme_ring_cq_doorbell():
         :                      writel(head, nvmeq->q_db + nvmeq->dev->db_stride);
    0.01 :   ffff8000107bd6b4:       dmb     oshst
    4.22 :   ffff8000107bd6b8:       ldr     x0, [x1]
    0.10 :   ffff8000107bd6bc:       ldr     x2, [x1, #104]
    0.00 :   ffff8000107bd6c0:       ldr     w0, [x0, #392]
    0.00 :   ffff8000107bd6c4:       add     x0, x2, x0, lsl #2
         :                      __raw_writel():
         :                      }
         :
         :                      #define __raw_writel __raw_writel
         :                      static inline void __raw_writel(u32 val, volatile void __iomem *addr)
         :                      {
         :                      asm volatile("str %w0, [%1]" : : "rZ" (val), "r" (addr));
    0.09 :   ffff8000107bd6c8:       str     w20, [x0]
         :                      nvme_irq():
         :                      nvme_process_cq(nvmeq, &start, &end, -1);
         :                      wmb();
    0.23 :   ffff8000107bd6cc:       dsb     st
         :                      if (start != end) {
         :                      nvme_complete_cqes(nvmeq, start, end);
         :                      return IRQ_HANDLED;
         :                      }
         :
         :                      return ret;
   26.33 :   ffff8000107bd6d0:       mov     w0, #0x0                        // #0
         :                      if (start != end) {
    0.00 :   ffff8000107bd6d4:       cmp     w19, w20
    0.00 :   ffff8000107bd6d8:       b.eq    ffff8000107bd784 <nvme_irq+0x184>  // b.none
    0.00 :   ffff8000107bd6dc:       stp     x21, x22, [x29, #32]
    0.01 :   ffff8000107bd6e0:       mov     x22, x1
    1.45 :   ffff8000107bd6e4:       stp     x23, x24, [x29, #48]
         :                      nvme_handle_cqe():
         :                      dev_warn(nvmeq->dev->ctrl.device,
    0.00 :   ffff8000107bd6e8:       adrp    x23, ffff80001123e000 <kallsyms_token_index+0xc4330>
    0.10 :   ffff8000107bd6ec:       str     x25, [x29, #64]
    0.00 :   ffff8000107bd6f0:       add     x23, x23, #0xd20
    0.04 :   ffff8000107bd6f4:       ldrh    w0, [x1, #112]
         :                      volatile struct nvme_completion *cqe = &nvmeq->cqes[idx];
    0.22 :   ffff8000107bd6f8:       ldr     x25, [x22, #72]
    0.00 :   ffff8000107bd6fc:       ubfiz   x24, x19, #4, #16
    0.17 :   ffff8000107bd700:       add     x21, x25, x24
         :                      if (unlikely(cqe->command_id >= nvmeq->q_depth)) {
    2.00 :   ffff8000107bd704:       ldrh    w1, [x21, #12]
    0.00 :   ffff8000107bd708:       cmp     w0, w1, uxth
    0.23 :   ffff8000107bd70c:       b.ls    ffff8000107bd7ac <nvme_irq+0x1ac>  // b.plast
         :                      if (unlikely(nvme_is_aen_req(nvmeq->qid, cqe->command_id))) {
   12.43 :   ffff8000107bd710:       ldrh    w0, [x21, #12]
         :                      nvme_is_aen_req():
         :                      put_device(ctrl->device);
         :                      }
         :
         :                      static inline bool nvme_is_aen_req(u16 qid, __u16 command_id)
         :                      {
         :                      return !qid && command_id >= NVME_AQ_BLK_MQ_DEPTH;
    0.96 :   ffff8000107bd714:       ldrh    w1, [x22, #122]
         :                      nvme_handle_cqe():
    0.00 :   ffff8000107bd718:       and     w0, w0, #0xffff
         :                      nvme_is_aen_req():
    0.00 :   ffff8000107bd71c:       cmp     w1, #0x0
         :                      nvme_handle_cqe():
    0.00 :   ffff8000107bd720:       ccmp    w0, #0x1e, #0x0, eq  // eq = none
    0.01 :   ffff8000107bd724:       b.hi    ffff8000107bd7d0 <nvme_irq+0x1d0>  // b.pmore
         :                      req = blk_mq_tag_to_rq(*nvmeq->tags, cqe->command_id);
    0.01 :   ffff8000107bd728:       ldr     x0, [x22, #80]
    0.35 :   ffff8000107bd72c:       ldrh    w1, [x21, #12]
    0.24 :   ffff8000107bd730:       ldr     x0, [x0]
    0.00 :   ffff8000107bd734:       and     w1, w1, #0xffff
    0.00 :   ffff8000107bd738:       bl      ffff80001045d390 <blk_mq_tag_to_rq>
         :                      trace_nvme_sq(req, cqe->sq_head, nvmeq->sq_tail);
    3.12 :   ffff8000107bd73c:       ldrh    w1, [x21, #8]
         :                      nvme_end_request(req, cqe->status, cqe->result);
    0.01 :   ffff8000107bd740:       ldrh    w1, [x21, #14]
    0.01 :   ffff8000107bd744:       ldr     x3, [x25, x24]
         :                      nvme_end_request():
         :                      rq->result = result;
    0.01 :   ffff8000107bd748:       str     x3, [x0, #288]
         :                      rq->status = le16_to_cpu(status) >> 1;
    0.00 :   ffff8000107bd74c:       ubfx    x1, x1, #1, #15
    1.00 :   ffff8000107bd750:       strh    w1, [x0, #298]
         :                      blk_mq_complete_request(req);
    0.01 :   ffff8000107bd754:       bl      ffff80001045d8b0 <blk_mq_complete_request>
         :                      nvme_complete_cqes():
         :                      if (++start == nvmeq->q_depth)
    0.39 :   ffff8000107bd758:       add     w19, w19, #0x1
    0.02 :   ffff8000107bd75c:       ldrh    w0, [x22, #112]
    0.08 :   ffff8000107bd760:       and     w19, w19, #0xffff
         :                      start = 0;
    0.59 :   ffff8000107bd764:       cmp     w19, w0
    0.00 :   ffff8000107bd768:       csel    w19, w19, wzr, ne  // ne = any
         :                      while (start != end) {
    7.38 :   ffff8000107bd76c:       cmp     w20, w19
    0.00 :   ffff8000107bd770:       b.ne    ffff8000107bd6f8 <nvme_irq+0xf8>  // b.any
         :                      nvme_irq():
         :                      return IRQ_HANDLED;
    0.31 :   ffff8000107bd774:       ldp     x21, x22, [x29, #32]
    0.00 :   ffff8000107bd778:       mov     w0, #0x1                        // #1
    0.99 :   ffff8000107bd77c:       ldp     x23, x24, [x29, #48]
    0.09 :   ffff8000107bd780:       ldr     x25, [x29, #64]
         :                      }
    0.22 :   ffff8000107bd784:       ldp     x19, x20, [sp, #16]
    0.08 :   ffff8000107bd788:       ldp     x29, x30, [sp], #80
    0.00 :   ffff8000107bd78c:       ret
         :                      nvme_update_cq_head():
         :                      nvmeq->cq_phase = !nvmeq->cq_phase;
    0.00 :   ffff8000107bd790:       cmp     w3, #0x0
         :                      nvmeq->cq_head = 0;
    0.00 :   ffff8000107bd794:       strh    wzr, [x1, #120]
         :                      nvmeq->cq_phase = !nvmeq->cq_phase;
    0.00 :   ffff8000107bd798:       cset    w3, eq  // eq = none
    0.00 :   ffff8000107bd79c:       mov     w20, #0x0                       // #0
    0.00 :   ffff8000107bd7a0:       strb    w3, [x1, #124]
    0.00 :   ffff8000107bd7a4:       and     w4, w3, #0xffff
    0.00 :   ffff8000107bd7a8:       b       ffff8000107bd624 <nvme_irq+0x24>
         :                      nvme_handle_cqe():
         :                      dev_warn(nvmeq->dev->ctrl.device,
    0.00 :   ffff8000107bd7ac:       ldr     x0, [x22]
    0.00 :   ffff8000107bd7b0:       mov     x1, x23
    0.00 :   ffff8000107bd7b4:       ldrh    w2, [x21, #12]
    0.00 :   ffff8000107bd7b8:       ldrh    w3, [x21, #10]
    0.00 :   ffff8000107bd7bc:       ldr     x0, [x0, #1448]
    0.00 :   ffff8000107bd7c0:       and     w2, w2, #0xffff
    0.00 :   ffff8000107bd7c4:       and     w3, w3, #0xffff
    0.00 :   ffff8000107bd7c8:       bl      ffff800010718e88 <_dev_warn>
    0.00 :   ffff8000107bd7cc:       b       ffff8000107bd758 <nvme_irq+0x158>
         :                      nvme_complete_async_event(&nvmeq->dev->ctrl,
    0.00 :   ffff8000107bd7d0:       ldrh    w1, [x21, #14]
    0.00 :   ffff8000107bd7d4:       mov     x2, x21
    0.00 :   ffff8000107bd7d8:       ldr     x0, [x22]
    0.00 :   ffff8000107bd7dc:       add     x0, x0, #0x200
    0.00 :   ffff8000107bd7e0:       bl      ffff8000107b9e20 <nvme_complete_async_event>
    0.00 :   ffff8000107bd7e4:       b       ffff8000107bd758 <nvme_irq+0x158>
 Percent |	Source code & Disassembly of vmlinux for cycles (28116 samples, percent: local period)
------------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102d70b8 <aio_complete_rw>:
         :                      aio_complete_rw():
         :                      list_del(&iocb->ki_list);
         :                      spin_unlock_irqrestore(&ctx->ctx_lock, flags);
         :                      }
         :
         :                      static void aio_complete_rw(struct kiocb *kiocb, long res, long res2)
         :                      {
    0.91 :   ffff8000102d70b8:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff8000102d70bc:       mov     x29, sp
    0.57 :   ffff8000102d70c0:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000102d70c4:       mov     x19, x0
    0.05 :   ffff8000102d70c8:       stp     x21, x22, [sp, #32]
    0.00 :   ffff8000102d70cc:       mov     x22, x1
         :                      struct aio_kiocb *iocb = container_of(kiocb, struct aio_kiocb, rw);
         :
         :                      if (!list_empty_careful(&iocb->ki_list))
    0.00 :   ffff8000102d70d0:       add     x0, x0, #0x90
         :                      {
    0.00 :   ffff8000102d70d4:       mov     x21, x2
         :                      list_empty_careful():
         :                      * if another CPU could re-list_add() it.
         :                      */
         :                      static inline int list_empty_careful(const struct list_head *head)
         :                      {
         :                      struct list_head *next = head->next;
         :                      return (next == head) && (next == head->prev);
    0.00 :   ffff8000102d70d8:       ldr     x1, [x19, #144]
    0.00 :   ffff8000102d70dc:       cmp     x0, x1
    0.02 :   ffff8000102d70e0:       b.eq    ffff8000102d73a8 <aio_complete_rw+0x2f0>  // b.none
         :                      spinlock_check():
         :                      * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
         :                      */
         :
         :                      static __always_inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
         :                      {
         :                      return &lock->rlock;
    0.00 :   ffff8000102d70e4:       ldr     x20, [x19, #96]
    0.00 :   ffff8000102d70e8:       add     x20, x20, #0x140
         :                      aio_remove_iocb():
         :                      spin_lock_irqsave(&ctx->ctx_lock, flags);
    0.00 :   ffff8000102d70ec:       mov     x0, x20
    0.00 :   ffff8000102d70f0:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
         :                      __list_del_entry():
         :                      __list_del(entry->prev, entry->next);
    0.00 :   ffff8000102d70f4:       ldp     x4, x3, [x19, #144]
         :                      __list_del():
         :                      next->prev = prev;
    0.00 :   ffff8000102d70f8:       str     x3, [x4, #8]
         :                      list_del():
         :                      entry->next = LIST_POISON1;
    0.00 :   ffff8000102d70fc:       mov     x5, #0x100                      // #256
         :                      entry->prev = LIST_POISON2;
    0.00 :   ffff8000102d7100:       mov     x2, #0x122                      // #290
         :                      entry->next = LIST_POISON1;
    0.00 :   ffff8000102d7104:       movk    x5, #0xdead, lsl #48
         :                      entry->prev = LIST_POISON2;
    0.00 :   ffff8000102d7108:       movk    x2, #0xdead, lsl #48
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff8000102d710c:       str     x4, [x3]
         :                      spin_unlock_irqrestore():
         :                      raw_spin_unlock_irq(&lock->rlock);
         :                      }
         :
         :                      static __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
         :                      {
         :                      raw_spin_unlock_irqrestore(&lock->rlock, flags);
    0.00 :   ffff8000102d7110:       mov     x1, x0
         :                      list_del():
    0.00 :   ffff8000102d7114:       stp     x5, x2, [x19, #144]
         :                      spin_unlock_irqrestore():
    0.00 :   ffff8000102d7118:       mov     x0, x20
    0.00 :   ffff8000102d711c:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      aio_complete_rw():
         :                      aio_remove_iocb(iocb);
         :
         :                      if (kiocb->ki_flags & IOCB_WRITE) {
    0.42 :   ffff8000102d7120:       ldr     w0, [x19, #32]
    0.00 :   ffff8000102d7124:       tbz     w0, #6, ffff8000102d7140 <aio_complete_rw+0x88>
         :                      struct inode *inode = file_inode(kiocb->ki_filp);
    0.00 :   ffff8000102d7128:       ldr     x0, [x19]
    0.00 :   ffff8000102d712c:       ldr     x1, [x0, #32]
         :                      file_end_write():
         :                      return __sb_start_write(file_inode(file)->i_sb, SB_FREEZE_WRITE, false);
         :                      }
         :
         :                      static inline void file_end_write(struct file *file)
         :                      {
         :                      if (!S_ISREG(file_inode(file)->i_mode))
    0.00 :   ffff8000102d7130:       ldrh    w0, [x1]
    0.00 :   ffff8000102d7134:       and     w0, w0, #0xf000
    0.00 :   ffff8000102d7138:       cmp     w0, #0x8, lsl #12
    0.00 :   ffff8000102d713c:       b.eq    ffff8000102d7404 <aio_complete_rw+0x34c>  // b.none
         :                      iocb_put():
         :                      if (refcount_dec_and_test(&iocb->ki_refcnt)) {
    0.54 :   ffff8000102d7140:       add     x0, x19, #0xa0
         :                      aio_complete_rw():
         :                      __sb_writers_acquired(inode->i_sb, SB_FREEZE_WRITE);
         :                      file_end_write(kiocb->ki_filp);
         :                      }
         :
         :                      iocb->ki_res.res = res;
         :                      iocb->ki_res.res2 = res2;
    0.09 :   ffff8000102d7144:       stp     x22, x21, [x19, #128]
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000102d7148:       b       ffff8000102d737c <aio_complete_rw+0x2c4>
    0.26 :   ffff8000102d714c:       b       ffff8000102d737c <aio_complete_rw+0x2c4>
         :                      __lse_atomic_fetch_sub_release():
         :                      return i;                                                       \
         :                      }
         :
         :                      ATOMIC_FETCH_OP_SUB(_relaxed,   )
         :                      ATOMIC_FETCH_OP_SUB(_acquire,  a, "memory")
         :                      ATOMIC_FETCH_OP_SUB(_release,  l, "memory")
    0.31 :   ffff8000102d7150:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000102d7154:       neg     w1, w1
    0.00 :   ffff8000102d7158:       ldaddl  w1, w1, [x0]
         :                      refcount_sub_and_test():
         :                      */
         :                      static inline __must_check bool refcount_sub_and_test(int i, refcount_t *r)
         :                      {
         :                      int old = atomic_fetch_sub_release(i, &r->refs);
         :
         :                      if (old == i) {
   58.21 :   ffff8000102d715c:       cmp     w1, #0x1
    0.00 :   ffff8000102d7160:       b.ne    ffff8000102d7390 <aio_complete_rw+0x2d8>  // b.any
    0.00 :   ffff8000102d7164:       stp     x23, x24, [x29, #48]
         :                      smp_acquire__after_ctrl_dep();
    0.49 :   ffff8000102d7168:       dmb     ishld
         :                      aio_complete():
         :                      struct kioctx   *ctx = iocb->ki_ctx;
    6.44 :   ffff8000102d716c:       ldr     x20, [x19, #96]
         :                      spinlock_check():
         :                      return &lock->rlock;
    0.00 :   ffff8000102d7170:       add     x22, x20, #0x1c8
         :                      aio_complete():
         :                      spin_lock_irqsave(&ctx->completion_lock, flags);
    0.00 :   ffff8000102d7174:       mov     x0, x22
    0.00 :   ffff8000102d7178:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
         :                      pos = tail + AIO_EVENTS_OFFSET;
    0.00 :   ffff8000102d717c:       ldr     w2, [x20, #448]
         :                      spin_lock_irqsave(&ctx->completion_lock, flags);
    0.00 :   ffff8000102d7180:       mov     x23, x0
         :                      if (++tail >= ctx->nr_events)
    0.00 :   ffff8000102d7184:       ldr     w4, [x20, #144]
         :                      pos = tail + AIO_EVENTS_OFFSET;
    0.00 :   ffff8000102d7188:       add     w2, w2, #0x1
         :                      ev_page = kmap_atomic(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);
    0.00 :   ffff8000102d718c:       ldr     x0, [x20, #168]
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff8000102d7190:       mrs     x1, sp_el0
         :                      aio_complete():
    0.00 :   ffff8000102d7194:       lsr     w24, w2, #7
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d7198:       ldr     w3, [x1, #16]
         :                      aio_complete():
         :                      tail = 0;
    0.00 :   ffff8000102d719c:       cmp     w2, w4
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff8000102d71a0:       add     w3, w3, #0x1
         :                      aio_complete():
    0.00 :   ffff8000102d71a4:       csel    w21, w2, wzr, cc  // cc = lo, ul, last
         :                      ev_page = kmap_atomic(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);
    0.00 :   ffff8000102d71a8:       ldr     x0, [x0, x24, lsl #3]
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d71ac:       str     w3, [x1, #16]
         :                      pagefault_disabled_inc():
         :                      }
         :                      #endif
         :
         :                      static __always_inline void pagefault_disabled_inc(void)
         :                      {
         :                      current->pagefault_disabled++;
    0.00 :   ffff8000102d71b0:       ldr     w3, [x1, #2448]
    0.00 :   ffff8000102d71b4:       add     w3, w3, #0x1
    0.00 :   ffff8000102d71b8:       str     w3, [x1, #2448]
         :                      lowmem_page_address():
         :                      */
         :                      #include <linux/vmstat.h>
         :
         :                      static __always_inline void *lowmem_page_address(const struct page *page)
         :                      {
         :                      return page_to_virt(page);
    0.00 :   ffff8000102d71bc:       mov     x3, #0x200000                   // #2097152
         :                      aio_complete():
         :                      event = ev_page + pos % AIO_EVENTS_PER_PAGE;
    0.00 :   ffff8000102d71c0:       ubfiz   x4, x2, #5, #7
         :                      lowmem_page_address():
    0.00 :   ffff8000102d71c4:       movk    x3, #0x200, lsl #32
    0.00 :   ffff8000102d71c8:       add     x0, x0, x3
         :                      aio_complete():
         :                      *event = iocb->ki_res;
    0.00 :   ffff8000102d71cc:       mov     x5, #0xffff000000000000         // #-281474976710656
         :                      lowmem_page_address():
    0.00 :   ffff8000102d71d0:       lsr     x0, x0, #6
         :                      aio_complete():
    0.00 :   ffff8000102d71d4:       ldp     x2, x3, [x19, #112]
    0.00 :   ffff8000102d71d8:       add     x0, x4, x0, lsl #12
    0.00 :   ffff8000102d71dc:       add     x0, x0, x5
    0.00 :   ffff8000102d71e0:       stp     x2, x3, [x0]
    0.00 :   ffff8000102d71e4:       ldp     x2, x3, [x19, #128]
    0.00 :   ffff8000102d71e8:       stp     x2, x3, [x0, #16]
         :                      pagefault_disabled_dec():
         :                      }
         :
         :                      static __always_inline void pagefault_disabled_dec(void)
         :                      {
         :                      current->pagefault_disabled--;
    0.00 :   ffff8000102d71ec:       ldr     w0, [x1, #2448]
    0.00 :   ffff8000102d71f0:       sub     w0, w0, #0x1
    0.00 :   ffff8000102d71f4:       str     w0, [x1, #2448]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d71f8:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000102d71fc:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d7200:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102d7204:       cbnz    x0, ffff8000102d73c4 <aio_complete_rw+0x30c>
         :                      __kunmap_atomic():
         :                      #define kmap_atomic_prot(page, prot)    kmap_atomic(page)
         :
         :                      static inline void __kunmap_atomic(void *addr)
         :                      {
         :                      pagefault_enable();
         :                      preempt_enable();
    0.00 :   ffff8000102d7208:       bl      ffff800010cad640 <preempt_schedule>
         :                      aio_complete():
         :                      flush_dcache_page(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);
    0.00 :   ffff8000102d720c:       ldr     x0, [x20, #168]
    0.00 :   ffff8000102d7210:       ldr     x0, [x0, x24, lsl #3]
    0.00 :   ffff8000102d7214:       bl      ffff8000100a2608 <flush_dcache_page>
         :                      smp_wmb();      /* make event visible before updating tail */
    0.00 :   ffff8000102d7218:       dmb     ishst
         :                      ctx->tail = tail;
    0.00 :   ffff8000102d721c:       str     w21, [x20, #448]
         :                      ring = kmap_atomic(ctx->ring_pages[0]);
    0.00 :   ffff8000102d7220:       ldr     x1, [x20, #168]
         :                      get_current():
    0.00 :   ffff8000102d7224:       mrs     x0, sp_el0
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d7228:       ldr     w2, [x0, #16]
         :                      aio_complete():
    0.00 :   ffff8000102d722c:       ldr     x1, [x1]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff8000102d7230:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d7234:       str     w2, [x0, #16]
         :                      pagefault_disabled_inc():
         :                      current->pagefault_disabled++;
    0.00 :   ffff8000102d7238:       ldr     w2, [x0, #2448]
    0.00 :   ffff8000102d723c:       add     w2, w2, #0x1
    0.00 :   ffff8000102d7240:       str     w2, [x0, #2448]
         :                      lowmem_page_address():
    0.00 :   ffff8000102d7244:       mov     x2, #0x200000                   // #2097152
    0.00 :   ffff8000102d7248:       movk    x2, #0x200, lsl #32
    0.00 :   ffff8000102d724c:       add     x1, x1, x2
    0.00 :   ffff8000102d7250:       mov     x2, #0xffff000000000000         // #-281474976710656
    0.00 :   ffff8000102d7254:       lsr     x1, x1, #6
    0.00 :   ffff8000102d7258:       add     x1, x2, x1, lsl #12
         :                      aio_complete():
         :                      head = ring->head;
    0.00 :   ffff8000102d725c:       ldr     w24, [x1, #8]
         :                      ring->tail = tail;
    0.00 :   ffff8000102d7260:       str     w21, [x1, #12]
         :                      pagefault_disabled_dec():
         :                      current->pagefault_disabled--;
    0.00 :   ffff8000102d7264:       ldr     w1, [x0, #2448]
    0.00 :   ffff8000102d7268:       sub     w1, w1, #0x1
    0.00 :   ffff8000102d726c:       str     w1, [x0, #2448]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d7270:       ldr     x1, [x0, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000102d7274:       sub     x1, x1, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d7278:       str     w1, [x0, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102d727c:       cbnz    x1, ffff8000102d73b8 <aio_complete_rw+0x300>
         :                      __kunmap_atomic():
    0.00 :   ffff8000102d7280:       bl      ffff800010cad640 <preempt_schedule>
         :                      aio_complete():
         :                      flush_dcache_page(ctx->ring_pages[0]);
    0.00 :   ffff8000102d7284:       ldr     x0, [x20, #168]
    0.00 :   ffff8000102d7288:       ldr     x0, [x0]
    0.00 :   ffff8000102d728c:       bl      ffff8000100a2608 <flush_dcache_page>
         :                      ctx->completed_events++;
    0.00 :   ffff8000102d7290:       ldr     w0, [x20, #452]
    0.00 :   ffff8000102d7294:       add     w0, w0, #0x1
    0.00 :   ffff8000102d7298:       str     w0, [x20, #452]
         :                      if (ctx->completed_events > 1)
    0.00 :   ffff8000102d729c:       cmp     w0, #0x1
    0.00 :   ffff8000102d72a0:       b.hi    ffff8000102d73d0 <aio_complete_rw+0x318>  // b.pmore
         :                      spin_unlock_irqrestore():
         :                      raw_spin_unlock_irqrestore(&lock->rlock, flags);
    0.00 :   ffff8000102d72a4:       mov     x0, x22
    0.00 :   ffff8000102d72a8:       mov     x1, x23
    0.00 :   ffff8000102d72ac:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      aio_complete():
         :                      if (iocb->ki_eventfd)
    0.00 :   ffff8000102d72b0:       ldr     x0, [x19, #168]
    0.00 :   ffff8000102d72b4:       cbz     x0, ffff8000102d72c0 <aio_complete_rw+0x208>
         :                      eventfd_signal(iocb->ki_eventfd, 1);
    0.00 :   ffff8000102d72b8:       mov     x1, #0x1                        // #1
    0.00 :   ffff8000102d72bc:       bl      ffff8000102d49a0 <eventfd_signal>
         :                      smp_mb();
    0.01 :   ffff8000102d72c0:       dmb     ish
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    5.63 :   ffff8000102d72c4:       ldr     x1, [x20, #424]
         :                      waitqueue_active():
         :                      * Also note that this 'optimization' trades a spin_lock() for an smp_mb(),
         :                      * which (when the lock is uncontended) are of roughly equal cost.
         :                      */
         :                      static inline int waitqueue_active(struct wait_queue_head *wq_head)
         :                      {
         :                      return !list_empty(&wq_head->head);
    0.00 :   ffff8000102d72c8:       add     x2, x20, #0x1a8
    0.00 :   ffff8000102d72cc:       add     x0, x20, #0x1a0
         :                      aio_complete():
         :                      if (waitqueue_active(&ctx->wait))
    0.00 :   ffff8000102d72d0:       cmp     x2, x1
    0.44 :   ffff8000102d72d4:       b.eq    ffff8000102d72e8 <aio_complete_rw+0x230>  // b.none
         :                      wake_up(&ctx->wait);
    0.01 :   ffff8000102d72d8:       mov     x3, #0x0                        // #0
    0.00 :   ffff8000102d72dc:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000102d72e0:       mov     w1, #0x3                        // #3
    0.00 :   ffff8000102d72e4:       bl      ffff80001012e430 <__wake_up>
         :                      iocb_destroy():
         :                      if (iocb->ki_eventfd)
    0.11 :   ffff8000102d72e8:       ldr     x0, [x19, #168]
    0.00 :   ffff8000102d72ec:       cbz     x0, ffff8000102d72f4 <aio_complete_rw+0x23c>
         :                      eventfd_ctx_put(iocb->ki_eventfd);
    0.00 :   ffff8000102d72f0:       bl      ffff8000102d4f38 <eventfd_ctx_put>
         :                      if (iocb->ki_filp)
    1.69 :   ffff8000102d72f4:       ldr     x0, [x19]
    0.00 :   ffff8000102d72f8:       cbz     x0, ffff8000102d7300 <aio_complete_rw+0x248>
         :                      fput(iocb->ki_filp);
    0.59 :   ffff8000102d72fc:       bl      ffff80001027bc88 <fput>
         :                      percpu_ref_put(&iocb->ki_ctx->reqs);
    0.06 :   ffff8000102d7300:       ldr     x20, [x19, #96]
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff8000102d7304:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
    0.00 :   ffff8000102d7308:       ldr     x0, [x20, #72]
         :                      __ref_is_percpu():
         :                      * Theoretically, the following could test just ATOMIC; however,
         :                      * then we'd have to mask off DEAD separately as DEAD may be
         :                      * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
         :                      * implies ATOMIC anyway.  Test them together.
         :                      */
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff8000102d730c:       tst     x0, #0x3
    0.00 :   ffff8000102d7310:       b.ne    ffff8000102d742c <aio_complete_rw+0x374>  // b.any
         :                      get_current():
    0.09 :   ffff8000102d7314:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.37 :   ffff8000102d7318:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff8000102d731c:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.17 :   ffff8000102d7320:       str     w2, [x1, #16]
         :                      __percpu_add_case_64():
         :
         :                      PERCPU_RW_OPS(8)
         :                      PERCPU_RW_OPS(16)
         :                      PERCPU_RW_OPS(32)
         :                      PERCPU_RW_OPS(64)
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff8000102d7324:       mov     x3, #0xffffffffffffffff         // #-1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000102d7328:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff8000102d732c:       add     x0, x0, x2
    0.35 :   ffff8000102d7330:       ldxr    x5, [x0]
   12.39 :   ffff8000102d7334:       add     x5, x5, x3
    0.00 :   ffff8000102d7338:       stxr    w4, x5, [x0]
    0.00 :   ffff8000102d733c:       cbnz    w4, ffff8000102d7330 <aio_complete_rw+0x278>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d7340:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000102d7344:       add     x0, x0, x3
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.54 :   ffff8000102d7348:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102d734c:       cbnz    x0, ffff8000102d73f8 <aio_complete_rw+0x340>
         :                      percpu_ref_put_many():
         :                      unsigned long __percpu *percpu_count;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count))
         :                      this_cpu_sub(*percpu_count, nr);
    0.00 :   ffff8000102d7350:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.58 :   ffff8000102d7354:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      iocb_destroy():
         :                      kmem_cache_free(kiocb_cachep, iocb);
    0.01 :   ffff8000102d7358:       adrp    x0, ffff800011ab4000 <in_lookup_hashtable+0x1c08>
    0.00 :   ffff8000102d735c:       mov     x1, x19
    0.45 :   ffff8000102d7360:       ldr     x0, [x0, #3936]
    0.00 :   ffff8000102d7364:       bl      ffff800010250100 <kmem_cache_free>
         :                      aio_complete_rw():
         :                      iocb_put(iocb);
         :                      }
    0.01 :   ffff8000102d7368:       ldp     x23, x24, [x29, #48]
    0.62 :   ffff8000102d736c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000102d7370:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000102d7374:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000102d7378:       ret
         :                      __ll_sc_atomic_fetch_sub_release():
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff8000102d737c:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000102d7380:       add     x5, x19, #0xa0
    0.00 :   ffff8000102d7384:       b       ffff8000102da5e8 <__arm64_compat_sys_io_pgetevents_time64+0x328>
         :                      refcount_sub_and_test():
         :                      if (old == i) {
    0.00 :   ffff8000102d7388:       cmp     w1, #0x1
    0.00 :   ffff8000102d738c:       b.eq    ffff8000102d7164 <aio_complete_rw+0xac>  // b.none
         :                      return true;
         :                      }
         :
         :                      if (unlikely(old < 0 || old - i < 0))
    0.01 :   ffff8000102d7390:       cmp     w1, #0x0
    0.00 :   ffff8000102d7394:       b.le    ffff8000102d7414 <aio_complete_rw+0x35c>
         :                      aio_complete_rw():
    0.01 :   ffff8000102d7398:       ldp     x19, x20, [sp, #16]
    0.04 :   ffff8000102d739c:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000102d73a0:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000102d73a4:       ret
         :                      list_empty_careful():
         :                      return (next == head) && (next == head->prev);
    5.55 :   ffff8000102d73a8:       ldr     x1, [x19, #152]
    0.00 :   ffff8000102d73ac:       cmp     x0, x1
    0.00 :   ffff8000102d73b0:       b.ne    ffff8000102d70e4 <aio_complete_rw+0x2c>  // b.any
    0.61 :   ffff8000102d73b4:       b       ffff8000102d7120 <aio_complete_rw+0x68>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d73b8:       ldr     x0, [x0, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102d73bc:       cbz     x0, ffff8000102d7280 <aio_complete_rw+0x1c8>
    0.00 :   ffff8000102d73c0:       b       ffff8000102d7284 <aio_complete_rw+0x1cc>
         :                      __read_once_size():
    0.00 :   ffff8000102d73c4:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102d73c8:       cbz     x0, ffff8000102d7208 <aio_complete_rw+0x150>
    0.00 :   ffff8000102d73cc:       b       ffff8000102d720c <aio_complete_rw+0x154>
         :                      aio_complete():
         :                      refill_reqs_available(ctx, head, tail);
    0.00 :   ffff8000102d73d0:       mov     w2, w21
    0.00 :   ffff8000102d73d4:       mov     w1, w24
    0.00 :   ffff8000102d73d8:       mov     x0, x20
    0.00 :   ffff8000102d73dc:       bl      ffff8000102d5fa0 <refill_reqs_available>
         :                      spin_unlock_irqrestore():
    0.00 :   ffff8000102d73e0:       mov     x0, x22
    0.00 :   ffff8000102d73e4:       mov     x1, x23
    0.00 :   ffff8000102d73e8:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      aio_complete():
         :                      if (iocb->ki_eventfd)
    0.57 :   ffff8000102d73ec:       ldr     x0, [x19, #168]
    0.00 :   ffff8000102d73f0:       cbnz    x0, ffff8000102d72b8 <aio_complete_rw+0x200>
    0.13 :   ffff8000102d73f4:       b       ffff8000102d72c0 <aio_complete_rw+0x208>
         :                      __read_once_size():
    0.00 :   ffff8000102d73f8:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102d73fc:       cbz     x0, ffff8000102d7350 <aio_complete_rw+0x298>
    0.63 :   ffff8000102d7400:       b       ffff8000102d7354 <aio_complete_rw+0x29c>
         :                      file_end_write():
         :                      return;
         :                      __sb_end_write(file_inode(file)->i_sb, SB_FREEZE_WRITE);
    0.00 :   ffff8000102d7404:       ldr     x0, [x1, #40]
    0.00 :   ffff8000102d7408:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000102d740c:       bl      ffff80001027c570 <__sb_end_write>
    0.00 :   ffff8000102d7410:       b       ffff8000102d7140 <aio_complete_rw+0x88>
         :                      refcount_sub_and_test():
         :                      refcount_warn_saturate(r, REFCOUNT_SUB_UAF);
    0.00 :   ffff8000102d7414:       mov     w1, #0x3                        // #3
    0.00 :   ffff8000102d7418:       bl      ffff80001048ba28 <refcount_warn_saturate>
         :                      aio_complete_rw():
         :                      }
    0.00 :   ffff8000102d741c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000102d7420:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000102d7424:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000102d7428:       ret
         :                      iocb_destroy():
         :                      percpu_ref_put(&iocb->ki_ctx->reqs);
    0.00 :   ffff8000102d742c:       add     x0, x20, #0x40
         :                      arch_static_branch_jump():
    0.00 :   ffff8000102d7430:       b       ffff8000102d7458 <aio_complete_rw+0x3a0>
    0.00 :   ffff8000102d7434:       b       ffff8000102d7458 <aio_complete_rw+0x3a0>
         :                      __lse_atomic64_sub_return():
         :                      }
         :
         :                      ATOMIC64_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC64_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff8000102d7438:       mov     x1, #0x1                        // #1
    0.00 :   ffff8000102d743c:       neg     x1, x1
    0.00 :   ffff8000102d7440:       ldaddal x1, x2, [x0]
    0.00 :   ffff8000102d7444:       add     x1, x1, x2
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff8000102d7448:       cbnz    x1, ffff8000102d7354 <aio_complete_rw+0x29c>
         :                      ref->release(ref);
    0.00 :   ffff8000102d744c:       ldr     x1, [x0, #16]
    0.00 :   ffff8000102d7450:       blr     x1
    0.00 :   ffff8000102d7454:       b       ffff8000102d7354 <aio_complete_rw+0x29c>
         :                      __ll_sc_atomic64_sub_return():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(add, add, I)
         :                      ATOMIC64_OPS(sub, sub, J)
    0.00 :   ffff8000102d7458:       mov     x2, #0x1                        // #1
    0.00 :   ffff8000102d745c:       add     x4, x20, #0x40
    0.00 :   ffff8000102d7460:       b       ffff8000102da600 <__arm64_compat_sys_io_pgetevents_time64+0x340>
    0.00 :   ffff8000102d7464:       b       ffff8000102d7448 <aio_complete_rw+0x390>
 Percent |	Source code & Disassembly of vmlinux for cycles (20701 samples, percent: local period)
------------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104ab970 <sbitmap_queue_clear>:
         :                      sbitmap_queue_clear():
         :                      }
         :                      EXPORT_SYMBOL_GPL(sbitmap_queue_wake_up);
         :
         :                      void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
         :                      unsigned int cpu)
         :                      {
    0.82 :   ffff8000104ab970:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff8000104ab974:       mov     x29, sp
    0.00 :   ffff8000104ab978:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000104ab97c:       mov     x19, x0
    0.00 :   ffff8000104ab980:       str     x21, [sp, #32]
    0.00 :   ffff8000104ab984:       mov     w20, w1
    0.00 :   ffff8000104ab988:       mov     w21, w2
         :                      * and its pair is the memory barrier implied in __sbitmap_get_word.
         :                      *
         :                      * One invariant is that the clear bit has to be zero when the bit
         :                      * is in use.
         :                      */
         :                      smp_mb__before_atomic();
    0.01 :   ffff8000104ab98c:       dmb     ish
         :                      sbitmap_deferred_clear_bit(&sbq->sb, nr);
   13.38 :   ffff8000104ab990:       ldr     w4, [x0, #4]
         :                      sbitmap_deferred_clear_bit():
         :                      */
         :                      static inline void sbitmap_deferred_clear_bit(struct sbitmap *sb, unsigned int bitnr)
         :                      {
         :                      unsigned long *addr = &sb->map[SB_NR_TO_INDEX(sb, bitnr)].cleared;
         :
         :                      set_bit(SB_NR_TO_BIT(sb, bitnr), addr);
    0.00 :   ffff8000104ab994:       mov     w2, #0xffffffff                 // #-1
         :                      unsigned long *addr = &sb->map[SB_NR_TO_INDEX(sb, bitnr)].cleared;
    0.03 :   ffff8000104ab998:       ldr     x0, [x0, #16]
    0.00 :   ffff8000104ab99c:       mov     w5, #0xc0                       // #192
         :                      set_bit():
         :                      */
         :
         :                      static inline void set_bit(unsigned int nr, volatile unsigned long *p)
         :                      {
         :                      p += BIT_WORD(nr);
         :                      atomic_long_or(BIT_MASK(nr), (atomic_long_t *)p);
    0.00 :   ffff8000104ab9a0:       mov     x1, #0x1                        // #1
         :                      sbitmap_deferred_clear_bit():
    0.00 :   ffff8000104ab9a4:       lsr     w3, w20, w4
         :                      set_bit(SB_NR_TO_BIT(sb, bitnr), addr);
    0.00 :   ffff8000104ab9a8:       lsl     w2, w2, w4
    0.00 :   ffff8000104ab9ac:       bic     w2, w20, w2
         :                      unsigned long *addr = &sb->map[SB_NR_TO_INDEX(sb, bitnr)].cleared;
    0.00 :   ffff8000104ab9b0:       umaddl  x3, w3, w5, x0
         :                      set_bit():
         :                      p += BIT_WORD(nr);
    0.00 :   ffff8000104ab9b4:       lsr     w0, w2, #6
         :                      atomic_long_or(BIT_MASK(nr), (atomic_long_t *)p);
    0.00 :   ffff8000104ab9b8:       lsl     x1, x1, x2
         :                      sbitmap_deferred_clear_bit():
    0.00 :   ffff8000104ab9bc:       add     x2, x3, #0x80
         :                      set_bit():
         :                      p += BIT_WORD(nr);
    0.02 :   ffff8000104ab9c0:       add     x0, x2, x0, lsl #3
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000104ab9c4:       b       ffff8000104aba20 <sbitmap_queue_clear+0xb0>
    0.83 :   ffff8000104ab9c8:       b       ffff8000104aba20 <sbitmap_queue_clear+0xb0>
         :                      __lse_atomic64_or():
         :                      : [i] "+r" (i), [v] "+Q" (v->counter)                           \
         :                      : "r" (v));                                                     \
         :                      }
         :
         :                      ATOMIC64_OP(andnot, stclr)
         :                      ATOMIC64_OP(or, stset)
    0.01 :   ffff8000104ab9cc:       stset   x1, [x0]
         :                      sbitmap_queue_clear():
         :                      * Pairs with the memory barrier in set_current_state() to ensure the
         :                      * proper ordering of clear_bit_unlock()/waitqueue_active() in the waker
         :                      * and test_and_set_bit_lock()/prepare_to_wait()/finish_wait() in the
         :                      * waiter. See the comment on waitqueue_active().
         :                      */
         :                      smp_mb__after_atomic();
    1.34 :   ffff8000104ab9d0:       dmb     ish
   81.60 :   ffff8000104ab9d4:       nop
         :                      sbitmap_queue_wake_up():
         :                      while (__sbq_wake_up(sbq))
    0.00 :   ffff8000104ab9d8:       mov     x0, x19
    0.00 :   ffff8000104ab9dc:       bl      ffff8000104ab7f8 <__sbq_wake_up>
    0.00 :   ffff8000104ab9e0:       tst     w0, #0xff
    0.00 :   ffff8000104ab9e4:       b.ne    ffff8000104ab9d8 <sbitmap_queue_clear+0x68>  // b.any
         :                      sbitmap_queue_clear():
         :                      sbitmap_queue_wake_up(sbq);
         :
         :                      if (likely(!sbq->round_robin && nr < sbq->sb.depth))
    0.00 :   ffff8000104ab9e8:       ldrb    w0, [x19, #52]
    0.00 :   ffff8000104ab9ec:       cbnz    w0, ffff8000104aba10 <sbitmap_queue_clear+0xa0>
    0.08 :   ffff8000104ab9f0:       ldr     w0, [x19]
    0.00 :   ffff8000104ab9f4:       cmp     w0, w20
    0.00 :   ffff8000104ab9f8:       b.ls    ffff8000104aba10 <sbitmap_queue_clear+0xa0>  // b.plast
         :                      *per_cpu_ptr(sbq->alloc_hint, cpu) = nr;
    0.77 :   ffff8000104ab9fc:       adrp    x1, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000104aba00:       add     x1, x1, #0x8e8
    0.00 :   ffff8000104aba04:       ldr     x0, [x19, #24]
    0.00 :   ffff8000104aba08:       ldr     x1, [x1, w21, uxtw #3]
    0.04 :   ffff8000104aba0c:       str     w20, [x1, x0]
         :                      }
    0.80 :   ffff8000104aba10:       ldp     x19, x20, [sp, #16]
    0.16 :   ffff8000104aba14:       ldr     x21, [sp, #32]
    0.10 :   ffff8000104aba18:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000104aba1c:       ret
         :                      __ll_sc_atomic64_or():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(and, and, L)
         :                      ATOMIC64_OPS(or, orr, L)
    0.00 :   ffff8000104aba20:       b       ffff8000104abe54 <__sbitmap_queue_get+0x2a4>
    0.00 :   ffff8000104aba24:       b       ffff8000104ab9d0 <sbitmap_queue_clear+0x60>
 Percent |	Source code & Disassembly of vmlinux for cycles (20653 samples, percent: local period)
------------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001024fa68 <__slab_free>:
         :                      __slab_free():
         :                      */
         :                      static void __slab_free(struct kmem_cache *s, struct page *page,
         :                      void *head, void *tail, int cnt,
         :                      unsigned long addr)
         :
         :                      {
    0.05 :   ffff80001024fa68:       stp     x29, x30, [sp, #-208]!
    0.00 :   ffff80001024fa6c:       adrp    x6, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff80001024fa70:       add     x8, x6, #0x8c8
         :                      kmem_cache_debug():
         :                      return unlikely(s->flags & SLAB_DEBUG_FLAGS);
    0.00 :   ffff80001024fa74:       mov     w6, #0xd00                      // #3328
         :                      __slab_free():
         :                      {
    0.52 :   ffff80001024fa78:       mov     x29, sp
    0.73 :   ffff80001024fa7c:       str     x19, [sp, #16]
         :                      kmem_cache_debug():
         :                      return unlikely(s->flags & SLAB_DEBUG_FLAGS);
    0.04 :   ffff80001024fa80:       movk    w6, #0x21, lsl #16
         :                      __slab_free():
         :                      {
    0.37 :   ffff80001024fa84:       stp     x21, x22, [sp, #32]
    0.00 :   ffff80001024fa88:       mov     x19, x1
    0.44 :   ffff80001024fa8c:       stp     x23, x24, [sp, #48]
    0.00 :   ffff80001024fa90:       mov     x21, x0
    0.90 :   ffff80001024fa94:       str     x2, [x29, #120]
    0.00 :   ffff80001024fa98:       mov     x23, x3
    0.50 :   ffff80001024fa9c:       ldr     x7, [x8]
    0.58 :   ffff80001024faa0:       str     x7, [x29, #200]
    0.00 :   ffff80001024faa4:       mov     x7, #0x0                        // #0
         :                      kmem_cache_debug():
         :                      return unlikely(s->flags & SLAB_DEBUG_FLAGS);
    0.29 :   ffff80001024faa8:       ldr     w8, [x0, #8]
         :                      __slab_free():
         :                      {
    0.00 :   ffff80001024faac:       mov     w22, w4
         :                      struct kmem_cache_node *n = NULL;
         :                      unsigned long uninitialized_var(flags);
         :
         :                      stat(s, FREE_SLOWPATH);
         :
         :                      if (kmem_cache_debug(s) &&
    0.00 :   ffff80001024fab0:       tst     w8, w6
    0.00 :   ffff80001024fab4:       b.ne    ffff80001024fd4c <__slab_free+0x2e4>  // b.any
    0.81 :   ffff80001024fab8:       and     w22, w22, #0xffff
    0.60 :   ffff80001024fabc:       str     x20, [x29, #24]
    0.06 :   ffff80001024fac0:       stp     x25, x26, [x29, #64]
    0.30 :   ffff80001024fac4:       stp     x27, x28, [x29, #80]
         :                      if (unlikely(n)) {
         :                      spin_unlock_irqrestore(&n->list_lock, flags);
         :                      n = NULL;
         :                      }
         :                      prior = page->freelist;
         :                      counters = page->counters;
    1.10 :   ffff80001024fac8:       ldp     x25, x20, [x19, #32]
         :                      set_freepointer(s, tail, prior);
         :                      new.counters = counters;
    0.60 :   ffff80001024facc:       str     x20, [x29, #168]
         :                      set_freepointer():
         :                      unsigned long freeptr_addr = (unsigned long)object + s->offset;
    0.28 :   ffff80001024fad0:       ldr     w0, [x21, #32]
         :                      __slab_free():
    0.00 :   ffff80001024fad4:       mov     x26, #0x0                       // #0
         :                      was_frozen = new.frozen;
         :                      new.inuse -= cnt;
    0.00 :   ffff80001024fad8:       sub     w6, w20, w22
         :                      if ((!new.inuse || !prior) && !was_frozen) {
    0.00 :   ffff80001024fadc:       cmp     x25, #0x0
         :                      new.inuse -= cnt;
    0.27 :   ffff80001024fae0:       and     w27, w6, #0xffff
         :                      was_frozen = new.frozen;
    0.00 :   ffff80001024fae4:       lsr     w1, w20, #24
         :                      new.inuse -= cnt;
    0.65 :   ffff80001024fae8:       strh    w27, [x29, #168]
         :                      if ((!new.inuse || !prior) && !was_frozen) {
    0.00 :   ffff80001024faec:       cset    w24, eq  // eq = none
         :                      set_freepointer():
         :                      *(void **)freeptr_addr = freelist_ptr(s, fp, freeptr_addr);
    0.44 :   ffff80001024faf0:       str     x25, [x23, x0]
         :                      __slab_free():
         :                      if ((!new.inuse || !prior) && !was_frozen) {
    0.00 :   ffff80001024faf4:       cmp     w27, #0x0
         :                      was_frozen = new.frozen;
    0.00 :   ffff80001024faf8:       ubfx    x28, x1, #7, #1
         :                      if ((!new.inuse || !prior) && !was_frozen) {
    0.00 :   ffff80001024fafc:       ccmp    w24, #0x0, #0x0, ne  // ne = any
    1.42 :   ffff80001024fb00:       ldr     w0, [x21, #8]
    0.00 :   ffff80001024fb04:       b.eq    ffff80001024fb40 <__slab_free+0xd8>  // b.none
    0.06 :   ffff80001024fb08:       cbnz    w28, ffff80001024fb40 <__slab_free+0xd8>
         :
         :                      if (kmem_cache_has_cpu_partial(s) && !prior) {
    0.00 :   ffff80001024fb0c:       mov     w2, #0xd00                      // #3328
    0.00 :   ffff80001024fb10:       movk    w2, #0x21, lsl #16
    0.00 :   ffff80001024fb14:       tst     w0, w2
    0.00 :   ffff80001024fb18:       ccmp    w24, #0x0, #0x4, eq  // eq = none
    0.00 :   ffff80001024fb1c:       b.ne    ffff80001024fd14 <__slab_free+0x2ac>  // b.any
         :                      page_to_nid():
         :                      #else
         :                      static inline int page_to_nid(const struct page *page)
         :                      {
         :                      struct page *p = (struct page *)page;
         :
         :                      return (PF_POISONED_CHECK(p)->flags >> NODES_PGSHIFT) & NODES_MASK;
    0.00 :   ffff80001024fb20:       ldr     x0, [x19]
         :                      get_node():
         :
         :                      };
         :
         :                      static inline struct kmem_cache_node *get_node(struct kmem_cache *s, int node)
         :                      {
         :                      return s->node[node];
    0.00 :   ffff80001024fb24:       lsr     x0, x0, #62
    0.00 :   ffff80001024fb28:       add     x0, x0, #0x30
    0.00 :   ffff80001024fb2c:       ldr     x26, [x21, x0, lsl #3]
         :                      __slab_free():
         :                      * drop the list_lock without any processing.
         :                      *
         :                      * Otherwise the list_lock will synchronize with
         :                      * other processors updating the list of slabs.
         :                      */
         :                      spin_lock_irqsave(&n->list_lock, flags);
    0.00 :   ffff80001024fb30:       mov     x0, x26
    0.00 :   ffff80001024fb34:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
    0.00 :   ffff80001024fb38:       str     x0, [x29, #104]
    0.00 :   ffff80001024fb3c:       ldr     w0, [x21, #8]
         :
         :                      }
         :                      }
         :
         :                      } while (!cmpxchg_double_slab(s, page,
    3.79 :   ffff80001024fb40:       ldr     x3, [x29, #168]
         :                      cmpxchg_double_slab():
         :                      if (s->flags & __CMPXCHG_DOUBLE) {
    0.00 :   ffff80001024fb44:       tbz     w0, #30, ffff80001024fb90 <__slab_free+0x128>
         :                      if (cmpxchg_double(&page->freelist, &page->counters,
   20.07 :   ffff80001024fb48:       add     x4, x19, #0x20
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff80001024fb4c:       b       ffff80001024fc0c <__slab_free+0x1a4>
    0.00 :   ffff80001024fb50:       b       ffff80001024fc0c <__slab_free+0x1a4>
         :                      __lse__cmpxchg_double_mb():
         :                      \
         :                      return x0;                                                      \
         :                      }
         :
         :                      __CMPXCHG_DBL(   ,   )
         :                      __CMPXCHG_DBL(_mb, al, "memory")
    0.00 :   ffff80001024fb54:       mov     x0, x25
    0.00 :   ffff80001024fb58:       mov     x1, x20
    0.96 :   ffff80001024fb5c:       ldr     x2, [x29, #120]
    2.32 :   ffff80001024fb60:       caspal  x0, x1, x2, x3, [x4]
   50.69 :   ffff80001024fb64:       eor     x0, x0, x25
    0.00 :   ffff80001024fb68:       eor     x1, x1, x20
    0.00 :   ffff80001024fb6c:       orr     x0, x0, x1
         :                      cmpxchg_double_slab():
    0.00 :   ffff80001024fb70:       cbz     x0, ffff80001024fc1c <__slab_free+0x1b4>
    0.00 :   ffff80001024fb74:       nop
         :                      cpu_relax():
         :
         :                      unsigned long get_wchan(struct task_struct *p);
         :
         :                      static inline void cpu_relax(void)
         :                      {
         :                      asm volatile("yield" ::: "memory");
    0.00 :   ffff80001024fb78:       yield
         :                      __slab_free():
         :                      if (unlikely(n)) {
    0.00 :   ffff80001024fb7c:       cbz     x26, ffff80001024fac8 <__slab_free+0x60>
         :                      spin_unlock_irqrestore():
         :                      raw_spin_unlock_irq(&lock->rlock);
         :                      }
         :
         :                      static __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
         :                      {
         :                      raw_spin_unlock_irqrestore(&lock->rlock, flags);
    0.00 :   ffff80001024fb80:       ldr     x1, [x29, #104]
    0.00 :   ffff80001024fb84:       mov     x0, x26
    0.00 :   ffff80001024fb88:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
    0.00 :   ffff80001024fb8c:       b       ffff80001024fac8 <__slab_free+0x60>
         :                      arch_local_save_flags():
         :                      */
         :                      static inline unsigned long arch_local_save_flags(void)
         :                      {
         :                      unsigned long flags;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001024fb90:       mrs     x0, daif
         :                      arch_irqs_disabled_flags():
         :
         :                      static inline int arch_irqs_disabled_flags(unsigned long flags)
         :                      {
         :                      int res;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001024fb94:       and     w1, w0, #0x80
         :                      arch_local_irq_save():
         :
         :                      /*
         :                      * There are too many states with IRQs disabled, just keep the current
         :                      * state if interrupts are already disabled/masked.
         :                      */
         :                      if (!arch_irqs_disabled_flags(flags))
    0.00 :   ffff80001024fb98:       cbnz    w1, ffff80001024fba4 <__slab_free+0x13c>
         :                      arch_local_irq_disable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001024fb9c:       mov     x1, #0x60                       // #96
    0.00 :   ffff80001024fba0:       msr     daifset, #0x2
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80001024fba4:       mrs     x2, sp_el0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001024fba8:       ldr     w1, [x2, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff80001024fbac:       add     w1, w1, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001024fbb0:       str     w1, [x2, #16]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001024fbb4:       ldr     x1, [x19]
         :                      test_and_set_bit_lock():
         :                      {
         :                      long old;
         :                      unsigned long mask = BIT_MASK(nr);
         :
         :                      p += BIT_WORD(nr);
         :                      if (READ_ONCE(*p) & mask)
    0.00 :   ffff80001024fbb8:       tbz     w1, #0, ffff80001024fc68 <__slab_free+0x200>
         :                      get_current():
    0.00 :   ffff80001024fbbc:       mrs     x2, sp_el0
         :                      __read_once_size():
    0.00 :   ffff80001024fbc0:       ldr     x1, [x2, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff80001024fbc4:       sub     x1, x1, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001024fbc8:       str     w1, [x2, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff80001024fbcc:       cbz     x1, ffff80001024fbd8 <__slab_free+0x170>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001024fbd0:       ldr     x1, [x2, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff80001024fbd4:       cbnz    x1, ffff80001024fbf0 <__slab_free+0x188>
    0.00 :   ffff80001024fbd8:       str     x0, [x29, #96]
    0.00 :   ffff80001024fbdc:       str     x3, [x29, #112]
         :                      bit_spin_lock():
         :                      * attempt to acquire the lock bit.
         :                      */
         :                      preempt_disable();
         :                      #if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
         :                      while (unlikely(test_and_set_bit_lock(bitnum, addr))) {
         :                      preempt_enable();
    0.00 :   ffff80001024fbe0:       bl      ffff800010cad640 <preempt_schedule>
    0.00 :   ffff80001024fbe4:       ldr     x0, [x29, #96]
    0.00 :   ffff80001024fbe8:       ldr     x3, [x29, #112]
    0.00 :   ffff80001024fbec:       nop
         :                      cpu_relax():
    0.00 :   ffff80001024fbf0:       yield
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff80001024fbf4:       ldr     x1, [x19]
         :                      bit_spin_lock():
         :                      do {
         :                      cpu_relax();
         :                      } while (test_bit(bitnum, addr));
    0.00 :   ffff80001024fbf8:       tbz     w1, #0, ffff80001024fba4 <__slab_free+0x13c>
         :                      cpu_relax():
    0.00 :   ffff80001024fbfc:       yield
         :                      test_bit():
    0.00 :   ffff80001024fc00:       ldr     x1, [x19]
         :                      bit_spin_lock():
    0.00 :   ffff80001024fc04:       tbnz    w1, #0, ffff80001024fbf0 <__slab_free+0x188>
    0.00 :   ffff80001024fc08:       b       ffff80001024fba4 <__slab_free+0x13c>
         :                      __ll_sc__cmpxchg_double_mb():
         :                      \
         :                      return ret;                                                     \
         :                      }
         :
         :                      __CMPXCHG_DBL(   ,        ,  ,         )
         :                      __CMPXCHG_DBL(_mb, dmb ish, l, "memory")
    0.00 :   ffff80001024fc0c:       add     x2, x19, #0x20
    0.00 :   ffff80001024fc10:       ldr     x4, [x29, #120]
    0.00 :   ffff80001024fc14:       b       ffff800010254344 <slabinfo_write+0x254>
         :                      cmpxchg_double_slab():
         :                      if (cmpxchg_double(&page->freelist, &page->counters,
    0.00 :   ffff80001024fc18:       cbnz    x0, ffff80001024fb78 <__slab_free+0x110>
         :                      __slab_free():
         :                      prior, counters,
         :                      head, new.counters,
         :                      "__slab_free"));
         :
         :                      if (likely(!n)) {
    0.01 :   ffff80001024fc1c:       cbnz    x26, ffff80001024fd58 <__slab_free+0x2f0>
         :
         :                      /*
         :                      * If we just froze the page then put it onto the
         :                      * per cpu partial list.
         :                      */
         :                      if (new.frozen && !was_frozen) {
    5.39 :   ffff80001024fc20:       ldrb    w0, [x29, #171]
    0.00 :   ffff80001024fc24:       eor     w10, w28, #0x1
    0.00 :   ffff80001024fc28:       tst     w10, w0, lsr #7
    0.00 :   ffff80001024fc2c:       b.ne    ffff80001024fd2c <__slab_free+0x2c4>  // b.any
    1.59 :   ffff80001024fc30:       ldr     x20, [x29, #24]
    0.60 :   ffff80001024fc34:       ldp     x25, x26, [x29, #64]
    1.45 :   ffff80001024fc38:       ldp     x27, x28, [x29, #80]
         :                      }
         :
         :                      spin_unlock_irqrestore(&n->list_lock, flags);
         :                      stat(s, FREE_SLAB);
         :                      discard_slab(s, page);
         :                      }
    0.00 :   ffff80001024fc3c:       adrp    x0, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff80001024fc40:       add     x24, x0, #0x8c8
    0.10 :   ffff80001024fc44:       ldr     x1, [x29, #200]
    0.09 :   ffff80001024fc48:       ldr     x0, [x24]
    0.00 :   ffff80001024fc4c:       eor     x0, x1, x0
    0.12 :   ffff80001024fc50:       cbnz    x0, ffff80001024fdf8 <__slab_free+0x390>
    0.21 :   ffff80001024fc54:       ldr     x19, [sp, #16]
    1.47 :   ffff80001024fc58:       ldp     x21, x22, [sp, #32]
    0.07 :   ffff80001024fc5c:       ldp     x23, x24, [sp, #48]
    0.05 :   ffff80001024fc60:       ldp     x29, x30, [sp], #208
    0.00 :   ffff80001024fc64:       ret
         :                      arch_static_branch_jump():
    0.00 :   ffff80001024fc68:       b       ffff80001024fd0c <__slab_free+0x2a4>
    0.00 :   ffff80001024fc6c:       b       ffff80001024fd0c <__slab_free+0x2a4>
         :                      __lse_atomic64_fetch_or_acquire():
         :                      ATOMIC64_FETCH_OPS(or, ldset)
    0.00 :   ffff80001024fc70:       mov     x1, #0x1                        // #1
    0.00 :   ffff80001024fc74:       ldseta  x1, x1, [x19]
         :                      bit_spin_lock():
         :                      while (unlikely(test_and_set_bit_lock(bitnum, addr))) {
    0.00 :   ffff80001024fc78:       tbnz    w1, #0, ffff80001024fbbc <__slab_free+0x154>
         :                      cmpxchg_double_slab():
         :                      if (page->freelist == freelist_old &&
    0.00 :   ffff80001024fc7c:       ldr     x1, [x19, #32]
    0.00 :   ffff80001024fc80:       cmp     x25, x1
    0.00 :   ffff80001024fc84:       b.eq    ffff80001024fcc4 <__slab_free+0x25c>  // b.none
         :                      __read_once_size():
    0.00 :   ffff80001024fc88:       ldr     x1, [x19]
         :                      __clear_bit_unlock():
         :                      {
         :                      unsigned long old;
         :
         :                      p += BIT_WORD(nr);
         :                      old = READ_ONCE(*p);
         :                      old &= ~BIT_MASK(nr);
    0.00 :   ffff80001024fc8c:       and     x1, x1, #0xfffffffffffffffe
         :                      atomic64_set_release():
         :
         :                      #ifndef atomic64_set_release
         :                      static inline void
         :                      atomic64_set_release(atomic64_t *v, s64 i)
         :                      {
         :                      smp_store_release(&(v)->counter, i);
    0.00 :   ffff80001024fc90:       stlr    x1, [x19]
         :                      get_current():
    0.00 :   ffff80001024fc94:       mrs     x2, sp_el0
         :                      __read_once_size():
    0.00 :   ffff80001024fc98:       ldr     x1, [x2, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff80001024fc9c:       sub     x1, x1, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001024fca0:       str     w1, [x2, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff80001024fca4:       cbz     x1, ffff80001024fcb0 <__slab_free+0x248>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001024fca8:       ldr     x1, [x2, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff80001024fcac:       cbnz    x1, ffff80001024fcbc <__slab_free+0x254>
    0.00 :   ffff80001024fcb0:       str     x0, [x29, #112]
         :                      __bit_spin_unlock():
         :                      BUG_ON(!test_bit(bitnum, addr));
         :                      #endif
         :                      #if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
         :                      __clear_bit_unlock(bitnum, addr);
         :                      #endif
         :                      preempt_enable();
    0.00 :   ffff80001024fcb4:       bl      ffff800010cad640 <preempt_schedule>
    0.00 :   ffff80001024fcb8:       ldr     x0, [x29, #112]
         :                      arch_local_irq_restore():
         :                      /*
         :                      * restore saved IRQ state
         :                      */
         :                      static inline void arch_local_irq_restore(unsigned long flags)
         :                      {
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001024fcbc:       msr     daif, x0
    0.00 :   ffff80001024fcc0:       b       ffff80001024fb78 <__slab_free+0x110>
         :                      cmpxchg_double_slab():
    0.00 :   ffff80001024fcc4:       ldr     x1, [x19, #40]
    0.00 :   ffff80001024fcc8:       cmp     x20, x1
    0.00 :   ffff80001024fccc:       b.ne    ffff80001024fc88 <__slab_free+0x220>  // b.any
         :                      __read_once_size():
    0.00 :   ffff80001024fcd0:       ldr     x1, [x19]
         :                      cmpxchg_double_slab():
         :                      page->freelist = freelist_new;
    0.00 :   ffff80001024fcd4:       ldr     x2, [x29, #120]
         :                      __clear_bit_unlock():
    0.00 :   ffff80001024fcd8:       and     x1, x1, #0xfffffffffffffffe
         :                      cmpxchg_double_slab():
         :                      page->counters = counters_new;
    0.00 :   ffff80001024fcdc:       stp     x2, x3, [x19, #32]
         :                      atomic64_set_release():
    0.00 :   ffff80001024fce0:       stlr    x1, [x19]
         :                      get_current():
    0.00 :   ffff80001024fce4:       mrs     x2, sp_el0
         :                      __read_once_size():
    0.00 :   ffff80001024fce8:       ldr     x1, [x2, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff80001024fcec:       sub     x1, x1, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001024fcf0:       str     w1, [x2, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff80001024fcf4:       cbnz    x1, ffff80001024fd20 <__slab_free+0x2b8>
    0.00 :   ffff80001024fcf8:       str     x0, [x29, #120]
         :                      __bit_spin_unlock():
    0.00 :   ffff80001024fcfc:       bl      ffff800010cad640 <preempt_schedule>
    0.00 :   ffff80001024fd00:       ldr     x0, [x29, #120]
         :                      cmpxchg_double_slab():
         :                      local_irq_restore(flags);
    0.00 :   ffff80001024fd04:       bl      ffff80001024af90 <arch_local_irq_restore>
    0.00 :   ffff80001024fd08:       b       ffff80001024fc1c <__slab_free+0x1b4>
         :                      __ll_sc_atomic64_fetch_or_acquire():
         :                      ATOMIC64_OPS(or, orr, L)
    0.00 :   ffff80001024fd0c:       b       ffff80001025436c <slabinfo_write+0x27c>
    0.00 :   ffff80001024fd10:       b       ffff80001024fc78 <__slab_free+0x210>
         :                      __slab_free():
         :                      new.frozen = 1;
    0.00 :   ffff80001024fd14:       orr     w1, w1, #0xffffff80
    0.00 :   ffff80001024fd18:       strb    w1, [x29, #171]
    0.00 :   ffff80001024fd1c:       b       ffff80001024fb40 <__slab_free+0xd8>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001024fd20:       ldr     x1, [x2, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff80001024fd24:       cbz     x1, ffff80001024fcf8 <__slab_free+0x290>
    0.00 :   ffff80001024fd28:       b       ffff80001024fd04 <__slab_free+0x29c>
         :                      __slab_free():
         :                      put_cpu_partial(s, page, 1);
    0.00 :   ffff80001024fd2c:       mov     w2, #0x1                        // #1
    0.00 :   ffff80001024fd30:       mov     x1, x19
    0.00 :   ffff80001024fd34:       mov     x0, x21
    0.00 :   ffff80001024fd38:       bl      ffff80001024ee38 <put_cpu_partial>
    0.00 :   ffff80001024fd3c:       ldr     x20, [x29, #24]
    0.00 :   ffff80001024fd40:       ldp     x25, x26, [x29, #64]
    0.00 :   ffff80001024fd44:       ldp     x27, x28, [x29, #80]
    0.00 :   ffff80001024fd48:       b       ffff80001024fc3c <__slab_free+0x1d4>
         :                      !free_debug_processing(s, page, head, tail, cnt, addr))
    0.00 :   ffff80001024fd4c:       bl      ffff80001024f660 <free_debug_processing>
         :                      if (kmem_cache_debug(s) &&
    0.00 :   ffff80001024fd50:       cbz     w0, ffff80001024fc3c <__slab_free+0x1d4>
    0.00 :   ffff80001024fd54:       b       ffff80001024fab8 <__slab_free+0x50>
         :                      if (unlikely(!new.inuse && n->nr_partial >= s->min_partial))
    0.00 :   ffff80001024fd58:       cbz     w27, ffff80001024fd90 <__slab_free+0x328>
         :                      kmem_cache_has_cpu_partial():
         :                      return !kmem_cache_debug(s);
    0.00 :   ffff80001024fd5c:       ldr     w0, [x21, #8]
         :                      kmem_cache_debug():
         :                      return unlikely(s->flags & SLAB_DEBUG_FLAGS);
    0.00 :   ffff80001024fd60:       mov     w1, #0xd00                      // #3328
    0.00 :   ffff80001024fd64:       movk    w1, #0x21, lsl #16
         :                      __slab_free():
         :                      if (!kmem_cache_has_cpu_partial(s) && unlikely(!prior)) {
    0.00 :   ffff80001024fd68:       tst     w0, w1
    0.00 :   ffff80001024fd6c:       ccmp    w24, #0x0, #0x4, ne  // ne = any
    0.00 :   ffff80001024fd70:       b.ne    ffff80001024fe08 <__slab_free+0x3a0>  // b.any
         :                      spin_unlock_irqrestore():
    0.00 :   ffff80001024fd74:       ldr     x1, [x29, #104]
    0.00 :   ffff80001024fd78:       mov     x0, x26
    0.00 :   ffff80001024fd7c:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
    0.00 :   ffff80001024fd80:       ldr     x20, [x29, #24]
    0.00 :   ffff80001024fd84:       ldp     x25, x26, [x29, #64]
    0.00 :   ffff80001024fd88:       ldp     x27, x28, [x29, #80]
    0.00 :   ffff80001024fd8c:       b       ffff80001024fc3c <__slab_free+0x1d4>
         :                      __slab_free():
         :                      if (unlikely(!new.inuse && n->nr_partial >= s->min_partial))
    0.00 :   ffff80001024fd90:       ldr     x0, [x21, #16]
    0.00 :   ffff80001024fd94:       ldr     x1, [x26, #8]
    0.00 :   ffff80001024fd98:       cmp     x1, x0
    0.00 :   ffff80001024fd9c:       b.cc    ffff80001024fd5c <__slab_free+0x2f4>  // b.lo, b.ul, b.last
         :                      if (prior) {
    0.00 :   ffff80001024fda0:       cbz     x25, ffff80001024fe34 <__slab_free+0x3cc>
         :                      __list_del_entry():
         :                      static inline void __list_del_entry(struct list_head *entry)
         :                      {
         :                      if (!__list_del_entry_valid(entry))
         :                      return;
         :
         :                      __list_del(entry->prev, entry->next);
    0.00 :   ffff80001024fda4:       ldp     x1, x0, [x19, #8]
         :                      __list_del():
         :                      next->prev = prev;
    0.00 :   ffff80001024fda8:       str     x0, [x1, #8]
         :                      list_del():
         :                      }
         :
         :                      static inline void list_del(struct list_head *entry)
         :                      {
         :                      __list_del_entry(entry);
         :                      entry->next = LIST_POISON1;
    0.00 :   ffff80001024fdac:       mov     x3, #0x100                      // #256
         :                      entry->prev = LIST_POISON2;
    0.00 :   ffff80001024fdb0:       mov     x2, #0x122                      // #290
         :                      entry->next = LIST_POISON1;
    0.00 :   ffff80001024fdb4:       movk    x3, #0xdead, lsl #48
         :                      entry->prev = LIST_POISON2;
    0.00 :   ffff80001024fdb8:       movk    x2, #0xdead, lsl #48
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff80001024fdbc:       str     x1, [x0]
         :                      list_del():
    0.00 :   ffff80001024fdc0:       stp     x3, x2, [x19, #8]
         :                      remove_partial():
         :                      n->nr_partial--;
    0.00 :   ffff80001024fdc4:       ldr     x0, [x26, #8]
    0.00 :   ffff80001024fdc8:       sub     x0, x0, #0x1
    0.00 :   ffff80001024fdcc:       str     x0, [x26, #8]
         :                      spin_unlock_irqrestore():
    0.00 :   ffff80001024fdd0:       ldr     x1, [x29, #104]
    0.00 :   ffff80001024fdd4:       mov     x0, x26
    0.00 :   ffff80001024fdd8:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      __slab_free():
         :                      discard_slab(s, page);
    0.00 :   ffff80001024fddc:       mov     x1, x19
    0.00 :   ffff80001024fde0:       mov     x0, x21
    0.00 :   ffff80001024fde4:       bl      ffff80001024e328 <discard_slab>
    0.00 :   ffff80001024fde8:       ldr     x20, [x29, #24]
    0.00 :   ffff80001024fdec:       ldp     x25, x26, [x29, #64]
    0.00 :   ffff80001024fdf0:       ldp     x27, x28, [x29, #80]
    0.00 :   ffff80001024fdf4:       b       ffff80001024fc3c <__slab_free+0x1d4>
    0.00 :   ffff80001024fdf8:       str     x20, [x29, #24]
    0.00 :   ffff80001024fdfc:       stp     x25, x26, [x29, #64]
    0.00 :   ffff80001024fe00:       stp     x27, x28, [x29, #80]
         :                      }
    0.00 :   ffff80001024fe04:       bl      ffff8000100e5630 <__stack_chk_fail>
         :                      remove_full():
         :                      if (!(s->flags & SLAB_STORE_USER))
    0.00 :   ffff80001024fe08:       tbnz    w0, #16, ffff80001024fe60 <__slab_free+0x3f8>
         :                      __add_partial():
         :                      n->nr_partial++;
    0.00 :   ffff80001024fe0c:       ldr     x0, [x26, #8]
         :                      list_add_tail(&page->slab_list, &n->partial);
    0.00 :   ffff80001024fe10:       add     x2, x19, #0x8
         :                      list_add_tail():
         :                      __list_add(new, head->prev, head);
    0.00 :   ffff80001024fe14:       ldr     x1, [x26, #24]
         :                      __add_partial():
         :                      n->nr_partial++;
    0.00 :   ffff80001024fe18:       add     x0, x0, #0x1
    0.00 :   ffff80001024fe1c:       str     x0, [x26, #8]
         :                      __list_add():
         :                      next->prev = new;
    0.00 :   ffff80001024fe20:       str     x2, [x26, #24]
         :                      __add_partial():
         :                      list_add_tail(&page->slab_list, &n->partial);
    0.00 :   ffff80001024fe24:       add     x0, x26, #0x10
         :                      __list_add():
         :                      new->prev = prev;
    0.00 :   ffff80001024fe28:       stp     x0, x1, [x19, #8]
         :                      __write_once_size():
    0.00 :   ffff80001024fe2c:       str     x2, [x1]
    0.00 :   ffff80001024fe30:       b       ffff80001024fd74 <__slab_free+0x30c>
         :                      remove_full():
         :                      if (!(s->flags & SLAB_STORE_USER))
    0.00 :   ffff80001024fe34:       ldr     w0, [x21, #8]
    0.00 :   ffff80001024fe38:       tbz     w0, #16, ffff80001024fdd0 <__slab_free+0x368>
         :                      __list_del_entry():
         :                      __list_del(entry->prev, entry->next);
    0.00 :   ffff80001024fe3c:       ldp     x1, x0, [x19, #8]
         :                      __list_del():
         :                      next->prev = prev;
    0.00 :   ffff80001024fe40:       str     x0, [x1, #8]
         :                      list_del():
         :                      entry->next = LIST_POISON1;
    0.00 :   ffff80001024fe44:       mov     x3, #0x100                      // #256
         :                      entry->prev = LIST_POISON2;
    0.00 :   ffff80001024fe48:       mov     x2, #0x122                      // #290
         :                      entry->next = LIST_POISON1;
    0.00 :   ffff80001024fe4c:       movk    x3, #0xdead, lsl #48
         :                      entry->prev = LIST_POISON2;
    0.00 :   ffff80001024fe50:       movk    x2, #0xdead, lsl #48
         :                      __write_once_size():
    0.00 :   ffff80001024fe54:       str     x1, [x0]
         :                      list_del():
    0.00 :   ffff80001024fe58:       stp     x3, x2, [x19, #8]
    0.00 :   ffff80001024fe5c:       b       ffff80001024fdd0 <__slab_free+0x368>
         :                      __list_del_entry():
         :                      __list_del(entry->prev, entry->next);
    0.00 :   ffff80001024fe60:       ldp     x1, x0, [x19, #8]
         :                      __list_del():
         :                      next->prev = prev;
    0.00 :   ffff80001024fe64:       str     x0, [x1, #8]
         :                      list_del():
         :                      entry->next = LIST_POISON1;
    0.00 :   ffff80001024fe68:       mov     x3, #0x100                      // #256
         :                      entry->prev = LIST_POISON2;
    0.00 :   ffff80001024fe6c:       mov     x2, #0x122                      // #290
         :                      entry->next = LIST_POISON1;
    0.00 :   ffff80001024fe70:       movk    x3, #0xdead, lsl #48
         :                      entry->prev = LIST_POISON2;
    0.00 :   ffff80001024fe74:       movk    x2, #0xdead, lsl #48
         :                      __write_once_size():
    0.00 :   ffff80001024fe78:       str     x1, [x0]
         :                      list_del():
    0.00 :   ffff80001024fe7c:       stp     x3, x2, [x19, #8]
    0.00 :   ffff80001024fe80:       b       ffff80001024fe0c <__slab_free+0x3a4>
 Percent |	Source code & Disassembly of vmlinux for cycles (20083 samples, percent: local period)
------------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001045e6c8 <blk_mq_free_request>:
         :                      blk_mq_free_request():
         :                      blk_mq_sched_restart(hctx);
         :                      blk_queue_exit(q);
         :                      }
         :
         :                      void blk_mq_free_request(struct request *rq)
         :                      {
    0.65 :   ffff80001045e6c8:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff80001045e6cc:       mov     x29, sp
    0.08 :   ffff80001045e6d0:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001045e6d4:       mov     x19, x0
    0.02 :   ffff80001045e6d8:       stp     x21, x22, [sp, #32]
         :                      struct request_queue *q = rq->q;
         :                      struct elevator_queue *e = q->elevator;
         :                      struct blk_mq_ctx *ctx = rq->mq_ctx;
         :                      struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
         :
         :                      if (rq->rq_flags & RQF_ELVPRIV) {
    0.05 :   ffff80001045e6dc:       ldr     w1, [x0, #28]
         :                      struct blk_mq_ctx *ctx = rq->mq_ctx;
    0.76 :   ffff80001045e6e0:       ldp     x21, x22, [x0]
         :                      struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
    0.02 :   ffff80001045e6e4:       ldr     x20, [x0, #16]
         :                      if (rq->rq_flags & RQF_ELVPRIV) {
    0.00 :   ffff80001045e6e8:       tbz     w1, #12, ffff80001045e718 <blk_mq_free_request+0x50>
         :                      struct elevator_queue *e = q->elevator;
    0.00 :   ffff80001045e6ec:       ldr     x1, [x21, #8]
         :                      if (e && e->type->ops.finish_request)
    0.00 :   ffff80001045e6f0:       cbz     x1, ffff80001045e704 <blk_mq_free_request+0x3c>
    0.00 :   ffff80001045e6f4:       ldr     x1, [x1]
    0.00 :   ffff80001045e6f8:       ldr     x1, [x1, #104]
    0.00 :   ffff80001045e6fc:       cbz     x1, ffff80001045e704 <blk_mq_free_request+0x3c>
         :                      e->type->ops.finish_request(rq);
    0.00 :   ffff80001045e700:       blr     x1
         :                      if (rq->elv.icq) {
    0.00 :   ffff80001045e704:       ldr     x0, [x19, #128]
    0.00 :   ffff80001045e708:       cbz     x0, ffff80001045e718 <blk_mq_free_request+0x50>
         :                      put_io_context(rq->elv.icq->ioc);
    0.00 :   ffff80001045e70c:       ldr     x0, [x0, #8]
    0.00 :   ffff80001045e710:       bl      ffff800010459408 <put_io_context>
         :                      rq->elv.icq = NULL;
    0.00 :   ffff80001045e714:       str     xzr, [x19, #128]
         :                      }
         :                      }
         :
         :                      ctx->rq_completed[rq_is_sync(rq)]++;
    0.13 :   ffff80001045e718:       ldr     w1, [x19, #24]
         :                      op_is_sync():
         :                      * PREFLUSH flag.  Other operations may be marked as synchronous using the
         :                      * REQ_SYNC flag.
         :                      */
         :                      static inline bool op_is_sync(unsigned int op)
         :                      {
         :                      return (op & REQ_OP_MASK) == REQ_OP_READ ||
    0.00 :   ffff80001045e71c:       mov     w0, #0x1                        // #1
    0.00 :   ffff80001045e720:       and     w2, w1, #0xff
    0.00 :   ffff80001045e724:       cbz     w2, ffff80001045e738 <blk_mq_free_request+0x70>
    0.00 :   ffff80001045e728:       and     w1, w1, #0x7f800
    0.00 :   ffff80001045e72c:       and     w1, w1, #0xfffe0fff
    0.00 :   ffff80001045e730:       cmp     w1, #0x0
    0.00 :   ffff80001045e734:       cset    w0, ne  // ne = any
    0.05 :   ffff80001045e738:       sxtw    x0, w0
    0.00 :   ffff80001045e73c:       add     x0, x22, x0, lsl #3
         :                      blk_mq_free_request():
    0.79 :   ffff80001045e740:       ldr     x1, [x0, #128]
    0.01 :   ffff80001045e744:       add     x1, x1, #0x1
    7.75 :   ffff80001045e748:       str     x1, [x0, #128]
         :                      if (rq->rq_flags & RQF_MQ_INFLIGHT)
    0.08 :   ffff80001045e74c:       ldr     w0, [x19, #28]
    0.00 :   ffff80001045e750:       tbnz    w0, #6, ffff80001045e7dc <blk_mq_free_request+0x114>
         :                      atomic_dec(&hctx->nr_active);
         :
         :                      if (unlikely(laptop_mode && !blk_rq_is_passthrough(rq)))
    0.02 :   ffff80001045e754:       adrp    x0, ffff800011aaa000 <pmus_srcu+0x18>
    0.42 :   ffff80001045e758:       ldr     w0, [x0, #2452]
    0.00 :   ffff80001045e75c:       cbnz    w0, ffff80001045e800 <blk_mq_free_request+0x138>
         :                      laptop_io_completion(q->backing_dev_info);
         :
         :                      rq_qos_done(q, rq);
    0.40 :   ffff80001045e760:       ldr     x0, [x21, #24]
         :                      rq_qos_done():
         :                      __rq_qos_cleanup(q->rq_qos, bio);
         :                      }
         :
         :                      static inline void rq_qos_done(struct request_queue *q, struct request *rq)
         :                      {
         :                      if (q->rq_qos)
    0.00 :   ffff80001045e764:       cbz     x0, ffff80001045e770 <blk_mq_free_request+0xa8>
         :                      __rq_qos_done(q->rq_qos, rq);
    0.00 :   ffff80001045e768:       mov     x1, x19
    0.00 :   ffff80001045e76c:       bl      ffff80001046f040 <__rq_qos_done>
         :                      blk_mq_free_request():
         :
         :                      WRITE_ONCE(rq->state, MQ_RQ_IDLE);
         :                      if (refcount_dec_and_test(&rq->ref))
    0.02 :   ffff80001045e770:       add     x0, x19, #0xd4
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.01 :   ffff80001045e774:       str     wzr, [x19, #208]
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff80001045e778:       b       ffff80001045e7b0 <blk_mq_free_request+0xe8>
    0.42 :   ffff80001045e77c:       b       ffff80001045e7b0 <blk_mq_free_request+0xe8>
         :                      __lse_atomic_fetch_sub_release():
         :                      return i;                                                       \
         :                      }
         :
         :                      ATOMIC_FETCH_OP_SUB(_relaxed,   )
         :                      ATOMIC_FETCH_OP_SUB(_acquire,  a, "memory")
         :                      ATOMIC_FETCH_OP_SUB(_release,  l, "memory")
    0.28 :   ffff80001045e780:       mov     w1, #0x1                        // #1
    0.00 :   ffff80001045e784:       neg     w1, w1
    0.05 :   ffff80001045e788:       ldaddl  w1, w1, [x0]
         :                      refcount_sub_and_test():
         :                      */
         :                      static inline __must_check bool refcount_sub_and_test(int i, refcount_t *r)
         :                      {
         :                      int old = atomic_fetch_sub_release(i, &r->refs);
         :
         :                      if (old == i) {
   79.42 :   ffff80001045e78c:       cmp     w1, #0x1
    0.00 :   ffff80001045e790:       b.ne    ffff80001045e7c4 <blk_mq_free_request+0xfc>  // b.any
         :                      smp_acquire__after_ctrl_dep();
    0.02 :   ffff80001045e794:       dmb     ishld
         :                      blk_mq_free_request():
         :                      __blk_mq_free_request(rq);
    7.83 :   ffff80001045e798:       mov     x0, x19
    0.00 :   ffff80001045e79c:       bl      ffff80001045d810 <__blk_mq_free_request>
         :                      }
    0.58 :   ffff80001045e7a0:       ldp     x19, x20, [sp, #16]
    0.12 :   ffff80001045e7a4:       ldp     x21, x22, [sp, #32]
    0.02 :   ffff80001045e7a8:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001045e7ac:       ret
         :                      __ll_sc_atomic_fetch_sub_release():
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff80001045e7b0:       mov     w2, #0x1                        // #1
    0.00 :   ffff80001045e7b4:       add     x5, x19, #0xd4
    0.00 :   ffff80001045e7b8:       b       ffff800010463388 <blk_mq_update_nr_requests+0x1b0>
         :                      refcount_sub_and_test():
         :                      if (old == i) {
    0.00 :   ffff80001045e7bc:       cmp     w1, #0x1
    0.00 :   ffff80001045e7c0:       b.eq    ffff80001045e794 <blk_mq_free_request+0xcc>  // b.none
         :                      return true;
         :                      }
         :
         :                      if (unlikely(old < 0 || old - i < 0))
    0.00 :   ffff80001045e7c4:       cmp     w1, #0x0
    0.00 :   ffff80001045e7c8:       b.le    ffff80001045e82c <blk_mq_free_request+0x164>
         :                      blk_mq_free_request():
    0.00 :   ffff80001045e7cc:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001045e7d0:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff80001045e7d4:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001045e7d8:       ret
         :                      atomic_dec(&hctx->nr_active);
    0.00 :   ffff80001045e7dc:       add     x1, x20, #0x1b0
         :                      arch_static_branch_jump():
    0.00 :   ffff80001045e7e0:       b       ffff80001045e81c <blk_mq_free_request+0x154>
    0.00 :   ffff80001045e7e4:       b       ffff80001045e81c <blk_mq_free_request+0x154>
         :                      __lse_atomic_sub():
         :                      asm volatile(
    0.00 :   ffff80001045e7e8:       mov     w0, #0x1                        // #1
    0.00 :   ffff80001045e7ec:       neg     w0, w0
    0.00 :   ffff80001045e7f0:       stadd   w0, [x1]
         :                      blk_mq_free_request():
         :                      if (unlikely(laptop_mode && !blk_rq_is_passthrough(rq)))
    0.00 :   ffff80001045e7f4:       adrp    x0, ffff800011aaa000 <pmus_srcu+0x18>
    0.00 :   ffff80001045e7f8:       ldr     w0, [x0, #2452]
    0.00 :   ffff80001045e7fc:       cbz     w0, ffff80001045e760 <blk_mq_free_request+0x98>
         :                      blk_rq_is_scsi():
         :                      return op == REQ_OP_DRV_IN || op == REQ_OP_DRV_OUT;
         :                      }
         :
         :                      static inline bool blk_rq_is_scsi(struct request *rq)
         :                      {
         :                      return blk_op_is_scsi(req_op(rq));
    0.00 :   ffff80001045e800:       ldrb    w0, [x19, #24]
         :                      blk_rq_is_passthrough():
         :                      return blk_op_is_private(req_op(rq));
         :                      }
         :
         :                      static inline bool blk_rq_is_passthrough(struct request *rq)
         :                      {
         :                      return blk_rq_is_scsi(rq) || blk_rq_is_private(rq);
    0.00 :   ffff80001045e804:       sub     w0, w0, #0x20
    0.00 :   ffff80001045e808:       cmp     w0, #0x3
    0.00 :   ffff80001045e80c:       b.ls    ffff80001045e760 <blk_mq_free_request+0x98>  // b.plast
         :                      blk_mq_free_request():
         :                      laptop_io_completion(q->backing_dev_info);
    0.00 :   ffff80001045e810:       ldr     x0, [x21, #88]
    0.00 :   ffff80001045e814:       bl      ffff8000101dc248 <laptop_io_completion>
    0.00 :   ffff80001045e818:       b       ffff80001045e760 <blk_mq_free_request+0x98>
         :                      __ll_sc_atomic_sub():
    0.00 :   ffff80001045e81c:       mov     w0, #0x1                        // #1
    0.00 :   ffff80001045e820:       add     x3, x20, #0x1b0
    0.00 :   ffff80001045e824:       b       ffff8000104633a0 <blk_mq_update_nr_requests+0x1c8>
    0.00 :   ffff80001045e828:       b       ffff80001045e754 <blk_mq_free_request+0x8c>
         :                      refcount_sub_and_test():
         :                      refcount_warn_saturate(r, REFCOUNT_SUB_UAF);
    0.00 :   ffff80001045e82c:       mov     w1, #0x3                        // #3
    0.00 :   ffff80001045e830:       bl      ffff80001048ba28 <refcount_warn_saturate>
         :                      blk_mq_free_request():
         :                      }
    0.00 :   ffff80001045e834:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001045e838:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff80001045e83c:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001045e840:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (30361 samples, percent: local period)
------------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010cb2bf0 <_raw_spin_unlock_irq>:
         :                      _raw_spin_unlock_irq():
         :                      EXPORT_SYMBOL(_raw_spin_unlock_irqrestore);
         :                      #endif
         :
         :                      #ifndef CONFIG_INLINE_SPIN_UNLOCK_IRQ
         :                      void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)
         :                      {
    0.00 :   ffff800010cb2bf0:       stp     x29, x30, [sp, #-16]!
         :                      queued_spin_unlock():
         :                      static __always_inline void queued_spin_unlock(struct qspinlock *lock)
         :                      {
         :                      /*
         :                      * unlock() needs release semantics:
         :                      */
         :                      smp_store_release(&lock->locked, 0);
    0.00 :   ffff800010cb2bf4:       mov     w1, #0x0                        // #0
         :                      _raw_spin_unlock_irq():
    0.00 :   ffff800010cb2bf8:       mov     x29, sp
         :                      queued_spin_unlock():
    0.00 :   ffff800010cb2bfc:       stlrb   w1, [x0]
         :                      arch_local_irq_enable():
         :                      u32 pmr = read_sysreg_s(SYS_ICC_PMR_EL1);
         :
         :                      WARN_ON_ONCE(pmr != GIC_PRIO_IRQON && pmr != GIC_PRIO_IRQOFF);
         :                      }
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010cb2c00:       mov     x0, #0xe0                       // #224
    0.00 :   ffff800010cb2c04:       msr     daifclr, #0x2
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
   98.91 :   ffff800010cb2c08:       mrs     x1, sp_el0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.07 :   ffff800010cb2c0c:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800010cb2c10:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.65 :   ffff800010cb2c14:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800010cb2c18:       cbz     x0, ffff800010cb2c24 <_raw_spin_unlock_irq+0x34>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010cb2c1c:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.02 :   ffff800010cb2c20:       cbnz    x0, ffff800010cb2c28 <_raw_spin_unlock_irq+0x38>
         :                      __raw_spin_unlock_irq():
         :                      static inline void __raw_spin_unlock_irq(raw_spinlock_t *lock)
         :                      {
         :                      spin_release(&lock->dep_map, _RET_IP_);
         :                      do_raw_spin_unlock(lock);
         :                      local_irq_enable();
         :                      preempt_enable();
    0.00 :   ffff800010cb2c24:       bl      ffff800010cad640 <preempt_schedule>
         :                      _raw_spin_unlock_irq():
         :                      __raw_spin_unlock_irq(lock);
         :                      }
    0.35 :   ffff800010cb2c28:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010cb2c2c:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (15348 samples, percent: local period)
------------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001044f238 <bio_release_pages.part.42>:
         :                      bio_release_pages():
         :                      }
         :                      return len;
         :                      }
         :                      EXPORT_SYMBOL(bio_add_page);
         :
         :                      void bio_release_pages(struct bio *bio, bool mark_dirty)
    0.03 :   ffff80001044f238:       stp     x29, x30, [sp, #-80]!
    0.00 :   ffff80001044f23c:       mov     x29, sp
    0.24 :   ffff80001044f240:       stp     x23, x24, [sp, #48]
    0.00 :   ffff80001044f244:       mov     x24, x0
    0.95 :   ffff80001044f248:       str     x25, [sp, #64]
    0.00 :   ffff80001044f24c:       and     w25, w1, #0xff
    0.01 :   ffff80001044f250:       stp     x19, x20, [sp, #16]
         :                      bvec_init_iter_all():
         :                      }
         :
         :                      static inline struct bio_vec *bvec_init_iter_all(struct bvec_iter_all *iter_all)
         :                      {
         :                      iter_all->done = 0;
         :                      iter_all->idx = 0;
    0.00 :   ffff80001044f254:       mov     w23, #0x0                       // #0
         :                      bio_release_pages():
    0.05 :   ffff80001044f258:       stp     x21, x22, [sp, #32]
    0.00 :   ffff80001044f25c:       mov     w20, #0x1000                    // #4096
         :                      bvec_init_iter_all():
         :                      iter_all->done = 0;
    0.01 :   ffff80001044f260:       mov     w21, #0x0                       // #0
         :                      __ll_sc_atomic_sub_return():
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff80001044f264:       mov     w22, #0x1                       // #1
         :                      bio_next_segment():
         :                      }
         :
         :                      static inline bool bio_next_segment(const struct bio *bio,
         :                      struct bvec_iter_all *iter)
         :                      {
         :                      if (iter->idx >= bio->bi_vcnt)
    0.23 :   ffff80001044f268:       ldrh    w0, [x24, #96]
    0.00 :   ffff80001044f26c:       cmp     w0, w23
    0.00 :   ffff80001044f270:       b.le    ffff80001044f300 <bio_release_pages.part.42+0xc8>
         :                      return false;
         :
         :                      bvec_advance(&bio->bi_io_vec[iter->idx], iter);
    0.86 :   ffff80001044f274:       ldr     x4, [x24, #104]
    0.00 :   ffff80001044f278:       sbfiz   x0, x23, #4, #32
    0.00 :   ffff80001044f27c:       add     x3, x4, x0
         :                      bvec_advance():
         :                      static inline void bvec_advance(const struct bio_vec *bvec,
         :                      struct bvec_iter_all *iter_all)
         :                      {
         :                      struct bio_vec *bv = &iter_all->bv;
         :
         :                      if (iter_all->done) {
    0.03 :   ffff80001044f280:       cbz     w21, ffff80001044f334 <bio_release_pages.part.42+0xfc>
         :                      bv->bv_page++;
    0.00 :   ffff80001044f284:       add     x19, x19, #0x40
    0.00 :   ffff80001044f288:       mov     w2, #0x1000                     // #4096
         :                      bv->bv_offset = 0;
         :                      } else {
         :                      bv->bv_page = bvec->bv_page + (bvec->bv_offset >> PAGE_SHIFT);
         :                      bv->bv_offset = bvec->bv_offset & ~PAGE_MASK;
         :                      }
         :                      bv->bv_len = min_t(unsigned int, PAGE_SIZE - bv->bv_offset,
    0.65 :   ffff80001044f28c:       ldr     w3, [x3, #8]
    0.05 :   ffff80001044f290:       sub     w0, w3, w21
    0.01 :   ffff80001044f294:       cmp     w0, w2
    0.00 :   ffff80001044f298:       csel    w0, w0, w2, ls  // ls = plast
         :                      bvec->bv_len - iter_all->done);
         :                      iter_all->done += bv->bv_len;
    0.36 :   ffff80001044f29c:       add     w21, w21, w0
         :
         :                      if (iter_all->done == bvec->bv_len) {
    0.00 :   ffff80001044f2a0:       cmp     w3, w21
    0.00 :   ffff80001044f2a4:       b.eq    ffff80001044f318 <bio_release_pages.part.42+0xe0>  // b.none
         :                      bio_release_pages():
         :
         :                      if (bio_flagged(bio, BIO_NO_PAGE_REF))
         :                      return;
         :
         :                      bio_for_each_segment_all(bvec, bio, iter_all) {
         :                      if (mark_dirty && !PageCompound(bvec->bv_page))
    1.19 :   ffff80001044f2a8:       cbz     w25, ffff80001044f2c0 <bio_release_pages.part.42+0x88>
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff80001044f2ac:       ldr     x0, [x19]
         :                      PageCompound():
         :                      return READ_ONCE(page->compound_head) & 1;
         :                      }
         :
         :                      static __always_inline int PageCompound(struct page *page)
         :                      {
         :                      return test_bit(PG_head, &page->flags) || PageTail(page);
    0.00 :   ffff80001044f2b0:       tbnz    w0, #16, ffff80001044f2c0 <bio_release_pages.part.42+0x88>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001044f2b4:       ldr     x0, [x19, #8]
         :                      PageCompound():
    0.00 :   ffff80001044f2b8:       tbz     w0, #0, ffff80001044f350 <bio_release_pages.part.42+0x118>
    0.00 :   ffff80001044f2bc:       nop
         :                      __read_once_size():
    1.63 :   ffff80001044f2c0:       ldr     x2, [x19, #8]
         :                      compound_head():
         :                      return (struct page *) (head - 1);
    0.10 :   ffff80001044f2c4:       sub     x0, x2, #0x1
    0.02 :   ffff80001044f2c8:       tst     x2, #0x1
    0.01 :   ffff80001044f2cc:       csel    x0, x0, x19, ne  // ne = any
         :                      page_ref_dec_and_test():
         :                      return ret;
         :                      }
         :
         :                      static inline int page_ref_dec_and_test(struct page *page)
         :                      {
         :                      int ret = atomic_dec_and_test(&page->_refcount);
    2.71 :   ffff80001044f2d0:       add     x3, x0, #0x34
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.01 :   ffff80001044f2d4:       b       ffff80001044f324 <bio_release_pages.part.42+0xec>
    0.14 :   ffff80001044f2d8:       b       ffff80001044f324 <bio_release_pages.part.42+0xec>
         :                      __lse_atomic_sub_return():
         :                      }
         :
         :                      ATOMIC_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC_OP_SUB_RETURN(        , al, "memory")
    0.26 :   ffff80001044f2dc:       mov     w2, w22
    0.00 :   ffff80001044f2e0:       neg     w2, w2
    0.22 :   ffff80001044f2e4:       ldaddal w2, w1, [x3]
   84.24 :   ffff80001044f2e8:       add     w2, w2, w1
         :                      put_page():
         :                      * include/linux/memremap.h and HMM for details.
         :                      */
         :                      if (put_devmap_managed_page(page))
         :                      return;
         :
         :                      if (put_page_testzero(page))
    0.00 :   ffff80001044f2ec:       cbnz    w2, ffff80001044f268 <bio_release_pages.part.42+0x30>
         :                      __put_page(page);
    0.00 :   ffff80001044f2f0:       bl      ffff8000101de470 <__put_page>
         :                      bio_next_segment():
         :                      if (iter->idx >= bio->bi_vcnt)
    0.00 :   ffff80001044f2f4:       ldrh    w0, [x24, #96]
    0.00 :   ffff80001044f2f8:       cmp     w0, w23
    0.00 :   ffff80001044f2fc:       b.gt    ffff80001044f274 <bio_release_pages.part.42+0x3c>
         :                      bio_release_pages():
         :                      set_page_dirty_lock(bvec->bv_page);
         :                      put_page(bvec->bv_page);
         :                      }
         :                      }
    2.14 :   ffff80001044f300:       ldp     x19, x20, [sp, #16]
    0.02 :   ffff80001044f304:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff80001044f308:       ldp     x23, x24, [sp, #48]
    0.01 :   ffff80001044f30c:       ldr     x25, [sp, #64]
    1.15 :   ffff80001044f310:       ldp     x29, x30, [sp], #80
    0.00 :   ffff80001044f314:       ret
         :                      bvec_advance():
         :                      iter_all->idx++;
    1.02 :   ffff80001044f318:       add     w23, w23, #0x1
         :                      iter_all->done = 0;
    0.21 :   ffff80001044f31c:       mov     w21, #0x0                       // #0
    0.26 :   ffff80001044f320:       b       ffff80001044f2a8 <bio_release_pages.part.42+0x70>
         :                      __ll_sc_atomic_sub_return():
    0.00 :   ffff80001044f324:       add     x3, x0, #0x34
    0.00 :   ffff80001044f328:       b       ffff80001045208c <bio_associate_blkg_from_page+0x54>
         :                      put_page():
         :                      if (put_page_testzero(page))
    0.00 :   ffff80001044f32c:       cbnz    w2, ffff80001044f268 <bio_release_pages.part.42+0x30>
    0.00 :   ffff80001044f330:       b       ffff80001044f2f0 <bio_release_pages.part.42+0xb8>
         :                      bvec_advance():
         :                      bv->bv_page = bvec->bv_page + (bvec->bv_offset >> PAGE_SHIFT);
    0.02 :   ffff80001044f334:       ldr     w2, [x3, #12]
    0.29 :   ffff80001044f338:       ldr     x1, [x4, x0]
         :                      bv->bv_offset = bvec->bv_offset & ~PAGE_MASK;
    0.00 :   ffff80001044f33c:       and     w0, w2, #0xfff
         :                      bv->bv_page = bvec->bv_page + (bvec->bv_offset >> PAGE_SHIFT);
    0.01 :   ffff80001044f340:       lsr     w19, w2, #12
    0.00 :   ffff80001044f344:       sub     w2, w20, w0
    0.81 :   ffff80001044f348:       add     x19, x1, x19, lsl #6
    0.03 :   ffff80001044f34c:       b       ffff80001044f28c <bio_release_pages.part.42+0x54>
         :                      bio_release_pages():
         :                      set_page_dirty_lock(bvec->bv_page);
    0.00 :   ffff80001044f350:       mov     x0, x19
    0.00 :   ffff80001044f354:       bl      ffff8000101db068 <set_page_dirty_lock>
    0.00 :   ffff80001044f358:       b       ffff80001044f2c0 <bio_release_pages.part.42+0x88>
 Percent |	Source code & Disassembly of vmlinux for cycles (12478 samples, percent: local period)
------------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104556a8 <blk_account_io_done>:
         :                      blk_account_io_done():
         :                      /*
         :                      * Account IO completion.  flush_rq isn't accounted as a
         :                      * normal IO on queueing nor completion.  Accounting the
         :                      * containing request is enough.
         :                      */
         :                      if (req->part && blk_do_io_stat(req) &&
    0.06 :   ffff8000104556a8:       ldr     x2, [x0, #168]
    0.00 :   ffff8000104556ac:       cbz     x2, ffff8000104558f8 <blk_account_io_done+0x250>
         :                      blk_do_io_stat():
         :                      *       a) it's attached to a gendisk, and
         :                      *       b) the queue had IO stats enabled when this request was started
         :                      */
         :                      static inline bool blk_do_io_stat(struct request *rq)
         :                      {
         :                      return rq->rq_disk && (rq->rq_flags & RQF_IO_STAT);
    0.02 :   ffff8000104556b0:       ldr     x2, [x0, #160]
    0.00 :   ffff8000104556b4:       cbz     x2, ffff8000104558f8 <blk_account_io_done+0x250>
         :                      blk_account_io_done():
    0.02 :   ffff8000104556b8:       ldr     w2, [x0, #28]
         :                      blk_do_io_stat():
    0.00 :   ffff8000104556bc:       tbz     w2, #13, ffff8000104558f4 <blk_account_io_done+0x24c>
         :                      blk_account_io_done():
    1.12 :   ffff8000104556c0:       tbnz    w2, #4, ffff800010455950 <blk_account_io_done+0x2a8>
         :                      {
    0.01 :   ffff8000104556c4:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff8000104556c8:       mov     x29, sp
    0.01 :   ffff8000104556cc:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000104556d0:       mov     x19, x0
    0.01 :   ffff8000104556d4:       stp     x21, x22, [sp, #32]
    0.00 :   ffff8000104556d8:       mov     x21, x1
    1.18 :   ffff8000104556dc:       stp     x23, x24, [sp, #48]
         :                      op_is_write():
         :                      bio->bi_opf = op | op_flags;
         :                      }
         :
         :                      static inline bool op_is_write(unsigned int op)
         :                      {
         :                      return (op & 1);
    0.00 :   ffff8000104556e0:       mov     w22, #0x2                       // #2
         :                      blk_account_io_done():
         :                      !(req->rq_flags & RQF_FLUSH_SEQ)) {
         :                      const int sgrp = op_stat_group(req_op(req));
    0.01 :   ffff8000104556e4:       ldr     w0, [x0, #24]
    0.00 :   ffff8000104556e8:       and     w1, w0, #0xff
         :                      op_is_write():
    0.00 :   ffff8000104556ec:       and     w0, w0, #0x1
    0.00 :   ffff8000104556f0:       cmp     w1, #0x3
    0.04 :   ffff8000104556f4:       csel    w22, w0, w22, ne  // ne = any
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff8000104556f8:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.04 :   ffff8000104556fc:       mrs     x1, sp_el0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.02 :   ffff800010455700:       ldr     w0, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff800010455704:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.97 :   ffff800010455708:       str     w0, [x1, #16]
         :                      blk_account_io_done():
         :                      struct hd_struct *part;
         :
         :                      part_stat_lock();
         :                      part = req->part;
         :
         :                      update_io_ticks(part, jiffies);
    0.00 :   ffff80001045570c:       adrp    x0, ffff800011897000 <bit_wait_table+0xe80>
         :                      part = req->part;
    0.21 :   ffff800010455710:       ldr     x20, [x19, #168]
         :                      part_stat_inc(part, ios[sgrp]);
    0.00 :   ffff800010455714:       adrp    x23, ffff8000114ca000 <bp_hardening_data>
    0.00 :   ffff800010455718:       adrp    x24, ffff800011899000 <page_wait_table+0x1500>
         :                      update_io_ticks(part, jiffies);
    0.04 :   ffff80001045571c:       ldr     x1, [x0, #2432]
    0.00 :   ffff800010455720:       mov     x0, x20
    0.00 :   ffff800010455724:       bl      ffff800010451bf0 <update_io_ticks>
         :                      __my_cpu_offset():
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
         :                      "mrs %0, tpidr_el2",
         :                      ARM64_HAS_VIRT_HOST_EXTN)
         :                      : "=r" (off) :
         :                      "Q" (*(const unsigned long *)current_stack_pointer));
    0.61 :   ffff800010455728:       mov     x4, sp
         :                      blk_account_io_done():
         :                      part_stat_inc(part, ios[sgrp]);
    0.00 :   ffff80001045572c:       add     x0, x23, #0x18
    0.00 :   ffff800010455730:       add     x1, x24, #0x8e8
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800010455734:       mrs     x2, tpidr_el1
         :                      blk_account_io_done():
    0.00 :   ffff800010455738:       ldrsw   x3, [x0, x2]
    0.00 :   ffff80001045573c:       sxtw    x0, w22
    2.56 :   ffff800010455740:       ldr     x2, [x20, #840]
    0.00 :   ffff800010455744:       lsl     x5, x0, #3
    0.57 :   ffff800010455748:       ldr     x1, [x1, x3, lsl #3]
    0.00 :   ffff80001045574c:       add     x2, x2, x1
    0.00 :   ffff800010455750:       add     x2, x2, x5
    2.28 :   ffff800010455754:       ldr     x3, [x2, #64]
    0.00 :   ffff800010455758:       add     x3, x3, #0x1
   10.88 :   ffff80001045575c:       str     x3, [x2, #64]
    0.85 :   ffff800010455760:       ldr     w1, [x20, #820]
    0.00 :   ffff800010455764:       cbz     w1, ffff8000104557a0 <blk_account_io_done+0xf8>
         :                      part_to_disk():
         :                      struct lockdep_map lockdep_map;
         :                      };
         :
         :                      static inline struct gendisk *part_to_disk(struct hd_struct *part)
         :                      {
         :                      if (likely(part)) {
    0.02 :   ffff800010455768:       cbz     x20, ffff8000104559a4 <blk_account_io_done+0x2fc>
         :                      if (part->partno)
         :                      return dev_to_disk(part_to_dev(part)->parent);
    0.00 :   ffff80001045576c:       ldr     x1, [x20, #104]
    0.00 :   ffff800010455770:       sub     x1, x1, #0x70
         :                      __my_cpu_offset():
    1.16 :   ffff800010455774:       mrs     x6, tpidr_el1
         :                      blk_account_io_done():
    0.14 :   ffff800010455778:       add     x2, x23, #0x18
    0.00 :   ffff80001045577c:       add     x3, x24, #0x8e8
    0.02 :   ffff800010455780:       ldrsw   x6, [x2, x6]
    0.01 :   ffff800010455784:       ldr     x1, [x1, #912]
    1.12 :   ffff800010455788:       ldr     x2, [x3, x6, lsl #3]
    0.00 :   ffff80001045578c:       add     x1, x1, x2
    0.00 :   ffff800010455790:       add     x1, x1, x5
    0.27 :   ffff800010455794:       ldr     x2, [x1, #64]
    0.00 :   ffff800010455798:       add     x2, x2, #0x1
    1.27 :   ffff80001045579c:       str     x2, [x1, #64]
         :                      __my_cpu_offset():
    0.25 :   ffff8000104557a0:       mrs     x4, tpidr_el1
         :                      blk_account_io_done():
         :                      part_stat_add(part, nsecs[sgrp], now - req->start_time_ns);
    0.74 :   ffff8000104557a4:       add     x2, x23, #0x18
    0.00 :   ffff8000104557a8:       add     x3, x24, #0x8e8
    0.09 :   ffff8000104557ac:       ldrsw   x4, [x2, x4]
    0.47 :   ffff8000104557b0:       ldr     x1, [x20, #840]
    0.03 :   ffff8000104557b4:       ldr     x2, [x19, #176]
    0.63 :   ffff8000104557b8:       ldr     x4, [x3, x4, lsl #3]
    0.00 :   ffff8000104557bc:       sub     x3, x21, x2
    0.01 :   ffff8000104557c0:       add     x1, x1, x4
    0.15 :   ffff8000104557c4:       ldr     x2, [x1, x0, lsl #3]
    0.00 :   ffff8000104557c8:       add     x2, x2, x3
    1.06 :   ffff8000104557cc:       str     x2, [x1, x0, lsl #3]
    0.08 :   ffff8000104557d0:       ldr     w1, [x20, #820]
    0.00 :   ffff8000104557d4:       cbz     w1, ffff800010455814 <blk_account_io_done+0x16c>
         :                      part_to_disk():
         :                      if (likely(part)) {
    0.45 :   ffff8000104557d8:       cbz     x20, ffff80001045599c <blk_account_io_done+0x2f4>
         :                      return dev_to_disk(part_to_dev(part)->parent);
    0.10 :   ffff8000104557dc:       ldr     x1, [x20, #104]
    0.00 :   ffff8000104557e0:       sub     x1, x1, #0x70
         :                      __my_cpu_offset():
    0.66 :   ffff8000104557e4:       mrs     x4, tpidr_el1
         :                      blk_account_io_done():
    0.02 :   ffff8000104557e8:       add     x2, x23, #0x18
    0.00 :   ffff8000104557ec:       add     x3, x24, #0x8e8
    0.53 :   ffff8000104557f0:       ldrsw   x4, [x2, x4]
    0.10 :   ffff8000104557f4:       ldr     x1, [x1, #912]
    0.54 :   ffff8000104557f8:       ldr     x2, [x19, #176]
    0.05 :   ffff8000104557fc:       ldr     x4, [x3, x4, lsl #3]
    0.01 :   ffff800010455800:       sub     x3, x21, x2
    0.00 :   ffff800010455804:       add     x1, x1, x4
    0.95 :   ffff800010455808:       ldr     x2, [x1, x0, lsl #3]
    0.00 :   ffff80001045580c:       add     x2, x2, x3
    1.52 :   ffff800010455810:       str     x2, [x1, x0, lsl #3]
         :                      part_stat_add(part, time_in_queue, nsecs_to_jiffies64(now - req->start_time_ns));
    0.45 :   ffff800010455814:       ldr     x0, [x19, #176]
    0.00 :   ffff800010455818:       sub     x0, x21, x0
    0.00 :   ffff80001045581c:       bl      ffff800010165638 <nsecs_to_jiffies64>
    0.51 :   ffff800010455820:       add     x2, x23, #0x18
    0.02 :   ffff800010455824:       add     x3, x24, #0x8e8
         :                      __my_cpu_offset():
    0.34 :   ffff800010455828:       mrs     x4, tpidr_el1
         :                      blk_account_io_done():
    0.05 :   ffff80001045582c:       ldrsw   x4, [x2, x4]
    0.43 :   ffff800010455830:       ldr     x1, [x20, #840]
    0.51 :   ffff800010455834:       ldr     x2, [x3, x4, lsl #3]
    0.00 :   ffff800010455838:       add     x1, x1, x2
    0.96 :   ffff80001045583c:       ldr     x2, [x1, #136]
    0.16 :   ffff800010455840:       add     x0, x2, x0
    1.90 :   ffff800010455844:       str     x0, [x1, #136]
    0.23 :   ffff800010455848:       ldr     w0, [x20, #820]
    0.00 :   ffff80001045584c:       cbnz    w0, ffff8000104558fc <blk_account_io_done+0x254>
         :                      part_dec_in_flight(req->q, part, rq_data_dir(req));
    0.09 :   ffff800010455850:       ldr     w2, [x19, #24]
    0.00 :   ffff800010455854:       mov     x1, x20
    0.15 :   ffff800010455858:       ldr     x0, [x19]
    0.06 :   ffff80001045585c:       and     w2, w2, #0x1
    0.00 :   ffff800010455860:       bl      ffff800010469598 <part_dec_in_flight>
         :                      rcu_read_lock():
    0.99 :   ffff800010455864:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.96 :   ffff800010455868:       ldr     x0, [x20, #856]
         :                      __ref_is_percpu():
         :                      * Theoretically, the following could test just ATOMIC; however,
         :                      * then we'd have to mask off DEAD separately as DEAD may be
         :                      * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
         :                      * implies ATOMIC anyway.  Test them together.
         :                      */
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff80001045586c:       tst     x0, #0x3
    0.00 :   ffff800010455870:       b.ne    ffff800010455960 <blk_account_io_done+0x2b8>  // b.any
         :                      get_current():
    0.10 :   ffff800010455874:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.06 :   ffff800010455878:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
    0.06 :   ffff80001045587c:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.96 :   ffff800010455880:       str     w2, [x1, #16]
         :                      __percpu_add_case_64():
         :
         :                      PERCPU_RW_OPS(8)
         :                      PERCPU_RW_OPS(16)
         :                      PERCPU_RW_OPS(32)
         :                      PERCPU_RW_OPS(64)
         :                      PERCPU_OP(add, add, stadd)
    0.11 :   ffff800010455884:       mov     x3, #0xffffffffffffffff         // #-1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.79 :   ffff800010455888:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.02 :   ffff80001045588c:       add     x0, x0, x2
    0.07 :   ffff800010455890:       ldxr    x5, [x0]
   13.13 :   ffff800010455894:       add     x5, x5, x3
    0.01 :   ffff800010455898:       stxr    w4, x5, [x0]
    0.00 :   ffff80001045589c:       cbnz    w4, ffff800010455890 <blk_account_io_done+0x1e8>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.14 :   ffff8000104558a0:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000104558a4:       add     x0, x0, x3
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    1.59 :   ffff8000104558a8:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000104558ac:       cbz     x0, ffff800010455948 <blk_account_io_done+0x2a0>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.06 :   ffff8000104558b0:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000104558b4:       cbz     x0, ffff800010455948 <blk_account_io_done+0x2a0>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    3.05 :   ffff8000104558b8:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      get_current():
    1.41 :   ffff8000104558bc:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.28 :   ffff8000104558c0:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000104558c4:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.83 :   ffff8000104558c8:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000104558cc:       cbz     x0, ffff8000104558d8 <blk_account_io_done+0x230>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.10 :   ffff8000104558d0:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000104558d4:       cbnz    x0, ffff8000104558dc <blk_account_io_done+0x234>
         :                      blk_account_io_done():
         :
         :                      hd_struct_put(part);
         :                      part_stat_unlock();
    0.00 :   ffff8000104558d8:       bl      ffff800010cad640 <preempt_schedule>
         :                      rcu_read_unlock():
   24.05 :   ffff8000104558dc:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_account_io_done():
         :                      }
         :                      }
    0.02 :   ffff8000104558e0:       ldp     x19, x20, [sp, #16]
    0.02 :   ffff8000104558e4:       ldp     x21, x22, [sp, #32]
    1.26 :   ffff8000104558e8:       ldp     x23, x24, [sp, #48]
    0.02 :   ffff8000104558ec:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000104558f0:       ret
    0.00 :   ffff8000104558f4:       ret
    0.00 :   ffff8000104558f8:       ret
         :                      part_stat_add(part, time_in_queue, nsecs_to_jiffies64(now - req->start_time_ns));
    0.20 :   ffff8000104558fc:       ldr     x0, [x19, #176]
    0.07 :   ffff800010455900:       sub     x0, x21, x0
    0.00 :   ffff800010455904:       bl      ffff800010165638 <nsecs_to_jiffies64>
         :                      part_to_disk():
         :                      if (likely(part)) {
    0.15 :   ffff800010455908:       cbz     x20, ffff8000104559ac <blk_account_io_done+0x304>
         :                      if (part->partno)
    0.25 :   ffff80001045590c:       ldr     w1, [x20, #820]
    0.06 :   ffff800010455910:       cbnz    w1, ffff800010455954 <blk_account_io_done+0x2ac>
         :                      else
         :                      return dev_to_disk(part_to_dev(part));
    0.00 :   ffff800010455914:       sub     x2, x20, #0x48
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.92 :   ffff800010455918:       mrs     x3, tpidr_el1
         :                      blk_account_io_done():
    0.12 :   ffff80001045591c:       add     x1, x23, #0x18
    0.00 :   ffff800010455920:       add     x24, x24, #0x8e8
    0.27 :   ffff800010455924:       ldrsw   x3, [x1, x3]
    0.95 :   ffff800010455928:       ldr     x2, [x2, #912]
    0.00 :   ffff80001045592c:       mov     x1, x2
    0.88 :   ffff800010455930:       ldr     x2, [x24, x3, lsl #3]
    0.00 :   ffff800010455934:       add     x1, x1, x2
    1.89 :   ffff800010455938:       ldr     x2, [x1, #136]
    0.00 :   ffff80001045593c:       add     x0, x2, x0
    3.98 :   ffff800010455940:       str     x0, [x1, #136]
    0.00 :   ffff800010455944:       b       ffff800010455850 <blk_account_io_done+0x1a8>
         :                      percpu_ref_put_many():
         :                      unsigned long __percpu *percpu_count;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count))
         :                      this_cpu_sub(*percpu_count, nr);
    0.00 :   ffff800010455948:       bl      ffff800010cad668 <preempt_schedule_notrace>
    0.00 :   ffff80001045594c:       b       ffff8000104558b8 <blk_account_io_done+0x210>
         :                      blk_account_io_done():
    0.00 :   ffff800010455950:       ret
         :                      part_to_disk():
         :                      return dev_to_disk(part_to_dev(part)->parent);
    0.39 :   ffff800010455954:       ldr     x2, [x20, #104]
    0.00 :   ffff800010455958:       sub     x2, x2, #0x70
    0.00 :   ffff80001045595c:       b       ffff800010455918 <blk_account_io_done+0x270>
         :                      hd_struct_put():
         :                      return percpu_ref_tryget_live(&part->ref);
         :                      }
         :
         :                      static inline void hd_struct_put(struct hd_struct *part)
         :                      {
         :                      percpu_ref_put(&part->ref);
    0.00 :   ffff800010455960:       add     x0, x20, #0x350
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010455964:       b       ffff80001045598c <blk_account_io_done+0x2e4>
    0.00 :   ffff800010455968:       b       ffff80001045598c <blk_account_io_done+0x2e4>
         :                      __lse_atomic64_sub_return():
         :                      }
         :
         :                      ATOMIC64_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC64_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff80001045596c:       mov     x1, #0x1                        // #1
    0.00 :   ffff800010455970:       neg     x1, x1
    0.00 :   ffff800010455974:       ldaddal x1, x2, [x0]
    0.00 :   ffff800010455978:       add     x1, x1, x2
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff80001045597c:       cbnz    x1, ffff8000104558b8 <blk_account_io_done+0x210>
         :                      ref->release(ref);
    0.00 :   ffff800010455980:       ldr     x1, [x0, #16]
    0.00 :   ffff800010455984:       blr     x1
    0.00 :   ffff800010455988:       b       ffff8000104558b8 <blk_account_io_done+0x210>
         :                      __ll_sc_atomic64_sub_return():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(add, add, I)
         :                      ATOMIC64_OPS(sub, sub, J)
    0.00 :   ffff80001045598c:       mov     x2, #0x1                        // #1
    0.00 :   ffff800010455990:       add     x4, x20, #0x350
    0.00 :   ffff800010455994:       b       ffff8000104564b0 <blk_finish_plug+0x1c8>
    0.00 :   ffff800010455998:       b       ffff80001045597c <blk_account_io_done+0x2d4>
         :                      part_to_disk():
         :                      return NULL;
    0.00 :   ffff80001045599c:       mov     x1, #0x0                        // #0
    0.00 :   ffff8000104559a0:       b       ffff8000104557e4 <blk_account_io_done+0x13c>
    0.00 :   ffff8000104559a4:       mov     x1, #0x0                        // #0
    0.00 :   ffff8000104559a8:       b       ffff800010455774 <blk_account_io_done+0xcc>
    0.00 :   ffff8000104559ac:       mov     x2, #0x0                        // #0
    0.00 :   ffff8000104559b0:       b       ffff800010455918 <blk_account_io_done+0x270>
 Percent |	Source code & Disassembly of vmlinux for cycles (12033 samples, percent: local period)
------------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010250100 <kmem_cache_free>:
         :                      kmem_cache_free():
         :                      do_slab_free(cache, virt_to_head_page(x), x, NULL, 1, addr);
         :                      }
         :                      #endif
         :
         :                      void kmem_cache_free(struct kmem_cache *s, void *x)
         :                      {
    0.18 :   ffff800010250100:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff800010250104:       mov     x29, sp
    0.08 :   ffff800010250108:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001025010c:       mov     x19, x0
    0.02 :   ffff800010250110:       stp     x21, x22, [sp, #32]
    0.00 :   ffff800010250114:       mov     x20, x1
    0.00 :   ffff800010250118:       mov     x22, x30
         :                      arch_static_branch():
         :                      #define JUMP_LABEL_NOP_SIZE             AARCH64_INSN_SIZE
         :
         :                      static __always_inline bool arch_static_branch(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff80001025011c:       nop
         :                      cache_from_obj():
         :                      * to not do even the assignment. In that case, slab_equal_or_root
         :                      * will also be a constant.
         :                      */
         :                      if (!memcg_kmem_enabled() &&
         :                      !IS_ENABLED(CONFIG_SLAB_FREELIST_HARDENED) &&
         :                      !unlikely(s->flags & SLAB_CONSISTENCY_CHECKS))
    0.00 :   ffff800010250120:       ldr     w0, [x0, #8]
         :                      !IS_ENABLED(CONFIG_SLAB_FREELIST_HARDENED) &&
    0.00 :   ffff800010250124:       tbnz    w0, #8, ffff8000102502b4 <kmem_cache_free+0x1b4>
         :                      kmem_cache_free():
         :                      s = cache_from_obj(s, x);
         :                      if (!s)
    0.00 :   ffff800010250128:       cbz     x19, ffff800010250164 <kmem_cache_free+0x64>
    0.00 :   ffff80001025012c:       mov     x21, #0x1000000000000           // #281474976710656
    0.00 :   ffff800010250130:       add     x21, x1, x21
    0.00 :   ffff800010250134:       mov     x0, #0xffffffffffe00000         // #-2097152
    0.00 :   ffff800010250138:       lsr     x21, x21, #12
    0.00 :   ffff80001025013c:       movk    x0, #0xfdff, lsl #32
    0.00 :   ffff800010250140:       add     x21, x0, x21, lsl #6
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    1.63 :   ffff800010250144:       ldr     x0, [x21, #8]
         :                      compound_head():
         :                      static inline struct page *compound_head(struct page *page)
         :                      {
         :                      unsigned long head = READ_ONCE(page->compound_head);
         :
         :                      if (unlikely(head & 1))
         :                      return (struct page *) (head - 1);
    0.00 :   ffff800010250148:       sub     x1, x0, #0x1
    0.00 :   ffff80001025014c:       tst     x0, #0x1
    0.00 :   ffff800010250150:       csel    x21, x1, x21, ne  // ne = any
         :                      arch_static_branch():
    0.32 :   ffff800010250154:       nop
         :                      set_freepointer():
         :                      unsigned long freeptr_addr = (unsigned long)object + s->offset;
    0.01 :   ffff800010250158:       ldr     w0, [x19, #32]
         :                      *(void **)freeptr_addr = freelist_ptr(s, fp, freeptr_addr);
    2.91 :   ffff80001025015c:       str     xzr, [x20, x0]
         :                      slab_free():
         :                      if (slab_free_freelist_hook(s, &head, &tail))
    0.00 :   ffff800010250160:       cbnz    x20, ffff800010250174 <kmem_cache_free+0x74>
         :                      kmem_cache_free():
         :                      return;
         :                      slab_free(s, virt_to_head_page(x), x, NULL, 1, _RET_IP_);
         :                      trace_kmem_cache_free(_RET_IP_, x);
         :                      }
    2.45 :   ffff800010250164:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010250168:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff80001025016c:       ldp     x29, x30, [sp], #64
    0.00 :   ffff800010250170:       ret
    0.76 :   ffff800010250174:       str     x23, [x29, #48]
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.25 :   ffff800010250178:       mrs     x2, sp_el0
         :                      __read_once_size():
    0.01 :   ffff80001025017c:       ldr     w0, [x2, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff800010250180:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    2.09 :   ffff800010250184:       str     w0, [x2, #16]
         :                      do_slab_free():
         :                      tid = this_cpu_read(s->cpu_slab->tid);
    0.13 :   ffff800010250188:       ldr     x1, [x19]
    0.00 :   ffff80001025018c:       add     x1, x1, #0x8
    0.00 :   ffff800010250190:       bl      ffff80001024af98 <__my_cpu_offset>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    2.15 :   ffff800010250194:       ldr     x23, [x1, x0]
    6.75 :   ffff800010250198:       ldr     x0, [x2, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff80001025019c:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
   13.81 :   ffff8000102501a0:       str     w0, [x2, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102501a4:       cbnz    x0, ffff800010250298 <kmem_cache_free+0x198>
         :                      do_slab_free():
    0.00 :   ffff8000102501a8:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      c = raw_cpu_ptr(s->cpu_slab);
    2.49 :   ffff8000102501ac:       ldr     x1, [x19]
    0.00 :   ffff8000102501b0:       bl      ffff80001024af98 <__my_cpu_offset>
    0.00 :   ffff8000102501b4:       mov     x2, x0
    0.01 :   ffff8000102501b8:       add     x0, x1, x0
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    2.35 :   ffff8000102501bc:       ldr     x3, [x0, #8]
         :                      do_slab_free():
         :                      unlikely(tid != READ_ONCE(c->tid)));
    0.25 :   ffff8000102501c0:       cmp     x3, x23
    0.00 :   ffff8000102501c4:       b.ne    ffff800010250178 <kmem_cache_free+0x78>  // b.any
         :                      if (likely(page == c->page)) {
    2.82 :   ffff8000102501c8:       ldr     x0, [x0, #16]
         :                      get_current():
    0.76 :   ffff8000102501cc:       mrs     x7, sp_el0
         :                      do_slab_free():
    0.08 :   ffff8000102501d0:       cmp     x0, x21
    0.42 :   ffff8000102501d4:       b.ne    ffff80001025033c <kmem_cache_free+0x23c>  // b.any
         :                      set_freepointer():
         :                      unsigned long freeptr_addr = (unsigned long)object + s->offset;
    0.17 :   ffff8000102501d8:       ldr     w0, [x19, #32]
         :                      do_slab_free():
         :                      set_freepointer(s, tail_obj, c->freelist);
    0.06 :   ffff8000102501dc:       ldr     x3, [x1, x2]
         :                      set_freepointer():
         :                      *(void **)freeptr_addr = freelist_ptr(s, fp, freeptr_addr);
    0.01 :   ffff8000102501e0:       str     x3, [x20, x0]
         :                      __read_once_size():
    0.38 :   ffff8000102501e4:       ldr     w0, [x7, #16]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff8000102501e8:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.50 :   ffff8000102501ec:       str     w0, [x7, #16]
         :                      do_slab_free():
         :                      if (unlikely(!this_cpu_cmpxchg_double(
    0.05 :   ffff8000102501f0:       ldr     x4, [x19]
    0.00 :   ffff8000102501f4:       bl      ffff80001024af98 <__my_cpu_offset>
    0.18 :   ffff8000102501f8:       add     x4, x4, x0
    0.06 :   ffff8000102501fc:       ldr     x0, [x1, x2]
    0.00 :   ffff800010250200:       add     x3, x23, #0x100
    0.00 :   ffff800010250204:       mov     x1, x23
    0.00 :   ffff800010250208:       mov     x2, x20
    0.05 :   ffff80001025020c:       bl      ffff80001024d338 <__cmpxchg_double>
    0.00 :   ffff800010250210:       mov     x23, x0
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.06 :   ffff800010250214:       ldr     x1, [x7, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800010250218:       sub     x1, x1, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.10 :   ffff80001025021c:       str     w1, [x7, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800010250220:       cbnz    x1, ffff8000102502a4 <kmem_cache_free+0x1a4>
         :                      do_slab_free():
    0.00 :   ffff800010250224:       bl      ffff800010cad668 <preempt_schedule_notrace>
    0.42 :   ffff800010250228:       cbnz    x23, ffff800010250178 <kmem_cache_free+0x78>
    0.05 :   ffff80001025022c:       ldr     x23, [x29, #48]
         :                      kmem_cache_free():
         :                      }
    0.68 :   ffff800010250230:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010250234:       ldp     x21, x22, [sp, #32]
    0.03 :   ffff800010250238:       ldp     x29, x30, [sp], #64
    0.00 :   ffff80001025023c:       ret
         :                      slab_want_init_on_free():
         :                      }
         :
         :                      static inline bool slab_want_init_on_free(struct kmem_cache *c)
         :                      {
         :                      if (static_branch_unlikely(&init_on_free))
         :                      return !(c->ctor ||
    0.00 :   ffff800010250240:       ldr     x0, [x19, #64]
    0.00 :   ffff800010250244:       cbnz    x0, ffff800010250158 <kmem_cache_free+0x58>
         :                      (c->flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON)));
    0.00 :   ffff800010250248:       ldr     w0, [x19, #8]
    0.00 :   ffff80001025024c:       and     w0, w0, #0xff800
    0.00 :   ffff800010250250:       and     w0, w0, #0xfff80fff
         :                      return !(c->ctor ||
    0.00 :   ffff800010250254:       cbnz    w0, ffff800010250158 <kmem_cache_free+0x58>
         :                      slab_free_freelist_hook():
         :                      memset(object, 0, s->object_size);
    0.00 :   ffff800010250258:       ldr     w2, [x19, #28]
    0.00 :   ffff80001025025c:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010250260:       mov     x0, x20
    0.00 :   ffff800010250264:       bl      ffff800010c92840 <__memset>
         :                      rsize = (s->flags & SLAB_RED_ZONE) ? s->red_left_pad
    0.00 :   ffff800010250268:       ldr     w0, [x19, #8]
         :                      : 0;
    0.00 :   ffff80001025026c:       mov     w3, #0x0                        // #0
    0.00 :   ffff800010250270:       tbz     w0, #10, ffff800010250278 <kmem_cache_free+0x178>
         :                      rsize = (s->flags & SLAB_RED_ZONE) ? s->red_left_pad
    0.00 :   ffff800010250274:       ldr     w3, [x19, #80]
         :                      memset((char *)object + s->inuse, 0,
    0.00 :   ffff800010250278:       ldr     w0, [x19, #72]
    0.00 :   ffff80001025027c:       mov     w1, #0x0                        // #0
         :                      s->size - s->inuse - rsize);
    0.00 :   ffff800010250280:       ldr     w2, [x19, #24]
    0.00 :   ffff800010250284:       sub     w2, w2, w0
         :                      memset((char *)object + s->inuse, 0,
    0.00 :   ffff800010250288:       add     x0, x20, w0, uxtw
    0.00 :   ffff80001025028c:       sub     w2, w2, w3
    0.00 :   ffff800010250290:       bl      ffff800010c92840 <__memset>
    0.00 :   ffff800010250294:       b       ffff800010250158 <kmem_cache_free+0x58>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    5.84 :   ffff800010250298:       ldr     x0, [x2, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff80001025029c:       cbz     x0, ffff8000102501a8 <kmem_cache_free+0xa8>
   28.83 :   ffff8000102502a0:       b       ffff8000102501ac <kmem_cache_free+0xac>
         :                      __read_once_size():
    0.65 :   ffff8000102502a4:       ldr     x0, [x7, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102502a8:       cbnz    x0, ffff800010250228 <kmem_cache_free+0x128>
         :                      do_slab_free():
         :                      if (unlikely(!this_cpu_cmpxchg_double(
    0.00 :   ffff8000102502ac:       bl      ffff800010cad668 <preempt_schedule_notrace>
    0.00 :   ffff8000102502b0:       b       ffff800010250228 <kmem_cache_free+0x128>
         :                      virt_to_head_page():
         :                      }
         :                      #endif
         :
         :                      static inline struct page *virt_to_head_page(const void *x)
         :                      {
         :                      struct page *page = virt_to_page(x);
    2.66 :   ffff8000102502b4:       mov     x21, #0x1000000000000           // #281474976710656
    0.00 :   ffff8000102502b8:       add     x21, x20, x21
    0.00 :   ffff8000102502bc:       mov     x0, #0xffffffffffe00000         // #-2097152
    0.02 :   ffff8000102502c0:       lsr     x21, x21, #12
    0.04 :   ffff8000102502c4:       movk    x0, #0xfdff, lsl #32
    0.00 :   ffff8000102502c8:       add     x21, x0, x21, lsl #6
         :                      __read_once_size():
    0.01 :   ffff8000102502cc:       ldr     x1, [x21, #8]
         :                      compound_head():
    0.00 :   ffff8000102502d0:       sub     x0, x1, #0x1
    0.00 :   ffff8000102502d4:       tst     x1, #0x1
    0.00 :   ffff8000102502d8:       csel    x0, x0, x21, ne  // ne = any
         :                      __read_once_size():
    2.92 :   ffff8000102502dc:       ldr     x2, [x0, #8]
         :                      compound_head():
    0.00 :   ffff8000102502e0:       sub     x1, x2, #0x1
    0.00 :   ffff8000102502e4:       tst     x2, #0x1
    0.00 :   ffff8000102502e8:       csel    x1, x1, x0, ne  // ne = any
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    4.80 :   ffff8000102502ec:       ldr     x1, [x1]
         :                      virt_to_cache():
         :                      if (WARN_ONCE(!PageSlab(page), "%s: Object is not a Slab page!\n",
    0.00 :   ffff8000102502f0:       tst     w1, #0x200
    0.00 :   ffff8000102502f4:       b.eq    ffff800010250360 <kmem_cache_free+0x260>  // b.none
    2.34 :   ffff8000102502f8:       str     x23, [x29, #48]
         :                      return page->slab_cache;
    0.30 :   ffff8000102502fc:       ldr     x23, [x0, #24]
         :                      cache_from_obj():
         :                      WARN_ONCE(cachep && !slab_equal_or_root(cachep, s),
    0.00 :   ffff800010250300:       cbz     x23, ffff80001025022c <kmem_cache_free+0x12c>
         :                      slab_equal_or_root():
         :                      return p == s || p == s->memcg_params.root_cache;
    0.03 :   ffff800010250304:       cmp     x19, x23
    0.01 :   ffff800010250308:       b.eq    ffff800010250334 <kmem_cache_free+0x234>  // b.none
    0.00 :   ffff80001025030c:       ldr     x0, [x23, #208]
    0.00 :   ffff800010250310:       cmp     x19, x0
    0.00 :   ffff800010250314:       b.eq    ffff800010250328 <kmem_cache_free+0x228>  // b.none
         :                      cache_from_obj():
         :                      WARN_ONCE(cachep && !slab_equal_or_root(cachep, s),
    0.00 :   ffff800010250318:       adrp    x4, ffff800011a5d000 <__print_once.28906>
    0.00 :   ffff80001025031c:       add     x4, x4, #0x21
    0.00 :   ffff800010250320:       ldrb    w0, [x4, #1]
    0.00 :   ffff800010250324:       cbz     w0, ffff800010250394 <kmem_cache_free+0x294>
    0.00 :   ffff800010250328:       mov     x19, x23
    0.00 :   ffff80001025032c:       ldr     x23, [x29, #48]
    0.00 :   ffff800010250330:       b       ffff800010250144 <kmem_cache_free+0x44>
    3.91 :   ffff800010250334:       ldr     x23, [x29, #48]
    0.00 :   ffff800010250338:       b       ffff800010250144 <kmem_cache_free+0x44>
         :                      do_slab_free():
         :                      __slab_free(s, page, head, tail_obj, cnt, addr);
    1.15 :   ffff80001025033c:       mov     x5, x22
    0.02 :   ffff800010250340:       mov     w4, #0x1                        // #1
    0.00 :   ffff800010250344:       mov     x3, x20
    0.00 :   ffff800010250348:       mov     x2, x20
    0.71 :   ffff80001025034c:       mov     x1, x21
    0.03 :   ffff800010250350:       mov     x0, x19
    0.00 :   ffff800010250354:       bl      ffff80001024fa68 <__slab_free>
    0.21 :   ffff800010250358:       ldr     x23, [x29, #48]
    0.00 :   ffff80001025035c:       b       ffff800010250164 <kmem_cache_free+0x64>
         :                      virt_to_cache():
         :                      if (WARN_ONCE(!PageSlab(page), "%s: Object is not a Slab page!\n",
    0.00 :   ffff800010250360:       adrp    x2, ffff800011a5d000 <__print_once.28906>
    0.00 :   ffff800010250364:       ldrb    w0, [x2, #33]
    0.00 :   ffff800010250368:       cbnz    w0, ffff800010250164 <kmem_cache_free+0x64>
    0.00 :   ffff80001025036c:       mov     w3, #0x1                        // #1
    0.00 :   ffff800010250370:       adrp    x1, ffff800010cf7000 <pageflag_names+0xe8>
    0.00 :   ffff800010250374:       add     x1, x1, #0xb58
    0.00 :   ffff800010250378:       strb    w3, [x2, #33]
    0.00 :   ffff80001025037c:       add     x1, x1, #0x18
    0.00 :   ffff800010250380:       adrp    x0, ffff800011194000 <kallsyms_token_index+0x1a330>
    0.00 :   ffff800010250384:       add     x0, x0, #0x9f8
    0.00 :   ffff800010250388:       bl      ffff8000100e5288 <__warn_printk>
    0.00 :   ffff80001025038c:       brk     #0x800
    0.00 :   ffff800010250390:       b       ffff800010250164 <kmem_cache_free+0x64>
         :                      cache_from_obj():
         :                      WARN_ONCE(cachep && !slab_equal_or_root(cachep, s),
    0.00 :   ffff800010250394:       ldr     x2, [x19, #88]
    0.00 :   ffff800010250398:       mov     w5, #0x1                        // #1
    0.00 :   ffff80001025039c:       ldr     x3, [x23, #88]
    0.00 :   ffff8000102503a0:       adrp    x1, ffff800010cf7000 <pageflag_names+0xe8>
    0.00 :   ffff8000102503a4:       add     x1, x1, #0xb58
    0.00 :   ffff8000102503a8:       strb    w5, [x4, #1]
    0.00 :   ffff8000102503ac:       add     x1, x1, #0x28
    0.00 :   ffff8000102503b0:       adrp    x0, ffff800011194000 <kallsyms_token_index+0x1a330>
    0.00 :   ffff8000102503b4:       add     x0, x0, #0xa18
    0.00 :   ffff8000102503b8:       bl      ffff8000100e5288 <__warn_printk>
    0.00 :   ffff8000102503bc:       brk     #0x800
    0.00 :   ffff8000102503c0:       mov     x19, x23
    0.00 :   ffff8000102503c4:       ldr     x23, [x29, #48]
    0.00 :   ffff8000102503c8:       b       ffff800010250144 <kmem_cache_free+0x44>
 Percent |	Source code & Disassembly of vmlinux for cycles (9373 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001045d8b0 <blk_mq_complete_request>:
         :                      blk_mq_complete_request():
         :                      * Description:
         :                      *       Ends all I/O on a request. It does not handle partial completions.
         :                      *       The actual completion happens out-of-order, through a IPI handler.
         :                      **/
         :                      bool blk_mq_complete_request(struct request *rq)
         :                      {
    3.76 :   ffff80001045d8b0:       stp     x29, x30, [sp, #-48]!
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001045d8b4:       mov     w1, #0x2                        // #2
         :                      blk_mq_complete_request():
    0.00 :   ffff80001045d8b8:       mov     x29, sp
    0.00 :   ffff80001045d8bc:       stp     x20, x21, [sp, #24]
         :                      __write_once_size():
    0.02 :   ffff80001045d8c0:       str     w1, [x0, #208]
         :                      __blk_mq_complete_request():
         :                      struct blk_mq_ctx *ctx = rq->mq_ctx;
    0.87 :   ffff80001045d8c4:       ldp     x20, x21, [x0]
         :                      if (q->nr_hw_queues == 1) {
   11.10 :   ffff80001045d8c8:       ldr     w1, [x20, #80]
    0.00 :   ffff80001045d8cc:       cmp     w1, #0x1
    1.91 :   ffff80001045d8d0:       b.eq    ffff80001045d908 <blk_mq_complete_request+0x58>  // b.none
   23.26 :   ffff80001045d8d4:       str     x19, [x29, #16]
    0.00 :   ffff80001045d8d8:       mov     x19, x0
         :                      if ((rq->cmd_flags & REQ_HIPRI) ||
    1.98 :   ffff80001045d8dc:       ldr     w0, [x0, #24]
    0.00 :   ffff80001045d8e0:       tbz     w0, #25, ffff80001045d91c <blk_mq_complete_request+0x6c>
         :                      q->mq_ops->complete(rq);
    0.00 :   ffff80001045d8e4:       ldr     x1, [x20, #48]
    0.00 :   ffff80001045d8e8:       mov     x0, x19
    0.00 :   ffff80001045d8ec:       ldr     x1, [x1, #48]
    0.00 :   ffff80001045d8f0:       blr     x1
    0.00 :   ffff80001045d8f4:       ldr     x19, [x29, #16]
         :                      blk_mq_complete_request():
         :                      if (unlikely(blk_should_fake_timeout(rq->q)))
         :                      return false;
         :                      __blk_mq_complete_request(rq);
         :                      return true;
         :                      }
    0.19 :   ffff80001045d8f8:       mov     w0, #0x1                        // #1
    0.00 :   ffff80001045d8fc:       ldp     x20, x21, [sp, #24]
    0.02 :   ffff80001045d900:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001045d904:       ret
         :                      __blk_mq_complete_request():
         :                      __blk_complete_request(rq);
    0.00 :   ffff80001045d908:       bl      ffff80001045c6d0 <__blk_complete_request>
         :                      blk_mq_complete_request():
         :                      }
    0.00 :   ffff80001045d90c:       mov     w0, #0x1                        // #1
    0.00 :   ffff80001045d910:       ldp     x20, x21, [sp, #24]
    0.00 :   ffff80001045d914:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001045d918:       ret
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.02 :   ffff80001045d91c:       ldr     x0, [x20, #104]
         :                      __blk_mq_complete_request():
         :                      if ((rq->cmd_flags & REQ_HIPRI) ||
    0.00 :   ffff80001045d920:       tst     w0, #0x10
    0.01 :   ffff80001045d924:       b.eq    ffff80001045d8e4 <blk_mq_complete_request+0x34>  // b.none
    0.57 :   ffff80001045d928:       str     x22, [x29, #40]
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    1.54 :   ffff80001045d92c:       mrs     x1, sp_el0
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.05 :   ffff80001045d930:       ldr     w0, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff80001045d934:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001045d938:       str     w0, [x1, #16]
         :                      test_bit():
    0.02 :   ffff80001045d93c:       ldr     x2, [x20, #104]
         :                      __blk_mq_complete_request():
         :                      cpu = get_cpu();
    0.00 :   ffff80001045d940:       adrp    x0, ffff8000114ca000 <bp_hardening_data>
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    2.10 :   ffff80001045d944:       mrs     x1, tpidr_el1
         :                      __blk_mq_complete_request():
    0.09 :   ffff80001045d948:       add     x0, x0, #0x18
         :                      if (!test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags))
    0.00 :   ffff80001045d94c:       tst     w2, #0x1000
         :                      cpu = get_cpu();
    0.00 :   ffff80001045d950:       ldr     w22, [x0, x1]
         :                      shared = cpus_share_cache(cpu, ctx->cpu);
    0.10 :   ffff80001045d954:       ldr     w1, [x21, #64]
         :                      if (!test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags))
    0.05 :   ffff80001045d958:       b.eq    ffff80001045d99c <blk_mq_complete_request+0xec>  // b.none
         :                      if (cpu != ctx->cpu && !shared && cpu_online(ctx->cpu)) {
    0.00 :   ffff80001045d95c:       cmp     w22, w1
    0.00 :   ffff80001045d960:       b.ne    ffff80001045d9b8 <blk_mq_complete_request+0x108>  // b.any
         :                      q->mq_ops->complete(rq);
    0.09 :   ffff80001045d964:       ldr     x1, [x20, #48]
    0.89 :   ffff80001045d968:       mov     x0, x19
    5.25 :   ffff80001045d96c:       ldr     x1, [x1, #48]
    0.06 :   ffff80001045d970:       blr     x1
         :                      get_current():
    2.19 :   ffff80001045d974:       mrs     x1, sp_el0
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    1.68 :   ffff80001045d978:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff80001045d97c:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    2.76 :   ffff80001045d980:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff80001045d984:       cbz     x0, ffff80001045da00 <blk_mq_complete_request+0x150>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.06 :   ffff80001045d988:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff80001045d98c:       cbz     x0, ffff80001045da00 <blk_mq_complete_request+0x150>
   11.97 :   ffff80001045d990:       ldr     x19, [x29, #16]
    1.64 :   ffff80001045d994:       ldr     x22, [x29, #40]
    0.00 :   ffff80001045d998:       b       ffff80001045d8f8 <blk_mq_complete_request+0x48>
         :                      __blk_mq_complete_request():
         :                      shared = cpus_share_cache(cpu, ctx->cpu);
   11.95 :   ffff80001045d99c:       mov     w0, w22
    0.02 :   ffff80001045d9a0:       bl      ffff8000101163d8 <cpus_share_cache>
         :                      if (cpu != ctx->cpu && !shared && cpu_online(ctx->cpu)) {
   13.77 :   ffff80001045d9a4:       ldr     w1, [x21, #64]
    0.00 :   ffff80001045d9a8:       tst     w0, #0xff
    0.00 :   ffff80001045d9ac:       ccmp    w22, w1, #0x4, eq  // eq = none
    0.00 :   ffff80001045d9b0:       b.eq    ffff80001045d964 <blk_mq_complete_request+0xb4>  // b.none
    0.00 :   ffff80001045d9b4:       nop
         :                      test_bit():
    0.00 :   ffff80001045d9b8:       add     w0, w1, #0x3f
    0.00 :   ffff80001045d9bc:       cmp     w1, #0x0
    0.00 :   ffff80001045d9c0:       csel    w0, w0, w1, lt  // lt = tstop
    0.00 :   ffff80001045d9c4:       adrp    x2, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff80001045d9c8:       add     x2, x2, #0x120
    0.00 :   ffff80001045d9cc:       asr     w0, w0, #6
    0.00 :   ffff80001045d9d0:       sxtw    x0, w0
    0.00 :   ffff80001045d9d4:       ldr     x0, [x2, x0, lsl #3]
    0.00 :   ffff80001045d9d8:       lsr     x1, x0, x1
         :                      __blk_mq_complete_request():
    0.00 :   ffff80001045d9dc:       tbz     w1, #0, ffff80001045d964 <blk_mq_complete_request+0xb4>
         :                      rq->csd.flags = 0;
    0.00 :   ffff80001045d9e0:       str     wzr, [x19, #256]
         :                      rq->csd.func = __blk_mq_complete_request_remote;
    0.00 :   ffff80001045d9e4:       adrp    x0, ffff80001045d000 <__blkdev_issue_zero_pages+0x60>
    0.00 :   ffff80001045d9e8:       add     x0, x0, #0x370
         :                      rq->csd.info = rq;
    0.00 :   ffff80001045d9ec:       stp     x0, x19, [x19, #240]
         :                      smp_call_function_single_async(ctx->cpu, &rq->csd);
    0.00 :   ffff80001045d9f0:       add     x1, x19, #0xe8
    0.00 :   ffff80001045d9f4:       ldr     w0, [x21, #64]
    0.00 :   ffff80001045d9f8:       bl      ffff8000101800b8 <smp_call_function_single_async>
    0.00 :   ffff80001045d9fc:       b       ffff80001045d974 <blk_mq_complete_request+0xc4>
         :                      put_cpu();
    0.00 :   ffff80001045da00:       bl      ffff800010cad640 <preempt_schedule>
    0.00 :   ffff80001045da04:       ldr     x19, [x29, #16]
    0.00 :   ffff80001045da08:       ldr     x22, [x29, #40]
    0.00 :   ffff80001045da0c:       b       ffff80001045d8f8 <blk_mq_complete_request+0x48>
 Percent |	Source code & Disassembly of vmlinux for cycles (9230 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010455308 <blk_update_request>:
         :                      blk_update_request():
         :                      *     %false - this request doesn't have any more data
         :                      *     %true  - this request has more data
         :                      **/
         :                      bool blk_update_request(struct request *req, blk_status_t error,
         :                      unsigned int nr_bytes)
         :                      {
    0.80 :   ffff800010455308:       sub     sp, sp, #0x50
    0.37 :   ffff80001045530c:       stp     x29, x30, [sp, #16]
    0.00 :   ffff800010455310:       add     x29, sp, #0x10
    0.18 :   ffff800010455314:       str     x20, [sp, #40]
    0.00 :   ffff800010455318:       mov     w20, w2
    0.37 :   ffff80001045531c:       str     x22, [sp, #56]
    0.00 :   ffff800010455320:       mov     x22, x0
    0.70 :   ffff800010455324:       str     x24, [sp, #72]
    0.00 :   ffff800010455328:       and     w24, w1, #0xff
         :                      blk_status_to_errno():
         :                      if (WARN_ON_ONCE(idx >= ARRAY_SIZE(blk_errors)))
    0.00 :   ffff80001045532c:       cmp     w24, #0xd
    0.00 :   ffff800010455330:       b.hi    ffff800010455594 <blk_update_request+0x28c>  // b.pmore
         :                      blk_update_request():
         :                      int total_bytes;
         :
         :                      trace_block_rq_complete(req, blk_status_to_errno(error), nr_bytes);
         :
         :                      if (!req->bio)
    0.50 :   ffff800010455334:       ldr     x0, [x22, #56]
         :                      return false;
    0.00 :   ffff800010455338:       mov     w1, #0x0                        // #0
         :                      if (!req->bio)
    0.00 :   ffff80001045533c:       cbz     x0, ffff8000104553f4 <blk_update_request+0xec>
    0.17 :   ffff800010455340:       str     x19, [x29, #16]
    0.55 :   ffff800010455344:       str     x21, [x29, #32]
    0.50 :   ffff800010455348:       str     x23, [x29, #48]
         :
         :                      #ifdef CONFIG_BLK_DEV_INTEGRITY
         :                      if (blk_integrity_rq(req) && req_op(req) == REQ_OP_READ &&
    0.39 :   ffff80001045534c:       ldr     w0, [x22, #24]
    0.00 :   ffff800010455350:       tbnz    w0, #16, ffff800010455410 <blk_update_request+0x108>
         :                      error == BLK_STS_OK)
         :                      req->q->integrity.profile->complete_fn(req, nr_bytes);
         :                      #endif
         :
         :                      if (unlikely(error && !blk_rq_is_passthrough(req) &&
    0.24 :   ffff800010455354:       cbnz    w24, ffff80001045559c <blk_update_request+0x294>
         :                      !(req->rq_flags & RQF_QUIET)))
         :                      print_req_error(req, error, __func__);
         :
         :                      blk_account_io_completion(req, nr_bytes);
    0.64 :   ffff800010455358:       mov     w1, w20
    0.00 :   ffff80001045535c:       mov     x0, x22
         :
         :                      total_bytes = 0;
    0.00 :   ffff800010455360:       mov     w23, #0x0                       // #0
         :                      blk_account_io_completion(req, nr_bytes);
    0.00 :   ffff800010455364:       bl      ffff8000104551e0 <blk_account_io_completion>
         :                      while (req->bio) {
    1.20 :   ffff800010455368:       b       ffff80001045539c <blk_update_request+0x94>
         :                      req_bio_endio():
         :                      bio_advance(bio, nbytes);
    1.44 :   ffff80001045536c:       mov     x0, x19
    0.00 :   ffff800010455370:       mov     w1, w21
    0.00 :   ffff800010455374:       bl      ffff80001044e768 <bio_advance>
         :                      if (bio->bi_iter.bi_size == 0 && !(rq->rq_flags & RQF_FLUSH_SEQ))
    0.45 :   ffff800010455378:       ldr     w0, [x19, #40]
    0.00 :   ffff80001045537c:       cbnz    w0, ffff800010455390 <blk_update_request+0x88>
    2.63 :   ffff800010455380:       ldr     w0, [x22, #28]
    0.00 :   ffff800010455384:       tbnz    w0, #4, ffff800010455390 <blk_update_request+0x88>
         :                      bio_endio(bio);
    0.71 :   ffff800010455388:       mov     x0, x19
    0.00 :   ffff80001045538c:       bl      ffff800010450080 <bio_endio>
         :                      blk_update_request():
         :
         :                      /* Completion has already been traced */
         :                      bio_clear_flag(bio, BIO_TRACE_COMPLETION);
         :                      req_bio_endio(req, bio, bio_bytes, error);
         :
         :                      total_bytes += bio_bytes;
    0.05 :   ffff800010455390:       add     w23, w23, w21
         :                      nr_bytes -= bio_bytes;
         :
         :                      if (!nr_bytes)
    0.00 :   ffff800010455394:       subs    w20, w20, w21
    0.00 :   ffff800010455398:       b.eq    ffff800010455468 <blk_update_request+0x160>  // b.none
         :                      while (req->bio) {
    0.10 :   ffff80001045539c:       ldr     x19, [x22, #56]
    0.22 :   ffff8000104553a0:       cbz     x19, ffff800010455438 <blk_update_request+0x130>
         :                      unsigned bio_bytes = min(bio->bi_iter.bi_size, nr_bytes);
    0.47 :   ffff8000104553a4:       ldr     w0, [x19, #40]
    0.00 :   ffff8000104553a8:       cmp     w20, w0
    0.00 :   ffff8000104553ac:       csel    w21, w20, w0, ls  // ls = plast
         :                      if (bio_bytes == bio->bi_iter.bi_size)
    0.00 :   ffff8000104553b0:       b.cc    ffff8000104553bc <blk_update_request+0xb4>  // b.lo, b.ul, b.last
         :                      req->bio = bio->bi_next;
   57.16 :   ffff8000104553b4:       ldr     x0, [x19]
   18.38 :   ffff8000104553b8:       str     x0, [x22, #56]
         :                      bio_clear_flag():
         :                      bio->bi_flags |= (1U << bit);
         :                      }
         :
         :                      static inline void bio_clear_flag(struct bio *bio, unsigned int bit)
         :                      {
         :                      bio->bi_flags &= ~(1U << bit);
    0.07 :   ffff8000104553bc:       ldrh    w0, [x19, #20]
    0.12 :   ffff8000104553c0:       and     w0, w0, #0xfffffbff
    0.00 :   ffff8000104553c4:       and     w0, w0, #0xffff
    1.65 :   ffff8000104553c8:       strh    w0, [x19, #20]
         :                      req_bio_endio():
         :                      if (error)
    0.00 :   ffff8000104553cc:       cbz     w24, ffff8000104553d4 <blk_update_request+0xcc>
         :                      bio->bi_status = error;
    0.00 :   ffff8000104553d0:       strb    w24, [x19, #26]
         :                      if (unlikely(rq->rq_flags & RQF_QUIET))
    0.01 :   ffff8000104553d4:       ldr     w1, [x22, #28]
    0.00 :   ffff8000104553d8:       tbz     w1, #11, ffff80001045536c <blk_update_request+0x64>
         :                      bio_set_flag():
         :                      bio->bi_flags |= (1U << bit);
    0.00 :   ffff8000104553dc:       orr     w0, w0, #0x40
    0.00 :   ffff8000104553e0:       strh    w0, [x19, #20]
    0.00 :   ffff8000104553e4:       b       ffff80001045536c <blk_update_request+0x64>
    0.00 :   ffff8000104553e8:       ldr     x19, [x29, #16]
    0.00 :   ffff8000104553ec:       ldr     x21, [x29, #32]
    0.00 :   ffff8000104553f0:       ldr     x23, [x29, #48]
         :                      blk_update_request():
         :                      /* recalculate the number of segments */
         :                      req->nr_phys_segments = blk_recalc_rq_segments(req);
         :                      }
         :
         :                      return true;
         :                      }
    0.00 :   ffff8000104553f4:       mov     w0, w1
    0.00 :   ffff8000104553f8:       ldr     x20, [sp, #40]
    0.00 :   ffff8000104553fc:       ldp     x29, x30, [sp, #16]
    0.00 :   ffff800010455400:       ldr     x22, [sp, #56]
    0.00 :   ffff800010455404:       ldr     x24, [sp, #72]
    0.00 :   ffff800010455408:       add     sp, sp, #0x50
    0.00 :   ffff80001045540c:       ret
         :                      if (blk_integrity_rq(req) && req_op(req) == REQ_OP_READ &&
    0.00 :   ffff800010455410:       and     w1, w0, #0xff
    0.00 :   ffff800010455414:       cbnz    w1, ffff800010455354 <blk_update_request+0x4c>
    0.00 :   ffff800010455418:       cbnz    w24, ffff8000104555b8 <blk_update_request+0x2b0>
         :                      req->q->integrity.profile->complete_fn(req, nr_bytes);
    0.00 :   ffff80001045541c:       ldr     x2, [x22]
    0.00 :   ffff800010455420:       mov     w1, w20
    0.00 :   ffff800010455424:       mov     x0, x22
    0.00 :   ffff800010455428:       ldr     x2, [x2, #200]
    0.00 :   ffff80001045542c:       ldr     x2, [x2, #24]
    0.00 :   ffff800010455430:       blr     x2
    0.00 :   ffff800010455434:       b       ffff800010455358 <blk_update_request+0x50>
         :                      req->__data_len = 0;
    2.82 :   ffff800010455438:       str     wzr, [x22, #40]
         :                      return false;
    0.00 :   ffff80001045543c:       mov     w1, #0x0                        // #0
         :                      }
    0.00 :   ffff800010455440:       mov     w0, w1
         :                      return false;
    1.62 :   ffff800010455444:       ldr     x19, [x29, #16]
    0.55 :   ffff800010455448:       ldr     x21, [x29, #32]
    0.04 :   ffff80001045544c:       ldr     x23, [x29, #48]
         :                      }
    0.20 :   ffff800010455450:       ldp     x29, x30, [sp, #16]
    1.15 :   ffff800010455454:       ldr     x20, [sp, #40]
    0.20 :   ffff800010455458:       ldr     x22, [sp, #56]
    3.31 :   ffff80001045545c:       ldr     x24, [sp, #72]
    0.00 :   ffff800010455460:       add     sp, sp, #0x50
    0.00 :   ffff800010455464:       ret
         :                      if (!req->bio) {
    0.04 :   ffff800010455468:       ldr     x4, [x22, #56]
    0.00 :   ffff80001045546c:       cbz     x4, ffff800010455438 <blk_update_request+0x130>
         :                      req->__data_len -= total_bytes;
    0.00 :   ffff800010455470:       ldr     w0, [x22, #40]
         :                      blk_rq_is_passthrough():
         :                      return blk_op_is_private(req_op(rq));
         :                      }
         :
         :                      static inline bool blk_rq_is_passthrough(struct request *rq)
         :                      {
         :                      return blk_rq_is_scsi(rq) || blk_rq_is_private(rq);
    0.00 :   ffff800010455474:       ldr     w2, [x22, #24]
         :                      blk_update_request():
    0.00 :   ffff800010455478:       sub     w0, w0, w23
    0.00 :   ffff80001045547c:       str     w0, [x22, #40]
         :                      blk_rq_is_scsi():
         :                      return blk_op_is_scsi(req_op(rq));
    0.00 :   ffff800010455480:       and     w1, w2, #0xff
         :                      blk_rq_is_passthrough():
         :                      return blk_rq_is_scsi(rq) || blk_rq_is_private(rq);
    0.00 :   ffff800010455484:       sub     w1, w1, #0x20
    0.00 :   ffff800010455488:       cmp     w1, #0x3
    0.00 :   ffff80001045548c:       b.ls    ffff8000104554a0 <blk_update_request+0x198>  // b.plast
         :                      blk_update_request():
         :                      req->__sector += total_bytes >> 9;
    0.00 :   ffff800010455490:       ldr     x1, [x22, #48]
    0.00 :   ffff800010455494:       asr     w23, w23, #9
    0.00 :   ffff800010455498:       add     x23, x1, w23, sxtw
    0.00 :   ffff80001045549c:       str     x23, [x22, #48]
         :                      if (req->rq_flags & RQF_MIXED_MERGE) {
    0.00 :   ffff8000104554a0:       ldr     w3, [x22, #28]
    0.00 :   ffff8000104554a4:       tbz     w3, #5, ffff8000104554c0 <blk_update_request+0x1b8>
         :                      req->cmd_flags &= ~REQ_FAILFAST_MASK;
    0.00 :   ffff8000104554a8:       and     w2, w2, #0xfffff8ff
    0.00 :   ffff8000104554ac:       str     w2, [x22, #24]
         :                      req->cmd_flags |= req->bio->bi_opf & REQ_FAILFAST_MASK;
    0.00 :   ffff8000104554b0:       ldr     w1, [x4, #16]
    0.00 :   ffff8000104554b4:       and     w1, w1, #0x700
    0.00 :   ffff8000104554b8:       orr     w2, w1, w2
    0.00 :   ffff8000104554bc:       str     w2, [x22, #24]
         :                      return true;
    0.00 :   ffff8000104554c0:       mov     w1, #0x1                        // #1
         :                      if (!(req->rq_flags & RQF_SPECIAL_PAYLOAD)) {
    0.00 :   ffff8000104554c4:       tbnz    w3, #18, ffff8000104553e8 <blk_update_request+0xe0>
         :                      bio_has_data():
         :                      bio->bi_iter.bi_size &&
    0.00 :   ffff8000104554c8:       ldr     w1, [x4, #40]
         :                      if (bio &&
    0.00 :   ffff8000104554cc:       cbz     w1, ffff80001045555c <blk_update_request+0x254>
         :                      bio_op(bio) != REQ_OP_DISCARD &&
    0.00 :   ffff8000104554d0:       ldrb    w2, [x4, #16]
         :                      bio->bi_iter.bi_size &&
    0.00 :   ffff8000104554d4:       sub     w3, w2, #0x3
         :                      bio_op(bio) != REQ_OP_SECURE_ERASE &&
    0.00 :   ffff8000104554d8:       tst     w3, #0xfffffffd
    0.00 :   ffff8000104554dc:       ccmp    w2, #0x9, #0x4, ne  // ne = any
    0.00 :   ffff8000104554e0:       b.ne    ffff800010455658 <blk_update_request+0x350>  // b.any
         :                      blk_update_request():
         :                      if (blk_rq_bytes(req) < blk_rq_cur_bytes(req)) {
    0.00 :   ffff8000104554e4:       cmp     w0, w1
    0.00 :   ffff8000104554e8:       b.cs    ffff80001045555c <blk_update_request+0x254>  // b.hs, b.nlast
         :                      blk_dump_rq_flags(req, "request botched");
    0.00 :   ffff8000104554ec:       mov     x0, x22
    0.00 :   ffff8000104554f0:       adrp    x1, ffff8000111ab000 <kallsyms_token_index+0x31330>
    0.00 :   ffff8000104554f4:       add     x1, x1, #0x398
    0.00 :   ffff8000104554f8:       bl      ffff800010453f00 <blk_dump_rq_flags>
         :                      req->__data_len = blk_rq_cur_bytes(req);
    0.00 :   ffff8000104554fc:       ldr     x0, [x22, #56]
         :                      blk_rq_cur_bytes():
         :                      return rq->__data_len;
         :                      }
         :
         :                      static inline int blk_rq_cur_bytes(const struct request *rq)
         :                      {
         :                      return rq->bio ? bio_cur_bytes(rq->bio) : 0;
    0.00 :   ffff800010455500:       cbz     x0, ffff800010455558 <blk_update_request+0x250>
         :                      bio_has_data():
         :                      bio->bi_iter.bi_size &&
    0.00 :   ffff800010455504:       ldr     w20, [x0, #40]
         :                      if (bio &&
    0.00 :   ffff800010455508:       cbz     w20, ffff800010455558 <blk_update_request+0x250>
         :                      bio_op(bio) != REQ_OP_DISCARD &&
    0.00 :   ffff80001045550c:       ldrb    w1, [x0, #16]
         :                      bio->bi_iter.bi_size &&
    0.00 :   ffff800010455510:       sub     w2, w1, #0x3
         :                      bio_op(bio) != REQ_OP_SECURE_ERASE &&
    0.00 :   ffff800010455514:       tst     w2, #0xfffffffd
    0.00 :   ffff800010455518:       ccmp    w1, #0x9, #0x4, ne  // ne = any
    0.00 :   ffff80001045551c:       b.eq    ffff800010455558 <blk_update_request+0x250>  // b.none
         :                      bio_cur_bytes():
         :                      return bio_iovec(bio).bv_len;
    0.00 :   ffff800010455520:       ldp     w3, w1, [x0, #44]
    0.00 :   ffff800010455524:       mov     w2, #0x1000                     // #4096
    0.00 :   ffff800010455528:       ldr     x4, [x0, #104]
    0.00 :   ffff80001045552c:       add     x3, x4, x3, lsl #4
    0.00 :   ffff800010455530:       ldp     w0, w3, [x3, #8]
    0.00 :   ffff800010455534:       sub     w0, w0, w1
    0.00 :   ffff800010455538:       add     w1, w1, w3
    0.00 :   ffff80001045553c:       cmp     w0, w20
    0.00 :   ffff800010455540:       and     w1, w1, #0xfff
    0.00 :   ffff800010455544:       sub     w1, w2, w1
    0.00 :   ffff800010455548:       csel    w20, w0, w20, ls  // ls = plast
    0.00 :   ffff80001045554c:       cmp     w1, w20
    0.00 :   ffff800010455550:       csel    w20, w1, w20, ls  // ls = plast
    0.00 :   ffff800010455554:       nop
         :                      blk_update_request():
    0.00 :   ffff800010455558:       str     w20, [x22, #40]
         :                      req->nr_phys_segments = blk_recalc_rq_segments(req);
    0.00 :   ffff80001045555c:       mov     x0, x22
    0.00 :   ffff800010455560:       bl      ffff80001045aea0 <blk_recalc_rq_segments>
    0.00 :   ffff800010455564:       strh    w0, [x22, #194]
         :                      return true;
    0.00 :   ffff800010455568:       mov     w1, #0x1                        // #1
         :                      }
    0.00 :   ffff80001045556c:       mov     w0, w1
    0.00 :   ffff800010455570:       ldr     x19, [x29, #16]
    0.00 :   ffff800010455574:       ldr     x21, [x29, #32]
    0.00 :   ffff800010455578:       ldr     x23, [x29, #48]
    0.00 :   ffff80001045557c:       ldp     x29, x30, [sp, #16]
    0.00 :   ffff800010455580:       ldr     x20, [sp, #40]
    0.00 :   ffff800010455584:       ldr     x22, [sp, #56]
    0.00 :   ffff800010455588:       ldr     x24, [sp, #72]
    0.00 :   ffff80001045558c:       add     sp, sp, #0x50
    0.00 :   ffff800010455590:       ret
         :                      blk_status_to_errno():
         :                      if (WARN_ON_ONCE(idx >= ARRAY_SIZE(blk_errors)))
    0.00 :   ffff800010455594:       brk     #0x800
    0.00 :   ffff800010455598:       b       ffff800010455334 <blk_update_request+0x2c>
         :                      blk_rq_is_scsi():
         :                      return blk_op_is_scsi(req_op(rq));
    0.00 :   ffff80001045559c:       and     w0, w0, #0xff
         :                      blk_op_is_scsi():
         :                      return op == REQ_OP_SCSI_IN || op == REQ_OP_SCSI_OUT;
    0.00 :   ffff8000104555a0:       sub     w1, w0, #0x20
         :                      blk_rq_is_passthrough():
         :                      return blk_rq_is_scsi(rq) || blk_rq_is_private(rq);
    0.00 :   ffff8000104555a4:       cmp     w1, #0x1
    0.00 :   ffff8000104555a8:       b.ls    ffff800010455358 <blk_update_request+0x50>  // b.plast
         :                      blk_op_is_private():
         :                      return op == REQ_OP_DRV_IN || op == REQ_OP_DRV_OUT;
    0.00 :   ffff8000104555ac:       sub     w0, w0, #0x22
         :                      blk_rq_is_passthrough():
         :                      return blk_rq_is_scsi(rq) || blk_rq_is_private(rq);
    0.00 :   ffff8000104555b0:       cmp     w0, #0x1
    0.00 :   ffff8000104555b4:       b.ls    ffff800010455358 <blk_update_request+0x50>  // b.plast
         :                      blk_update_request():
         :                      if (unlikely(error && !blk_rq_is_passthrough(req) &&
    0.00 :   ffff8000104555b8:       ldr     w0, [x22, #28]
    0.00 :   ffff8000104555bc:       tbnz    w0, #11, ffff800010455358 <blk_update_request+0x50>
         :                      print_req_error():
         :                      if (WARN_ON_ONCE(idx >= ARRAY_SIZE(blk_errors)))
    0.00 :   ffff8000104555c0:       cmp     w24, #0xd
    0.00 :   ffff8000104555c4:       b.hi    ffff80001045569c <blk_update_request+0x394>  // b.pmore
         :                      printk_ratelimited(KERN_ERR
    0.00 :   ffff8000104555c8:       adrp    x19, ffff800010d1b000 <x509_machine+0x28>
    0.00 :   ffff8000104555cc:       adrp    x0, ffff8000118c8000 <rng_algs+0x180>
    0.00 :   ffff8000104555d0:       add     x21, x19, #0x3f8
    0.00 :   ffff8000104555d4:       add     x0, x0, #0x4d0
    0.00 :   ffff8000104555d8:       add     x1, x21, #0x218
    0.00 :   ffff8000104555dc:       add     x0, x0, #0x10
    0.00 :   ffff8000104555e0:       bl      ffff800010c9eac8 <___ratelimit>
    0.00 :   ffff8000104555e4:       cbz     w0, ffff800010455358 <blk_update_request+0x50>
    0.00 :   ffff8000104555e8:       ldr     x0, [x22, #160]
    0.00 :   ffff8000104555ec:       add     x1, x21, w24, sxtw #4
    0.00 :   ffff8000104555f0:       ldr     w7, [x22, #24]
    0.00 :   ffff8000104555f4:       adrp    x3, ffff80001119f000 <kallsyms_token_index+0x25330>
    0.00 :   ffff8000104555f8:       cmp     x0, #0x0
    0.00 :   ffff8000104555fc:       add     x3, x3, #0xc30
    0.00 :   ffff800010455600:       and     w5, w7, #0xff
    0.00 :   ffff800010455604:       add     x0, x0, #0xc
    0.00 :   ffff800010455608:       csel    x3, x3, x0, eq  // eq = none
    0.00 :   ffff80001045560c:       ldr     x4, [x22, #48]
         :                      blk_op_str():
         :                      if (op < ARRAY_SIZE(blk_op_name) && blk_op_name[op])
    0.00 :   ffff800010455610:       cmp     w5, #0x23
         :                      print_req_error():
         :                      printk_ratelimited(KERN_ERR
    0.00 :   ffff800010455614:       ldr     x2, [x1, #296]
         :                      blk_op_str():
         :                      if (op < ARRAY_SIZE(blk_op_name) && blk_op_name[op])
    0.00 :   ffff800010455618:       b.hi    ffff800010455690 <blk_update_request+0x388>  // b.pmore
    0.00 :   ffff80001045561c:       mov     w0, w5
    0.00 :   ffff800010455620:       ldr     x6, [x21, x0, lsl #3]
    0.00 :   ffff800010455624:       cbz     x6, ffff800010455690 <blk_update_request+0x388>
         :                      print_req_error():
         :                      printk_ratelimited(KERN_ERR
    0.00 :   ffff800010455628:       ldrh    w8, [x22, #200]
    0.00 :   ffff80001045562c:       add     x1, x19, #0x3f8
    0.00 :   ffff800010455630:       ldrh    w0, [x22, #194]
    0.00 :   ffff800010455634:       and     w7, w7, #0xffffff00
    0.00 :   ffff800010455638:       str     w0, [sp]
    0.00 :   ffff80001045563c:       add     x1, x1, #0x200
    0.00 :   ffff800010455640:       lsr     w8, w8, #13
    0.00 :   ffff800010455644:       str     w8, [sp, #8]
    0.00 :   ffff800010455648:       adrp    x0, ffff8000111ab000 <kallsyms_token_index+0x31330>
    0.00 :   ffff80001045564c:       add     x0, x0, #0x340
    0.00 :   ffff800010455650:       bl      ffff800010148e94 <printk>
    0.00 :   ffff800010455654:       b       ffff800010455358 <blk_update_request+0x50>
         :                      bio_cur_bytes():
    0.00 :   ffff800010455658:       ldp     w5, w2, [x4, #44]
    0.00 :   ffff80001045565c:       mov     w3, #0x1000                     // #4096
    0.00 :   ffff800010455660:       ldr     x6, [x4, #104]
    0.00 :   ffff800010455664:       add     x5, x6, x5, lsl #4
    0.00 :   ffff800010455668:       ldp     w4, w5, [x5, #8]
    0.00 :   ffff80001045566c:       sub     w4, w4, w2
    0.00 :   ffff800010455670:       add     w2, w2, w5
    0.00 :   ffff800010455674:       cmp     w4, w1
    0.00 :   ffff800010455678:       and     w2, w2, #0xfff
    0.00 :   ffff80001045567c:       sub     w2, w3, w2
    0.00 :   ffff800010455680:       csel    w1, w4, w1, ls  // ls = plast
    0.00 :   ffff800010455684:       cmp     w2, w1
    0.00 :   ffff800010455688:       csel    w1, w2, w1, ls  // ls = plast
    0.00 :   ffff80001045568c:       b       ffff8000104554e4 <blk_update_request+0x1dc>
         :                      blk_op_str():
         :                      const char *op_str = "UNKNOWN";
    0.00 :   ffff800010455690:       adrp    x6, ffff8000111ab000 <kallsyms_token_index+0x31330>
    0.00 :   ffff800010455694:       add     x6, x6, #0x128
    0.00 :   ffff800010455698:       b       ffff800010455628 <blk_update_request+0x320>
         :                      print_req_error():
         :                      if (WARN_ON_ONCE(idx >= ARRAY_SIZE(blk_errors)))
    0.00 :   ffff80001045569c:       brk     #0x800
    0.00 :   ffff8000104556a0:       b       ffff800010455358 <blk_update_request+0x50>
 Percent |	Source code & Disassembly of vmlinux for cycles (7772 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000107bd7e8 <nvme_pci_complete_rq>:
         :                      nvme_pci_complete_rq():
         :                      nvme_cleanup_cmd(req);
         :                      return ret;
         :                      }
         :
         :                      static void nvme_pci_complete_rq(struct request *req)
         :                      {
    5.11 :   ffff8000107bd7e8:       stp     x29, x30, [sp, #-32]!
    0.04 :   ffff8000107bd7ec:       mov     x29, sp
    0.64 :   ffff8000107bd7f0:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000107bd7f4:       mov     x19, x0
         :                      struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
         :                      struct nvme_dev *dev = iod->nvmeq->dev;
         :
         :                      if (blk_integrity_rq(req))
    0.48 :   ffff8000107bd7f8:       ldr     w3, [x0, #24]
         :                      struct nvme_dev *dev = iod->nvmeq->dev;
    1.11 :   ffff8000107bd7fc:       ldr     x0, [x0, #312]
   26.34 :   ffff8000107bd800:       ldr     x20, [x0]
         :                      if (blk_integrity_rq(req))
    1.21 :   ffff8000107bd804:       tbnz    w3, #16, ffff8000107bd838 <nvme_pci_complete_rq+0x50>
         :                      blk_rq_nr_phys_segments():
         :                      * own special payload.  In that case we still return 1 here so that this
         :                      * special payload will be mapped.
         :                      */
         :                      static inline unsigned short blk_rq_nr_phys_segments(struct request *rq)
         :                      {
         :                      if (rq->rq_flags & RQF_SPECIAL_PAYLOAD)
   63.34 :   ffff8000107bd808:       ldr     w0, [x19, #28]
    0.00 :   ffff8000107bd80c:       tbnz    w0, #18, ffff8000107bd818 <nvme_pci_complete_rq+0x30>
         :                      nvme_pci_complete_rq():
         :                      dma_unmap_page(dev->dev, iod->meta_dma,
         :                      rq_integrity_vec(req)->bv_len, rq_data_dir(req));
         :                      if (blk_rq_nr_phys_segments(req))
    0.07 :   ffff8000107bd810:       ldrh    w0, [x19, #194]
    0.13 :   ffff8000107bd814:       cbz     w0, ffff8000107bd824 <nvme_pci_complete_rq+0x3c>
         :                      nvme_unmap_data(dev, req);
    0.47 :   ffff8000107bd818:       mov     x1, x19
    0.00 :   ffff8000107bd81c:       mov     x0, x20
    0.00 :   ffff8000107bd820:       bl      ffff8000107bce88 <nvme_unmap_data>
         :                      nvme_complete_rq(req);
    0.06 :   ffff8000107bd824:       mov     x0, x19
    0.00 :   ffff8000107bd828:       bl      ffff8000107b9868 <nvme_complete_rq>
         :                      }
    0.26 :   ffff8000107bd82c:       ldp     x19, x20, [sp, #16]
    0.75 :   ffff8000107bd830:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000107bd834:       ret
         :                      rq_integrity_vec():
         :                      * Return the first bvec that contains integrity data.  Only drivers that are
         :                      * limited to a single integrity segment should use this helper.
         :                      */
         :                      static inline struct bio_vec *rq_integrity_vec(struct request *rq)
         :                      {
         :                      if (WARN_ON_ONCE(queue_max_integrity_segments(rq->q) > 1))
    0.00 :   ffff8000107bd838:       ldr     x2, [x19]
         :                      nvme_pci_complete_rq():
         :                      dma_unmap_page(dev->dev, iod->meta_dma,
    0.00 :   ffff8000107bd83c:       ldr     x1, [x19, #352]
    0.00 :   ffff8000107bd840:       ldr     x0, [x20, #336]
         :                      rq_integrity_vec():
    0.00 :   ffff8000107bd844:       ldrh    w2, [x2, #1130]
    0.00 :   ffff8000107bd848:       cmp     w2, #0x1
    0.00 :   ffff8000107bd84c:       b.hi    ffff8000107bd88c <nvme_pci_complete_rq+0xa4>  // b.pmore
         :                      return NULL;
         :                      return rq->bio->bi_integrity->bip_vec;
    0.00 :   ffff8000107bd850:       ldr     x2, [x19, #56]
    0.00 :   ffff8000107bd854:       ldr     x2, [x2, #88]
    0.00 :   ffff8000107bd858:       ldr     x2, [x2, #96]
         :                      dma_unmap_page_attrs():
         :                      }
         :
         :                      static inline void dma_unmap_page_attrs(struct device *dev, dma_addr_t addr,
         :                      size_t size, enum dma_data_direction dir, unsigned long attrs)
         :                      {
         :                      const struct dma_map_ops *ops = get_dma_ops(dev);
    0.00 :   ffff8000107bd85c:       ldr     x4, [x0, #576]
    0.00 :   ffff8000107bd860:       and     w3, w3, #0x1
         :                      nvme_pci_complete_rq():
    0.00 :   ffff8000107bd864:       ldr     w2, [x2, #8]
         :                      get_dma_ops():
         :                      if (dev->dma_ops)
    0.00 :   ffff8000107bd868:       cbz     x4, ffff8000107bd880 <nvme_pci_complete_rq+0x98>
         :                      dma_unmap_page_attrs():
         :
         :                      BUG_ON(!valid_dma_direction(dir));
         :                      if (dma_is_direct(ops))
         :                      dma_direct_unmap_page(dev, addr, size, dir, attrs);
         :                      else if (ops->unmap_page)
    0.00 :   ffff8000107bd86c:       ldr     x5, [x4, #40]
    0.00 :   ffff8000107bd870:       cbz     x5, ffff8000107bd808 <nvme_pci_complete_rq+0x20>
         :                      ops->unmap_page(dev, addr, size, dir, attrs);
    0.00 :   ffff8000107bd874:       mov     x4, #0x0                        // #0
    0.00 :   ffff8000107bd878:       blr     x5
    0.00 :   ffff8000107bd87c:       b       ffff8000107bd808 <nvme_pci_complete_rq+0x20>
         :                      dma_direct_unmap_page(dev, addr, size, dir, attrs);
    0.00 :   ffff8000107bd880:       mov     x4, #0x0                        // #0
    0.00 :   ffff8000107bd884:       bl      ffff800010161d58 <dma_direct_unmap_page>
    0.00 :   ffff8000107bd888:       b       ffff8000107bd808 <nvme_pci_complete_rq+0x20>
         :                      rq_integrity_vec():
         :                      if (WARN_ON_ONCE(queue_max_integrity_segments(rq->q) > 1))
    0.00 :   ffff8000107bd88c:       brk     #0x800
         :                      return NULL;
    0.00 :   ffff8000107bd890:       mov     x2, #0x0                        // #0
    0.00 :   ffff8000107bd894:       ldr     w3, [x19, #24]
    0.00 :   ffff8000107bd898:       b       ffff8000107bd85c <nvme_pci_complete_rq+0x74>
 Percent |	Source code & Disassembly of vmlinux for cycles (7726 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104551e0 <blk_account_io_completion>:
         :                      blk_account_io_completion():
         :                      }
         :                      EXPORT_SYMBOL_GPL(blk_rq_err_bytes);
         :
         :                      void blk_account_io_completion(struct request *req, unsigned int bytes)
         :                      {
         :                      if (req->part && blk_do_io_stat(req)) {
    0.74 :   ffff8000104551e0:       ldr     x2, [x0, #168]
    0.00 :   ffff8000104551e4:       cbz     x2, ffff8000104552ec <blk_account_io_completion+0x10c>
         :                      blk_do_io_stat():
         :                      *       a) it's attached to a gendisk, and
         :                      *       b) the queue had IO stats enabled when this request was started
         :                      */
         :                      static inline bool blk_do_io_stat(struct request *rq)
         :                      {
         :                      return rq->rq_disk && (rq->rq_flags & RQF_IO_STAT);
    2.29 :   ffff8000104551e8:       ldr     x2, [x0, #160]
    0.00 :   ffff8000104551ec:       cbz     x2, ffff8000104552ec <blk_account_io_completion+0x10c>
    0.58 :   ffff8000104551f0:       ldr     w2, [x0, #28]
    0.00 :   ffff8000104551f4:       tbz     w2, #13, ffff8000104552f0 <blk_account_io_completion+0x110>
         :                      blk_account_io_completion():
         :                      {
    0.88 :   ffff8000104551f8:       stp     x29, x30, [sp, #-48]!
         :                      op_is_write():
         :                      bio->bi_opf = op | op_flags;
         :                      }
         :
         :                      static inline bool op_is_write(unsigned int op)
         :                      {
         :                      return (op & 1);
    0.00 :   ffff8000104551fc:       mov     w2, #0x2                        // #2
         :                      blk_account_io_completion():
    0.00 :   ffff800010455200:       mov     x29, sp
    0.88 :   ffff800010455204:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010455208:       mov     x19, x0
    0.33 :   ffff80001045520c:       str     x21, [sp, #32]
    0.00 :   ffff800010455210:       mov     w21, w1
         :                      const int sgrp = op_stat_group(req_op(req));
    0.19 :   ffff800010455214:       ldr     w0, [x0, #24]
    0.00 :   ffff800010455218:       and     w1, w0, #0xff
         :                      op_is_write():
    0.00 :   ffff80001045521c:       and     w0, w0, #0x1
    0.00 :   ffff800010455220:       cmp     w1, #0x3
    1.53 :   ffff800010455224:       csel    w20, w0, w2, ne  // ne = any
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff800010455228:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    2.17 :   ffff80001045522c:       mrs     x1, sp_el0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.40 :   ffff800010455230:       ldr     w0, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff800010455234:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    1.31 :   ffff800010455238:       str     w0, [x1, #16]
         :                      blk_account_io_completion():
         :                      struct hd_struct *part;
         :
         :                      part_stat_lock();
         :                      part = req->part;
         :                      part_stat_add(part, sectors[sgrp], bytes >> 9);
    0.00 :   ffff80001045523c:       adrp    x3, ffff8000114ca000 <bp_hardening_data>
         :                      __my_cpu_offset():
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
         :                      "mrs %0, tpidr_el2",
         :                      ARM64_HAS_VIRT_HOST_EXTN)
         :                      : "=r" (off) :
         :                      "Q" (*(const unsigned long *)current_stack_pointer));
    0.03 :   ffff800010455240:       mov     x7, sp
         :                      blk_account_io_completion():
    0.00 :   ffff800010455244:       add     x0, x3, #0x18
    2.04 :   ffff800010455248:       adrp    x6, ffff800011899000 <page_wait_table+0x1500>
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.43 :   ffff80001045524c:       mrs     x1, tpidr_el1
         :                      blk_account_io_completion():
    0.05 :   ffff800010455250:       ldrsw   x8, [x0, x1]
    0.00 :   ffff800010455254:       add     x4, x6, #0x8e8
         :                      part = req->part;
    7.44 :   ffff800010455258:       ldr     x5, [x19, #168]
    0.00 :   ffff80001045525c:       sbfiz   x2, x20, #3, #32
         :                      part_stat_add(part, sectors[sgrp], bytes >> 9);
    0.05 :   ffff800010455260:       lsr     w1, w21, #9
    0.41 :   ffff800010455264:       ldr     x4, [x4, x8, lsl #3]
   10.06 :   ffff800010455268:       ldr     x0, [x5, #840]
    0.00 :   ffff80001045526c:       add     x0, x0, x4
    2.02 :   ffff800010455270:       add     x0, x0, x2
    4.59 :   ffff800010455274:       ldr     x4, [x0, #32]
    0.00 :   ffff800010455278:       add     x4, x4, x1
   29.52 :   ffff80001045527c:       str     x4, [x0, #32]
    2.62 :   ffff800010455280:       ldr     w0, [x5, #820]
    0.00 :   ffff800010455284:       cbz     w0, ffff8000104552c0 <blk_account_io_completion+0xe0>
         :                      part_to_disk():
         :                      struct lockdep_map lockdep_map;
         :                      };
         :
         :                      static inline struct gendisk *part_to_disk(struct hd_struct *part)
         :                      {
         :                      if (likely(part)) {
    0.08 :   ffff800010455288:       cbz     x5, ffff800010455300 <blk_account_io_completion+0x120>
         :                      if (part->partno)
         :                      return dev_to_disk(part_to_dev(part)->parent);
    0.01 :   ffff80001045528c:       ldr     x0, [x5, #104]
    0.00 :   ffff800010455290:       sub     x0, x0, #0x70
         :                      __my_cpu_offset():
    1.27 :   ffff800010455294:       mrs     x4, tpidr_el1
         :                      blk_account_io_completion():
    0.54 :   ffff800010455298:       add     x3, x3, #0x18
    0.00 :   ffff80001045529c:       add     x6, x6, #0x8e8
    0.00 :   ffff8000104552a0:       ldrsw   x3, [x3, x4]
    0.09 :   ffff8000104552a4:       ldr     x0, [x0, #912]
    6.96 :   ffff8000104552a8:       ldr     x3, [x6, x3, lsl #3]
    0.05 :   ffff8000104552ac:       add     x0, x0, x3
    0.00 :   ffff8000104552b0:       add     x2, x0, x2
    0.27 :   ffff8000104552b4:       ldr     x0, [x2, #32]
    0.01 :   ffff8000104552b8:       add     x1, x0, x1
   10.62 :   ffff8000104552bc:       str     x1, [x2, #32]
         :                      get_current():
    1.19 :   ffff8000104552c0:       mrs     x1, sp_el0
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.31 :   ffff8000104552c4:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000104552c8:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.22 :   ffff8000104552cc:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000104552d0:       cbnz    x0, ffff8000104552f4 <blk_account_io_completion+0x114>
         :                      blk_account_io_completion():
         :                      part_stat_unlock();
    0.00 :   ffff8000104552d4:       bl      ffff800010cad640 <preempt_schedule>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.48 :   ffff8000104552d8:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_account_io_completion():
         :                      }
         :                      }
    0.13 :   ffff8000104552dc:       ldr     x21, [sp, #32]
    0.50 :   ffff8000104552e0:       ldp     x19, x20, [sp, #16]
    0.25 :   ffff8000104552e4:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000104552e8:       ret
    0.00 :   ffff8000104552ec:       ret
    0.00 :   ffff8000104552f0:       ret
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    3.86 :   ffff8000104552f4:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000104552f8:       cbz     x0, ffff8000104552d4 <blk_account_io_completion+0xf4>
    2.59 :   ffff8000104552fc:       b       ffff8000104552d8 <blk_account_io_completion+0xf8>
         :                      part_to_disk():
         :                      else
         :                      return dev_to_disk(part_to_dev(part));
         :                      }
         :                      return NULL;
    0.00 :   ffff800010455300:       mov     x0, #0x0                        // #0
    0.00 :   ffff800010455304:       b       ffff800010455294 <blk_account_io_completion+0xb4>
 Percent |	Source code & Disassembly of vmlinux for cycles (7066 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001044fa80 <bio_uninit>:
         :                      bio_uninit():
         :                      (*idx)++;
         :                      return bvl;
         :                      }
         :
         :                      void bio_uninit(struct bio *bio)
         :                      {
    0.60 :   ffff80001044fa80:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff80001044fa84:       mov     x29, sp
    4.35 :   ffff80001044fa88:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001044fa8c:       mov     x19, x0
         :                      bio_disassociate_blkg():
         :                      *
         :                      * Helper to disassociate the blkg from @bio if a blkg is associated.
         :                      */
         :                      void bio_disassociate_blkg(struct bio *bio)
         :                      {
         :                      if (bio->bi_blkg) {
    0.90 :   ffff80001044fa90:       ldr     x20, [x0, #72]
    0.00 :   ffff80001044fa94:       cbz     x20, ffff80001044faf8 <bio_uninit+0x78>
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    1.12 :   ffff80001044fa98:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.62 :   ffff80001044fa9c:       ldr     x0, [x20, #72]
         :                      __ref_is_percpu():
         :                      * Theoretically, the following could test just ATOMIC; however,
         :                      * then we'd have to mask off DEAD separately as DEAD may be
         :                      * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
         :                      * implies ATOMIC anyway.  Test them together.
         :                      */
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.13 :   ffff80001044faa0:       tst     x0, #0x3
    0.00 :   ffff80001044faa4:       b.ne    ffff80001044fb1c <bio_uninit+0x9c>  // b.any
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
   22.65 :   ffff80001044faa8:       mrs     x1, sp_el0
         :                      __read_once_size():
    2.03 :   ffff80001044faac:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff80001044fab0:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.30 :   ffff80001044fab4:       str     w2, [x1, #16]
         :                      __percpu_add_case_64():
         :
         :                      PERCPU_RW_OPS(8)
         :                      PERCPU_RW_OPS(16)
         :                      PERCPU_RW_OPS(32)
         :                      PERCPU_RW_OPS(64)
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff80001044fab8:       mov     x3, #0xffffffffffffffff         // #-1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff80001044fabc:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff80001044fac0:       add     x0, x0, x2
    2.48 :   ffff80001044fac4:       ldxr    x5, [x0]
   32.14 :   ffff80001044fac8:       add     x5, x5, x3
    0.00 :   ffff80001044facc:       stxr    w4, x5, [x0]
    0.00 :   ffff80001044fad0:       cbnz    w4, ffff80001044fac4 <bio_uninit+0x44>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.85 :   ffff80001044fad4:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.01 :   ffff80001044fad8:       add     x0, x0, x3
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    2.86 :   ffff80001044fadc:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff80001044fae0:       cbz     x0, ffff80001044faec <bio_uninit+0x6c>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    9.43 :   ffff80001044fae4:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff80001044fae8:       cbnz    x0, ffff80001044faf0 <bio_uninit+0x70>
         :                      percpu_ref_put_many():
         :                      unsigned long __percpu *percpu_count;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count))
         :                      this_cpu_sub(*percpu_count, nr);
    0.00 :   ffff80001044faec:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
   11.75 :   ffff80001044faf0:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      bio_disassociate_blkg():
         :                      blkg_put(bio->bi_blkg);
         :                      bio->bi_blkg = NULL;
    0.07 :   ffff80001044faf4:       str     xzr, [x19, #72]
         :                      bio_integrity():
         :
         :                      #if defined(CONFIG_BLK_DEV_INTEGRITY)
         :
         :                      static inline struct bio_integrity_payload *bio_integrity(struct bio *bio)
         :                      {
         :                      if (bio->bi_opf & REQ_INTEGRITY)
    2.25 :   ffff80001044faf8:       ldr     w0, [x19, #16]
    0.00 :   ffff80001044fafc:       tbz     w0, #16, ffff80001044fb10 <bio_uninit+0x90>
         :                      bio_uninit():
         :                      if (bio_integrity(bio))
    0.00 :   ffff80001044fb00:       ldr     x0, [x19, #88]
    0.00 :   ffff80001044fb04:       cbz     x0, ffff80001044fb10 <bio_uninit+0x90>
         :                      bio_integrity_free(bio);
    0.00 :   ffff80001044fb08:       mov     x0, x19
    0.00 :   ffff80001044fb0c:       bl      ffff80001047aa58 <bio_integrity_free>
         :                      }
    2.36 :   ffff80001044fb10:       ldp     x19, x20, [sp, #16]
    3.12 :   ffff80001044fb14:       ldp     x29, x30, [sp], #32
    0.00 :   ffff80001044fb18:       ret
         :                      blkg_put():
         :                      * blkg_put - put a blkg reference
         :                      * @blkg: blkg to put
         :                      */
         :                      static inline void blkg_put(struct blkcg_gq *blkg)
         :                      {
         :                      percpu_ref_put(&blkg->refcnt);
    0.00 :   ffff80001044fb1c:       add     x0, x20, #0x40
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff80001044fb20:       b       ffff80001044fb48 <bio_uninit+0xc8>
    0.00 :   ffff80001044fb24:       b       ffff80001044fb48 <bio_uninit+0xc8>
         :                      __lse_atomic64_sub_return():
         :                      }
         :
         :                      ATOMIC64_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC64_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff80001044fb28:       mov     x1, #0x1                        // #1
    0.00 :   ffff80001044fb2c:       neg     x1, x1
    0.00 :   ffff80001044fb30:       ldaddal x1, x2, [x0]
    0.00 :   ffff80001044fb34:       add     x1, x1, x2
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff80001044fb38:       cbnz    x1, ffff80001044faf0 <bio_uninit+0x70>
         :                      ref->release(ref);
    0.00 :   ffff80001044fb3c:       ldr     x1, [x0, #16]
    0.00 :   ffff80001044fb40:       blr     x1
    0.00 :   ffff80001044fb44:       b       ffff80001044faf0 <bio_uninit+0x70>
         :                      __ll_sc_atomic64_sub_return():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(add, add, I)
         :                      ATOMIC64_OPS(sub, sub, J)
    0.00 :   ffff80001044fb48:       mov     x2, #0x1                        // #1
    0.00 :   ffff80001044fb4c:       add     x4, x20, #0x40
    0.00 :   ffff80001044fb50:       b       ffff8000104520dc <bio_associate_blkg_from_page+0xa4>
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff80001044fb54:       cbnz    x1, ffff80001044faf0 <bio_uninit+0x70>
    0.00 :   ffff80001044fb58:       b       ffff80001044fb3c <bio_uninit+0xbc>
 Percent |	Source code & Disassembly of vmlinux for cycles (6817 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010451ac0 <bio_check_pages_dirty>:
         :                      bio_check_pages_dirty():
         :                      bio_put(bio);
         :                      }
         :                      }
         :
         :                      void bio_check_pages_dirty(struct bio *bio)
         :                      {
    0.00 :   ffff800010451ac0:       stp     x29, x30, [sp, #-48]!
         :                      bvec_init_iter_all():
         :                      .bi_bvec_done   = 0,                                            \
         :                      }
         :
         :                      static inline struct bio_vec *bvec_init_iter_all(struct bvec_iter_all *iter_all)
         :                      {
         :                      iter_all->done = 0;
    0.00 :   ffff800010451ac4:       mov     w2, #0x0                        // #0
         :                      iter_all->idx = 0;
    0.00 :   ffff800010451ac8:       mov     w4, #0x0                        // #0
    0.00 :   ffff800010451acc:       mov     w8, #0x1000                     // #4096
         :                      bio_check_pages_dirty():
    0.18 :   ffff800010451ad0:       mov     x29, sp
    2.20 :   ffff800010451ad4:       str     x19, [sp, #16]
    0.00 :   ffff800010451ad8:       mov     x19, x0
         :                      bio_next_segment():
         :                      }
         :
         :                      static inline bool bio_next_segment(const struct bio *bio,
         :                      struct bvec_iter_all *iter)
         :                      {
         :                      if (iter->idx >= bio->bi_vcnt)
    0.26 :   ffff800010451adc:       ldrh    w7, [x0, #96]
         :                      return false;
         :
         :                      bvec_advance(&bio->bi_io_vec[iter->idx], iter);
   23.22 :   ffff800010451ae0:       sbfiz   x0, x4, #4, #32
         :                      if (iter->idx >= bio->bi_vcnt)
    0.00 :   ffff800010451ae4:       cmp     w7, w4
    0.00 :   ffff800010451ae8:       b.le    ffff800010451bc0 <bio_check_pages_dirty+0x100>
         :                      bvec_advance(&bio->bi_io_vec[iter->idx], iter);
    0.09 :   ffff800010451aec:       ldr     x6, [x19, #104]
         :                      bvec_advance():
         :                      struct bvec_iter_all *iter_all)
         :                      {
         :                      struct bio_vec *bv = &iter_all->bv;
         :
         :                      if (iter_all->done) {
         :                      bv->bv_page++;
    0.00 :   ffff800010451af0:       add     x1, x1, #0x40
    0.00 :   ffff800010451af4:       mov     w3, #0x1000                     // #4096
         :                      bio_next_segment():
    0.00 :   ffff800010451af8:       add     x5, x6, x0
         :                      bvec_advance():
         :                      if (iter_all->done) {
    0.32 :   ffff800010451afc:       cbnz    w2, ffff800010451b18 <bio_check_pages_dirty+0x58>
         :                      bv->bv_offset = 0;
         :                      } else {
         :                      bv->bv_page = bvec->bv_page + (bvec->bv_offset >> PAGE_SHIFT);
    1.67 :   ffff800010451b00:       ldr     x1, [x6, x0]
    0.69 :   ffff800010451b04:       ldr     w0, [x5, #12]
         :                      bv->bv_offset = bvec->bv_offset & ~PAGE_MASK;
    0.00 :   ffff800010451b08:       and     w3, w0, #0xfff
         :                      bv->bv_page = bvec->bv_page + (bvec->bv_offset >> PAGE_SHIFT);
    0.00 :   ffff800010451b0c:       lsr     w0, w0, #12
    0.00 :   ffff800010451b10:       sub     w3, w8, w3
    0.15 :   ffff800010451b14:       add     x1, x1, x0, lsl #6
         :                      }
         :                      bv->bv_len = min_t(unsigned int, PAGE_SIZE - bv->bv_offset,
    0.13 :   ffff800010451b18:       ldr     w5, [x5, #8]
    0.01 :   ffff800010451b1c:       sub     w0, w5, w2
    0.00 :   ffff800010451b20:       cmp     w0, w3
    0.00 :   ffff800010451b24:       csel    w0, w0, w3, ls  // ls = plast
         :                      bvec->bv_len - iter_all->done);
         :                      iter_all->done += bv->bv_len;
    3.47 :   ffff800010451b28:       add     w2, w2, w0
         :
         :                      if (iter_all->done == bvec->bv_len) {
    0.00 :   ffff800010451b2c:       cmp     w5, w2
    0.00 :   ffff800010451b30:       b.ne    ffff800010451b3c <bio_check_pages_dirty+0x7c>  // b.any
         :                      iter_all->idx++;
    0.00 :   ffff800010451b34:       add     w4, w4, #0x1
         :                      iter_all->done = 0;
    0.00 :   ffff800010451b38:       mov     w2, #0x0                        // #0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    1.99 :   ffff800010451b3c:       ldr     x3, [x1, #8]
         :                      compound_head():
         :                      {
         :                      unsigned long head = READ_ONCE(page->compound_head);
         :
         :                      if (unlikely(head & 1))
         :                      return (struct page *) (head - 1);
         :                      return page;
    0.48 :   ffff800010451b40:       sub     x0, x3, #0x1
    0.00 :   ffff800010451b44:       tst     x3, #0x1
    0.00 :   ffff800010451b48:       csel    x0, x0, x1, ne  // ne = any
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
   62.20 :   ffff800010451b4c:       ldr     x0, [x0]
         :                      bio_check_pages_dirty():
         :                      struct bio_vec *bvec;
         :                      unsigned long flags;
         :                      struct bvec_iter_all iter_all;
         :
         :                      bio_for_each_segment_all(bvec, bio, iter_all) {
         :                      if (!PageDirty(bvec->bv_page) && !PageCompound(bvec->bv_page))
    0.00 :   ffff800010451b50:       tbnz    w0, #3, ffff800010451ae0 <bio_check_pages_dirty+0x20>
         :                      test_bit():
    0.00 :   ffff800010451b54:       ldr     x0, [x1]
         :                      PageCompound():
         :                      return READ_ONCE(page->compound_head) & 1;
         :                      }
         :
         :                      static __always_inline int PageCompound(struct page *page)
         :                      {
         :                      return test_bit(PG_head, &page->flags) || PageTail(page);
    0.00 :   ffff800010451b58:       tbnz    w0, #16, ffff800010451ae0 <bio_check_pages_dirty+0x20>
         :                      __read_once_size():
    0.00 :   ffff800010451b5c:       ldr     x0, [x1, #8]
         :                      PageCompound():
    0.00 :   ffff800010451b60:       tbnz    w0, #0, ffff800010451ae0 <bio_check_pages_dirty+0x20>
    0.00 :   ffff800010451b64:       stp     x20, x21, [x29, #24]
         :                      bio_check_pages_dirty():
         :
         :                      bio_release_pages(bio, false);
         :                      bio_put(bio);
         :                      return;
         :                      defer:
         :                      spin_lock_irqsave(&bio_dirty_lock, flags);
    0.00 :   ffff800010451b68:       adrp    x20, ffff800011abc000 <drbg_algs+0x2b80>
    0.00 :   ffff800010451b6c:       add     x20, x20, #0xa0
    0.00 :   ffff800010451b70:       add     x21, x20, #0x188
    0.00 :   ffff800010451b74:       mov     x0, x21
    0.00 :   ffff800010451b78:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
         :                      bio->bi_private = bio_dirty_list;
    0.00 :   ffff800010451b7c:       ldr     x2, [x20, #400]
         :                      spin_unlock_irqrestore():
         :                      raw_spin_unlock_irq(&lock->rlock);
         :                      }
         :
         :                      static __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
         :                      {
         :                      raw_spin_unlock_irqrestore(&lock->rlock, flags);
    0.00 :   ffff800010451b80:       mov     x1, x0
         :                      bio_check_pages_dirty():
    0.00 :   ffff800010451b84:       str     x2, [x19, #64]
         :                      spin_unlock_irqrestore():
    0.00 :   ffff800010451b88:       mov     x0, x21
         :                      bio_check_pages_dirty():
         :                      bio_dirty_list = bio;
    0.00 :   ffff800010451b8c:       str     x19, [x20, #400]
         :                      spin_unlock_irqrestore():
    0.00 :   ffff800010451b90:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      schedule_work():
         :                      * queued and leaves it in the same position on the kernel-global
         :                      * workqueue otherwise.
         :                      */
         :                      static inline bool schedule_work(struct work_struct *work)
         :                      {
         :                      return queue_work(system_wq, work);
    0.00 :   ffff800010451b94:       adrp    x0, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      queue_work():
         :                      return queue_work_on(WORK_CPU_UNBOUND, wq, work);
    0.00 :   ffff800010451b98:       adrp    x2, ffff8000118c8000 <rng_algs+0x180>
    0.00 :   ffff800010451b9c:       add     x2, x2, #0x448
    0.00 :   ffff800010451ba0:       ldr     x1, [x0, #432]
    0.00 :   ffff800010451ba4:       add     x2, x2, #0x20
    0.00 :   ffff800010451ba8:       mov     w0, #0x100                      // #256
    0.00 :   ffff800010451bac:       bl      ffff800010103da8 <queue_work_on>
         :                      bio_check_pages_dirty():
         :                      spin_unlock_irqrestore(&bio_dirty_lock, flags);
         :                      schedule_work(&bio_dirty_work);
         :                      }
    0.00 :   ffff800010451bb0:       ldr     x19, [sp, #16]
    0.00 :   ffff800010451bb4:       ldp     x20, x21, [x29, #24]
    0.00 :   ffff800010451bb8:       ldp     x29, x30, [sp], #48
    0.00 :   ffff800010451bbc:       ret
         :                      bio_release_pages():
         :                      if (bio_flagged(bio, BIO_NO_PAGE_REF))
    2.21 :   ffff800010451bc0:       ldrh    w0, [x19, #20]
    0.00 :   ffff800010451bc4:       tbz     w0, #0, ffff800010451bdc <bio_check_pages_dirty+0x11c>
         :                      bio_check_pages_dirty():
         :                      bio_put(bio);
    0.01 :   ffff800010451bc8:       mov     x0, x19
    0.00 :   ffff800010451bcc:       bl      ffff800010450020 <bio_put>
         :                      }
    0.38 :   ffff800010451bd0:       ldr     x19, [sp, #16]
    0.24 :   ffff800010451bd4:       ldp     x29, x30, [sp], #48
    0.00 :   ffff800010451bd8:       ret
         :                      bio_release_pages():
    0.07 :   ffff800010451bdc:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010451be0:       mov     x0, x19
    0.00 :   ffff800010451be4:       bl      ffff80001044f238 <bio_release_pages.part.42>
    0.03 :   ffff800010451be8:       b       ffff800010451bc8 <bio_check_pages_dirty+0x108>
 Percent |	Source code & Disassembly of vmlinux for cycles (6630 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000107bce88 <nvme_unmap_data>:
         :                      nvme_unmap_data():
         :                      return false;
         :                      return true;
         :                      }
         :
         :                      static void nvme_unmap_data(struct nvme_dev *dev, struct request *req)
         :                      {
    0.54 :   ffff8000107bce88:       stp     x29, x30, [sp, #-64]!
    0.51 :   ffff8000107bce8c:       mov     x29, sp
    1.23 :   ffff8000107bce90:       str     x19, [sp, #16]
         :                      struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
         :                      const int last_prp = dev->ctrl.page_size / sizeof(__le64) - 1;
         :                      dma_addr_t dma_addr = iod->first_dma, next_dma_addr;
    0.00 :   ffff8000107bce94:       add     x19, x1, #0x118
         :                      {
    0.03 :   ffff8000107bce98:       stp     x21, x22, [sp, #32]
    0.03 :   ffff8000107bce9c:       mov     x21, x1
    0.48 :   ffff8000107bcea0:       str     x24, [sp, #56]
    0.02 :   ffff8000107bcea4:       mov     x22, x0
         :                      int i;
         :
         :                      if (iod->dma_len) {
    0.53 :   ffff8000107bcea8:       ldr     w2, [x19, #64]
         :                      dma_addr_t dma_addr = iod->first_dma, next_dma_addr;
    4.69 :   ffff8000107bceac:       ldr     x24, [x19, #56]
         :                      if (iod->dma_len) {
    0.00 :   ffff8000107bceb0:       cbz     w2, ffff8000107bcefc <nvme_unmap_data+0x74>
         :                      dma_unmap_page(dev->dev, dma_addr, iod->dma_len,
    0.08 :   ffff8000107bceb4:       ldr     x0, [x0, #336]
    4.32 :   ffff8000107bceb8:       mov     w2, w2
         :                      op_is_write():
         :                      bio->bi_opf = op | op_flags;
         :                      }
         :
         :                      static inline bool op_is_write(unsigned int op)
         :                      {
         :                      return (op & 1);
   20.65 :   ffff8000107bcebc:       ldr     w3, [x1, #24]
         :                      dma_unmap_page_attrs():
         :                      }
         :
         :                      static inline void dma_unmap_page_attrs(struct device *dev, dma_addr_t addr,
         :                      size_t size, enum dma_data_direction dir, unsigned long attrs)
         :                      {
         :                      const struct dma_map_ops *ops = get_dma_ops(dev);
    0.06 :   ffff8000107bcec0:       ldr     x1, [x0, #576]
         :                      nvme_unmap_data():
    8.28 :   ffff8000107bcec4:       tst     x3, #0x1
    1.43 :   ffff8000107bcec8:       cset    w3, eq  // eq = none
    0.15 :   ffff8000107bcecc:       add     w3, w3, #0x1
         :                      get_dma_ops():
         :                      if (dev->dma_ops)
   17.23 :   ffff8000107bced0:       cbz     x1, ffff8000107bcf48 <nvme_unmap_data+0xc0>
         :                      dma_unmap_page_attrs():
         :
         :                      BUG_ON(!valid_dma_direction(dir));
         :                      if (dma_is_direct(ops))
         :                      dma_direct_unmap_page(dev, addr, size, dir, attrs);
         :                      else if (ops->unmap_page)
    0.03 :   ffff8000107bced4:       ldr     x5, [x1, #40]
    6.75 :   ffff8000107bced8:       cbz     x5, ffff8000107bcee8 <nvme_unmap_data+0x60>
         :                      ops->unmap_page(dev, addr, size, dir, attrs);
   29.12 :   ffff8000107bcedc:       mov     x4, #0x0                        // #0
    0.11 :   ffff8000107bcee0:       mov     x1, x24
    1.08 :   ffff8000107bcee4:       blr     x5
         :                      nvme_unmap_data():
         :                      dma_pool_free(dev->prp_page_pool, addr, dma_addr);
         :                      dma_addr = next_dma_addr;
         :                      }
         :
         :                      mempool_free(iod->sg, dev->iod_mempool);
         :                      }
    0.39 :   ffff8000107bcee8:       ldr     x19, [sp, #16]
    1.06 :   ffff8000107bceec:       ldp     x21, x22, [sp, #32]
    1.16 :   ffff8000107bcef0:       ldr     x24, [sp, #56]
    0.05 :   ffff8000107bcef4:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000107bcef8:       ret
    0.00 :   ffff8000107bcefc:       str     x23, [x29, #48]
         :                      WARN_ON_ONCE(!iod->nents);
    0.00 :   ffff8000107bcf00:       ldr     w2, [x19, #52]
         :                      const int last_prp = dev->ctrl.page_size / sizeof(__le64) - 1;
    0.00 :   ffff8000107bcf04:       ldr     w23, [x0, #1720]
         :                      WARN_ON_ONCE(!iod->nents);
    0.00 :   ffff8000107bcf08:       cbz     w2, ffff8000107bd04c <nvme_unmap_data+0x1c4>
         :                      dma_unmap_sg(dev->dev, iod->sg, iod->nents, rq_dma_dir(req));
    0.00 :   ffff8000107bcf0c:       ldr     x0, [x22, #336]
         :                      op_is_write():
    0.00 :   ffff8000107bcf10:       ldr     w3, [x21, #24]
         :                      nvme_unmap_data():
         :                      if (is_pci_p2pdma_page(sg_page(iod->sg)))
    0.00 :   ffff8000107bcf14:       ldr     x4, [x19, #80]
         :                      dma_unmap_sg_attrs():
         :
         :                      static inline void dma_unmap_sg_attrs(struct device *dev, struct scatterlist *sg,
         :                      int nents, enum dma_data_direction dir,
         :                      unsigned long attrs)
         :                      {
         :                      const struct dma_map_ops *ops = get_dma_ops(dev);
    0.00 :   ffff8000107bcf18:       ldr     x1, [x0, #576]
         :                      nvme_unmap_data():
         :                      dma_unmap_sg(dev->dev, iod->sg, iod->nents, rq_dma_dir(req));
    0.00 :   ffff8000107bcf1c:       tst     x3, #0x1
    0.00 :   ffff8000107bcf20:       cset    w3, eq  // eq = none
    0.00 :   ffff8000107bcf24:       add     w3, w3, #0x1
         :                      get_dma_ops():
         :                      if (dev->dma_ops)
    0.00 :   ffff8000107bcf28:       cbz     x1, ffff8000107bcf68 <nvme_unmap_data+0xe0>
         :                      dma_unmap_sg_attrs():
         :
         :                      BUG_ON(!valid_dma_direction(dir));
         :                      debug_dma_unmap_sg(dev, sg, nents, dir);
         :                      if (dma_is_direct(ops))
         :                      dma_direct_unmap_sg(dev, sg, nents, dir, attrs);
         :                      else if (ops->unmap_sg)
    0.00 :   ffff8000107bcf2c:       ldr     x5, [x1, #56]
    0.00 :   ffff8000107bcf30:       cbz     x5, ffff8000107bcf78 <nvme_unmap_data+0xf0>
         :                      ops->unmap_sg(dev, sg, nents, dir, attrs);
    0.00 :   ffff8000107bcf34:       mov     x1, x4
    0.00 :   ffff8000107bcf38:       mov     x4, #0x0                        // #0
    0.00 :   ffff8000107bcf3c:       blr     x5
    0.00 :   ffff8000107bcf40:       ldr     x4, [x19, #80]
    0.00 :   ffff8000107bcf44:       b       ffff8000107bcf78 <nvme_unmap_data+0xf0>
         :                      dma_unmap_page_attrs():
         :                      dma_direct_unmap_page(dev, addr, size, dir, attrs);
    0.00 :   ffff8000107bcf48:       mov     x1, x24
    0.00 :   ffff8000107bcf4c:       mov     x4, #0x0                        // #0
    0.00 :   ffff8000107bcf50:       bl      ffff800010161d58 <dma_direct_unmap_page>
         :                      nvme_unmap_data():
         :                      }
    0.00 :   ffff8000107bcf54:       ldr     x19, [sp, #16]
    0.00 :   ffff8000107bcf58:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000107bcf5c:       ldr     x24, [sp, #56]
    0.00 :   ffff8000107bcf60:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000107bcf64:       ret
         :                      dma_unmap_sg_attrs():
         :                      dma_direct_unmap_sg(dev, sg, nents, dir, attrs);
    0.00 :   ffff8000107bcf68:       mov     x1, x4
    0.00 :   ffff8000107bcf6c:       mov     x4, #0x0                        // #0
    0.00 :   ffff8000107bcf70:       bl      ffff800010161de0 <dma_direct_unmap_sg>
    0.00 :   ffff8000107bcf74:       ldr     x4, [x19, #80]
         :                      nvme_unmap_data():
         :                      if (iod->npages == 0)
    0.00 :   ffff8000107bcf78:       ldr     w0, [x19, #48]
    0.00 :   ffff8000107bcf7c:       cbnz    w0, ffff8000107bcfa4 <nvme_unmap_data+0x11c>
         :                      blk_rq_nr_phys_segments():
         :                      * own special payload.  In that case we still return 1 here so that this
         :                      * special payload will be mapped.
         :                      */
         :                      static inline unsigned short blk_rq_nr_phys_segments(struct request *rq)
         :                      {
         :                      if (rq->rq_flags & RQF_SPECIAL_PAYLOAD)
    0.00 :   ffff8000107bcf80:       ldr     w2, [x21, #28]
    0.00 :   ffff8000107bcf84:       mov     x1, #0x20                       // #32
         :                      nvme_unmap_data():
         :                      dma_pool_free(dev->prp_small_pool, nvme_pci_iod_list(req)[0],
    0.00 :   ffff8000107bcf88:       ldr     x0, [x22, #352]
         :                      blk_rq_nr_phys_segments():
    0.00 :   ffff8000107bcf8c:       tbz     w2, #18, ffff8000107bd040 <nvme_unmap_data+0x1b8>
         :                      nvme_unmap_data():
    0.00 :   ffff8000107bcf90:       ldr     x1, [x4, x1]
    0.00 :   ffff8000107bcf94:       mov     x2, x24
    0.00 :   ffff8000107bcf98:       bl      ffff800010238390 <dma_pool_free>
    0.00 :   ffff8000107bcf9c:       ldr     w0, [x19, #48]
    0.00 :   ffff8000107bcfa0:       ldr     x4, [x19, #80]
         :                      for (i = 0; i < iod->npages; i++) {
    0.00 :   ffff8000107bcfa4:       cmp     w0, #0x0
    0.00 :   ffff8000107bcfa8:       b.le    ffff8000107bd01c <nvme_unmap_data+0x194>
         :                      const int last_prp = dev->ctrl.page_size / sizeof(__le64) - 1;
    0.00 :   ffff8000107bcfac:       lsr     w23, w23, #3
    0.00 :   ffff8000107bcfb0:       str     x20, [x29, #24]
    0.00 :   ffff8000107bcfb4:       sub     w23, w23, #0x1
         :                      next_dma_addr = le64_to_cpu(prp_list[last_prp]);
    0.00 :   ffff8000107bcfb8:       mov     w20, #0x0                       // #0
    0.00 :   ffff8000107bcfbc:       sbfiz   x23, x23, #3, #32
    0.00 :   ffff8000107bcfc0:       b       ffff8000107bcfec <nvme_unmap_data+0x164>
         :                      next_dma_addr =
    0.00 :   ffff8000107bcfc4:       ldr     x3, [x1, #4080]
         :                      dma_pool_free(dev->prp_page_pool, addr, dma_addr);
    0.00 :   ffff8000107bcfc8:       ldr     x0, [x22, #344]
    0.00 :   ffff8000107bcfcc:       mov     x2, x24
         :                      dma_addr = next_dma_addr;
    0.00 :   ffff8000107bcfd0:       mov     x24, x3
         :                      for (i = 0; i < iod->npages; i++) {
    0.00 :   ffff8000107bcfd4:       add     w20, w20, #0x1
         :                      dma_pool_free(dev->prp_page_pool, addr, dma_addr);
    0.00 :   ffff8000107bcfd8:       bl      ffff800010238390 <dma_pool_free>
         :                      for (i = 0; i < iod->npages; i++) {
    0.00 :   ffff8000107bcfdc:       ldr     w0, [x19, #48]
    0.00 :   ffff8000107bcfe0:       ldr     x4, [x19, #80]
    0.00 :   ffff8000107bcfe4:       cmp     w0, w20
    0.00 :   ffff8000107bcfe8:       b.le    ffff8000107bd018 <nvme_unmap_data+0x190>
         :                      blk_rq_nr_phys_segments():
    0.00 :   ffff8000107bcfec:       ldr     w0, [x21, #28]
    0.00 :   ffff8000107bcff0:       mov     x1, #0x20                       // #32
    0.00 :   ffff8000107bcff4:       tbnz    w0, #18, ffff8000107bd000 <nvme_unmap_data+0x178>
    0.00 :   ffff8000107bcff8:       ldrh    w1, [x21, #194]
    0.00 :   ffff8000107bcffc:       lsl     x1, x1, #5
         :                      nvme_unmap_data():
         :                      if (iod->use_sgl) {
    0.00 :   ffff8000107bd000:       ldrb    w0, [x19, #40]
         :                      void *addr = nvme_pci_iod_list(req)[i];
    0.00 :   ffff8000107bd004:       add     x1, x1, w20, sxtw #3
    0.00 :   ffff8000107bd008:       ldr     x1, [x4, x1]
         :                      if (iod->use_sgl) {
    0.00 :   ffff8000107bd00c:       cbnz    w0, ffff8000107bcfc4 <nvme_unmap_data+0x13c>
         :                      next_dma_addr = le64_to_cpu(prp_list[last_prp]);
    0.00 :   ffff8000107bd010:       ldr     x3, [x1, x23]
    0.00 :   ffff8000107bd014:       b       ffff8000107bcfc8 <nvme_unmap_data+0x140>
    0.00 :   ffff8000107bd018:       ldr     x20, [x29, #24]
         :                      mempool_free(iod->sg, dev->iod_mempool);
    0.00 :   ffff8000107bd01c:       ldr     x1, [x22, #3176]
    0.00 :   ffff8000107bd020:       mov     x0, x4
    0.00 :   ffff8000107bd024:       bl      ffff8000101d5b40 <mempool_free>
    0.00 :   ffff8000107bd028:       ldr     x23, [x29, #48]
         :                      }
    0.00 :   ffff8000107bd02c:       ldr     x19, [sp, #16]
    0.00 :   ffff8000107bd030:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000107bd034:       ldr     x24, [sp, #56]
    0.00 :   ffff8000107bd038:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000107bd03c:       ret
    0.00 :   ffff8000107bd040:       ldrh    w1, [x21, #194]
    0.00 :   ffff8000107bd044:       lsl     x1, x1, #5
    0.00 :   ffff8000107bd048:       b       ffff8000107bcf90 <nvme_unmap_data+0x108>
         :                      WARN_ON_ONCE(!iod->nents);
    0.00 :   ffff8000107bd04c:       brk     #0x800
    0.00 :   ffff8000107bd050:       ldr     w2, [x19, #52]
    0.00 :   ffff8000107bd054:       b       ffff8000107bcf0c <nvme_unmap_data+0x84>
 Percent |	Source code & Disassembly of vmlinux for cycles (10808 samples, percent: local period)
------------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010cad738 <preempt_schedule_irq>:
         :                      preempt_schedule_irq():
         :                      * off of irq context.
         :                      * Note, that this is called and return with irqs disabled. This will
         :                      * protect us against recursive calling from irq.
         :                      */
         :                      asmlinkage __visible void __sched preempt_schedule_irq(void)
         :                      {
    0.00 :   ffff800010cad738:       stp     x29, x30, [sp, #-48]!
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800010cad73c:       mrs     x0, sp_el0
         :                      preempt_schedule_irq():
    0.00 :   ffff800010cad740:       mov     x29, sp
    0.00 :   ffff800010cad744:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010cad748:       str     x21, [sp, #32]
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010cad74c:       ldr     w0, [x0, #16]
         :                      preempt_schedule_irq():
         :                      enum ctx_state prev_state;
         :
         :                      /* Catch callers which need to be fixed */
         :                      BUG_ON(preempt_count() || !irqs_disabled());
    0.00 :   ffff800010cad750:       cbnz    w0, ffff800010cad7ac <preempt_schedule_irq+0x74>
         :                      arch_local_save_flags():
         :                      */
         :                      static inline unsigned long arch_local_save_flags(void)
         :                      {
         :                      unsigned long flags;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010cad754:       mrs     x1, daif
         :                      arch_irqs_disabled_flags():
         :
         :                      static inline int arch_irqs_disabled_flags(unsigned long flags)
         :                      {
         :                      int res;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010cad758:       and     w0, w1, #0x80
         :                      preempt_schedule_irq():
    0.00 :   ffff800010cad75c:       cbz     w0, ffff800010cad7ac <preempt_schedule_irq+0x74>
         :                      arch_local_irq_enable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010cad760:       mov     x21, #0xe0                      // #224
         :                      arch_local_irq_disable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010cad764:       mov     x20, #0x60                      // #96
         :                      get_current():
    0.00 :   ffff800010cad768:       mrs     x19, sp_el0
         :                      __read_once_size():
    0.00 :   ffff800010cad76c:       ldr     w0, [x19, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff800010cad770:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010cad774:       str     w0, [x19, #16]
         :                      arch_local_irq_enable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010cad778:       msr     daifclr, #0x2
         :                      preempt_schedule_irq():
         :                      prev_state = exception_enter();
         :
         :                      do {
         :                      preempt_disable();
         :                      local_irq_enable();
         :                      __schedule(true);
  100.00 :   ffff800010cad77c:       mov     w0, #0x1                        // #1
    0.00 :   ffff800010cad780:       bl      ffff800010caccd8 <__schedule>
         :                      arch_local_irq_disable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010cad784:       msr     daifset, #0x2
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010cad788:       ldr     w0, [x19, #16]
         :                      __preempt_count_sub():
         :                      }
         :
         :                      static inline void __preempt_count_sub(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc -= val;
    0.00 :   ffff800010cad78c:       sub     w0, w0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010cad790:       str     w0, [x19, #16]
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff800010cad794:       ldr     x0, [x19]
         :                      preempt_schedule_irq():
         :                      local_irq_disable();
         :                      sched_preempt_enable_no_resched();
         :                      } while (need_resched());
    0.00 :   ffff800010cad798:       tbnz    w0, #1, ffff800010cad768 <preempt_schedule_irq+0x30>
         :
         :                      exception_exit(prev_state);
         :                      }
    0.00 :   ffff800010cad79c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010cad7a0:       ldr     x21, [sp, #32]
    0.00 :   ffff800010cad7a4:       ldp     x29, x30, [sp], #48
    0.00 :   ffff800010cad7a8:       ret
         :                      BUG_ON(preempt_count() || !irqs_disabled());
    0.00 :   ffff800010cad7ac:       brk     #0x800
 Percent |	Source code & Disassembly of vmlinux for cycles (5535 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000106fde00 <__iommu_dma_unmap>:
         :                      __iommu_dma_unmap():
         :                      size >> iova_shift(iovad));
         :                      }
         :
         :                      static void __iommu_dma_unmap(struct device *dev, dma_addr_t dma_addr,
         :                      size_t size)
         :                      {
    0.23 :   ffff8000106fde00:       stp     x29, x30, [sp, #-96]!
    0.02 :   ffff8000106fde04:       mov     x29, sp
    0.20 :   ffff8000106fde08:       stp     x19, x20, [sp, #16]
    0.02 :   ffff8000106fde0c:       mov     x20, x1
    0.56 :   ffff8000106fde10:       stp     x21, x22, [sp, #32]
    0.02 :   ffff8000106fde14:       adrp    x21, ffff800011899000 <page_wait_table+0x1500>
    1.98 :   ffff8000106fde18:       stp     x23, x24, [sp, #48]
    0.05 :   ffff8000106fde1c:       add     x3, x21, #0x8c8
    0.02 :   ffff8000106fde20:       mov     x19, x2
         :                      iommu_iotlb_gather_init():
         :                      return (struct iommu_device *)dev_get_drvdata(dev);
         :                      }
         :
         :                      static inline void iommu_iotlb_gather_init(struct iommu_iotlb_gather *gather)
         :                      {
         :                      *gather = (struct iommu_iotlb_gather) {
    0.02 :   ffff8000106fde24:       add     x22, x29, #0x60
         :                      __iommu_dma_unmap():
    0.22 :   ffff8000106fde28:       ldr     x1, [x3]
    0.18 :   ffff8000106fde2c:       str     x1, [x29, #88]
    0.07 :   ffff8000106fde30:       mov     x1, #0x0                        // #0
         :                      iova_align():
         :                      return iova & iova_mask(iovad);
         :                      }
         :
         :                      static inline size_t iova_align(struct iova_domain *iovad, size_t size)
         :                      {
         :                      return ALIGN(size, iovad->granule);
    0.02 :   ffff8000106fde34:       sub     x19, x19, #0x1
         :                      __iommu_dma_unmap():
         :                      struct iommu_domain *domain = iommu_get_dma_domain(dev);
    0.02 :   ffff8000106fde38:       bl      ffff8000106fcb18 <iommu_get_dma_domain>
         :                      struct iommu_dma_cookie *cookie = domain->iova_cookie;
   10.75 :   ffff8000106fde3c:       ldr     x23, [x0, #64]
         :                      iommu_iotlb_gather_init():
    2.28 :   ffff8000106fde40:       mov     x1, #0xffffffffffffffff         // #-1
    4.91 :   ffff8000106fde44:       str     x1, [x22, #-32]!
         :                      __iommu_dma_unmap():
         :                      struct iommu_domain *domain = iommu_get_dma_domain(dev);
    0.00 :   ffff8000106fde48:       mov     x24, x0
         :                      iommu_iotlb_gather_init():
    0.63 :   ffff8000106fde4c:       stp     xzr, xzr, [x29, #72]
         :                      iova_offset():
         :                      return iova & iova_mask(iovad);
    0.42 :   ffff8000106fde50:       ldr     x2, [x23, #40]
         :                      __iommu_dma_unmap():
         :
         :                      dma_addr -= iova_off;
         :                      size = iova_align(iovad, size + iova_off);
         :                      iommu_iotlb_gather_init(&iotlb_gather);
         :
         :                      unmapped = iommu_unmap_fast(domain, dma_addr, size, &iotlb_gather);
    1.17 :   ffff8000106fde54:       mov     x3, x22
         :                      iova_mask():
         :                      return iovad->granule - 1;
    0.07 :   ffff8000106fde58:       add     x1, x2, x1
         :                      iova_align():
         :                      return ALIGN(size, iovad->granule);
    0.22 :   ffff8000106fde5c:       neg     x4, x2
         :                      iova_offset():
         :                      return iova & iova_mask(iovad);
   31.76 :   ffff8000106fde60:       and     x1, x1, x20
         :                      iova_align():
         :                      return ALIGN(size, iovad->granule);
    0.00 :   ffff8000106fde64:       add     x2, x2, x1
         :                      __iommu_dma_unmap():
         :                      dma_addr -= iova_off;
    0.00 :   ffff8000106fde68:       sub     x20, x20, x1
         :                      iova_align():
    0.00 :   ffff8000106fde6c:       add     x19, x19, x2
         :                      __iommu_dma_unmap():
         :                      unmapped = iommu_unmap_fast(domain, dma_addr, size, &iotlb_gather);
    0.07 :   ffff8000106fde70:       mov     x1, x20
         :                      iova_align():
    0.02 :   ffff8000106fde74:       and     x19, x19, x4
         :                      __iommu_dma_unmap():
    0.00 :   ffff8000106fde78:       mov     x2, x19
    0.00 :   ffff8000106fde7c:       bl      ffff8000106fbb50 <iommu_unmap_fast>
         :                      WARN_ON(unmapped != size);
    1.05 :   ffff8000106fde80:       cmp     x0, x19
    0.00 :   ffff8000106fde84:       b.ne    ffff8000106fdef0 <__iommu_dma_unmap+0xf0>  // b.any
         :
         :                      if (!cookie->fq_domain)
    0.38 :   ffff8000106fde88:       ldr     x0, [x23, #1896]
    0.00 :   ffff8000106fde8c:       cbz     x0, ffff8000106fdec8 <__iommu_dma_unmap+0xc8>
         :                      iommu_tlb_sync(domain, &iotlb_gather);
         :                      iommu_dma_free_iova(cookie, dma_addr, size);
    2.84 :   ffff8000106fde90:       mov     x1, x20
    0.00 :   ffff8000106fde94:       mov     x0, x23
    0.00 :   ffff8000106fde98:       mov     x2, x19
         :                      }
    0.00 :   ffff8000106fde9c:       add     x21, x21, #0x8c8
         :                      iommu_dma_free_iova(cookie, dma_addr, size);
    0.00 :   ffff8000106fdea0:       bl      ffff8000106fdda0 <iommu_dma_free_iova>
         :                      }
    0.04 :   ffff8000106fdea4:       ldr     x1, [x29, #88]
    3.79 :   ffff8000106fdea8:       ldr     x0, [x21]
    0.00 :   ffff8000106fdeac:       eor     x0, x1, x0
    0.00 :   ffff8000106fdeb0:       cbnz    x0, ffff8000106fdef8 <__iommu_dma_unmap+0xf8>
    3.32 :   ffff8000106fdeb4:       ldp     x19, x20, [sp, #16]
    2.98 :   ffff8000106fdeb8:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000106fdebc:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff8000106fdec0:       ldp     x29, x30, [sp], #96
    0.00 :   ffff8000106fdec4:       ret
         :                      iommu_tlb_sync():
         :                      }
         :
         :                      static inline void iommu_tlb_sync(struct iommu_domain *domain,
         :                      struct iommu_iotlb_gather *iotlb_gather)
         :                      {
         :                      if (domain->ops->iotlb_sync)
    3.32 :   ffff8000106fdec8:       ldr     x0, [x24, #8]
    6.00 :   ffff8000106fdecc:       ldr     x2, [x0, #72]
    6.82 :   ffff8000106fded0:       cbz     x2, ffff8000106fdee0 <__iommu_dma_unmap+0xe0>
         :                      domain->ops->iotlb_sync(domain, iotlb_gather);
   13.28 :   ffff8000106fded4:       mov     x1, x22
    0.00 :   ffff8000106fded8:       mov     x0, x24
    0.00 :   ffff8000106fdedc:       blr     x2
         :                      iommu_iotlb_gather_init():
         :                      *gather = (struct iommu_iotlb_gather) {
    0.04 :   ffff8000106fdee0:       mov     x0, #0xffffffffffffffff         // #-1
    0.09 :   ffff8000106fdee4:       str     x0, [x29, #64]
    0.13 :   ffff8000106fdee8:       stp     xzr, xzr, [x29, #72]
    0.00 :   ffff8000106fdeec:       b       ffff8000106fde90 <__iommu_dma_unmap+0x90>
         :                      __iommu_dma_unmap():
         :                      WARN_ON(unmapped != size);
    0.00 :   ffff8000106fdef0:       brk     #0x800
    0.00 :   ffff8000106fdef4:       b       ffff8000106fde88 <__iommu_dma_unmap+0x88>
         :                      }
    0.00 :   ffff8000106fdef8:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (5523 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001045d390 <blk_mq_tag_to_rq>:
         :                      blk_mq_tag_to_rq():
         :                      }
         :                      EXPORT_SYMBOL(blk_mq_delay_kick_requeue_list);
         :
         :                      struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)
         :                      {
         :                      if (tag < tags->nr_tags) {
   15.24 :   ffff80001045d390:       ldr     w2, [x0]
    0.00 :   ffff80001045d394:       cmp     w2, w1
    0.00 :   ffff80001045d398:       b.ls    ffff80001045d3ac <blk_mq_tag_to_rq+0x1c>  // b.plast
         :                      prefetch(tags->rqs[tag]);
   13.54 :   ffff80001045d39c:       ldr     x0, [x0, #144]
   14.73 :   ffff80001045d3a0:       ldr     x0, [x0, w1, uxtw #3]
         :                      prefetch():
         :                      * Prefetching support
         :                      */
         :                      #define ARCH_HAS_PREFETCH
         :                      static inline void prefetch(const void *ptr)
         :                      {
         :                      asm volatile("prfm pldl1keep, %a0\n" : : "p" (ptr));
   55.65 :   ffff80001045d3a4:       prfm    pldl1keep, [x0]
         :                      blk_mq_tag_to_rq():
         :                      return tags->rqs[tag];
         :                      }
         :
         :                      return NULL;
         :                      }
    0.84 :   ffff80001045d3a8:       ret
         :                      return NULL;
    0.00 :   ffff80001045d3ac:       mov     x0, #0x0                        // #0
         :                      }
    0.00 :   ffff80001045d3b0:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (5484 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102c4cd8 <blkdev_bio_end_io>:
         :                      blkdev_bio_end_io():
         :
         :                      return blk_poll(q, READ_ONCE(kiocb->ki_cookie), wait);
         :                      }
         :
         :                      static void blkdev_bio_end_io(struct bio *bio)
         :                      {
    1.22 :   ffff8000102c4cd8:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff8000102c4cdc:       mov     x29, sp
    3.79 :   ffff8000102c4ce0:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000102c4ce4:       mov     x20, x0
    0.80 :   ffff8000102c4ce8:       str     x22, [sp, #40]
         :                      struct blkdev_dio *dio = bio->bi_private;
    0.49 :   ffff8000102c4cec:       ldr     x19, [x0, #64]
         :                      bool should_dirty = dio->should_dirty;
         :
         :                      if (bio->bi_status && !dio->bio.bi_status)
    1.25 :   ffff8000102c4cf0:       ldrb    w1, [x0, #26]
         :                      bool should_dirty = dio->should_dirty;
    1.50 :   ffff8000102c4cf4:       ldrb    w0, [x19, #20]
    0.00 :   ffff8000102c4cf8:       ubfx    x22, x0, #1, #1
         :                      if (bio->bi_status && !dio->bio.bi_status)
    0.00 :   ffff8000102c4cfc:       cbz     w1, ffff8000102c4d08 <blkdev_bio_end_io+0x30>
    0.00 :   ffff8000102c4d00:       ldrb    w2, [x19, #50]
    0.00 :   ffff8000102c4d04:       cbz     w2, ffff8000102c4db0 <blkdev_bio_end_io+0xd8>
         :                      dio->bio.bi_status = bio->bi_status;
         :
         :                      if (!dio->multi_bio || atomic_dec_and_test(&dio->ref)) {
    3.44 :   ffff8000102c4d08:       tbnz    w0, #0, ffff8000102c4db8 <blkdev_bio_end_io+0xe0>
    0.24 :   ffff8000102c4d0c:       str     x21, [x29, #32]
    1.39 :   ffff8000102c4d10:       ldr     x21, [x19]
         :                      if (!dio->is_sync) {
    0.00 :   ffff8000102c4d14:       tbnz    w0, #2, ffff8000102c4d78 <blkdev_bio_end_io+0xa0>
         :                      struct kiocb *iocb = dio->iocb;
         :                      ssize_t ret;
         :
         :                      if (likely(!dio->bio.bi_status)) {
    1.49 :   ffff8000102c4d18:       ldrb    w0, [x19, #50]
    0.00 :   ffff8000102c4d1c:       cbnz    w0, ffff8000102c4e14 <blkdev_bio_end_io+0x13c>
         :                      ret = dio->size;
    0.13 :   ffff8000102c4d20:       ldr     x1, [x19, #8]
         :                      iocb->ki_pos += ret;
    1.48 :   ffff8000102c4d24:       ldr     x2, [x21, #8]
    0.00 :   ffff8000102c4d28:       add     x2, x2, x1
   74.38 :   ffff8000102c4d2c:       str     x2, [x21, #8]
         :                      } else {
         :                      ret = blk_status_to_errno(dio->bio.bi_status);
         :                      }
         :
         :                      dio->iocb->ki_complete(iocb, ret, 0);
    0.98 :   ffff8000102c4d30:       ldr     x3, [x19]
    0.00 :   ffff8000102c4d34:       mov     x0, x21
    0.00 :   ffff8000102c4d38:       mov     x2, #0x0                        // #0
    0.02 :   ffff8000102c4d3c:       ldr     x3, [x3, #16]
    0.07 :   ffff8000102c4d40:       blr     x3
         :                      if (dio->multi_bio)
    0.00 :   ffff8000102c4d44:       ldrb    w0, [x19, #20]
    0.00 :   ffff8000102c4d48:       tbnz    w0, #0, ffff8000102c4e04 <blkdev_bio_end_io+0x12c>
    3.76 :   ffff8000102c4d4c:       ldr     x21, [x29, #32]
         :                      WRITE_ONCE(dio->waiter, NULL);
         :                      blk_wake_io_task(waiter);
         :                      }
         :                      }
         :
         :                      if (should_dirty) {
    0.00 :   ffff8000102c4d50:       cbnz    w22, ffff8000102c4d98 <blkdev_bio_end_io+0xc0>
         :                      bio_check_pages_dirty(bio);
         :                      } else {
         :                      bio_release_pages(bio, false);
    0.00 :   ffff8000102c4d54:       mov     w1, #0x0                        // #0
    0.00 :   ffff8000102c4d58:       mov     x0, x20
    0.00 :   ffff8000102c4d5c:       bl      ffff800010450a80 <bio_release_pages>
         :                      bio_put(bio);
    0.00 :   ffff8000102c4d60:       mov     x0, x20
    0.00 :   ffff8000102c4d64:       bl      ffff800010450020 <bio_put>
         :                      }
         :                      }
    0.00 :   ffff8000102c4d68:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000102c4d6c:       ldr     x22, [sp, #40]
    0.00 :   ffff8000102c4d70:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000102c4d74:       ret
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff8000102c4d78:       str     xzr, [x19]
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff8000102c4d7c:       mrs     x0, sp_el0
         :                      blk_wake_io_task():
         :                      /*
         :                      * If we're polling, the task itself is doing the completions. For
         :                      * that case, we don't need to signal a wakeup, it's enough to just
         :                      * mark us as RUNNING.
         :                      */
         :                      if (waiter == current)
    0.00 :   ffff8000102c4d80:       cmp     x21, x0
    0.00 :   ffff8000102c4d84:       b.eq    ffff8000102c4df8 <blkdev_bio_end_io+0x120>  // b.none
         :                      __set_current_state(TASK_RUNNING);
         :                      else
         :                      wake_up_process(waiter);
    0.00 :   ffff8000102c4d88:       mov     x0, x21
    0.00 :   ffff8000102c4d8c:       bl      ffff800010115b88 <wake_up_process>
    0.00 :   ffff8000102c4d90:       ldr     x21, [x29, #32]
         :                      blkdev_bio_end_io():
         :                      if (should_dirty) {
    0.00 :   ffff8000102c4d94:       cbz     w22, ffff8000102c4d54 <blkdev_bio_end_io+0x7c>
         :                      bio_check_pages_dirty(bio);
    0.29 :   ffff8000102c4d98:       mov     x0, x20
    0.00 :   ffff8000102c4d9c:       bl      ffff800010451ac0 <bio_check_pages_dirty>
         :                      }
    0.81 :   ffff8000102c4da0:       ldp     x19, x20, [sp, #16]
    2.49 :   ffff8000102c4da4:       ldr     x22, [sp, #40]
    0.00 :   ffff8000102c4da8:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000102c4dac:       ret
         :                      dio->bio.bi_status = bio->bi_status;
    0.00 :   ffff8000102c4db0:       strb    w1, [x19, #50]
         :                      if (!dio->multi_bio || atomic_dec_and_test(&dio->ref)) {
    0.00 :   ffff8000102c4db4:       tbz     w0, #0, ffff8000102c4d0c <blkdev_bio_end_io+0x34>
    0.00 :   ffff8000102c4db8:       add     x1, x19, #0x10
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000102c4dbc:       b       ffff8000102c4de4 <blkdev_bio_end_io+0x10c>
    0.00 :   ffff8000102c4dc0:       b       ffff8000102c4de4 <blkdev_bio_end_io+0x10c>
         :                      __lse_atomic_sub_return():
         :                      }
         :
         :                      ATOMIC_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff8000102c4dc4:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000102c4dc8:       neg     w0, w0
    0.00 :   ffff8000102c4dcc:       ldaddal w0, w2, [x1]
    0.00 :   ffff8000102c4dd0:       add     w0, w0, w2
         :                      blkdev_bio_end_io():
    0.00 :   ffff8000102c4dd4:       cbnz    w0, ffff8000102c4d50 <blkdev_bio_end_io+0x78>
    0.00 :   ffff8000102c4dd8:       str     x21, [x29, #32]
    0.00 :   ffff8000102c4ddc:       ldrb    w0, [x19, #20]
    0.00 :   ffff8000102c4de0:       b       ffff8000102c4d10 <blkdev_bio_end_io+0x38>
         :                      __ll_sc_atomic_sub_return():
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff8000102c4de4:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000102c4de8:       add     x3, x19, #0x10
    0.00 :   ffff8000102c4dec:       b       ffff8000102c567c <iterate_bdevs+0x214>
         :                      blkdev_bio_end_io():
    0.00 :   ffff8000102c4df0:       cbnz    w0, ffff8000102c4d50 <blkdev_bio_end_io+0x78>
    0.00 :   ffff8000102c4df4:       b       ffff8000102c4dd8 <blkdev_bio_end_io+0x100>
         :                      blk_wake_io_task():
         :                      __set_current_state(TASK_RUNNING);
    0.00 :   ffff8000102c4df8:       str     xzr, [x21, #24]
    0.00 :   ffff8000102c4dfc:       ldr     x21, [x29, #32]
    0.00 :   ffff8000102c4e00:       b       ffff8000102c4d50 <blkdev_bio_end_io+0x78>
         :                      blkdev_bio_end_io():
         :                      bio_put(&dio->bio);
    0.00 :   ffff8000102c4e04:       add     x0, x19, #0x18
    0.00 :   ffff8000102c4e08:       bl      ffff800010450020 <bio_put>
    0.00 :   ffff8000102c4e0c:       ldr     x21, [x29, #32]
    0.00 :   ffff8000102c4e10:       b       ffff8000102c4d50 <blkdev_bio_end_io+0x78>
         :                      ret = blk_status_to_errno(dio->bio.bi_status);
    0.00 :   ffff8000102c4e14:       bl      ffff800010453588 <blk_status_to_errno>
    0.00 :   ffff8000102c4e18:       sxtw    x1, w0
    0.00 :   ffff8000102c4e1c:       b       ffff8000102c4d30 <blkdev_bio_end_io+0x58>
 Percent |	Source code & Disassembly of vmlinux for cycles (4969 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010450080 <bio_endio>:
         :                      bio_endio():
         :                      *   using bio_chain().  The ->bi_end_io() function will only be called the
         :                      *   last time.  At this point the BLK_TA_COMPLETE tracing event will be
         :                      *   generated if BIO_TRACE_COMPLETION is set.
         :                      **/
         :                      void bio_endio(struct bio *bio)
         :                      {
    1.32 :   ffff800010450080:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff800010450084:       mov     x29, sp
    1.50 :   ffff800010450088:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001045008c:       mov     x19, x0
    1.70 :   ffff800010450090:       str     x21, [sp, #32]
         :                      * save/restore bi_end_io) - however, we want to avoid unbounded
         :                      * recursion and blowing the stack. Tail call optimization would
         :                      * handle this, but compiling with frame pointers also disables
         :                      * gcc's sibling call optimization.
         :                      */
         :                      if (bio->bi_end_io == bio_chain_endio) {
    0.00 :   ffff800010450094:       adrp    x21, ffff800010450000 <bio_free+0x40>
    0.00 :   ffff800010450098:       add     x21, x21, #0x1b8
         :                      __ll_sc_atomic_sub_return():
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff80001045009c:       mov     w20, #0x1                       // #1
         :                      bio_remaining_done():
         :                      if (!bio_flagged(bio, BIO_CHAIN))
    1.25 :   ffff8000104500a0:       ldrh    w0, [x19, #20]
    0.00 :   ffff8000104500a4:       tbz     w0, #7, ffff8000104500e0 <bio_endio+0x60>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000104500a8:       ldr     w0, [x19, #28]
         :                      bio_remaining_done():
         :                      BUG_ON(atomic_read(&bio->__bi_remaining) <= 0);
    0.00 :   ffff8000104500ac:       cmp     w0, #0x0
    0.00 :   ffff8000104500b0:       b.le    ffff800010450164 <bio_endio+0xe4>
         :                      if (atomic_dec_and_test(&bio->__bi_remaining)) {
    0.00 :   ffff8000104500b4:       add     x0, x19, #0x1c
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000104500b8:       b       ffff80001045014c <bio_endio+0xcc>
    0.00 :   ffff8000104500bc:       b       ffff80001045014c <bio_endio+0xcc>
         :                      __lse_atomic_sub_return():
         :                      }
         :
         :                      ATOMIC_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff8000104500c0:       mov     w1, w20
    0.00 :   ffff8000104500c4:       neg     w1, w1
    0.00 :   ffff8000104500c8:       ldaddal w1, w2, [x0]
    0.00 :   ffff8000104500cc:       add     w1, w1, w2
         :                      bio_remaining_done():
    0.00 :   ffff8000104500d0:       cbnz    w1, ffff800010450154 <bio_endio+0xd4>
         :                      bio_clear_flag():
         :                      bio->bi_flags |= (1U << bit);
         :                      }
         :
         :                      static inline void bio_clear_flag(struct bio *bio, unsigned int bit)
         :                      {
         :                      bio->bi_flags &= ~(1U << bit);
    0.00 :   ffff8000104500d4:       ldrh    w0, [x19, #20]
    0.00 :   ffff8000104500d8:       and     w0, w0, #0xffffff7f
    0.00 :   ffff8000104500dc:       strh    w0, [x19, #20]
         :                      bio_integrity():
         :
         :                      #if defined(CONFIG_BLK_DEV_INTEGRITY)
         :
         :                      static inline struct bio_integrity_payload *bio_integrity(struct bio *bio)
         :                      {
         :                      if (bio->bi_opf & REQ_INTEGRITY)
    4.36 :   ffff8000104500e0:       ldr     w0, [x19, #16]
    0.00 :   ffff8000104500e4:       tbz     w0, #16, ffff800010450100 <bio_endio+0x80>
         :                      bio_integrity_endio():
         :                      void blk_flush_integrity(void);
         :                      bool __bio_integrity_endio(struct bio *);
         :                      void bio_integrity_free(struct bio *bio);
         :                      static inline bool bio_integrity_endio(struct bio *bio)
         :                      {
         :                      if (bio_integrity(bio))
    0.00 :   ffff8000104500e8:       ldr     x0, [x19, #88]
    0.00 :   ffff8000104500ec:       cbz     x0, ffff800010450100 <bio_endio+0x80>
         :                      return __bio_integrity_endio(bio);
    0.00 :   ffff8000104500f0:       mov     x0, x19
    0.00 :   ffff8000104500f4:       bl      ffff80001047ab80 <__bio_integrity_endio>
         :                      bio_endio():
         :                      if (!bio_integrity_endio(bio))
    0.00 :   ffff8000104500f8:       tst     w0, #0xff
    0.00 :   ffff8000104500fc:       b.eq    ffff800010450154 <bio_endio+0xd4>  // b.none
         :                      if (bio->bi_disk)
   34.43 :   ffff800010450100:       ldr     x0, [x19, #8]
    0.00 :   ffff800010450104:       cbz     x0, ffff80001045011c <bio_endio+0x9c>
         :                      rq_qos_done_bio(bio->bi_disk->queue, bio);
    7.30 :   ffff800010450108:       ldr     x0, [x0, #1040]
    4.84 :   ffff80001045010c:       ldr     x0, [x0, #24]
         :                      rq_qos_done_bio():
         :                      __rq_qos_requeue(q->rq_qos, rq);
         :                      }
         :
         :                      static inline void rq_qos_done_bio(struct request_queue *q, struct bio *bio)
         :                      {
         :                      if (q->rq_qos)
    0.02 :   ffff800010450110:       cbz     x0, ffff80001045011c <bio_endio+0x9c>
         :                      __rq_qos_done_bio(q->rq_qos, bio);
    0.00 :   ffff800010450114:       mov     x1, x19
    0.00 :   ffff800010450118:       bl      ffff80001046f210 <__rq_qos_done_bio>
         :                      bio_endio():
         :                      if (bio->bi_end_io == bio_chain_endio) {
   25.68 :   ffff80001045011c:       ldr     x0, [x19, #56]
    4.29 :   ffff800010450120:       cmp     x0, x21
    0.00 :   ffff800010450124:       b.ne    ffff800010450168 <bio_endio+0xe8>  // b.any
         :                      __bio_chain_endio():
         :                      struct bio *parent = bio->bi_private;
    0.00 :   ffff800010450128:       ldr     x1, [x19, #64]
         :                      if (!parent->bi_status)
    0.00 :   ffff80001045012c:       ldrb    w0, [x1, #26]
    0.00 :   ffff800010450130:       cbnz    w0, ffff80001045013c <bio_endio+0xbc>
         :                      parent->bi_status = bio->bi_status;
    0.00 :   ffff800010450134:       ldrb    w0, [x19, #26]
    0.00 :   ffff800010450138:       strb    w0, [x1, #26]
         :                      bio_put(bio);
    0.00 :   ffff80001045013c:       mov     x0, x19
         :                      bio_endio():
         :                      bio = __bio_chain_endio(bio);
    0.00 :   ffff800010450140:       mov     x19, x1
         :                      __bio_chain_endio():
         :                      bio_put(bio);
    0.00 :   ffff800010450144:       bl      ffff800010450020 <bio_put>
         :                      bio_endio():
         :                      goto again;
    0.00 :   ffff800010450148:       b       ffff8000104500a0 <bio_endio+0x20>
         :                      __ll_sc_atomic_sub_return():
    0.00 :   ffff80001045014c:       b       ffff800010452150 <bio_associate_blkg_from_page+0x118>
         :                      bio_remaining_done():
         :                      if (atomic_dec_and_test(&bio->__bi_remaining)) {
    0.00 :   ffff800010450150:       cbz     w1, ffff8000104500d4 <bio_endio+0x54>
         :                      bio_endio():
         :                      blk_throtl_bio_endio(bio);
         :                      /* release cgroup info */
         :                      bio_uninit(bio);
         :                      if (bio->bi_end_io)
         :                      bio->bi_end_io(bio);
         :                      }
    0.00 :   ffff800010450154:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010450158:       ldr     x21, [sp, #32]
    0.00 :   ffff80001045015c:       ldp     x29, x30, [sp], #48
    0.00 :   ffff800010450160:       ret
         :                      bio_remaining_done():
         :                      BUG_ON(atomic_read(&bio->__bi_remaining) <= 0);
    0.00 :   ffff800010450164:       brk     #0x800
         :                      bio_endio():
         :                      if (bio->bi_disk && bio_flagged(bio, BIO_TRACE_COMPLETION)) {
    3.82 :   ffff800010450168:       ldr     x0, [x19, #8]
    0.00 :   ffff80001045016c:       cbz     x0, ffff80001045018c <bio_endio+0x10c>
         :                      bio_flagged():
         :                      return (bio->bi_flags & (1U << bit)) != 0;
    2.21 :   ffff800010450170:       ldrh    w0, [x19, #20]
         :                      bio_endio():
    0.00 :   ffff800010450174:       tbz     w0, #10, ffff80001045018c <bio_endio+0x10c>
         :                      trace_block_bio_complete(bio->bi_disk->queue, bio,
    0.00 :   ffff800010450178:       ldrb    w0, [x19, #26]
    0.00 :   ffff80001045017c:       bl      ffff800010453588 <blk_status_to_errno>
         :                      bio_clear_flag():
         :                      bio->bi_flags &= ~(1U << bit);
    0.00 :   ffff800010450180:       ldrh    w0, [x19, #20]
    0.00 :   ffff800010450184:       and     w0, w0, #0xfffffbff
    0.00 :   ffff800010450188:       strh    w0, [x19, #20]
         :                      bio_endio():
         :                      bio_uninit(bio);
    1.09 :   ffff80001045018c:       mov     x0, x19
    0.00 :   ffff800010450190:       bl      ffff80001044fa80 <bio_uninit>
         :                      if (bio->bi_end_io)
    0.30 :   ffff800010450194:       ldr     x1, [x19, #56]
    0.00 :   ffff800010450198:       cbz     x1, ffff800010450154 <bio_endio+0xd4>
         :                      bio->bi_end_io(bio);
    1.73 :   ffff80001045019c:       mov     x0, x19
    0.00 :   ffff8000104501a0:       blr     x1
         :                      }
    0.28 :   ffff8000104501a4:       ldp     x19, x20, [sp, #16]
    1.64 :   ffff8000104501a8:       ldr     x21, [sp, #32]
    2.22 :   ffff8000104501ac:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000104501b0:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (4442 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000107079c0 <arm_smmu_cmdq_build_cmd>:
         :                      arm_smmu_cmdq_build_cmd():
         :                      }
         :
         :                      /* High-level queue accessors */
         :                      static int arm_smmu_cmdq_build_cmd(u64 *cmd, struct arm_smmu_cmdq_ent *ent)
         :                      {
         :                      memset(cmd, 0, 1 << CMDQ_ENT_SZ_SHIFT);
    0.02 :   ffff8000107079c0:       stp     xzr, xzr, [x0]
         :                      cmd[0] |= FIELD_PREP(CMDQ_0_OP, ent->opcode);
   23.60 :   ffff8000107079c4:       ldr     x3, [x0]
   25.53 :   ffff8000107079c8:       ldrb    w2, [x1]
    0.00 :   ffff8000107079cc:       orr     x2, x2, x3
    1.14 :   ffff8000107079d0:       str     x2, [x0]
         :
         :                      switch (ent->opcode) {
    2.07 :   ffff8000107079d4:       ldrb    w3, [x1]
    0.00 :   ffff8000107079d8:       cmp     w3, #0x20
    0.00 :   ffff8000107079dc:       b.eq    ffff800010707b58 <arm_smmu_cmdq_build_cmd+0x198>  // b.none
    0.21 :   ffff8000107079e0:       b.ls    ffff800010707a10 <arm_smmu_cmdq_build_cmd+0x50>  // b.plast
    0.00 :   ffff8000107079e4:       cmp     w3, #0x30
    0.00 :   ffff8000107079e8:       b.eq    ffff800010707b58 <arm_smmu_cmdq_build_cmd+0x198>  // b.none
    0.00 :   ffff8000107079ec:       b.ls    ffff800010707b60 <arm_smmu_cmdq_build_cmd+0x1a0>  // b.plast
    0.00 :   ffff8000107079f0:       cmp     w3, #0x41
    0.00 :   ffff8000107079f4:       b.eq    ffff800010707a98 <arm_smmu_cmdq_build_cmd+0xd8>  // b.none
    0.00 :   ffff8000107079f8:       cmp     w3, #0x46
    0.00 :   ffff8000107079fc:       b.eq    ffff800010707c00 <arm_smmu_cmdq_build_cmd+0x240>  // b.none
    0.00 :   ffff800010707a00:       cmp     w3, #0x40
    0.00 :   ffff800010707a04:       b.eq    ffff800010707af4 <arm_smmu_cmdq_build_cmd+0x134>  // b.none
         :                      }
         :                      cmd[0] |= FIELD_PREP(CMDQ_SYNC_0_MSH, ARM_SMMU_SH_ISH);
         :                      cmd[0] |= FIELD_PREP(CMDQ_SYNC_0_MSIATTR, ARM_SMMU_MEMATTR_OIWB);
         :                      break;
         :                      default:
         :                      return -ENOENT;
    0.00 :   ffff800010707a08:       mov     w3, #0xfffffffe                 // #-2
    0.00 :   ffff800010707a0c:       b       ffff800010707a4c <arm_smmu_cmdq_build_cmd+0x8c>
         :                      switch (ent->opcode) {
    3.67 :   ffff800010707a10:       cmp     w3, #0x4
    0.00 :   ffff800010707a14:       b.eq    ffff800010707bc8 <arm_smmu_cmdq_build_cmd+0x208>  // b.none
    1.55 :   ffff800010707a18:       b.hi    ffff800010707a54 <arm_smmu_cmdq_build_cmd+0x94>  // b.pmore
    0.00 :   ffff800010707a1c:       cmp     w3, #0x1
    0.00 :   ffff800010707a20:       b.eq    ffff800010707bdc <arm_smmu_cmdq_build_cmd+0x21c>  // b.none
    0.00 :   ffff800010707a24:       cmp     w3, #0x3
    0.00 :   ffff800010707a28:       b.ne    ffff800010707a08 <arm_smmu_cmdq_build_cmd+0x48>  // b.any
         :                      cmd[0] |= FIELD_PREP(CMDQ_CFGI_0_SID, ent->cfgi.sid);
    0.00 :   ffff800010707a2c:       ldr     w5, [x1, #8]
         :                      }
         :
         :                      return 0;
    0.00 :   ffff800010707a30:       mov     w3, #0x0                        // #0
         :                      cmd[1] |= FIELD_PREP(CMDQ_CFGI_1_LEAF, ent->cfgi.leaf);
    0.00 :   ffff800010707a34:       ldr     x4, [x0, #8]
         :                      cmd[0] |= FIELD_PREP(CMDQ_CFGI_0_SID, ent->cfgi.sid);
    0.00 :   ffff800010707a38:       orr     x2, x2, x5, lsl #32
    0.00 :   ffff800010707a3c:       str     x2, [x0]
         :                      cmd[1] |= FIELD_PREP(CMDQ_CFGI_1_LEAF, ent->cfgi.leaf);
    0.00 :   ffff800010707a40:       ldrb    w1, [x1, #12]
    0.00 :   ffff800010707a44:       orr     x1, x4, x1
    0.00 :   ffff800010707a48:       str     x1, [x0, #8]
         :                      }
    2.55 :   ffff800010707a4c:       mov     w0, w3
    0.00 :   ffff800010707a50:       ret
         :                      switch (ent->opcode) {
    0.43 :   ffff800010707a54:       cmp     w3, #0x11
    0.00 :   ffff800010707a58:       b.eq    ffff800010707ba4 <arm_smmu_cmdq_build_cmd+0x1e4>  // b.none
    1.50 :   ffff800010707a5c:       cmp     w3, #0x12
    1.13 :   ffff800010707a60:       b.ne    ffff800010707a08 <arm_smmu_cmdq_build_cmd+0x48>  // b.any
         :                      cmd[0] |= FIELD_PREP(CMDQ_TLBI_0_ASID, ent->tlbi.asid);
    1.10 :   ffff800010707a64:       ldrh    w4, [x1, #8]
         :                      return 0;
    0.00 :   ffff800010707a68:       mov     w3, #0x0                        // #0
         :                      cmd[1] |= FIELD_PREP(CMDQ_TLBI_1_LEAF, ent->tlbi.leaf);
   12.94 :   ffff800010707a6c:       ldr     x5, [x0, #8]
         :                      cmd[0] |= FIELD_PREP(CMDQ_TLBI_0_ASID, ent->tlbi.asid);
    0.00 :   ffff800010707a70:       orr     x2, x2, x4, lsl #48
    9.00 :   ffff800010707a74:       str     x2, [x0]
         :                      cmd[1] |= FIELD_PREP(CMDQ_TLBI_1_LEAF, ent->tlbi.leaf);
    1.17 :   ffff800010707a78:       ldrb    w2, [x1, #12]
    0.00 :   ffff800010707a7c:       orr     x2, x2, x5
    1.19 :   ffff800010707a80:       str     x2, [x0, #8]
         :                      cmd[1] |= ent->tlbi.addr & CMDQ_TLBI_1_VA_MASK;
    7.41 :   ffff800010707a84:       ldr     x1, [x1, #16]
    0.00 :   ffff800010707a88:       and     x1, x1, #0xfffffffffffff000
    0.00 :   ffff800010707a8c:       orr     x1, x1, x2
    3.77 :   ffff800010707a90:       str     x1, [x0, #8]
         :                      break;
    0.00 :   ffff800010707a94:       b       ffff800010707a4c <arm_smmu_cmdq_build_cmd+0x8c>
         :                      cmd[0] |= FIELD_PREP(CMDQ_0_SSV, ent->substream_valid);
    0.00 :   ffff800010707a98:       ldrb    w3, [x1, #1]
         :                      cmd[1] |= FIELD_PREP(CMDQ_PRI_1_GRPID, ent->pri.grpid);
    0.00 :   ffff800010707a9c:       ldr     x4, [x0, #8]
         :                      cmd[0] |= FIELD_PREP(CMDQ_0_SSV, ent->substream_valid);
    0.00 :   ffff800010707aa0:       orr     x3, x2, x3, lsl #11
    0.00 :   ffff800010707aa4:       str     x3, [x0]
         :                      cmd[0] |= FIELD_PREP(CMDQ_PRI_0_SSID, ent->pri.ssid);
    0.00 :   ffff800010707aa8:       ldr     w2, [x1, #12]
    0.00 :   ffff800010707aac:       lsl     w2, w2, #12
    0.00 :   ffff800010707ab0:       orr     x2, x2, x3
    0.00 :   ffff800010707ab4:       str     x2, [x0]
         :                      cmd[0] |= FIELD_PREP(CMDQ_PRI_0_SID, ent->pri.sid);
    0.00 :   ffff800010707ab8:       ldr     w3, [x1, #8]
    0.00 :   ffff800010707abc:       orr     x2, x2, x3, lsl #32
    0.00 :   ffff800010707ac0:       str     x2, [x0]
         :                      cmd[1] |= FIELD_PREP(CMDQ_PRI_1_GRPID, ent->pri.grpid);
    0.00 :   ffff800010707ac4:       ldrh    w2, [x1, #16]
    0.00 :   ffff800010707ac8:       and     x2, x2, #0x1ff
    0.00 :   ffff800010707acc:       orr     x2, x2, x4
    0.00 :   ffff800010707ad0:       str     x2, [x0, #8]
         :                      switch (ent->pri.resp) {
    0.00 :   ffff800010707ad4:       ldr     w1, [x1, #20]
    0.00 :   ffff800010707ad8:       cmp     w1, #0x2
    0.00 :   ffff800010707adc:       b.hi    ffff800010707c38 <arm_smmu_cmdq_build_cmd+0x278>  // b.pmore
         :                      cmd[1] |= FIELD_PREP(CMDQ_PRI_1_RESP, ent->pri.resp);
    0.00 :   ffff800010707ae0:       ubfiz   x1, x1, #12, #32
         :                      return 0;
    0.00 :   ffff800010707ae4:       mov     w3, #0x0                        // #0
         :                      cmd[1] |= FIELD_PREP(CMDQ_PRI_1_RESP, ent->pri.resp);
    0.00 :   ffff800010707ae8:       orr     x2, x1, x2
    0.00 :   ffff800010707aec:       str     x2, [x0, #8]
         :                      break;
    0.00 :   ffff800010707af0:       b       ffff800010707a4c <arm_smmu_cmdq_build_cmd+0x8c>
         :                      cmd[0] |= FIELD_PREP(CMDQ_0_SSV, ent->substream_valid);
    0.00 :   ffff800010707af4:       ldrb    w4, [x1, #1]
         :                      return 0;
    0.00 :   ffff800010707af8:       mov     w3, #0x0                        // #0
         :                      cmd[1] |= FIELD_PREP(CMDQ_ATC_1_SIZE, ent->atc.size);
    0.00 :   ffff800010707afc:       ldr     x6, [x0, #8]
         :                      cmd[0] |= FIELD_PREP(CMDQ_0_SSV, ent->substream_valid);
    0.00 :   ffff800010707b00:       orr     x2, x2, x4, lsl #11
    0.00 :   ffff800010707b04:       str     x2, [x0]
         :                      cmd[0] |= FIELD_PREP(CMDQ_ATC_0_GLOBAL, ent->atc.global);
    0.00 :   ffff800010707b08:       ldrb    w4, [x1, #25]
    0.00 :   ffff800010707b0c:       orr     x2, x2, x4, lsl #9
    0.00 :   ffff800010707b10:       str     x2, [x0]
         :                      cmd[0] |= FIELD_PREP(CMDQ_ATC_0_SSID, ent->atc.ssid);
    0.00 :   ffff800010707b14:       ldr     w4, [x1, #12]
    0.00 :   ffff800010707b18:       lsl     w4, w4, #12
    0.00 :   ffff800010707b1c:       orr     x4, x4, x2
    0.00 :   ffff800010707b20:       str     x4, [x0]
         :                      cmd[0] |= FIELD_PREP(CMDQ_ATC_0_SID, ent->atc.sid);
    0.00 :   ffff800010707b24:       ldr     w5, [x1, #8]
    0.00 :   ffff800010707b28:       orr     x4, x4, x5, lsl #32
    0.00 :   ffff800010707b2c:       str     x4, [x0]
         :                      cmd[1] |= FIELD_PREP(CMDQ_ATC_1_SIZE, ent->atc.size);
    0.00 :   ffff800010707b30:       ldrb    w2, [x1, #24]
    0.00 :   ffff800010707b34:       and     x2, x2, #0x3f
    0.00 :   ffff800010707b38:       orr     x2, x2, x6
    0.00 :   ffff800010707b3c:       str     x2, [x0, #8]
         :                      cmd[1] |= ent->atc.addr & CMDQ_ATC_1_ADDR_MASK;
    0.00 :   ffff800010707b40:       ldr     x1, [x1, #16]
    0.00 :   ffff800010707b44:       and     x1, x1, #0xfffffffffffff000
    0.00 :   ffff800010707b48:       orr     x2, x1, x2
    0.00 :   ffff800010707b4c:       str     x2, [x0, #8]
         :                      }
    0.00 :   ffff800010707b50:       mov     w0, w3
    0.00 :   ffff800010707b54:       ret
         :                      return 0;
    0.00 :   ffff800010707b58:       mov     w3, #0x0                        // #0
    0.00 :   ffff800010707b5c:       b       ffff800010707a4c <arm_smmu_cmdq_build_cmd+0x8c>
         :                      switch (ent->opcode) {
    0.00 :   ffff800010707b60:       cmp     w3, #0x28
    0.00 :   ffff800010707b64:       b.eq    ffff800010707bb0 <arm_smmu_cmdq_build_cmd+0x1f0>  // b.none
    0.00 :   ffff800010707b68:       cmp     w3, #0x2a
    0.00 :   ffff800010707b6c:       b.ne    ffff800010707a08 <arm_smmu_cmdq_build_cmd+0x48>  // b.any
         :                      cmd[0] |= FIELD_PREP(CMDQ_TLBI_0_VMID, ent->tlbi.vmid);
    0.00 :   ffff800010707b70:       ldrh    w4, [x1, #10]
         :                      return 0;
    0.00 :   ffff800010707b74:       mov     w3, #0x0                        // #0
         :                      cmd[1] |= FIELD_PREP(CMDQ_TLBI_1_LEAF, ent->tlbi.leaf);
    0.00 :   ffff800010707b78:       ldr     x5, [x0, #8]
         :                      cmd[0] |= FIELD_PREP(CMDQ_TLBI_0_VMID, ent->tlbi.vmid);
    0.00 :   ffff800010707b7c:       orr     x2, x2, x4, lsl #32
    0.00 :   ffff800010707b80:       str     x2, [x0]
         :                      cmd[1] |= FIELD_PREP(CMDQ_TLBI_1_LEAF, ent->tlbi.leaf);
    0.00 :   ffff800010707b84:       ldrb    w2, [x1, #12]
    0.00 :   ffff800010707b88:       orr     x2, x2, x5
    0.00 :   ffff800010707b8c:       str     x2, [x0, #8]
         :                      cmd[1] |= ent->tlbi.addr & CMDQ_TLBI_1_IPA_MASK;
    0.00 :   ffff800010707b90:       ldr     x1, [x1, #16]
    0.00 :   ffff800010707b94:       and     x1, x1, #0xffffffffff000
    0.00 :   ffff800010707b98:       orr     x1, x1, x2
    0.00 :   ffff800010707b9c:       str     x1, [x0, #8]
         :                      break;
    0.00 :   ffff800010707ba0:       b       ffff800010707a4c <arm_smmu_cmdq_build_cmd+0x8c>
         :                      cmd[0] |= FIELD_PREP(CMDQ_TLBI_0_ASID, ent->tlbi.asid);
    0.00 :   ffff800010707ba4:       ldrh    w3, [x1, #8]
    0.00 :   ffff800010707ba8:       orr     x2, x2, x3, lsl #48
    0.00 :   ffff800010707bac:       str     x2, [x0]
         :                      cmd[0] |= FIELD_PREP(CMDQ_TLBI_0_VMID, ent->tlbi.vmid);
    0.00 :   ffff800010707bb0:       ldrh    w1, [x1, #10]
         :                      return 0;
    0.00 :   ffff800010707bb4:       mov     w3, #0x0                        // #0
         :                      cmd[0] |= FIELD_PREP(CMDQ_TLBI_0_VMID, ent->tlbi.vmid);
    0.00 :   ffff800010707bb8:       orr     x2, x2, x1, lsl #32
    0.00 :   ffff800010707bbc:       str     x2, [x0]
         :                      }
    0.00 :   ffff800010707bc0:       mov     w0, w3
    0.00 :   ffff800010707bc4:       ret
         :                      cmd[1] |= FIELD_PREP(CMDQ_CFGI_1_RANGE, 31);
    0.00 :   ffff800010707bc8:       ldr     x1, [x0, #8]
         :                      return 0;
    0.00 :   ffff800010707bcc:       mov     w3, #0x0                        // #0
         :                      cmd[1] |= FIELD_PREP(CMDQ_CFGI_1_RANGE, 31);
    0.00 :   ffff800010707bd0:       orr     x1, x1, #0x1f
    0.00 :   ffff800010707bd4:       str     x1, [x0, #8]
         :                      break;
    0.00 :   ffff800010707bd8:       b       ffff800010707a4c <arm_smmu_cmdq_build_cmd+0x8c>
         :                      cmd[0] |= FIELD_PREP(CMDQ_PREFETCH_0_SID, ent->prefetch.sid);
    0.00 :   ffff800010707bdc:       ldr     w4, [x1, #8]
         :                      return 0;
    0.00 :   ffff800010707be0:       mov     w3, #0x0                        // #0
         :                      cmd[1] |= FIELD_PREP(CMDQ_PREFETCH_1_SIZE, ent->prefetch.size);
    0.00 :   ffff800010707be4:       ldr     x5, [x0, #8]
         :                      cmd[0] |= FIELD_PREP(CMDQ_PREFETCH_0_SID, ent->prefetch.sid);
    0.00 :   ffff800010707be8:       orr     x2, x2, x4, lsl #32
    0.00 :   ffff800010707bec:       str     x2, [x0]
         :                      cmd[1] |= FIELD_PREP(CMDQ_PREFETCH_1_SIZE, ent->prefetch.size);
    0.00 :   ffff800010707bf0:       ldrb    w2, [x1, #12]
    0.00 :   ffff800010707bf4:       and     x2, x2, #0x1f
    0.00 :   ffff800010707bf8:       orr     x2, x2, x5
    0.00 :   ffff800010707bfc:       b       ffff800010707b3c <arm_smmu_cmdq_build_cmd+0x17c>
         :                      if (ent->sync.msiaddr) {
    0.00 :   ffff800010707c00:       ldr     x4, [x1, #8]
         :                      cmd[0] |= FIELD_PREP(CMDQ_SYNC_0_CS, CMDQ_SYNC_0_CS_SEV);
    0.00 :   ffff800010707c04:       orr     x3, x2, #0x2000
         :                      if (ent->sync.msiaddr) {
    0.00 :   ffff800010707c08:       cbz     x4, ffff800010707c28 <arm_smmu_cmdq_build_cmd+0x268>
         :                      cmd[0] |= FIELD_PREP(CMDQ_SYNC_0_CS, CMDQ_SYNC_0_CS_IRQ);
    0.00 :   ffff800010707c0c:       orr     x3, x2, #0x1000
    0.00 :   ffff800010707c10:       str     x3, [x0]
         :                      cmd[1] |= ent->sync.msiaddr & CMDQ_SYNC_1_MSIADDR_MASK;
    0.00 :   ffff800010707c14:       ldr     x2, [x0, #8]
    0.00 :   ffff800010707c18:       ldr     x1, [x1, #8]
    0.00 :   ffff800010707c1c:       and     x1, x1, #0xffffffffffffc
    0.00 :   ffff800010707c20:       orr     x1, x2, x1
    0.00 :   ffff800010707c24:       str     x1, [x0, #8]
         :                      cmd[0] |= FIELD_PREP(CMDQ_SYNC_0_MSIATTR, ARM_SMMU_MEMATTR_OIWB);
    0.00 :   ffff800010707c28:       orr     x1, x3, #0xfc00000
         :                      return 0;
    0.00 :   ffff800010707c2c:       mov     w3, #0x0                        // #0
         :                      cmd[0] |= FIELD_PREP(CMDQ_SYNC_0_MSIATTR, ARM_SMMU_MEMATTR_OIWB);
    0.00 :   ffff800010707c30:       str     x1, [x0]
         :                      break;
    0.00 :   ffff800010707c34:       b       ffff800010707a4c <arm_smmu_cmdq_build_cmd+0x8c>
         :                      return -EINVAL;
    0.00 :   ffff800010707c38:       mov     w3, #0xffffffea                 // #-22
    0.00 :   ffff800010707c3c:       b       ffff800010707a4c <arm_smmu_cmdq_build_cmd+0x8c>
 Percent |	Source code & Disassembly of vmlinux for cycles (5593 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001016ad10 <ktime_get>:
         :                      ktime_get():
         :                      timespec64_add_ns(ts, nsecs);
         :                      }
         :                      EXPORT_SYMBOL(ktime_get_real_ts64);
         :
         :                      ktime_t ktime_get(void)
         :                      {
    5.21 :   ffff80001016ad10:       stp     x29, x30, [sp, #-48]!
         :                      struct timekeeper *tk = &tk_core.timekeeper;
         :                      unsigned int seq;
         :                      ktime_t base;
         :                      u64 nsecs;
         :
         :                      WARN_ON(timekeeping_suspended);
    0.00 :   ffff80001016ad14:       adrp    x0, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      {
    0.00 :   ffff80001016ad18:       mov     x29, sp
         :                      WARN_ON(timekeeping_suspended);
    0.55 :   ffff80001016ad1c:       ldr     w0, [x0, #644]
         :                      {
    1.15 :   ffff80001016ad20:       stp     x19, x20, [sp, #16]
    0.18 :   ffff80001016ad24:       stp     x21, x22, [sp, #32]
         :                      WARN_ON(timekeeping_suspended);
    0.00 :   ffff80001016ad28:       cbnz    w0, ffff80001016ada0 <ktime_get+0x90>
    2.49 :   ffff80001016ad2c:       adrp    x20, ffff800011aa4000 <__log_buf+0x1fc10>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001016ad30:       add     x20, x20, #0xec0
    0.59 :   ffff80001016ad34:       ldr     w22, [x20, #64]
         :                      __read_seqcount_begin():
         :                      {
         :                      unsigned ret;
         :
         :                      repeat:
         :                      ret = READ_ONCE(s->sequence);
         :                      if (unlikely(ret & 1)) {
    0.00 :   ffff80001016ad38:       tbnz    w22, #0, ffff80001016ad98 <ktime_get+0x88>
         :                      raw_read_seqcount_begin():
         :                      * section is tested by checking read_seqcount_retry function.
         :                      */
         :                      static inline unsigned raw_read_seqcount_begin(const seqcount_t *s)
         :                      {
         :                      unsigned ret = __read_seqcount_begin(s);
         :                      smp_rmb();
    3.70 :   ffff80001016ad3c:       dmb     ishld
         :                      __read_once_size():
   49.46 :   ffff80001016ad40:       ldr     x1, [x20, #72]
         :                      ktime_get():
         :
         :                      do {
         :                      seq = read_seqcount_begin(&tk_core.seq);
         :                      base = tk->tkr_mono.base;
    0.07 :   ffff80001016ad44:       ldr     x21, [x20, #112]
         :                      tk_clock_read():
         :                      return clock->read(clock);
    0.00 :   ffff80001016ad48:       mov     x0, x1
    0.00 :   ffff80001016ad4c:       ldr     x1, [x1]
    0.00 :   ffff80001016ad50:       blr     x1
         :                      timekeeping_get_ns():
         :                      return timekeeping_delta_to_ns(tkr, delta);
    0.05 :   ffff80001016ad54:       ldp     w1, w3, [x20, #96]
         :                      timekeeping_get_delta():
         :                      delta = clocksource_delta(cycle_now, tkr->cycle_last, tkr->mask);
    0.20 :   ffff80001016ad58:       ldp     x5, x6, [x20, #80]
         :                      timekeeping_get_ns():
         :                      return timekeeping_delta_to_ns(tkr, delta);
    0.00 :   ffff80001016ad5c:       ldr     x4, [x20, #104]
         :                      read_seqcount_retry():
         :                      * If the critical section was invalid, it must be ignored (and typically
         :                      * retried).
         :                      */
         :                      static inline int read_seqcount_retry(const seqcount_t *s, unsigned start)
         :                      {
         :                      smp_rmb();
    2.93 :   ffff80001016ad60:       dmb     ishld
         :                      ktime_get():
         :                      nsecs = timekeeping_get_ns(&tk->tkr_mono);
         :
         :                      } while (read_seqcount_retry(&tk_core.seq, seq));
   30.21 :   ffff80001016ad64:       ldr     w2, [x20, #64]
    0.00 :   ffff80001016ad68:       cmp     w22, w2
    0.00 :   ffff80001016ad6c:       b.ne    ffff80001016ad34 <ktime_get+0x24>  // b.any
         :                      clocksource_delta():
         :                      return ret & ~(mask >> 1) ? 0 : ret;
         :                      }
         :                      #else
         :                      static inline u64 clocksource_delta(u64 now, u64 last, u64 mask)
         :                      {
         :                      return (now - last) & mask;
    0.04 :   ffff80001016ad70:       sub     x0, x0, x6
         :                      timekeeping_delta_to_ns():
         :                      nsec = delta * tkr->mult + tkr->xtime_nsec;
    0.00 :   ffff80001016ad74:       mov     w1, w1
         :                      clocksource_delta():
    0.00 :   ffff80001016ad78:       and     x0, x0, x5
         :                      ktime_get():
         :
         :                      return ktime_add_ns(base, nsecs);
         :                      }
    0.00 :   ffff80001016ad7c:       ldp     x19, x20, [sp, #16]
         :                      timekeeping_delta_to_ns():
         :                      nsec = delta * tkr->mult + tkr->xtime_nsec;
    0.00 :   ffff80001016ad80:       madd    x0, x0, x1, x4
         :                      nsec >>= tkr->shift;
    0.00 :   ffff80001016ad84:       lsr     x0, x0, x3
         :                      ktime_get():
         :                      }
    0.00 :   ffff80001016ad88:       add     x0, x0, x21
    0.00 :   ffff80001016ad8c:       ldp     x21, x22, [sp, #32]
    3.13 :   ffff80001016ad90:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001016ad94:       ret
         :                      cpu_relax():
         :
         :                      unsigned long get_wchan(struct task_struct *p);
         :
         :                      static inline void cpu_relax(void)
         :                      {
         :                      asm volatile("yield" ::: "memory");
    0.02 :   ffff80001016ad98:       yield
    0.02 :   ffff80001016ad9c:       b       ffff80001016ad34 <ktime_get+0x24>
         :                      ktime_get():
         :                      WARN_ON(timekeeping_suspended);
    0.00 :   ffff80001016ada0:       brk     #0x800
    0.00 :   ffff80001016ada4:       b       ffff80001016ad2c <ktime_get+0x1c>
 Percent |	Source code & Disassembly of vmlinux for cycles (4151 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010455110 <blk_queue_exit>:
         :                      blk_queue_exit():
         :                      return -ENODEV;
         :                      }
         :                      }
         :
         :                      void blk_queue_exit(struct request_queue *q)
         :                      {
    0.00 :   ffff800010455110:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff800010455114:       mov     x29, sp
    0.02 :   ffff800010455118:       str     x19, [sp, #16]
    0.00 :   ffff80001045511c:       mov     x19, x0
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff800010455120:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010455124:       ldr     x1, [x19, #1480]
         :                      __ref_is_percpu():
         :                      * Theoretically, the following could test just ATOMIC; however,
         :                      * then we'd have to mask off DEAD separately as DEAD may be
         :                      * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
         :                      * implies ATOMIC anyway.  Test them together.
         :                      */
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff800010455128:       tst     x1, #0x3
    0.00 :   ffff80001045512c:       b.ne    ffff80001045519c <blk_queue_exit+0x8c>  // b.any
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.46 :   ffff800010455130:       mrs     x2, sp_el0
         :                      __read_once_size():
    3.35 :   ffff800010455134:       ldr     w0, [x2, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff800010455138:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.10 :   ffff80001045513c:       str     w0, [x2, #16]
         :                      __percpu_add_case_64():
         :
         :                      PERCPU_RW_OPS(8)
         :                      PERCPU_RW_OPS(16)
         :                      PERCPU_RW_OPS(32)
         :                      PERCPU_RW_OPS(64)
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff800010455140:       mov     x3, #0xffffffffffffffff         // #-1
         :                      percpu_ref_put_many():
         :                      unsigned long __percpu *percpu_count;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count))
         :                      this_cpu_sub(*percpu_count, nr);
    0.00 :   ffff800010455144:       mov     x0, x1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.14 :   ffff800010455148:       mrs     x1, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.36 :   ffff80001045514c:       add     x0, x0, x1
    3.74 :   ffff800010455150:       ldxr    x5, [x0]
    1.45 :   ffff800010455154:       add     x5, x5, x3
    0.05 :   ffff800010455158:       stxr    w4, x5, [x0]
    0.28 :   ffff80001045515c:       cbnz    w4, ffff800010455150 <blk_queue_exit+0x40>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
   21.10 :   ffff800010455160:       ldr     x0, [x2, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800010455164:       add     x0, x0, x3
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
   19.82 :   ffff800010455168:       str     w0, [x2, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff80001045516c:       cbnz    x0, ffff800010455184 <blk_queue_exit+0x74>
         :                      percpu_ref_put_many():
    0.00 :   ffff800010455170:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.00 :   ffff800010455174:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_queue_exit():
         :                      percpu_ref_put(&q->q_usage_counter);
         :                      }
    0.00 :   ffff800010455178:       ldr     x19, [sp, #16]
    0.00 :   ffff80001045517c:       ldp     x29, x30, [sp], #32
    0.00 :   ffff800010455180:       ret
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    8.78 :   ffff800010455184:       ldr     x0, [x2, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff800010455188:       cbz     x0, ffff800010455170 <blk_queue_exit+0x60>
         :                      rcu_read_unlock():
   38.97 :   ffff80001045518c:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_queue_exit():
    0.58 :   ffff800010455190:       ldr     x19, [sp, #16]
    0.79 :   ffff800010455194:       ldp     x29, x30, [sp], #32
    0.00 :   ffff800010455198:       ret
         :                      percpu_ref_put(&q->q_usage_counter);
    0.00 :   ffff80001045519c:       add     x0, x19, #0x5c0
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000104551a0:       b       ffff8000104551c8 <blk_queue_exit+0xb8>
    0.00 :   ffff8000104551a4:       b       ffff8000104551c8 <blk_queue_exit+0xb8>
         :                      __lse_atomic64_sub_return():
         :                      }
         :
         :                      ATOMIC64_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC64_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff8000104551a8:       mov     x1, #0x1                        // #1
    0.00 :   ffff8000104551ac:       neg     x1, x1
    0.00 :   ffff8000104551b0:       ldaddal x1, x2, [x0]
    0.00 :   ffff8000104551b4:       add     x1, x1, x2
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff8000104551b8:       cbnz    x1, ffff800010455174 <blk_queue_exit+0x64>
         :                      ref->release(ref);
    0.00 :   ffff8000104551bc:       ldr     x1, [x0, #16]
    0.00 :   ffff8000104551c0:       blr     x1
    0.00 :   ffff8000104551c4:       b       ffff800010455174 <blk_queue_exit+0x64>
         :                      __ll_sc_atomic64_sub_return():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(add, add, I)
         :                      ATOMIC64_OPS(sub, sub, J)
    0.00 :   ffff8000104551c8:       mov     x2, #0x1                        // #1
    0.00 :   ffff8000104551cc:       add     x4, x19, #0x5c0
    0.00 :   ffff8000104551d0:       b       ffff800010456494 <blk_finish_plug+0x1ac>
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff8000104551d4:       cbnz    x1, ffff800010455174 <blk_queue_exit+0x64>
    0.00 :   ffff8000104551d8:       b       ffff8000104551bc <blk_queue_exit+0xac>
 Percent |	Source code & Disassembly of vmlinux for cycles (5183 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001015eee0 <__rcu_read_unlock>:
         :                      __rcu_read_unlock():
         :                      * rcu_read_unlock()) and ->rcu_read_unlock_special is non-zero, then
         :                      * invoke rcu_read_unlock_special() to clean up after a context switch
         :                      * in an RCU read-side critical section and other special cases.
         :                      */
         :                      void __rcu_read_unlock(void)
         :                      {
   13.69 :   ffff80001015eee0:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff80001015eee4:       mov     x29, sp
    9.92 :   ffff80001015eee8:       str     x19, [sp, #16]
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    2.86 :   ffff80001015eeec:       mrs     x19, sp_el0
         :                      __rcu_read_unlock():
         :                      struct task_struct *t = current;
         :
         :                      if (t->rcu_read_lock_nesting != 1) {
    5.04 :   ffff80001015eef0:       ldr     w0, [x19, #776]
    1.87 :   ffff80001015eef4:       cmp     w0, #0x1
    0.06 :   ffff80001015eef8:       b.eq    ffff80001015ef10 <__rcu_read_unlock+0x30>  // b.none
         :                      --t->rcu_read_lock_nesting;
    5.45 :   ffff80001015eefc:       sub     w0, w0, #0x1
    1.06 :   ffff80001015ef00:       str     w0, [x19, #776]
         :                      if (IS_ENABLED(CONFIG_PROVE_LOCKING)) {
         :                      int rrln = t->rcu_read_lock_nesting;
         :
         :                      WARN_ON_ONCE(rrln < 0 && rrln > RCU_NEST_NMAX);
         :                      }
         :                      }
    4.16 :   ffff80001015ef04:       ldr     x19, [sp, #16]
    5.97 :   ffff80001015ef08:       ldp     x29, x30, [sp], #32
    0.02 :   ffff80001015ef0c:       ret
         :                      t->rcu_read_lock_nesting = -RCU_NEST_BIAS;
   19.38 :   ffff80001015ef10:       mov     w1, #0x80000001                 // #-2147483647
    6.77 :   ffff80001015ef14:       str     w1, [x19, #776]
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.73 :   ffff80001015ef18:       ldr     w1, [x19, #780]
         :                      __rcu_read_unlock():
         :                      if (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))
    0.00 :   ffff80001015ef1c:       cbnz    w1, ffff80001015ef30 <__rcu_read_unlock+0x50>
         :                      t->rcu_read_lock_nesting = 0;
    6.57 :   ffff80001015ef20:       str     wzr, [x19, #776]
         :                      }
   10.90 :   ffff80001015ef24:       ldr     x19, [sp, #16]
    5.43 :   ffff80001015ef28:       ldp     x29, x30, [sp], #32
    0.00 :   ffff80001015ef2c:       ret
         :                      __read_once_size():
    0.02 :   ffff80001015ef30:       ldr     w1, [x19, #16]
    0.02 :   ffff80001015ef34:       ldr     w2, [x19, #16]
         :                      rcu_read_unlock_special():
         :                      bool preempt_bh_were_disabled =
         :                      !!(preempt_count() & (PREEMPT_MASK | SOFTIRQ_MASK));
         :                      bool irqs_were_disabled;
         :
         :                      /* NMI handlers cannot block and cannot safely manipulate state. */
         :                      if (in_nmi())
    0.00 :   ffff80001015ef38:       tbnz    w2, #20, ffff80001015ef20 <__rcu_read_unlock+0x40>
    0.00 :   ffff80001015ef3c:       str     x20, [x29, #24]
         :                      arch_local_save_flags():
         :                      */
         :                      static inline unsigned long arch_local_save_flags(void)
         :                      {
         :                      unsigned long flags;
         :
         :                      asm volatile(ALTERNATIVE(
    0.02 :   ffff80001015ef40:       mrs     x20, daif
         :                      arch_irqs_disabled_flags():
         :                      asm volatile(ALTERNATIVE(
         :                      "and    %w0, %w1, #" __stringify(PSR_I_BIT),
         :                      "eor    %w0, %w1, #" __stringify(GIC_PRIO_IRQON),
         :                      ARM64_HAS_IRQ_PRIO_MASKING)
         :                      : "=&r" (res)
         :                      : "r" ((int) flags)
    0.00 :   ffff80001015ef44:       mov     w2, w20
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015ef48:       and     w3, w20, #0x80
         :                      arch_local_irq_save():
         :
         :                      /*
         :                      * There are too many states with IRQs disabled, just keep the current
         :                      * state if interrupts are already disabled/masked.
         :                      */
         :                      if (!arch_irqs_disabled_flags(flags))
    0.00 :   ffff80001015ef4c:       cbnz    w3, ffff80001015ef58 <__rcu_read_unlock+0x78>
         :                      arch_local_irq_disable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015ef50:       mov     x3, #0x60                       // #96
    0.08 :   ffff80001015ef54:       msr     daifset, #0x2
         :                      arch_irqs_disabled_flags():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015ef58:       and     w3, w2, #0x80
         :                      rcu_read_unlock_special():
         :                      return;
         :
         :                      local_irq_save(flags);
         :                      irqs_were_disabled = irqs_disabled_flags(flags);
         :                      if (preempt_bh_were_disabled || irqs_were_disabled) {
    0.00 :   ffff80001015ef5c:       and     w1, w1, #0xffff
    0.00 :   ffff80001015ef60:       orr     w1, w1, w3
    0.00 :   ffff80001015ef64:       cbz     w1, ffff80001015f04c <__rcu_read_unlock+0x16c>
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff80001015ef68:       mrs     x4, tpidr_el1
         :                      rcu_read_unlock_special():
         :                      bool exp;
         :                      struct rcu_data *rdp = this_cpu_ptr(&rcu_data);
    0.00 :   ffff80001015ef6c:       adrp    x1, ffff8000114d6000 <runqueues+0x280>
    0.00 :   ffff80001015ef70:       add     x1, x1, #0x9c0
    0.00 :   ffff80001015ef74:       add     x1, x1, x4
         :                      struct rcu_node *rnp = rdp->mynode;
         :
         :                      t->rcu_read_unlock_special.b.exp_hint = false;
         :                      exp = (t->rcu_blocked_node && t->rcu_blocked_node->exp_tasks) ||
    0.00 :   ffff80001015ef78:       ldr     x2, [x19, #800]
         :                      struct rcu_node *rnp = rdp->mynode;
    0.00 :   ffff80001015ef7c:       ldr     x4, [x1, #24]
         :                      t->rcu_read_unlock_special.b.exp_hint = false;
    0.00 :   ffff80001015ef80:       strb    wzr, [x19, #782]
         :                      (rdp->grpmask & rnp->expmask) ||
    0.00 :   ffff80001015ef84:       cbz     x2, ffff80001015efdc <__rcu_read_unlock+0xfc>
         :                      exp = (t->rcu_blocked_node && t->rcu_blocked_node->exp_tasks) ||
    0.00 :   ffff80001015ef88:       ldr     x2, [x2, #152]
    0.00 :   ffff80001015ef8c:       cbz     x2, ffff80001015efdc <__rcu_read_unlock+0xfc>
         :                      tick_nohz_full_cpu(rdp->cpu);
         :                      // Need to defer quiescent state until everything is enabled.
         :                      if (irqs_were_disabled && use_softirq &&
    0.00 :   ffff80001015ef90:       cbz     w3, ffff80001015eff0 <__rcu_read_unlock+0x110>
    0.00 :   ffff80001015ef94:       adrp    x2, ffff8000118b2000 <irq_gc_syscore_ops+0x18>
    0.00 :   ffff80001015ef98:       ldrb    w2, [x2, #2544]
    0.00 :   ffff80001015ef9c:       cbz     w2, ffff80001015eff0 <__rcu_read_unlock+0x110>
         :                      get_current():
    0.00 :   ffff80001015efa0:       mrs     x2, sp_el0
         :                      __read_once_size():
    0.00 :   ffff80001015efa4:       ldr     w2, [x2, #16]
         :                      rcu_read_unlock_special():
    0.00 :   ffff80001015efa8:       tst     w2, #0x1fff00
    0.00 :   ffff80001015efac:       b.ne    ffff80001015efbc <__rcu_read_unlock+0xdc>  // b.any
         :                      (in_interrupt() ||
    0.00 :   ffff80001015efb0:       cbz     w0, ffff80001015eff0 <__rcu_read_unlock+0x110>
         :                      (exp && !t->rcu_read_unlock_special.b.deferred_qs))) {
    0.00 :   ffff80001015efb4:       ldrb    w2, [x19, #783]
    0.00 :   ffff80001015efb8:       cbnz    w2, ffff80001015eff0 <__rcu_read_unlock+0x110>
         :                      // Using softirq, safe to awaken, and we get
         :                      // no help from enabling irqs, unlike bh/preempt.
         :                      raise_softirq_irqoff(RCU_SOFTIRQ);
    0.00 :   ffff80001015efbc:       mov     w0, #0x9                        // #9
    0.00 :   ffff80001015efc0:       bl      ffff8000100ebe30 <raise_softirq_irqoff>
         :                      rcu_preempt_deferred_qs_handler);
         :                      rdp->defer_qs_iw_pending = true;
         :                      irq_work_queue_on(&rdp->defer_qs_iw, rdp->cpu);
         :                      }
         :                      }
         :                      t->rcu_read_unlock_special.b.deferred_qs = true;
    0.00 :   ffff80001015efc4:       mov     w0, #0x1                        // #1
    0.00 :   ffff80001015efc8:       strb    w0, [x19, #783]
         :                      arch_local_irq_restore():
         :                      /*
         :                      * restore saved IRQ state
         :                      */
         :                      static inline void arch_local_irq_restore(unsigned long flags)
         :                      {
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015efcc:       msr     daif, x20
    0.00 :   ffff80001015efd0:       ldr     x20, [x29, #24]
         :                      __rcu_read_unlock():
         :                      t->rcu_read_lock_nesting = 0;
    0.00 :   ffff80001015efd4:       str     wzr, [x19, #776]
    0.00 :   ffff80001015efd8:       b       ffff80001015ef24 <__rcu_read_unlock+0x44>
         :                      rcu_read_unlock_special():
         :                      (rdp->grpmask & rnp->expmask) ||
    0.00 :   ffff80001015efdc:       ldr     x0, [x4, #64]
    0.00 :   ffff80001015efe0:       ldr     x2, [x1, #32]
         :                      exp = (t->rcu_blocked_node && t->rcu_blocked_node->exp_tasks) ||
    0.00 :   ffff80001015efe4:       tst     x2, x0
    0.00 :   ffff80001015efe8:       cset    w0, ne  // ne = any
    0.00 :   ffff80001015efec:       b       ffff80001015ef90 <__rcu_read_unlock+0xb0>
         :                      get_current():
    0.00 :   ffff80001015eff0:       mrs     x2, sp_el0
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff80001015eff4:       b       ffff80001015f044 <__rcu_read_unlock+0x164>
    0.00 :   ffff80001015eff8:       b       ffff80001015f044 <__rcu_read_unlock+0x164>
         :                      __lse_atomic64_or():
         :                      : [i] "+r" (i), [v] "+Q" (v->counter)                           \
         :                      : "r" (v));                                                     \
         :                      }
         :
         :                      ATOMIC64_OP(andnot, stclr)
         :                      ATOMIC64_OP(or, stset)
    0.00 :   ffff80001015effc:       mov     x4, #0x2                        // #2
    0.00 :   ffff80001015f000:       stset   x4, [x2]
         :                      get_current():
    0.00 :   ffff80001015f004:       mrs     x2, sp_el0
         :                      set_preempt_need_resched():
         :                      task_thread_info(p)->preempt_count = PREEMPT_ENABLED; \
         :                      } while (0)
         :
         :                      static inline void set_preempt_need_resched(void)
         :                      {
         :                      current_thread_info()->preempt.need_resched = 0;
    0.00 :   ffff80001015f008:       str     wzr, [x2, #20]
         :                      rcu_read_unlock_special():
         :                      if (IS_ENABLED(CONFIG_IRQ_WORK) && irqs_were_disabled &&
    0.00 :   ffff80001015f00c:       cbz     w3, ffff80001015efc4 <__rcu_read_unlock+0xe4>
    0.00 :   ffff80001015f010:       ldrb    w2, [x1, #72]
         :                      !rdp->defer_qs_iw_pending && exp) {
    0.00 :   ffff80001015f014:       bic     x0, x0, x2
    0.00 :   ffff80001015f018:       tbz     w0, #0, ffff80001015efc4 <__rcu_read_unlock+0xe4>
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001015f01c:       mov     x0, x1
         :                      init_irq_work():
         :
         :                      static inline
         :                      void init_irq_work(struct irq_work *work, void (*func)(struct irq_work *))
         :                      {
         :                      atomic_set(&work->flags, 0);
         :                      work->func = func;
    0.00 :   ffff80001015f020:       adrp    x2, ffff80001015a000 <__call_srcu+0x80>
    0.00 :   ffff80001015f024:       add     x2, x2, #0x6b0
         :                      rcu_read_unlock_special():
         :                      rdp->defer_qs_iw_pending = true;
    0.00 :   ffff80001015f028:       mov     w3, #0x1                        // #1
         :                      __write_once_size():
    0.00 :   ffff80001015f02c:       str     wzr, [x0, #48]!
         :                      init_irq_work():
    0.00 :   ffff80001015f030:       str     x2, [x0, #16]
         :                      rcu_read_unlock_special():
    0.00 :   ffff80001015f034:       strb    w3, [x1, #72]
         :                      irq_work_queue_on(&rdp->defer_qs_iw, rdp->cpu);
    0.00 :   ffff80001015f038:       ldr     w1, [x1, #360]
    0.00 :   ffff80001015f03c:       bl      ffff8000101b2f80 <irq_work_queue_on>
    0.00 :   ffff80001015f040:       b       ffff80001015efc4 <__rcu_read_unlock+0xe4>
         :                      __ll_sc_atomic64_or():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(and, and, L)
         :                      ATOMIC64_OPS(or, orr, L)
    0.00 :   ffff80001015f044:       b       ffff8000101607cc <rcu_needs_cpu+0x1ac>
    0.00 :   ffff80001015f048:       b       ffff80001015f004 <__rcu_read_unlock+0x124>
         :                      __write_once_size():
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
    0.00 :   ffff80001015f04c:       strb    wzr, [x19, #782]
         :                      rcu_read_unlock_special():
         :                      local_irq_restore(flags);
         :                      return;
         :                      }
         :                      WRITE_ONCE(t->rcu_read_unlock_special.b.exp_hint, false);
         :                      rcu_preempt_deferred_qs_irqrestore(t, flags);
    0.00 :   ffff80001015f050:       mov     x1, x20
    0.00 :   ffff80001015f054:       mov     x0, x19
    0.00 :   ffff80001015f058:       bl      ffff80001015e158 <rcu_preempt_deferred_qs_irqrestore>
    0.00 :   ffff80001015f05c:       ldr     x20, [x29, #24]
         :                      __rcu_read_unlock():
         :                      t->rcu_read_lock_nesting = 0;
    0.00 :   ffff80001015f060:       str     wzr, [x19, #776]
    0.00 :   ffff80001015f064:       b       ffff80001015ef24 <__rcu_read_unlock+0x44>
 Percent |	Source code & Disassembly of vmlinux for cycles (3802 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101d5b40 <mempool_free>:
         :                      mempool_free():
         :                      */
         :                      void mempool_free(void *element, mempool_t *pool)
         :                      {
         :                      unsigned long flags;
         :
         :                      if (unlikely(element == NULL))
    4.02 :   ffff8000101d5b40:       cbz     x0, ffff8000101d5b84 <mempool_free+0x44>
         :                      {
    0.02 :   ffff8000101d5b44:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff8000101d5b48:       mov     x29, sp
    1.25 :   ffff8000101d5b4c:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000101d5b50:       mov     x19, x1
    0.00 :   ffff8000101d5b54:       mov     x20, x0
         :                      * performs "p = mempool_alloc(...);" while another task is doing
         :                      * "while (!p) cpu_relax(); mempool_free(p, ...);".  This function
         :                      * may end up using curr_nr value which is from before allocation
         :                      * of @p without the following rmb.
         :                      */
         :                      smp_rmb();
    3.43 :   ffff8000101d5b58:       dmb     ishld
         :                      *
         :                      * Waiters happen iff curr_nr is 0 and the above guarantee also
         :                      * ensures that there will be frees which return elements to the
         :                      * pool waking up the waiters.
         :                      */
         :                      if (unlikely(pool->curr_nr < pool->min_nr)) {
   87.33 :   ffff8000101d5b5c:       ldp     w0, w1, [x1, #4]
    0.00 :   ffff8000101d5b60:       cmp     w1, w0
    0.00 :   ffff8000101d5b64:       b.lt    ffff8000101d5b88 <mempool_free+0x48>  // b.tstop
         :                      wake_up(&pool->wait);
         :                      return;
         :                      }
         :                      spin_unlock_irqrestore(&pool->lock, flags);
         :                      }
         :                      pool->free(element, pool->pool_data);
    0.00 :   ffff8000101d5b68:       ldr     x1, [x19, #24]
    0.00 :   ffff8000101d5b6c:       mov     x0, x20
    0.00 :   ffff8000101d5b70:       ldr     x2, [x19, #40]
    0.00 :   ffff8000101d5b74:       blr     x2
         :                      }
    3.74 :   ffff8000101d5b78:       ldp     x19, x20, [sp, #16]
    0.21 :   ffff8000101d5b7c:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000101d5b80:       ret
    0.00 :   ffff8000101d5b84:       ret
         :                      spin_lock_irqsave(&pool->lock, flags);
    0.00 :   ffff8000101d5b88:       mov     x0, x19
    0.00 :   ffff8000101d5b8c:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
         :                      if (likely(pool->curr_nr < pool->min_nr)) {
    0.00 :   ffff8000101d5b90:       ldp     w3, w2, [x19, #4]
    0.00 :   ffff8000101d5b94:       cmp     w2, w3
    0.00 :   ffff8000101d5b98:       b.ge    ffff8000101d5bd0 <mempool_free+0x90>  // b.tcont
         :                      add_element():
         :                      pool->elements[pool->curr_nr++] = element;
    0.00 :   ffff8000101d5b9c:       ldr     x3, [x19, #16]
    0.00 :   ffff8000101d5ba0:       add     w1, w2, #0x1
    0.00 :   ffff8000101d5ba4:       str     w1, [x19, #8]
         :                      spin_unlock_irqrestore():
         :                      raw_spin_unlock_irq(&lock->rlock);
         :                      }
         :
         :                      static __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
         :                      {
         :                      raw_spin_unlock_irqrestore(&lock->rlock, flags);
    0.00 :   ffff8000101d5ba8:       mov     x1, x0
    0.00 :   ffff8000101d5bac:       mov     x0, x19
         :                      add_element():
    0.00 :   ffff8000101d5bb0:       str     x20, [x3, w2, sxtw #3]
         :                      spin_unlock_irqrestore():
    0.00 :   ffff8000101d5bb4:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      mempool_free():
         :                      wake_up(&pool->wait);
    0.00 :   ffff8000101d5bb8:       mov     x3, #0x0                        // #0
    0.00 :   ffff8000101d5bbc:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000101d5bc0:       mov     w1, #0x3                        // #3
    0.00 :   ffff8000101d5bc4:       add     x0, x19, #0x30
    0.00 :   ffff8000101d5bc8:       bl      ffff80001012e430 <__wake_up>
         :                      return;
    0.00 :   ffff8000101d5bcc:       b       ffff8000101d5b78 <mempool_free+0x38>
         :                      spin_unlock_irqrestore():
    0.00 :   ffff8000101d5bd0:       mov     x1, x0
    0.00 :   ffff8000101d5bd4:       mov     x0, x19
    0.00 :   ffff8000101d5bd8:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
    0.00 :   ffff8000101d5bdc:       b       ffff8000101d5b68 <mempool_free+0x28>
 Percent |	Source code & Disassembly of vmlinux for cycles (4406 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010acc3e0 <arch_counter_read>:
         :                      arch_counter_read():
         :                      */
         :                      u64 (*arch_timer_read_counter)(void) = arch_counter_get_cntvct;
         :                      EXPORT_SYMBOL_GPL(arch_timer_read_counter);
         :
         :                      static u64 arch_counter_read(struct clocksource *cs)
         :                      {
    0.07 :   ffff800010acc3e0:       stp     x29, x30, [sp, #-16]!
         :                      return arch_timer_read_counter();
    0.00 :   ffff800010acc3e4:       adrp    x0, ffff800011a49000 <mc_algs+0x300>
         :                      {
    0.16 :   ffff800010acc3e8:       mov     x29, sp
         :                      return arch_timer_read_counter();
    3.92 :   ffff800010acc3ec:       ldr     x0, [x0, #3600]
    0.00 :   ffff800010acc3f0:       blr     x0
         :                      }
   95.85 :   ffff800010acc3f4:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010acc3f8:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (4418 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010acc3b0 <arch_counter_get_cntpct>:
         :                      __arch_counter_get_cntpct():
         :
         :                      static __always_inline u64 __arch_counter_get_cntpct(void)
         :                      {
         :                      u64 cnt;
         :
         :                      isb();
    0.00 :   ffff800010acc3b0:       isb
         :                      cnt = read_sysreg(cntpct_el0);
   91.51 :   ffff800010acc3b4:       mrs     x0, cntpct_el0
         :                      arch_counter_enforce_ordering(cnt);
    4.48 :   ffff800010acc3b8:       eor     x1, x0, x0
    0.00 :   ffff800010acc3bc:       add     x1, sp, x1
    4.01 :   ffff800010acc3c0:       ldr     xzr, [x1]
         :                      arch_counter_get_cntpct():
         :                      }
         :
         :                      static notrace u64 arch_counter_get_cntpct(void)
         :                      {
         :                      return __arch_counter_get_cntpct();
         :                      }
    0.00 :   ffff800010acc3c4:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (3706 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001014ba78 <wake_threads_waitq>:
         :                      wake_threads_waitq():
         :                      irq_finalize_oneshot(desc, action);
         :                      return ret;
         :                      }
         :
         :                      static void wake_threads_waitq(struct irq_desc *desc)
         :                      {
    0.33 :   ffff80001014ba78:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff80001014ba7c:       mov     x29, sp
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    5.20 :   ffff80001014ba80:       b       ffff80001014babc <wake_threads_waitq+0x44>
    0.08 :   ffff80001014ba84:       b       ffff80001014babc <wake_threads_waitq+0x44>
    0.14 :   ffff80001014ba88:       add     x2, x0, #0x100
         :                      __lse_atomic_sub_return():
         :                      }
         :
         :                      ATOMIC_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff80001014ba8c:       mov     w1, #0x1                        // #1
    0.00 :   ffff80001014ba90:       neg     w1, w1
    0.08 :   ffff80001014ba94:       ldaddal w1, w3, [x2]
   92.07 :   ffff80001014ba98:       add     w1, w1, w3
         :                      wake_threads_waitq():
         :                      if (atomic_dec_and_test(&desc->threads_active))
    0.00 :   ffff80001014ba9c:       cbnz    w1, ffff80001014bab4 <wake_threads_waitq+0x3c>
         :                      wake_up(&desc->wait_for_threads);
    1.69 :   ffff80001014baa0:       mov     x3, #0x0                        // #0
    0.00 :   ffff80001014baa4:       mov     w2, #0x1                        // #1
    0.00 :   ffff80001014baa8:       mov     w1, #0x3                        // #3
    0.00 :   ffff80001014baac:       add     x0, x0, #0x108
    0.14 :   ffff80001014bab0:       bl      ffff80001012e430 <__wake_up>
         :                      }
    0.27 :   ffff80001014bab4:       ldp     x29, x30, [sp], #16
    0.00 :   ffff80001014bab8:       ret
         :                      __ll_sc_atomic_sub_return():
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff80001014babc:       mov     w2, #0x1                        // #1
    0.00 :   ffff80001014bac0:       add     x4, x0, #0x100
    0.00 :   ffff80001014bac4:       b       ffff80001014e544 <__irq_get_irqchip_state+0x34>
    0.00 :   ffff80001014bac8:       b       ffff80001014ba9c <wake_threads_waitq+0x24>
 Percent |	Source code & Disassembly of vmlinux for cycles (3575 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000106fb9b0 <__iommu_unmap>:
         :                      __iommu_unmap():
         :                      EXPORT_SYMBOL_GPL(iommu_map_atomic);
         :
         :                      static size_t __iommu_unmap(struct iommu_domain *domain,
         :                      unsigned long iova, size_t size,
         :                      struct iommu_iotlb_gather *iotlb_gather)
         :                      {
    1.47 :   ffff8000106fb9b0:       stp     x29, x30, [sp, #-64]!
    0.03 :   ffff8000106fb9b4:       mov     x29, sp
    2.52 :   ffff8000106fb9b8:       stp     x20, x21, [sp, #24]
    0.00 :   ffff8000106fb9bc:       mov     x21, x0
    0.24 :   ffff8000106fb9c0:       str     x23, [sp, #48]
         :                      const struct iommu_ops *ops = domain->ops;
    0.33 :   ffff8000106fb9c4:       ldr     x23, [x0, #8]
         :                      size_t unmapped_page, unmapped = 0;
         :                      unsigned long orig_iova = iova;
         :                      unsigned int min_pagesz;
         :
         :                      if (unlikely(ops->unmap == NULL ||
    0.78 :   ffff8000106fb9c8:       ldr     x0, [x23, #48]
   21.28 :   ffff8000106fb9cc:       cbz     x0, ffff8000106fba84 <__iommu_unmap+0xd4>
   19.40 :   ffff8000106fb9d0:       ldr     x0, [x21, #16]
    0.19 :   ffff8000106fb9d4:       cbz     x0, ffff8000106fba84 <__iommu_unmap+0xd4>
    3.16 :   ffff8000106fb9d8:       str     x24, [x29, #56]
    0.00 :   ffff8000106fb9dc:       mov     x24, x3
         :                      domain->pgsize_bitmap == 0UL))
         :                      return 0;
         :
         :                      if (unlikely(!(domain->type & __IOMMU_DOMAIN_PAGING)))
    0.47 :   ffff8000106fb9e0:       ldr     w3, [x21]
    0.06 :   ffff8000106fb9e4:       tbz     w3, #0, ffff8000106fba9c <__iommu_unmap+0xec>
         :                      __ffs():
         :                      *
         :                      * Undefined if no bit exists, so code should check against 0 first.
         :                      */
         :                      static __always_inline unsigned long __ffs(unsigned long word)
         :                      {
         :                      return __builtin_ctzl(word);
    0.62 :   ffff8000106fb9e8:       rbit    x4, x0
         :                      __iommu_unmap():
         :                      return 0;
         :
         :                      /* find out the minimum page size supported */
         :                      min_pagesz = 1 << __ffs(domain->pgsize_bitmap);
    0.03 :   ffff8000106fb9ec:       mov     w3, #0x1                        // #1
         :                      __ffs():
    0.00 :   ffff8000106fb9f0:       clz     x4, x4
    0.59 :   ffff8000106fb9f4:       str     x19, [x29, #16]
    3.30 :   ffff8000106fb9f8:       str     x22, [x29, #40]
         :                      __iommu_unmap():
         :                      /*
         :                      * The virtual address, as well as the size of the mapping, must be
         :                      * aligned (at least) to the size of the smallest page supported
         :                      * by the hardware
         :                      */
         :                      if (!IS_ALIGNED(iova | size, min_pagesz)) {
    0.19 :   ffff8000106fb9fc:       orr     x5, x1, x2
         :                      min_pagesz = 1 << __ffs(domain->pgsize_bitmap);
    0.03 :   ffff8000106fba00:       lsl     w4, w3, w4
    0.16 :   ffff8000106fba04:       mov     x3, x4
         :                      if (!IS_ALIGNED(iova | size, min_pagesz)) {
    6.62 :   ffff8000106fba08:       sub     x4, x4, #0x1
    0.03 :   ffff8000106fba0c:       mov     x19, x1
    0.00 :   ffff8000106fba10:       mov     x22, x2
    0.00 :   ffff8000106fba14:       tst     x5, x4
    0.53 :   ffff8000106fba18:       b.ne    ffff8000106fbaa8 <__iommu_unmap+0xf8>  // b.any
         :                      size_t unmapped_page, unmapped = 0;
    0.36 :   ffff8000106fba1c:       mov     x20, #0x0                       // #0
         :
         :                      /*
         :                      * Keep iterating until we either unmap 'size' bytes (or more)
         :                      * or we hit an area that isn't mapped.
         :                      */
         :                      while (unmapped < size) {
    0.00 :   ffff8000106fba20:       cbnz    x2, ffff8000106fba3c <__iommu_unmap+0x8c>
    0.00 :   ffff8000106fba24:       b       ffff8000106fba64 <__iommu_unmap+0xb4>
         :
         :                      pr_debug("unmapped: iova 0x%lx size 0x%zx\n",
         :                      iova, unmapped_page);
         :
         :                      iova += unmapped_page;
         :                      unmapped += unmapped_page;
    1.73 :   ffff8000106fba28:       add     x20, x20, x0
         :                      iova += unmapped_page;
    0.03 :   ffff8000106fba2c:       add     x19, x19, x0
         :                      while (unmapped < size) {
    0.00 :   ffff8000106fba30:       cmp     x22, x20
    0.03 :   ffff8000106fba34:       b.ls    ffff8000106fba64 <__iommu_unmap+0xb4>  // b.plast
    0.00 :   ffff8000106fba38:       ldr     x0, [x21, #16]
         :                      size_t pgsize = iommu_pgsize(domain, iova, size - unmapped);
    6.58 :   ffff8000106fba3c:       sub     x2, x22, x20
    0.11 :   ffff8000106fba40:       mov     x1, x19
    0.03 :   ffff8000106fba44:       bl      ffff8000106fb958 <iommu_pgsize.isra.21>
         :                      unmapped_page = ops->unmap(domain, iova, pgsize, iotlb_gather);
    0.95 :   ffff8000106fba48:       mov     x2, x0
    0.30 :   ffff8000106fba4c:       ldr     x4, [x23, #48]
    0.09 :   ffff8000106fba50:       mov     x1, x19
    0.17 :   ffff8000106fba54:       mov     x3, x24
    0.03 :   ffff8000106fba58:       mov     x0, x21
    0.73 :   ffff8000106fba5c:       blr     x4
         :                      if (!unmapped_page)
    3.10 :   ffff8000106fba60:       cbnz    x0, ffff8000106fba28 <__iommu_unmap+0x78>
    0.25 :   ffff8000106fba64:       ldr     x19, [x29, #16]
   13.48 :   ffff8000106fba68:       ldr     x22, [x29, #40]
    4.34 :   ffff8000106fba6c:       ldr     x24, [x29, #56]
         :                      }
         :
         :                      trace_unmap(orig_iova, size, unmapped);
         :                      return unmapped;
         :                      }
    0.00 :   ffff8000106fba70:       mov     x0, x20
    2.18 :   ffff8000106fba74:       ldr     x23, [sp, #48]
    0.11 :   ffff8000106fba78:       ldp     x20, x21, [sp, #24]
    3.17 :   ffff8000106fba7c:       ldp     x29, x30, [sp], #64
    0.20 :   ffff8000106fba80:       ret
         :                      return 0;
    0.00 :   ffff8000106fba84:       mov     x20, #0x0                       // #0
         :                      }
    0.00 :   ffff8000106fba88:       ldr     x23, [sp, #48]
    0.00 :   ffff8000106fba8c:       mov     x0, x20
    0.00 :   ffff8000106fba90:       ldp     x20, x21, [sp, #24]
    0.00 :   ffff8000106fba94:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000106fba98:       ret
         :                      return 0;
    0.00 :   ffff8000106fba9c:       mov     x20, #0x0                       // #0
    0.00 :   ffff8000106fbaa0:       ldr     x24, [x29, #56]
    0.00 :   ffff8000106fbaa4:       b       ffff8000106fba70 <__iommu_unmap+0xc0>
         :                      pr_err("unaligned: iova 0x%lx size 0x%zx min_pagesz 0x%x\n",
    0.00 :   ffff8000106fbaa8:       adrp    x0, ffff800011227000 <kallsyms_token_index+0xad330>
         :                      return 0;
    0.00 :   ffff8000106fbaac:       mov     x20, #0x0                       // #0
         :                      pr_err("unaligned: iova 0x%lx size 0x%zx min_pagesz 0x%x\n",
    0.00 :   ffff8000106fbab0:       add     x0, x0, #0xfd0
    0.00 :   ffff8000106fbab4:       bl      ffff800010148e94 <printk>
         :                      return 0;
    0.00 :   ffff8000106fbab8:       ldr     x19, [x29, #16]
    0.00 :   ffff8000106fbabc:       ldr     x22, [x29, #40]
    0.00 :   ffff8000106fbac0:       ldr     x24, [x29, #56]
    0.00 :   ffff8000106fbac4:       b       ffff8000106fba70 <__iommu_unmap+0xc0>
 Percent |	Source code & Disassembly of vmlinux for cycles (5405 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010701200 <arm_lpae_map>:
         :                      arm_lpae_map():
         :                      return pte;
         :                      }
         :
         :                      static int arm_lpae_map(struct io_pgtable_ops *ops, unsigned long iova,
         :                      phys_addr_t paddr, size_t size, int iommu_prot)
         :                      {
    0.83 :   ffff800010701200:       mov     x5, x0
         :                      arm_lpae_iopte *ptep = data->pgd;
         :                      int ret, lvl = data->start_level;
         :                      arm_lpae_iopte prot;
         :
         :                      /* If no access, then nothing to do */
         :                      if (!(iommu_prot & (IOMMU_READ | IOMMU_WRITE)))
    0.00 :   ffff800010701204:       ands    w0, w4, #0x3
    0.00 :   ffff800010701208:       b.eq    ffff80001070124c <arm_lpae_map+0x4c>  // b.none
         :                      {
    0.20 :   ffff80001070120c:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff800010701210:       mov     x29, sp
         :                      return 0;
         :
         :                      if (WARN_ON(!size || (size & cfg->pgsize_bitmap) != size))
    0.00 :   ffff800010701214:       cbnz    x3, ffff800010701228 <arm_lpae_map+0x28>
    0.00 :   ffff800010701218:       brk     #0x800
         :                      return -EINVAL;
    0.00 :   ffff80001070121c:       mov     w0, #0xffffffea                 // #-22
         :                      * a chance for anything to kick off a table walk for the new iova.
         :                      */
         :                      wmb();
         :
         :                      return ret;
         :                      }
    0.00 :   ffff800010701220:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010701224:       ret
         :                      if (WARN_ON(!size || (size & cfg->pgsize_bitmap) != size))
    0.29 :   ffff800010701228:       ldur    x6, [x5, #-72]
    0.00 :   ffff80001070122c:       bics    xzr, x3, x6
    0.00 :   ffff800010701230:       b.ne    ffff800010701218 <arm_lpae_map+0x18>  // b.any
         :                      if (WARN_ON(iova >> data->iop.cfg.ias || paddr >> data->iop.cfg.oas))
    0.09 :   ffff800010701234:       ldur    w6, [x5, #-64]
    0.00 :   ffff800010701238:       lsr     x6, x1, x6
    0.00 :   ffff80001070123c:       cbz     x6, ffff800010701250 <arm_lpae_map+0x50>
    0.00 :   ffff800010701240:       brk     #0x800
         :                      return -ERANGE;
    0.00 :   ffff800010701244:       mov     w0, #0xffffffde                 // #-34
    0.00 :   ffff800010701248:       b       ffff800010701220 <arm_lpae_map+0x20>
         :                      }
    0.00 :   ffff80001070124c:       ret
         :                      if (WARN_ON(iova >> data->iop.cfg.ias || paddr >> data->iop.cfg.oas))
    0.18 :   ffff800010701250:       ldur    w6, [x5, #-60]
    0.00 :   ffff800010701254:       lsr     x6, x2, x6
    0.00 :   ffff800010701258:       cbnz    x6, ffff800010701240 <arm_lpae_map+0x40>
         :                      arm_lpae_prot_to_pte():
         :                      if (data->iop.fmt == ARM_64_LPAE_S1 ||
    0.78 :   ffff80001070125c:       ldur    w6, [x5, #-96]
    0.04 :   ffff800010701260:       ands    w6, w6, #0xfffffffd
    0.00 :   ffff800010701264:       b.ne    ffff8000107012a0 <arm_lpae_map+0xa0>  // b.any
         :                      pte = ARM_LPAE_PTE_nG;
    0.33 :   ffff800010701268:       cmp     w0, #0x1
    0.00 :   ffff80001070126c:       mov     x7, #0x880                      // #2176
    0.00 :   ffff800010701270:       mov     x0, #0x800                      // #2048
    0.00 :   ffff800010701274:       csel    x7, x7, x0, eq  // eq = none
         :                      pte |= ARM_LPAE_PTE_AP_UNPRIV;
    0.11 :   ffff800010701278:       orr     x0, x7, #0x40
    0.00 :   ffff80001070127c:       tst     x4, #0x20
    0.00 :   ffff800010701280:       csel    x7, x0, x7, eq  // eq = none
    0.00 :   ffff800010701284:       and     w0, w4, #0x10
         :                      if (prot & IOMMU_MMIO)
    0.00 :   ffff800010701288:       cbnz    w0, ffff8000107012f8 <arm_lpae_map+0xf8>
         :                      else if (prot & IOMMU_CACHE)
    0.02 :   ffff80001070128c:       tbnz    w4, #2, ffff8000107012b4 <arm_lpae_map+0xb4>
         :                      pte |= (ARM_LPAE_MAIR_ATTR_IDX_INC_OCACHE
    0.00 :   ffff800010701290:       orr     x0, x7, #0xc
    0.00 :   ffff800010701294:       tst     x4, #0x40
    0.00 :   ffff800010701298:       csel    x7, x0, x7, ne  // ne = any
    0.00 :   ffff80001070129c:       b       ffff8000107012b8 <arm_lpae_map+0xb8>
         :                      pte |= ARM_LPAE_PTE_HAP_WRITE;
    0.00 :   ffff8000107012a0:       ubfiz   x7, x4, #6, #2
    0.00 :   ffff8000107012a4:       and     w0, w4, #0x10
         :                      if (data->iop.fmt == ARM_64_LPAE_S2 ||
    0.00 :   ffff8000107012a8:       cmp     w6, #0x1
    0.00 :   ffff8000107012ac:       b.ne    ffff800010701288 <arm_lpae_map+0x88>  // b.any
         :                      if (prot & IOMMU_MMIO)
    0.00 :   ffff8000107012b0:       cbz     w0, ffff8000107012e0 <arm_lpae_map+0xe0>
         :                      pte |= (ARM_LPAE_MAIR_ATTR_IDX_CACHE
    0.39 :   ffff8000107012b4:       orr     x7, x7, #0x4
         :                      arm_lpae_map():
         :                      arm_lpae_iopte *ptep = data->pgd;
    0.00 :   ffff8000107012b8:       sub     x0, x5, #0x60
         :                      ret = __arm_lpae_map(data, iova, paddr, size, prot, lvl, ptep);
    0.04 :   ffff8000107012bc:       ldr     x6, [x0, #136]
         :                      arm_lpae_prot_to_pte():
         :                      pte |= ARM_LPAE_PTE_XN;
    0.69 :   ffff8000107012c0:       tst     x4, #0x8
    0.00 :   ffff8000107012c4:       orr     x4, x7, #0x60000000000000
         :                      arm_lpae_map():
         :                      ret = __arm_lpae_map(data, iova, paddr, size, prot, lvl, ptep);
    0.00 :   ffff8000107012c8:       csel    x4, x4, x7, ne  // ne = any
    0.53 :   ffff8000107012cc:       ldr     w5, [x0, #124]
    0.00 :   ffff8000107012d0:       bl      ffff800010700f80 <__arm_lpae_map>
         :                      wmb();
    0.02 :   ffff8000107012d4:       dsb     st
         :                      }
   95.46 :   ffff8000107012d8:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000107012dc:       ret
         :                      arm_lpae_prot_to_pte():
         :                      pte |= ARM_LPAE_PTE_MEMATTR_OIWB;
    0.00 :   ffff8000107012e0:       orr     x0, x7, #0x3c
    0.00 :   ffff8000107012e4:       mov     x6, #0x14                       // #20
    0.00 :   ffff8000107012e8:       tst     x4, #0x4
    0.00 :   ffff8000107012ec:       orr     x7, x7, x6
    0.00 :   ffff8000107012f0:       csel    x7, x7, x0, eq  // eq = none
    0.00 :   ffff8000107012f4:       b       ffff8000107012b8 <arm_lpae_map+0xb8>
         :                      pte |= (ARM_LPAE_MAIR_ATTR_IDX_DEV
    0.00 :   ffff8000107012f8:       orr     x7, x7, #0x8
    0.00 :   ffff8000107012fc:       b       ffff8000107012b8 <arm_lpae_map+0xb8>
 Percent |	Source code & Disassembly of vmlinux for cycles (6507 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010096848 <el0_svc_common.constprop.2>:
         :                      el0_svc_common():
         :                      }
         :                      #else
         :                      static void cortex_a76_erratum_1463225_svc_handler(void) { }
         :                      #endif /* CONFIG_ARM64_ERRATUM_1463225 */
         :
         :                      static void el0_svc_common(struct pt_regs *regs, int scno, int sc_nr,
    0.00 :   ffff800010096848:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff80001009684c:       mov     x29, sp
    0.00 :   ffff800010096850:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010096854:       mov     x19, x0
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800010096858:       mrs     x0, sp_el0
         :                      el0_svc_common():
         :                      const syscall_fn_t syscall_table[])
         :                      {
         :                      unsigned long flags = current_thread_info()->flags;
    0.00 :   ffff80001009685c:       ldr     x20, [x0]
         :
         :                      regs->orig_x0 = regs->regs[0];
    0.00 :   ffff800010096860:       ldr     x3, [x19]
    0.00 :   ffff800010096864:       str     x3, [x19, #272]
         :                      regs->syscallno = scno;
    0.00 :   ffff800010096868:       str     w1, [x19, #280]
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff80001009686c:       ldr     x0, [x0]
         :                      cortex_a76_erratum_1463225_svc_handler():
         :                      if (!unlikely(test_thread_flag(TIF_SINGLESTEP)))
    0.00 :   ffff800010096870:       tst     w0, #0x200000
    0.00 :   ffff800010096874:       b.ne    ffff80001009691c <el0_svc_common.constprop.2+0xd4>  // b.any
         :                      local_daif_restore():
         :                      * So we don't need additional synchronization here.
         :                      */
         :                      gic_write_pmr(pmr);
         :                      }
         :
         :                      write_sysreg(flags, daif);
    0.00 :   ffff800010096878:       msr     daif, xzr
         :                      el0_svc_common():
         :
         :                      cortex_a76_erratum_1463225_svc_handler();
         :                      local_daif_restore(DAIF_PROCCTX);
         :                      user_exit();
         :
         :                      if (has_syscall_work(flags)) {
   94.40 :   ffff80001009687c:       ands    x20, x20, #0x1f80
    0.03 :   ffff800010096880:       b.ne    ffff8000100968ec <el0_svc_common.constprop.2+0xa4>  // b.any
         :                      invoke_syscall():
         :                      if (scno < sc_nr) {
    1.12 :   ffff800010096884:       cmp     w1, #0x1b3
    0.00 :   ffff800010096888:       b.hi    ffff800010096980 <el0_svc_common.constprop.2+0x138>  // b.pmore
         :                      syscall_fn = syscall_table[array_index_nospec(scno, sc_nr)];
    3.13 :   ffff80001009688c:       mov     w0, w1
         :                      array_index_mask_nospec():
         :                      static inline unsigned long array_index_mask_nospec(unsigned long idx,
         :                      unsigned long sz)
         :                      {
         :                      unsigned long mask;
         :
         :                      asm volatile(
    0.00 :   ffff800010096890:       cmp     x0, #0x1b4
    0.00 :   ffff800010096894:       ngc     x0, xzr
         :                      "       sbc     %0, xzr, xzr\n"
         :                      : "=r" (mask)
         :                      : "r" (idx), "Ir" (sz)
         :                      : "cc");
         :
         :                      csdb();
    0.00 :   ffff800010096898:       csdb
         :                      invoke_syscall():
    1.07 :   ffff80001009689c:       and     w1, w1, w0
         :                      __invoke_syscall():
         :                      return syscall_fn(regs);
    0.00 :   ffff8000100968a0:       mov     x0, x19
    0.00 :   ffff8000100968a4:       ldr     x1, [x2, x1, lsl #3]
    0.00 :   ffff8000100968a8:       blr     x1
         :                      invoke_syscall():
         :                      regs->regs[0] = ret;
    0.08 :   ffff8000100968ac:       str     x0, [x19]
         :                      el0_svc_common():
         :                      /*
         :                      * The tracing status may have changed under our feet, so we have to
         :                      * check again. However, if we were tracing entry, then we always trace
         :                      * exit regardless, as the old entry assembly did.
         :                      */
         :                      if (!has_syscall_work(flags) && !IS_ENABLED(CONFIG_DEBUG_RSEQ)) {
    0.00 :   ffff8000100968b0:       cbnz    x20, ffff8000100968d8 <el0_svc_common.constprop.2+0x90>
         :                      local_daif_mask():
         :                      asm volatile(
    0.17 :   ffff8000100968b4:       msr     daifset, #0xf
         :                      get_current():
    0.00 :   ffff8000100968b8:       mrs     x0, sp_el0
         :                      has_syscall_work():
         :                      return unlikely(flags & _TIF_SYSCALL_WORK);
    0.00 :   ffff8000100968bc:       ldr     x0, [x0]
         :                      el0_svc_common():
         :                      local_daif_mask();
         :                      flags = current_thread_info()->flags;
         :                      if (!has_syscall_work(flags)) {
    0.00 :   ffff8000100968c0:       tst     x0, #0x1f80
    0.00 :   ffff8000100968c4:       b.ne    ffff8000100968d4 <el0_svc_common.constprop.2+0x8c>  // b.any
         :                      local_daif_restore(DAIF_PROCCTX);
         :                      }
         :
         :                      trace_exit:
         :                      syscall_trace_exit(regs);
         :                      }
    0.00 :   ffff8000100968c8:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000100968cc:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000100968d0:       ret
         :                      local_daif_restore():
         :                      write_sysreg(flags, daif);
    0.00 :   ffff8000100968d4:       msr     daif, xzr
         :                      el0_svc_common():
         :                      syscall_trace_exit(regs);
    0.00 :   ffff8000100968d8:       mov     x0, x19
    0.00 :   ffff8000100968dc:       bl      ffff80001008ad08 <syscall_trace_exit>
         :                      }
    0.00 :   ffff8000100968e0:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000100968e4:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000100968e8:       ret
         :                      if (scno == NO_SYSCALL)
    0.00 :   ffff8000100968ec:       cmn     w1, #0x1
    0.00 :   ffff8000100968f0:       b.ne    ffff8000100968fc <el0_svc_common.constprop.2+0xb4>  // b.any
         :                      regs->regs[0] = -ENOSYS;
    0.00 :   ffff8000100968f4:       mov     x0, #0xffffffffffffffda         // #-38
    0.00 :   ffff8000100968f8:       str     x0, [x19]
    0.00 :   ffff8000100968fc:       str     x2, [x29, #40]
         :                      scno = syscall_trace_enter(regs);
    0.00 :   ffff800010096900:       mov     x0, x19
    0.00 :   ffff800010096904:       bl      ffff80001008ab90 <syscall_trace_enter>
    0.00 :   ffff800010096908:       mov     w1, w0
         :                      if (scno == NO_SYSCALL)
    0.00 :   ffff80001009690c:       cmn     w0, #0x1
    0.00 :   ffff800010096910:       ldr     x2, [x29, #40]
    0.00 :   ffff800010096914:       b.ne    ffff800010096884 <el0_svc_common.constprop.2+0x3c>  // b.any
    0.00 :   ffff800010096918:       b       ffff8000100968d8 <el0_svc_common.constprop.2+0x90>
    0.00 :   ffff80001009691c:       str     x2, [x29, #32]
         :                      cortex_a76_erratum_1463225_svc_handler():
         :                      if (!unlikely(this_cpu_has_cap(ARM64_WORKAROUND_1463225)))
    0.00 :   ffff800010096920:       mov     w0, #0x2c                       // #44
    0.00 :   ffff800010096924:       str     w1, [x29, #40]
    0.00 :   ffff800010096928:       bl      ffff800010094d28 <this_cpu_has_cap>
    0.00 :   ffff80001009692c:       ldr     w1, [x29, #40]
    0.00 :   ffff800010096930:       tst     w0, #0xff
    0.00 :   ffff800010096934:       ldr     x2, [x29, #32]
    0.00 :   ffff800010096938:       b.eq    ffff800010096878 <el0_svc_common.constprop.2+0x30>  // b.none
         :                      __my_cpu_offset():
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
         :                      "mrs %0, tpidr_el2",
         :                      ARM64_HAS_VIRT_HOST_EXTN)
         :                      : "=r" (off) :
         :                      "Q" (*(const unsigned long *)current_stack_pointer));
    0.00 :   ffff80001009693c:       mov     x5, sp
         :                      cortex_a76_erratum_1463225_svc_handler():
         :                      __this_cpu_write(__in_cortex_a76_erratum_1463225_wa, 1);
    0.00 :   ffff800010096940:       mov     w4, #0x1                        // #1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800010096944:       mrs     x3, tpidr_el1
         :                      cortex_a76_erratum_1463225_svc_handler():
    0.00 :   ffff800010096948:       adrp    x0, ffff8000114cb000 <overflow_stack+0xd60>
    0.00 :   ffff80001009694c:       add     x0, x0, #0x698
    0.00 :   ffff800010096950:       str     w4, [x0, x3]
         :                      reg = read_sysreg(mdscr_el1);
    0.00 :   ffff800010096954:       mrs     x3, mdscr_el1
         :                      val = reg | DBG_MDSCR_SS | DBG_MDSCR_KDE;
    0.00 :   ffff800010096958:       mov     w4, #0x2001                     // #8193
         :                      write_sysreg(val, mdscr_el1);
    0.00 :   ffff80001009695c:       orr     w4, w4, w3
    0.00 :   ffff800010096960:       msr     mdscr_el1, x4
         :                      asm volatile("msr daifclr, #8");
    0.00 :   ffff800010096964:       msr     daifclr, #0x8
         :                      isb();
    0.00 :   ffff800010096968:       isb
         :                      write_sysreg(reg, mdscr_el1);
    0.00 :   ffff80001009696c:       and     x3, x3, #0xffffffff
    0.00 :   ffff800010096970:       msr     mdscr_el1, x3
         :                      __my_cpu_offset():
    0.00 :   ffff800010096974:       mrs     x3, tpidr_el1
         :                      cortex_a76_erratum_1463225_svc_handler():
         :                      __this_cpu_write(__in_cortex_a76_erratum_1463225_wa, 0);
    0.00 :   ffff800010096978:       str     wzr, [x0, x3]
    0.00 :   ffff80001009697c:       b       ffff800010096878 <el0_svc_common.constprop.2+0x30>
         :                      get_current():
    0.00 :   ffff800010096980:       mrs     x0, sp_el0
         :                      test_bit():
    0.00 :   ffff800010096984:       ldr     x0, [x0]
         :                      do_ni_syscall():
         :                      if (is_compat_task()) {
    0.00 :   ffff800010096988:       tst     w0, #0x400000
    0.00 :   ffff80001009698c:       b.eq    ffff8000100969a0 <el0_svc_common.constprop.2+0x158>  // b.none
         :                      ret = compat_arm_syscall(regs, scno);
    0.00 :   ffff800010096990:       mov     x0, x19
    0.00 :   ffff800010096994:       bl      ffff800010098f30 <compat_arm_syscall>
         :                      if (ret != -ENOSYS)
    0.00 :   ffff800010096998:       cmn     x0, #0x26
    0.00 :   ffff80001009699c:       b.ne    ffff8000100968ac <el0_svc_common.constprop.2+0x64>  // b.any
         :                      return sys_ni_syscall();
    0.00 :   ffff8000100969a0:       bl      ffff80001010c378 <sys_ni_syscall>
    0.00 :   ffff8000100969a4:       b       ffff8000100968ac <el0_svc_common.constprop.2+0x64>
 Percent |	Source code & Disassembly of vmlinux for cycles (3247 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010c92540 <__memcpy>:
         :                      __efistub_memcpy():
         :                      C_l     .req    x11
         :                      C_h     .req    x12
         :                      D_l     .req    x13
         :                      D_h     .req    x14
         :
         :                      mov     dst, dstin
    2.40 :   ffff800010c92540:       mov     x6, x0
         :                      cmp     count, #16
    0.00 :   ffff800010c92544:       cmp     x2, #0x10
         :                      /*When memory length is less than 16, the accessed are not aligned.*/
         :                      b.lo    .Ltiny15
    0.00 :   ffff800010c92548:       b.cc    ffff800010c925c0 <__memcpy+0x80>  // b.lo, b.ul, b.last
         :
         :                      neg     tmp2, src
    0.03 :   ffff800010c9254c:       neg     x4, x1
         :                      ands    tmp2, tmp2, #15/* Bytes to reach alignment. */
    0.00 :   ffff800010c92550:       ands    x4, x4, #0xf
         :                      b.eq    .LSrcAligned
    0.00 :   ffff800010c92554:       b.eq    ffff800010c9258c <__memcpy+0x4c>  // b.none
         :                      sub     count, count, tmp2
    0.00 :   ffff800010c92558:       sub     x2, x2, x4
         :                      * Copy the leading memory data from src to dst in an increasing
         :                      * address order.By this way,the risk of overwriting the source
         :                      * memory data is eliminated when the distance between src and
         :                      * dst is less than 16. The memory accesses here are alignment.
         :                      */
         :                      tbz     tmp2, #0, 1f
    0.00 :   ffff800010c9255c:       tbz     w4, #0, ffff800010c92568 <__memcpy+0x28>
         :                      ldrb1   tmp1w, src, #1
    0.00 :   ffff800010c92560:       ldrb    w3, [x1], #1
         :                      strb1   tmp1w, dst, #1
    0.00 :   ffff800010c92564:       strb    w3, [x6], #1
         :                      1:
         :                      tbz     tmp2, #1, 2f
    0.00 :   ffff800010c92568:       tbz     w4, #1, ffff800010c92574 <__memcpy+0x34>
         :                      ldrh1   tmp1w, src, #2
    0.00 :   ffff800010c9256c:       ldrh    w3, [x1], #2
         :                      strh1   tmp1w, dst, #2
    0.00 :   ffff800010c92570:       strh    w3, [x6], #2
         :                      2:
         :                      tbz     tmp2, #2, 3f
    0.00 :   ffff800010c92574:       tbz     w4, #2, ffff800010c92580 <__memcpy+0x40>
         :                      ldr1    tmp1w, src, #4
    0.00 :   ffff800010c92578:       ldr     w3, [x1], #4
         :                      str1    tmp1w, dst, #4
    0.00 :   ffff800010c9257c:       str     w3, [x6], #4
         :                      3:
         :                      tbz     tmp2, #3, .LSrcAligned
    0.00 :   ffff800010c92580:       tbz     w4, #3, ffff800010c9258c <__memcpy+0x4c>
         :                      ldr1    tmp1, src, #8
    0.00 :   ffff800010c92584:       ldr     x3, [x1], #8
         :                      str1    tmp1, dst, #8
    0.00 :   ffff800010c92588:       str     x3, [x6], #8
         :
         :                      .LSrcAligned:
         :                      cmp     count, #64
    1.85 :   ffff800010c9258c:       cmp     x2, #0x40
         :                      b.ge    .Lcpy_over64
    0.06 :   ffff800010c92590:       b.ge    ffff800010c925f4 <__memcpy+0xb4>  // b.tcont
         :                      .Ltail63:
         :                      /*
         :                      * Copy up to 48 bytes of data. At this point we only need the
         :                      * bottom 6 bits of count to be accurate.
         :                      */
         :                      ands    tmp1, count, #0x30
    0.00 :   ffff800010c92594:       ands    x3, x2, #0x30
         :                      b.eq    .Ltiny15
    0.00 :   ffff800010c92598:       b.eq    ffff800010c925c0 <__memcpy+0x80>  // b.none
         :                      cmp     tmp1w, #0x20
    0.00 :   ffff800010c9259c:       cmp     w3, #0x20
         :                      b.eq    1f
    0.00 :   ffff800010c925a0:       b.eq    ffff800010c925b0 <__memcpy+0x70>  // b.none
         :                      b.lt    2f
    0.00 :   ffff800010c925a4:       b.lt    ffff800010c925b8 <__memcpy+0x78>  // b.tstop
         :                      ldp1    A_l, A_h, src, #16
    0.00 :   ffff800010c925a8:       ldp     x7, x8, [x1], #16
         :                      stp1    A_l, A_h, dst, #16
    0.00 :   ffff800010c925ac:       stp     x7, x8, [x6], #16
         :                      1:
         :                      ldp1    A_l, A_h, src, #16
    0.00 :   ffff800010c925b0:       ldp     x7, x8, [x1], #16
         :                      stp1    A_l, A_h, dst, #16
    0.00 :   ffff800010c925b4:       stp     x7, x8, [x6], #16
         :                      2:
         :                      ldp1    A_l, A_h, src, #16
    0.00 :   ffff800010c925b8:       ldp     x7, x8, [x1], #16
         :                      stp1    A_l, A_h, dst, #16
    0.00 :   ffff800010c925bc:       stp     x7, x8, [x6], #16
         :                      * precondition that src address is at least 16 bytes bigger than dst
         :                      * address,otherwise some source data will be overwritten when memove
         :                      * call memcpy directly. To make memmove simpler and decouple the
         :                      * memcpy's dependency on memmove, withdrew the original process.
         :                      */
         :                      tbz     count, #3, 1f
    0.00 :   ffff800010c925c0:       tbz     w2, #3, ffff800010c925cc <__memcpy+0x8c>
         :                      ldr1    tmp1, src, #8
    0.00 :   ffff800010c925c4:       ldr     x3, [x1], #8
         :                      str1    tmp1, dst, #8
    0.00 :   ffff800010c925c8:       str     x3, [x6], #8
         :                      1:
         :                      tbz     count, #2, 2f
    0.00 :   ffff800010c925cc:       tbz     w2, #2, ffff800010c925d8 <__memcpy+0x98>
         :                      ldr1    tmp1w, src, #4
    0.00 :   ffff800010c925d0:       ldr     w3, [x1], #4
         :                      str1    tmp1w, dst, #4
    0.00 :   ffff800010c925d4:       str     w3, [x6], #4
         :                      2:
         :                      tbz     count, #1, 3f
    0.00 :   ffff800010c925d8:       tbz     w2, #1, ffff800010c925e4 <__memcpy+0xa4>
         :                      ldrh1   tmp1w, src, #2
    0.00 :   ffff800010c925dc:       ldrh    w3, [x1], #2
         :                      strh1   tmp1w, dst, #2
    0.00 :   ffff800010c925e0:       strh    w3, [x6], #2
         :                      3:
         :                      tbz     count, #0, .Lexitfunc
    0.00 :   ffff800010c925e4:       tbz     w2, #0, ffff800010c92690 <__memcpy+0x150>
         :                      ldrb1   tmp1w, src, #1
    0.00 :   ffff800010c925e8:       ldrb    w3, [x1], #1
         :                      strb1   tmp1w, dst, #1
    0.00 :   ffff800010c925ec:       strb    w3, [x6], #1
         :
         :                      b       .Lexitfunc
    0.00 :   ffff800010c925f0:       b       ffff800010c92690 <__memcpy+0x150>
         :
         :                      .Lcpy_over64:
         :                      subs    count, count, #128
    0.22 :   ffff800010c925f4:       subs    x2, x2, #0x80
         :                      b.ge    .Lcpy_body_large
    0.00 :   ffff800010c925f8:       b.ge    ffff800010c92640 <__memcpy+0x100>  // b.tcont
         :                      /*
         :                      * Less than 128 bytes to copy, so handle 64 here and then jump
         :                      * to the tail.
         :                      */
         :                      ldp1    A_l, A_h, src, #16
    0.00 :   ffff800010c925fc:       ldp     x7, x8, [x1], #16
         :                      stp1    A_l, A_h, dst, #16
    0.00 :   ffff800010c92600:       stp     x7, x8, [x6], #16
         :                      ldp1    B_l, B_h, src, #16
    0.00 :   ffff800010c92604:       ldp     x9, x10, [x1], #16
         :                      ldp1    C_l, C_h, src, #16
    0.00 :   ffff800010c92608:       ldp     x11, x12, [x1], #16
         :                      stp1    B_l, B_h, dst, #16
    0.00 :   ffff800010c9260c:       stp     x9, x10, [x6], #16
         :                      stp1    C_l, C_h, dst, #16
    0.00 :   ffff800010c92610:       stp     x11, x12, [x6], #16
         :                      ldp1    D_l, D_h, src, #16
    0.00 :   ffff800010c92614:       ldp     x13, x14, [x1], #16
         :                      stp1    D_l, D_h, dst, #16
    0.00 :   ffff800010c92618:       stp     x13, x14, [x6], #16
         :
         :                      tst     count, #0x3f
    0.00 :   ffff800010c9261c:       tst     x2, #0x3f
         :                      b.ne    .Ltail63
    0.00 :   ffff800010c92620:       b.ne    ffff800010c92594 <__memcpy+0x54>  // b.any
         :                      b       .Lexitfunc
    0.00 :   ffff800010c92624:       b       ffff800010c92690 <__memcpy+0x150>
    0.00 :   ffff800010c92628:       nop
    0.00 :   ffff800010c9262c:       nop
    0.00 :   ffff800010c92630:       nop
    0.00 :   ffff800010c92634:       nop
    0.00 :   ffff800010c92638:       nop
    0.00 :   ffff800010c9263c:       nop
         :                      * 64 bytes per line this ensures the entire loop is in one line.
         :                      */
         :                      .p2align        L1_CACHE_SHIFT
         :                      .Lcpy_body_large:
         :                      /* pre-get 64 bytes data. */
         :                      ldp1    A_l, A_h, src, #16
   19.13 :   ffff800010c92640:       ldp     x7, x8, [x1], #16
         :                      ldp1    B_l, B_h, src, #16
   24.65 :   ffff800010c92644:       ldp     x9, x10, [x1], #16
         :                      ldp1    C_l, C_h, src, #16
    5.15 :   ffff800010c92648:       ldp     x11, x12, [x1], #16
         :                      ldp1    D_l, D_h, src, #16
    0.61 :   ffff800010c9264c:       ldp     x13, x14, [x1], #16
         :                      1:
         :                      /*
         :                      * interlace the load of next 64 bytes data block with store of the last
         :                      * loaded 64 bytes data.
         :                      */
         :                      stp1    A_l, A_h, dst, #16
    1.92 :   ffff800010c92650:       stp     x7, x8, [x6], #16
         :                      ldp1    A_l, A_h, src, #16
    4.13 :   ffff800010c92654:       ldp     x7, x8, [x1], #16
         :                      stp1    B_l, B_h, dst, #16
    9.46 :   ffff800010c92658:       stp     x9, x10, [x6], #16
         :                      ldp1    B_l, B_h, src, #16
    2.55 :   ffff800010c9265c:       ldp     x9, x10, [x1], #16
         :                      stp1    C_l, C_h, dst, #16
    1.83 :   ffff800010c92660:       stp     x11, x12, [x6], #16
         :                      ldp1    C_l, C_h, src, #16
    2.89 :   ffff800010c92664:       ldp     x11, x12, [x1], #16
         :                      stp1    D_l, D_h, dst, #16
    5.23 :   ffff800010c92668:       stp     x13, x14, [x6], #16
         :                      ldp1    D_l, D_h, src, #16
    2.34 :   ffff800010c9266c:       ldp     x13, x14, [x1], #16
         :                      subs    count, count, #64
    0.00 :   ffff800010c92670:       subs    x2, x2, #0x40
         :                      b.ge    1b
    0.00 :   ffff800010c92674:       b.ge    ffff800010c92650 <__memcpy+0x110>  // b.tcont
         :                      stp1    A_l, A_h, dst, #16
    1.36 :   ffff800010c92678:       stp     x7, x8, [x6], #16
         :                      stp1    B_l, B_h, dst, #16
    2.13 :   ffff800010c9267c:       stp     x9, x10, [x6], #16
         :                      stp1    C_l, C_h, dst, #16
    3.60 :   ffff800010c92680:       stp     x11, x12, [x6], #16
         :                      stp1    D_l, D_h, dst, #16
    1.54 :   ffff800010c92684:       stp     x13, x14, [x6], #16
         :
         :                      tst     count, #0x3f
    0.00 :   ffff800010c92688:       tst     x2, #0x3f
         :                      b.ne    .Ltail63
    0.00 :   ffff800010c9268c:       b.ne    ffff800010c92594 <__memcpy+0x54>  // b.any
         :
         :                      .weak memcpy
         :                      ENTRY(__memcpy)
         :                      ENTRY(memcpy)
         :                      #include "copy_template.S"
         :                      ret
    6.93 :   ffff800010c92690:       ret
         :                      ...
 Percent |	Source code & Disassembly of vmlinux for cycles (3196 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001070a388 <arm_smmu_tlb_inv_range>:
         :                      arm_smmu_tlb_inv_range():
         :                      }
         :
         :                      static void arm_smmu_tlb_inv_range(unsigned long iova, size_t size,
         :                      size_t granule, bool leaf,
         :                      struct arm_smmu_domain *smmu_domain)
         :                      {
    1.62 :   ffff80001070a388:       sub     sp, sp, #0x490
    3.16 :   ffff80001070a38c:       stp     x29, x30, [sp]
    0.00 :   ffff80001070a390:       mov     x29, sp
    0.31 :   ffff80001070a394:       str     x21, [sp, #32]
    0.00 :   ffff80001070a398:       adrp    x21, ffff800011899000 <page_wait_table+0x1500>
    0.19 :   ffff80001070a39c:       str     x25, [sp, #64]
    0.00 :   ffff80001070a3a0:       add     x5, x21, #0x8c8
    1.47 :   ffff80001070a3a4:       str     x28, [sp, #88]
    0.00 :   ffff80001070a3a8:       mov     x25, x4
         :                      u64 cmds[CMDQ_BATCH_ENTRIES * CMDQ_ENT_DWORDS];
         :                      struct arm_smmu_device *smmu = smmu_domain->smmu;
         :                      unsigned long start = iova, end = iova + size;
         :                      int i = 0;
         :                      struct arm_smmu_cmdq_ent cmd = {
    3.51 :   ffff80001070a3ac:       stp     xzr, xzr, [x29, #104]
         :                      {
    0.28 :   ffff80001070a3b0:       ldr     x4, [x5]
    6.97 :   ffff80001070a3b4:       str     x4, [x29, #1160]
    0.00 :   ffff80001070a3b8:       mov     x4, #0x0                        // #0
         :                      struct arm_smmu_cmdq_ent cmd = {
    1.06 :   ffff80001070a3bc:       strb    w3, [x29, #116]
    2.60 :   ffff80001070a3c0:       stp     xzr, xzr, [x29, #120]
         :                      struct arm_smmu_device *smmu = smmu_domain->smmu;
    0.41 :   ffff80001070a3c4:       ldr     x28, [x25]
         :                      .tlbi = {
         :                      .leaf   = leaf,
         :                      },
         :                      };
         :
         :                      if (!size)
    0.00 :   ffff80001070a3c8:       cbz     x1, ffff80001070a4a8 <arm_smmu_tlb_inv_range+0x120>
    1.19 :   ffff80001070a3cc:       str     x19, [x29, #16]
    1.12 :   ffff80001070a3d0:       stp     x22, x23, [x29, #40]
    2.63 :   ffff80001070a3d4:       str     x24, [x29, #56]
    0.00 :   ffff80001070a3d8:       mov     x24, x2
    0.16 :   ffff80001070a3dc:       stp     x26, x27, [x29, #72]
    0.13 :   ffff80001070a3e0:       mov     x27, x0
    0.00 :   ffff80001070a3e4:       mov     x26, x1
         :                      return;
         :
         :                      if (smmu_domain->stage == ARM_SMMU_DOMAIN_S1) {
    1.34 :   ffff80001070a3e8:       ldr     w0, [x25, #56]
    0.00 :   ffff80001070a3ec:       cbz     w0, ffff80001070a4d4 <arm_smmu_tlb_inv_range+0x14c>
         :                      cmd.opcode      = CMDQ_OP_TLBI_NH_VA;
         :                      cmd.tlbi.asid   = smmu_domain->s1_cfg.cd.asid;
         :                      } else {
         :                      cmd.opcode      = CMDQ_OP_TLBI_S2_IPA;
         :                      cmd.tlbi.vmid   = smmu_domain->s2_cfg.vmid;
    0.00 :   ffff80001070a3f0:       ldrh    w0, [x25, #64]
         :                      cmd.opcode      = CMDQ_OP_TLBI_S2_IPA;
    0.00 :   ffff80001070a3f4:       mov     w1, #0x2a                       // #42
    0.00 :   ffff80001070a3f8:       strb    w1, [x29, #104]
         :                      cmd.tlbi.vmid   = smmu_domain->s2_cfg.vmid;
    0.00 :   ffff80001070a3fc:       strh    w0, [x29, #114]
         :                      unsigned long start = iova, end = iova + size;
    1.00 :   ffff80001070a400:       add     x23, x27, x26
         :                      }
         :
         :                      while (iova < end) {
    0.00 :   ffff80001070a404:       mov     w19, #0x0                       // #0
    0.03 :   ffff80001070a408:       add     x22, x29, #0x88
    0.00 :   ffff80001070a40c:       cmp     x27, x23
    1.32 :   ffff80001070a410:       b.cs    ffff80001070a474 <arm_smmu_tlb_inv_range+0xec>  // b.hs, b.nlast
    2.56 :   ffff80001070a414:       str     x20, [x29, #24]
    0.00 :   ffff80001070a418:       add     x22, x29, #0x88
    0.00 :   ffff80001070a41c:       mov     x20, x27
    0.03 :   ffff80001070a420:       mov     w19, #0x0                       // #0
    1.54 :   ffff80001070a424:       nop
         :                      arm_smmu_cmdq_issue_cmdlist(smmu, cmds, i, false);
         :                      i = 0;
         :                      }
         :
         :                      cmd.tlbi.addr = iova;
         :                      arm_smmu_cmdq_build_cmd(&cmds[i * CMDQ_ENT_DWORDS], &cmd);
    1.06 :   ffff80001070a428:       lsl     w0, w19, #1
         :                      cmd.tlbi.addr = iova;
    1.16 :   ffff80001070a42c:       str     x20, [x29, #120]
         :                      arm_smmu_cmdq_build_cmd(&cmds[i * CMDQ_ENT_DWORDS], &cmd);
    1.84 :   ffff80001070a430:       add     x1, x29, #0x68
         :                      iova += granule;
    0.28 :   ffff80001070a434:       add     x20, x20, x24
         :                      arm_smmu_cmdq_build_cmd(&cmds[i * CMDQ_ENT_DWORDS], &cmd);
    1.25 :   ffff80001070a438:       add     x0, x22, w0, sxtw #3
   31.62 :   ffff80001070a43c:       bl      ffff8000107079c0 <arm_smmu_cmdq_build_cmd>
         :                      i++;
    1.48 :   ffff80001070a440:       add     w19, w19, #0x1
         :                      while (iova < end) {
    0.00 :   ffff80001070a444:       cmp     x23, x20
    0.00 :   ffff80001070a448:       b.ls    ffff80001070a470 <arm_smmu_tlb_inv_range+0xe8>  // b.plast
         :                      if (i == CMDQ_BATCH_ENTRIES) {
    0.00 :   ffff80001070a44c:       cmp     w19, #0x40
    0.00 :   ffff80001070a450:       b.ne    ffff80001070a428 <arm_smmu_tlb_inv_range+0xa0>  // b.any
         :                      arm_smmu_cmdq_issue_cmdlist(smmu, cmds, i, false);
    0.00 :   ffff80001070a454:       mov     w2, w19
    0.00 :   ffff80001070a458:       mov     w3, #0x0                        // #0
    0.00 :   ffff80001070a45c:       mov     x1, x22
    0.00 :   ffff80001070a460:       mov     x0, x28
         :                      i = 0;
    0.00 :   ffff80001070a464:       mov     w19, #0x0                       // #0
         :                      arm_smmu_cmdq_issue_cmdlist(smmu, cmds, i, false);
    0.00 :   ffff80001070a468:       bl      ffff800010708a20 <arm_smmu_cmdq_issue_cmdlist>
    0.00 :   ffff80001070a46c:       b       ffff80001070a428 <arm_smmu_tlb_inv_range+0xa0>
    1.23 :   ffff80001070a470:       ldr     x20, [x29, #24]
         :                      }
         :
         :                      arm_smmu_cmdq_issue_cmdlist(smmu, cmds, i, true);
    0.00 :   ffff80001070a474:       mov     w3, #0x1                        // #1
    0.00 :   ffff80001070a478:       mov     w2, w19
    0.00 :   ffff80001070a47c:       mov     x1, x22
   10.46 :   ffff80001070a480:       mov     x0, x28
    0.00 :   ffff80001070a484:       bl      ffff800010708a20 <arm_smmu_cmdq_issue_cmdlist>
         :
         :                      /*
         :                      * Unfortunately, this can't be leaf-only since we may have
         :                      * zapped an entire table.
         :                      */
         :                      arm_smmu_atc_inv_domain(smmu_domain, 0, start, size);
    0.03 :   ffff80001070a488:       mov     x2, x26
    0.00 :   ffff80001070a48c:       mov     x1, x27
    0.00 :   ffff80001070a490:       mov     x0, x25
    0.00 :   ffff80001070a494:       bl      ffff8000107094d8 <arm_smmu_atc_inv_domain.constprop.42>
    0.06 :   ffff80001070a498:       ldr     x19, [x29, #16]
    0.12 :   ffff80001070a49c:       ldp     x22, x23, [x29, #40]
    5.78 :   ffff80001070a4a0:       ldr     x24, [x29, #56]
    0.00 :   ffff80001070a4a4:       ldp     x26, x27, [x29, #72]
         :                      }
    0.00 :   ffff80001070a4a8:       add     x21, x21, #0x8c8
    0.09 :   ffff80001070a4ac:       ldr     x1, [x29, #1160]
    0.00 :   ffff80001070a4b0:       ldr     x0, [x21]
    0.00 :   ffff80001070a4b4:       eor     x0, x1, x0
    0.00 :   ffff80001070a4b8:       cbnz    x0, ffff80001070a4e8 <arm_smmu_tlb_inv_range+0x160>
    4.72 :   ffff80001070a4bc:       ldp     x29, x30, [sp]
    0.06 :   ffff80001070a4c0:       ldr     x21, [sp, #32]
    0.00 :   ffff80001070a4c4:       ldr     x25, [sp, #64]
    0.00 :   ffff80001070a4c8:       ldr     x28, [sp, #88]
    0.00 :   ffff80001070a4cc:       add     sp, sp, #0x490
    0.00 :   ffff80001070a4d0:       ret
         :                      cmd.tlbi.asid   = smmu_domain->s1_cfg.cd.asid;
    1.00 :   ffff80001070a4d4:       ldrh    w0, [x25, #80]
         :                      cmd.opcode      = CMDQ_OP_TLBI_NH_VA;
    0.03 :   ffff80001070a4d8:       mov     w1, #0x12                       // #18
    2.95 :   ffff80001070a4dc:       strb    w1, [x29, #104]
         :                      cmd.tlbi.asid   = smmu_domain->s1_cfg.cd.asid;
    0.13 :   ffff80001070a4e0:       strh    w0, [x29, #112]
    0.00 :   ffff80001070a4e4:       b       ffff80001070a400 <arm_smmu_tlb_inv_range+0x78>
    0.00 :   ffff80001070a4e8:       stp     x19, x20, [x29, #16]
    0.00 :   ffff80001070a4ec:       stp     x22, x23, [x29, #40]
    0.00 :   ffff80001070a4f0:       str     x24, [x29, #56]
    0.00 :   ffff80001070a4f4:       stp     x26, x27, [x29, #72]
         :                      }
    0.00 :   ffff80001070a4f8:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (5457 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010cb2408 <_raw_spin_unlock>:
         :                      _raw_spin_unlock():
         :                      EXPORT_SYMBOL(_raw_spin_lock_bh);
         :                      #endif
         :
         :                      #ifdef CONFIG_UNINLINE_SPIN_UNLOCK
         :                      void __lockfunc _raw_spin_unlock(raw_spinlock_t *lock)
         :                      {
    0.00 :   ffff800010cb2408:       stp     x29, x30, [sp, #-16]!
         :                      queued_spin_unlock():
         :                      static __always_inline void queued_spin_unlock(struct qspinlock *lock)
         :                      {
         :                      /*
         :                      * unlock() needs release semantics:
         :                      */
         :                      smp_store_release(&lock->locked, 0);
    0.00 :   ffff800010cb240c:       mov     w1, #0x0                        // #0
         :                      _raw_spin_unlock():
    0.00 :   ffff800010cb2410:       mov     x29, sp
         :                      queued_spin_unlock():
    0.00 :   ffff800010cb2414:       stlrb   w1, [x0]
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
   99.01 :   ffff800010cb2418:       mrs     x1, sp_el0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.06 :   ffff800010cb241c:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800010cb2420:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.02 :   ffff800010cb2424:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800010cb2428:       cbz     x0, ffff800010cb2434 <_raw_spin_unlock+0x2c>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.07 :   ffff800010cb242c:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff800010cb2430:       cbnz    x0, ffff800010cb2438 <_raw_spin_unlock+0x30>
         :                      __raw_spin_unlock():
         :
         :                      static inline void __raw_spin_unlock(raw_spinlock_t *lock)
         :                      {
         :                      spin_release(&lock->dep_map, _RET_IP_);
         :                      do_raw_spin_unlock(lock);
         :                      preempt_enable();
    0.00 :   ffff800010cb2434:       bl      ffff800010cad640 <preempt_schedule>
         :                      _raw_spin_unlock():
         :                      __raw_spin_unlock(lock);
         :                      }
    0.84 :   ffff800010cb2438:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010cb243c:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (2744 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001014c190 <irq_thread>:
         :                      irq_thread():
         :
         :                      /*
         :                      * Interrupt handler thread
         :                      */
         :                      static int irq_thread(void *data)
         :                      {
    0.00 :   ffff80001014c190:       stp     x29, x30, [sp, #-128]!
    0.00 :   ffff80001014c194:       mov     x29, sp
    0.00 :   ffff80001014c198:       stp     x21, x22, [sp, #32]
    0.00 :   ffff80001014c19c:       mov     x21, x0
    0.00 :   ffff80001014c1a0:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001014c1a4:       stp     x23, x24, [sp, #48]
    0.00 :   ffff80001014c1a8:       stp     x25, x26, [sp, #64]
    0.00 :   ffff80001014c1ac:       adrp    x25, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff80001014c1b0:       stp     x27, x28, [sp, #80]
    0.00 :   ffff80001014c1b4:       add     x0, x25, #0x8c8
    0.00 :   ffff80001014c1b8:       ldr     x1, [x0]
    0.00 :   ffff80001014c1bc:       str     x1, [x29, #120]
    0.00 :   ffff80001014c1c0:       mov     x1, #0x0                        // #0
         :                      struct callback_head on_exit_work;
         :                      struct irqaction *action = data;
         :                      struct irq_desc *desc = irq_to_desc(action->irq);
    0.00 :   ffff80001014c1c4:       ldr     w0, [x21, #56]
    0.00 :   ffff80001014c1c8:       bl      ffff8000101499e0 <irq_to_desc>
    0.00 :   ffff80001014c1cc:       mov     x22, x0
         :                      irqreturn_t (*handler_fn)(struct irq_desc *desc,
         :                      struct irqaction *action);
         :
         :                      if (force_irqthreads && test_bit(IRQTF_FORCED_THREAD,
    0.00 :   ffff80001014c1d0:       adrp    x0, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff80001014c1d4:       ldrb    w0, [x0, #576]
    0.00 :   ffff80001014c1d8:       cbz     w0, ffff80001014c390 <irq_thread+0x200>
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff80001014c1dc:       ldr     x1, [x21, #64]
         :                      irq_thread():
         :                      &action->thread_flags))
         :                      handler_fn = irq_forced_thread_fn;
    0.00 :   ffff80001014c1e0:       adrp    x0, ffff80001014c000 <free_irq>
    0.00 :   ffff80001014c1e4:       adrp    x24, ffff80001014c000 <free_irq>
    0.00 :   ffff80001014c1e8:       add     x0, x0, #0x570
    0.00 :   ffff80001014c1ec:       add     x24, x24, #0x3d8
    0.00 :   ffff80001014c1f0:       tst     w1, #0x8
    0.00 :   ffff80001014c1f4:       csel    x24, x24, x0, eq  // eq = none
         :                      init_task_work():
         :                      typedef void (*task_work_func_t)(struct callback_head *);
         :
         :                      static inline void
         :                      init_task_work(struct callback_head *twork, task_work_func_t func)
         :                      {
         :                      twork->func = func;
    0.00 :   ffff80001014c1f8:       adrp    x26, ffff80001014c000 <free_irq>
         :                      irq_thread():
         :                      else
         :                      handler_fn = irq_thread_fn;
         :
         :                      init_task_work(&on_exit_work, irq_thread_dtor);
         :                      task_work_add(current, &on_exit_work, false);
    0.00 :   ffff80001014c1fc:       mov     w2, #0x0                        // #0
         :                      init_task_work():
    0.00 :   ffff80001014c200:       add     x0, x26, #0x470
         :                      irq_thread():
    0.00 :   ffff80001014c204:       add     x1, x29, #0x68
         :                      init_task_work():
    0.00 :   ffff80001014c208:       str     x0, [x29, #112]
         :                      irq_thread_check_affinity():
         :                      if (!test_and_clear_bit(IRQTF_AFFINITY, &action->thread_flags))
    0.00 :   ffff80001014c20c:       add     x19, x21, #0x40
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80001014c210:       mrs     x0, sp_el0
         :                      irq_thread():
         :                      task_work_add(current, &on_exit_work, false);
    0.00 :   ffff80001014c214:       bl      ffff800010108e28 <task_work_add>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001014c218:       ldr     x0, [x21, #64]
         :                      test_and_clear_bit():
         :                      {
         :                      long old;
         :                      unsigned long mask = BIT_MASK(nr);
         :
         :                      p += BIT_WORD(nr);
         :                      if (!(READ_ONCE(*p) & mask))
    0.00 :   ffff80001014c21c:       tbnz    w0, #2, ffff80001014c39c <irq_thread+0x20c>
         :                      irq_wake_secondary():
         :                      raw_spin_lock_irq(&desc->lock);
    0.00 :   ffff80001014c220:       add     x27, x22, #0xd4
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff80001014c224:       mov     x20, #0x1                       // #1
         :                      __ll_sc_atomic64_fetch_andnot():
         :                      /*
         :                      * GAS converts the mysterious and undocumented BIC (immediate) alias to
         :                      * an AND (immediate) instruction with the immediate inverted. We don't
         :                      * have a constraint for this, so fall back to register.
         :                      */
         :                      ATOMIC64_OPS(andnot, bic, )
    0.00 :   ffff80001014c228:       mov     x23, #0x4                       // #4
    0.00 :   ffff80001014c22c:       nop
         :                      get_current():
    0.29 :   ffff80001014c230:       mrs     x0, sp_el0
         :                      __write_once_size():
    0.00 :   ffff80001014c234:       str     x20, [x0, #24]
         :                      irq_wait_for_interrupt():
         :                      set_current_state(TASK_INTERRUPTIBLE);
    0.26 :   ffff80001014c238:       dmb     ish
         :                      if (kthread_should_stop()) {
   21.25 :   ffff80001014c23c:       bl      ffff80001010a8f8 <kthread_should_stop>
    0.07 :   ffff80001014c240:       tst     w0, #0xff
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001014c244:       ldr     x0, [x21, #64]
         :                      irq_wait_for_interrupt():
    0.00 :   ffff80001014c248:       b.ne    ffff80001014c274 <irq_thread+0xe4>  // b.any
    0.04 :   ffff80001014c24c:       nop
         :                      test_and_clear_bit():
    1.16 :   ffff80001014c250:       tbnz    w0, #0, ffff80001014c2bc <irq_thread+0x12c>
         :                      irq_wait_for_interrupt():
         :                      schedule();
    0.00 :   ffff80001014c254:       bl      ffff800010cad270 <schedule>
         :                      get_current():
    0.58 :   ffff80001014c258:       mrs     x0, sp_el0
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff80001014c25c:       str     x20, [x0, #24]
         :                      irq_wait_for_interrupt():
         :                      set_current_state(TASK_INTERRUPTIBLE);
    0.93 :   ffff80001014c260:       dmb     ish
         :                      if (kthread_should_stop()) {
    2.16 :   ffff80001014c264:       bl      ffff80001010a8f8 <kthread_should_stop>
    1.36 :   ffff80001014c268:       tst     w0, #0xff
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001014c26c:       ldr     x0, [x21, #64]
         :                      irq_wait_for_interrupt():
    0.00 :   ffff80001014c270:       b.eq    ffff80001014c250 <irq_thread+0xc0>  // b.none
         :                      test_and_clear_bit():
    0.00 :   ffff80001014c274:       tbnz    w0, #0, ffff80001014c36c <irq_thread+0x1dc>
         :                      get_current():
    0.00 :   ffff80001014c278:       mrs     x0, sp_el0
         :                      irq_wait_for_interrupt():
         :                      __set_current_state(TASK_RUNNING);
    0.00 :   ffff80001014c27c:       str     xzr, [x0, #24]
         :                      irq_thread():
         :                      * This is the regular exit path. __free_irq() is stopping the
         :                      * thread via kthread_stop() after calling
         :                      * synchronize_hardirq(). So neither IRQTF_RUNTHREAD nor the
         :                      * oneshot mask bit can be set.
         :                      */
         :                      task_work_cancel(current, irq_thread_dtor);
    0.00 :   ffff80001014c280:       add     x1, x26, #0x470
         :                      return 0;
         :                      }
    0.00 :   ffff80001014c284:       add     x25, x25, #0x8c8
         :                      task_work_cancel(current, irq_thread_dtor);
    0.00 :   ffff80001014c288:       bl      ffff800010108f18 <task_work_cancel>
         :                      }
    0.00 :   ffff80001014c28c:       mov     w0, #0x0                        // #0
    0.00 :   ffff80001014c290:       ldr     x2, [x29, #120]
    0.00 :   ffff80001014c294:       ldr     x1, [x25]
    0.00 :   ffff80001014c298:       eor     x1, x2, x1
    0.00 :   ffff80001014c29c:       cbnz    x1, ffff80001014c3d4 <irq_thread+0x244>
    0.00 :   ffff80001014c2a0:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001014c2a4:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff80001014c2a8:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff80001014c2ac:       ldp     x25, x26, [sp, #64]
    0.00 :   ffff80001014c2b0:       ldp     x27, x28, [sp, #80]
    0.00 :   ffff80001014c2b4:       ldp     x29, x30, [sp], #128
    0.00 :   ffff80001014c2b8:       ret
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.88 :   ffff80001014c2bc:       b       ffff80001014c300 <irq_thread+0x170>
    0.44 :   ffff80001014c2c0:       b       ffff80001014c300 <irq_thread+0x170>
         :                      __lse_atomic64_fetch_andnot():
         :                      ATOMIC64_FETCH_OP(_relaxed,   , op, asm_op)                     \
         :                      ATOMIC64_FETCH_OP(_acquire,  a, op, asm_op, "memory")           \
         :                      ATOMIC64_FETCH_OP(_release,  l, op, asm_op, "memory")           \
         :                      ATOMIC64_FETCH_OP(        , al, op, asm_op, "memory")
         :
         :                      ATOMIC64_FETCH_OPS(andnot, ldclr)
    0.04 :   ffff80001014c2c4:       mov     x0, x20
    0.99 :   ffff80001014c2c8:       ldclral x0, x0, [x19]
         :                      irq_wait_for_interrupt():
         :                      if (test_and_clear_bit(IRQTF_RUNTHREAD,
   35.23 :   ffff80001014c2cc:       tbz     w0, #0, ffff80001014c254 <irq_thread+0xc4>
         :                      get_current():
    0.00 :   ffff80001014c2d0:       mrs     x0, sp_el0
         :                      irq_wait_for_interrupt():
         :                      __set_current_state(TASK_RUNNING);
    1.27 :   ffff80001014c2d4:       str     xzr, [x0, #24]
         :                      __read_once_size():
   15.61 :   ffff80001014c2d8:       ldr     x0, [x21, #64]
         :                      test_and_clear_bit():
    0.00 :   ffff80001014c2dc:       tbnz    w0, #2, ffff80001014c30c <irq_thread+0x17c>
         :                      irq_thread():
         :                      action_ret = handler_fn(desc, action);
   14.65 :   ffff80001014c2e0:       mov     x1, x21
    0.00 :   ffff80001014c2e4:       mov     x0, x22
    0.00 :   ffff80001014c2e8:       blr     x24
         :                      if (action_ret == IRQ_WAKE_THREAD)
    0.07 :   ffff80001014c2ec:       cmp     w0, #0x2
    0.00 :   ffff80001014c2f0:       b.eq    ffff80001014c33c <irq_thread+0x1ac>  // b.none
         :                      wake_threads_waitq(desc);
    0.55 :   ffff80001014c2f4:       mov     x0, x22
    0.00 :   ffff80001014c2f8:       bl      ffff80001014ba78 <wake_threads_waitq>
    2.18 :   ffff80001014c2fc:       b       ffff80001014c230 <irq_thread+0xa0>
         :                      __ll_sc_atomic64_fetch_andnot():
    0.00 :   ffff80001014c300:       b       ffff80001014e5c0 <__irq_get_irqchip_state+0xb0>
         :                      irq_wait_for_interrupt():
         :                      if (test_and_clear_bit(IRQTF_RUNTHREAD,
    0.00 :   ffff80001014c304:       tbz     w0, #0, ffff80001014c254 <irq_thread+0xc4>
    0.00 :   ffff80001014c308:       b       ffff80001014c2d0 <irq_thread+0x140>
         :                      arch_static_branch_jump():
    0.00 :   ffff80001014c30c:       b       ffff80001014c364 <irq_thread+0x1d4>
    0.00 :   ffff80001014c310:       b       ffff80001014c364 <irq_thread+0x1d4>
         :                      __lse_atomic64_fetch_andnot():
    0.00 :   ffff80001014c314:       mov     x0, x23
    0.00 :   ffff80001014c318:       ldclral x0, x0, [x19]
         :                      irq_thread_check_affinity():
         :                      if (!test_and_clear_bit(IRQTF_AFFINITY, &action->thread_flags))
    0.00 :   ffff80001014c31c:       tbz     w0, #2, ffff80001014c2e0 <irq_thread+0x150>
    0.00 :   ffff80001014c320:       mov     x0, x22
    0.00 :   ffff80001014c324:       bl      ffff80001014b960 <irq_thread_check_affinity.part.52>
         :                      irq_thread():
         :                      action_ret = handler_fn(desc, action);
    0.00 :   ffff80001014c328:       mov     x1, x21
    0.00 :   ffff80001014c32c:       mov     x0, x22
    0.00 :   ffff80001014c330:       blr     x24
         :                      if (action_ret == IRQ_WAKE_THREAD)
    0.00 :   ffff80001014c334:       cmp     w0, #0x2
    0.00 :   ffff80001014c338:       b.ne    ffff80001014c2f4 <irq_thread+0x164>  // b.any
         :                      irq_wake_secondary(desc, action);
    0.00 :   ffff80001014c33c:       ldr     x28, [x21, #48]
         :                      irq_wake_secondary():
         :                      if (WARN_ON_ONCE(!secondary))
    0.00 :   ffff80001014c340:       cbz     x28, ffff80001014c3cc <irq_thread+0x23c>
         :                      raw_spin_lock_irq(&desc->lock);
    0.00 :   ffff80001014c344:       mov     x0, x27
    0.00 :   ffff80001014c348:       bl      ffff800010cb2cd0 <_raw_spin_lock_irq>
         :                      __irq_wake_thread(desc, secondary);
    0.00 :   ffff80001014c34c:       mov     x1, x28
    0.00 :   ffff80001014c350:       mov     x0, x22
    0.00 :   ffff80001014c354:       bl      ffff80001014a990 <__irq_wake_thread>
         :                      raw_spin_unlock_irq(&desc->lock);
    0.00 :   ffff80001014c358:       mov     x0, x27
    0.00 :   ffff80001014c35c:       bl      ffff800010cb2bf0 <_raw_spin_unlock_irq>
    0.00 :   ffff80001014c360:       b       ffff80001014c2f4 <irq_thread+0x164>
         :                      __ll_sc_atomic64_fetch_andnot():
    0.00 :   ffff80001014c364:       b       ffff80001014e5dc <__irq_get_irqchip_state+0xcc>
    0.00 :   ffff80001014c368:       b       ffff80001014c31c <irq_thread+0x18c>
         :                      arch_static_branch_jump():
    0.00 :   ffff80001014c36c:       b       ffff80001014c384 <irq_thread+0x1f4>
    0.00 :   ffff80001014c370:       b       ffff80001014c384 <irq_thread+0x1f4>
         :                      __lse_atomic64_fetch_andnot():
    0.00 :   ffff80001014c374:       mov     x0, x20
    0.00 :   ffff80001014c378:       ldclral x0, x0, [x19]
         :                      irq_wait_for_interrupt():
         :                      if (test_and_clear_bit(IRQTF_RUNTHREAD,
    0.00 :   ffff80001014c37c:       tbnz    w0, #0, ffff80001014c2d0 <irq_thread+0x140>
    0.00 :   ffff80001014c380:       b       ffff80001014c278 <irq_thread+0xe8>
         :                      __ll_sc_atomic64_fetch_andnot():
    0.00 :   ffff80001014c384:       b       ffff80001014e5f8 <__irq_get_irqchip_state+0xe8>
         :                      irq_wait_for_interrupt():
    0.00 :   ffff80001014c388:       tbnz    w0, #0, ffff80001014c2d0 <irq_thread+0x140>
    0.00 :   ffff80001014c38c:       b       ffff80001014c278 <irq_thread+0xe8>
         :                      irq_thread():
         :                      handler_fn = irq_thread_fn;
    0.00 :   ffff80001014c390:       adrp    x24, ffff80001014c000 <free_irq>
    0.00 :   ffff80001014c394:       add     x24, x24, #0x3d8
    0.00 :   ffff80001014c398:       b       ffff80001014c1f8 <irq_thread+0x68>
         :                      arch_static_branch_jump():
    0.00 :   ffff80001014c39c:       b       ffff80001014c3bc <irq_thread+0x22c>
    0.00 :   ffff80001014c3a0:       b       ffff80001014c3bc <irq_thread+0x22c>
         :                      __lse_atomic64_fetch_andnot():
    0.00 :   ffff80001014c3a4:       mov     x0, #0x4                        // #4
    0.00 :   ffff80001014c3a8:       ldclral x0, x0, [x19]
         :                      irq_thread_check_affinity():
         :                      if (!test_and_clear_bit(IRQTF_AFFINITY, &action->thread_flags))
    0.00 :   ffff80001014c3ac:       tbz     w0, #2, ffff80001014c220 <irq_thread+0x90>
    0.00 :   ffff80001014c3b0:       mov     x0, x22
    0.00 :   ffff80001014c3b4:       bl      ffff80001014b960 <irq_thread_check_affinity.part.52>
    0.00 :   ffff80001014c3b8:       b       ffff80001014c220 <irq_thread+0x90>
         :                      __ll_sc_atomic64_fetch_andnot():
    0.00 :   ffff80001014c3bc:       mov     x1, #0x4                        // #4
    0.00 :   ffff80001014c3c0:       add     x4, x21, #0x40
    0.00 :   ffff80001014c3c4:       b       ffff80001014e614 <__irq_get_irqchip_state+0x104>
    0.00 :   ffff80001014c3c8:       b       ffff80001014c3ac <irq_thread+0x21c>
         :                      irq_wake_secondary():
         :                      if (WARN_ON_ONCE(!secondary))
    0.00 :   ffff80001014c3cc:       brk     #0x800
    0.00 :   ffff80001014c3d0:       b       ffff80001014c2f4 <irq_thread+0x164>
         :                      irq_thread():
         :                      }
    0.00 :   ffff80001014c3d4:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (4686 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010205b38 <get_user_pages_fast>:
         :                      get_user_pages_fast():
         :                      * requested. If nr_pages is 0 or negative, returns 0. If no pages
         :                      * were pinned, returns -errno.
         :                      */
         :                      int get_user_pages_fast(unsigned long start, int nr_pages,
         :                      unsigned int gup_flags, struct page **pages)
         :                      {
    0.24 :   ffff800010205b38:       stp     x29, x30, [sp, #-80]!
         :                      unsigned long addr, len, end;
         :                      int nr = 0, ret = 0;
         :
         :                      if (WARN_ON_ONCE(gup_flags & ~(FOLL_WRITE | FOLL_LONGTERM)))
    0.00 :   ffff800010205b3c:       tst     w2, #0xfffefffe
         :                      {
    0.17 :   ffff800010205b40:       mov     x29, sp
    0.45 :   ffff800010205b44:       stp     x20, x21, [sp, #24]
    0.00 :   ffff800010205b48:       adrp    x20, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010205b4c:       add     x4, x20, #0x8c8
    0.34 :   ffff800010205b50:       ldr     x5, [x4]
    0.34 :   ffff800010205b54:       str     x5, [x29, #72]
    0.00 :   ffff800010205b58:       mov     x5, #0x0                        // #0
         :                      int nr = 0, ret = 0;
    0.08 :   ffff800010205b5c:       str     wzr, [x29, #68]
         :                      if (WARN_ON_ONCE(gup_flags & ~(FOLL_WRITE | FOLL_LONGTERM)))
    0.00 :   ffff800010205b60:       b.ne    ffff800010205ca4 <get_user_pages_fast+0x16c>  // b.any
    0.02 :   ffff800010205b64:       str     x19, [x29, #16]
         :                      sign_extend64():
         :                      * @index: 0 based bit index (0<=index<64) to sign bit
         :                      */
         :                      static inline __s64 sign_extend64(__u64 value, int index)
         :                      {
         :                      __u8 shift = 63 - index;
         :                      return (__s64)(value << shift) >> shift;
    0.00 :   ffff800010205b68:       sbfx    x19, x0, #0, #56
    0.00 :   ffff800010205b6c:       and     x0, x0, #0xfffffffffffff000
    0.26 :   ffff800010205b70:       stp     x22, x23, [x29, #40]
         :                      get_user_pages_fast():
         :                      return -EINVAL;
         :
         :                      start = untagged_addr(start) & PAGE_MASK;
    0.00 :   ffff800010205b74:       and     x19, x19, x0
    0.00 :   ffff800010205b78:       mov     w23, w2
         :                      addr = start;
         :                      len = (unsigned long) nr_pages << PAGE_SHIFT;
    0.00 :   ffff800010205b7c:       sbfiz   x2, x1, #12, #32
    0.19 :   ffff800010205b80:       mov     w22, w1
         :                      end = start + len;
    0.00 :   ffff800010205b84:       add     x1, x19, x2
         :
         :                      if (end <= start)
         :                      return 0;
    0.00 :   ffff800010205b88:       mov     w21, #0x0                       // #0
         :                      if (end <= start)
    0.00 :   ffff800010205b8c:       cmp     x19, x1
    0.11 :   ffff800010205b90:       b.cc    ffff800010205bc0 <get_user_pages_fast+0x88>  // b.lo, b.ul, b.last
    0.00 :   ffff800010205b94:       ldr     x19, [x29, #16]
    0.00 :   ffff800010205b98:       ldp     x22, x23, [x29, #40]
         :                      ret += nr;
         :                      }
         :                      }
         :
         :                      return ret;
         :                      }
    0.00 :   ffff800010205b9c:       add     x20, x20, #0x8c8
    0.00 :   ffff800010205ba0:       mov     w0, w21
    0.00 :   ffff800010205ba4:       ldr     x2, [x29, #72]
    0.02 :   ffff800010205ba8:       ldr     x1, [x20]
    0.00 :   ffff800010205bac:       eor     x1, x2, x1
    0.00 :   ffff800010205bb0:       cbnz    x1, ffff800010205d08 <get_user_pages_fast+0x1d0>
    0.21 :   ffff800010205bb4:       ldp     x20, x21, [sp, #24]
    0.04 :   ffff800010205bb8:       ldp     x29, x30, [sp], #80
    0.00 :   ffff800010205bbc:       ret
    0.15 :   ffff800010205bc0:       str     x24, [x29, #56]
    0.00 :   ffff800010205bc4:       mov     x24, x3
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    1.19 :   ffff800010205bc8:       mrs     x0, sp_el0
         :                      __range_ok():
         :                      * Asynchronous I/O running in a kernel thread does not have the
         :                      * TIF_TAGGED_ADDR flag of the process owning the mm, so always untag
         :                      * the user address before checking.
         :                      */
         :                      if (IS_ENABLED(CONFIG_ARM64_TAGGED_ADDR_ABI) &&
         :                      (current->flags & PF_KTHREAD || test_thread_flag(TIF_TAGGED_ADDR)))
    0.02 :   ffff800010205bcc:       ldr     w4, [x0, #44]
         :                      unsigned long ret, limit = current_thread_info()->addr_limit;
    0.68 :   ffff800010205bd0:       ldr     x3, [x0, #8]
         :                      if (IS_ENABLED(CONFIG_ARM64_TAGGED_ADDR_ABI) &&
    0.00 :   ffff800010205bd4:       tbz     w4, #21, ffff800010205c38 <get_user_pages_fast+0x100>
         :                      sign_extend64():
    0.00 :   ffff800010205bd8:       sbfx    x0, x19, #0, #56
         :                      __range_ok():
         :                      addr = untagged_addr(addr);
    0.00 :   ffff800010205bdc:       and     x0, x0, x19
         :
         :                      __chk_user_ptr(addr);
         :                      asm volatile(
    0.43 :   ffff800010205be0:       adds    x0, x0, x2
    0.00 :   ffff800010205be4:       csel    x3, xzr, x3, hi  // hi = pmore
    0.00 :   ffff800010205be8:       csinv   x0, x0, xzr, cc  // cc = lo, ul, last
    0.00 :   ffff800010205bec:       sbcs    xzr, x0, x3
    0.00 :   ffff800010205bf0:       cset    x0, ls  // ls = plast
         :                      get_user_pages_fast():
         :                      if (unlikely(!access_ok((void __user *)start, len)))
    0.00 :   ffff800010205bf4:       cbz     x0, ffff800010205cf4 <get_user_pages_fast+0x1bc>
         :                      arch_local_irq_disable():
         :                      u32 pmr = read_sysreg_s(SYS_ICC_PMR_EL1);
         :
         :                      WARN_ON_ONCE(pmr != GIC_PRIO_IRQON && pmr != GIC_PRIO_IRQOFF);
         :                      }
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010205bf8:       mov     x0, #0x60                       // #96
    0.96 :   ffff800010205bfc:       msr     daifset, #0x2
         :                      get_user_pages_fast():
         :                      gup_pgd_range(addr, end, gup_flags, pages, &nr);
    0.00 :   ffff800010205c00:       mov     x0, x19
    0.00 :   ffff800010205c04:       add     x4, x29, #0x44
    0.00 :   ffff800010205c08:       mov     x3, x24
    0.00 :   ffff800010205c0c:       mov     w2, w23
    0.00 :   ffff800010205c10:       bl      ffff800010203a58 <gup_pgd_range>
         :                      arch_local_irq_enable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010205c14:       mov     x0, #0xe0                       // #224
    0.00 :   ffff800010205c18:       msr     daifclr, #0x2
         :                      get_user_pages_fast():
         :                      ret = nr;
   85.32 :   ffff800010205c1c:       ldr     w21, [x29, #68]
         :                      if (nr < nr_pages) {
    0.00 :   ffff800010205c20:       cmp     w22, w21
    0.00 :   ffff800010205c24:       b.gt    ffff800010205c4c <get_user_pages_fast+0x114>
    4.63 :   ffff800010205c28:       ldr     x19, [x29, #16]
    0.36 :   ffff800010205c2c:       ldp     x22, x23, [x29, #40]
    3.23 :   ffff800010205c30:       ldr     x24, [x29, #56]
    0.00 :   ffff800010205c34:       b       ffff800010205b9c <get_user_pages_fast+0x64>
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.06 :   ffff800010205c38:       ldr     x0, [x0]
         :                      __range_ok():
         :                      if (IS_ENABLED(CONFIG_ARM64_TAGGED_ADDR_ABI) &&
    0.00 :   ffff800010205c3c:       tst     w0, #0x4000000
    0.15 :   ffff800010205c40:       b.ne    ffff800010205bd8 <get_user_pages_fast+0xa0>  // b.any
         :                      get_user_pages_fast():
         :                      if (unlikely(!access_ok((void __user *)start, len)))
    0.34 :   ffff800010205c44:       mov     x0, x19
    0.00 :   ffff800010205c48:       b       ffff800010205be0 <get_user_pages_fast+0xa8>
         :                      start += nr << PAGE_SHIFT;
    0.00 :   ffff800010205c4c:       lsl     w0, w21, #12
         :                      ret = __gup_longterm_unlocked(start, nr_pages - nr,
    0.00 :   ffff800010205c50:       sub     w22, w22, w21
         :                      pages += nr;
    0.00 :   ffff800010205c54:       add     x21, x24, w21, sxtw #3
         :                      start += nr << PAGE_SHIFT;
    0.00 :   ffff800010205c58:       add     x19, x19, w0, sxtw
    0.00 :   ffff800010205c5c:       sxtw    x22, w22
         :                      __gup_longterm_unlocked():
         :                      if (gup_flags & FOLL_LONGTERM) {
    0.00 :   ffff800010205c60:       tbnz    w23, #16, ffff800010205cb0 <get_user_pages_fast+0x178>
         :                      ret = get_user_pages_unlocked(start, nr_pages,
    0.00 :   ffff800010205c64:       mov     x2, x21
    0.00 :   ffff800010205c68:       mov     w3, w23
    0.00 :   ffff800010205c6c:       mov     x1, x22
    0.00 :   ffff800010205c70:       mov     x0, x19
    0.00 :   ffff800010205c74:       bl      ffff8000102054c8 <get_user_pages_unlocked>
    0.00 :   ffff800010205c78:       mov     w21, w0
         :                      get_user_pages_fast():
         :                      if (nr > 0) {
    0.00 :   ffff800010205c7c:       ldr     w0, [x29, #68]
    0.00 :   ffff800010205c80:       cmp     w0, #0x0
    0.00 :   ffff800010205c84:       b.le    ffff800010205c28 <get_user_pages_fast+0xf0>
         :                      ret += nr;
    0.00 :   ffff800010205c88:       add     w1, w21, w0
    0.00 :   ffff800010205c8c:       cmp     w21, #0x0
    0.00 :   ffff800010205c90:       csel    w21, w1, w0, ge  // ge = tcont
    0.00 :   ffff800010205c94:       ldr     x19, [x29, #16]
    0.00 :   ffff800010205c98:       ldp     x22, x23, [x29, #40]
    0.00 :   ffff800010205c9c:       ldr     x24, [x29, #56]
    0.00 :   ffff800010205ca0:       b       ffff800010205b9c <get_user_pages_fast+0x64>
         :                      if (WARN_ON_ONCE(gup_flags & ~(FOLL_WRITE | FOLL_LONGTERM)))
    0.00 :   ffff800010205ca4:       brk     #0x800
         :                      return -EINVAL;
    0.00 :   ffff800010205ca8:       mov     w21, #0xffffffea                // #-22
    0.00 :   ffff800010205cac:       b       ffff800010205b9c <get_user_pages_fast+0x64>
         :                      get_current():
    0.00 :   ffff800010205cb0:       mrs     x24, sp_el0
         :                      __gup_longterm_unlocked():
         :                      down_read(&current->mm->mmap_sem);
    0.00 :   ffff800010205cb4:       ldr     x0, [x24, #952]
    0.00 :   ffff800010205cb8:       add     x0, x0, #0x68
    0.00 :   ffff800010205cbc:       bl      ffff800010cb0aa0 <down_read>
         :                      ret = __gup_longterm_locked(current, current->mm,
    0.00 :   ffff800010205cc0:       ldr     x1, [x24, #952]
    0.00 :   ffff800010205cc4:       mov     x4, x21
    0.00 :   ffff800010205cc8:       mov     w6, w23
    0.00 :   ffff800010205ccc:       mov     x3, x22
    0.00 :   ffff800010205cd0:       mov     x2, x19
    0.00 :   ffff800010205cd4:       mov     x5, #0x0                        // #0
    0.00 :   ffff800010205cd8:       mov     x0, x24
    0.00 :   ffff800010205cdc:       bl      ffff800010205708 <__gup_longterm_locked>
    0.00 :   ffff800010205ce0:       mov     w21, w0
         :                      up_read(&current->mm->mmap_sem);
    0.00 :   ffff800010205ce4:       ldr     x0, [x24, #952]
    0.00 :   ffff800010205ce8:       add     x0, x0, #0x68
    0.00 :   ffff800010205cec:       bl      ffff8000101376e8 <up_read>
    0.00 :   ffff800010205cf0:       b       ffff800010205c7c <get_user_pages_fast+0x144>
         :                      get_user_pages_fast():
         :                      return -EFAULT;
    0.00 :   ffff800010205cf4:       mov     w21, #0xfffffff2                // #-14
    0.00 :   ffff800010205cf8:       ldr     x19, [x29, #16]
    0.00 :   ffff800010205cfc:       ldp     x22, x23, [x29, #40]
    0.00 :   ffff800010205d00:       ldr     x24, [x29, #56]
    0.00 :   ffff800010205d04:       b       ffff800010205b9c <get_user_pages_fast+0x64>
    0.00 :   ffff800010205d08:       str     x19, [x29, #16]
    0.00 :   ffff800010205d0c:       stp     x22, x23, [x29, #40]
    0.00 :   ffff800010205d10:       str     x24, [x29, #56]
         :                      }
    0.00 :   ffff800010205d14:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (2610 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010451bf0 <update_io_ticks>:
         :                      update_io_ticks():
         :                      spin_unlock_irqrestore(&bio_dirty_lock, flags);
         :                      schedule_work(&bio_dirty_work);
         :                      }
         :
         :                      void update_io_ticks(struct hd_struct *part, unsigned long now)
         :                      {
    0.69 :   ffff800010451bf0:       mov     x3, x0
         :                      unsigned long stamp;
         :                      again:
         :                      stamp = READ_ONCE(part->stamp);
         :                      if (unlikely(stamp != now)) {
         :                      if (likely(cmpxchg(&part->stamp, stamp, now) == stamp)) {
         :                      __part_stat_add(part, io_ticks, 1);
    0.00 :   ffff800010451bf4:       adrp    x4, ffff8000114ca000 <bp_hardening_data>
    0.00 :   ffff800010451bf8:       add     x8, x4, #0x18
         :                      {
    0.00 :   ffff800010451bfc:       mov     x5, x1
         :                      __part_stat_add(part, io_ticks, 1);
    4.38 :   ffff800010451c00:       adrp    x2, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010451c04:       add     x7, x2, #0x8e8
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    1.11 :   ffff800010451c08:       ldr     x4, [x3, #832]
         :                      update_io_ticks():
         :                      if (unlikely(stamp != now)) {
    0.00 :   ffff800010451c0c:       cmp     x5, x4
    0.00 :   ffff800010451c10:       b.ne    ffff800010451c3c <update_io_ticks+0x4c>  // b.any
    9.22 :   ffff800010451c14:       nop
         :                      }
         :                      }
         :                      if (part->partno) {
   66.60 :   ffff800010451c18:       ldr     w0, [x3, #820]
    0.00 :   ffff800010451c1c:       cbz     w0, ffff800010451c64 <update_io_ticks+0x74>
         :                      part_to_disk():
         :                      struct lockdep_map lockdep_map;
         :                      };
         :
         :                      static inline struct gendisk *part_to_disk(struct hd_struct *part)
         :                      {
         :                      if (likely(part)) {
    3.86 :   ffff800010451c20:       cbz     x3, ffff800010451c9c <update_io_ticks+0xac>
         :                      if (part->partno)
         :                      return dev_to_disk(part_to_dev(part)->parent);
    1.92 :   ffff800010451c24:       ldr     x3, [x3, #104]
    0.00 :   ffff800010451c28:       sub     x3, x3, #0x70
         :                      update_io_ticks():
         :                      part = &part_to_disk(part)->part0;
    0.00 :   ffff800010451c2c:       add     x3, x3, #0x48
         :                      __read_once_size():
    4.28 :   ffff800010451c30:       ldr     x4, [x3, #832]
         :                      update_io_ticks():
         :                      if (unlikely(stamp != now)) {
    0.00 :   ffff800010451c34:       cmp     x5, x4
    0.00 :   ffff800010451c38:       b.eq    ffff800010451c18 <update_io_ticks+0x28>  // b.none
         :                      if (likely(cmpxchg(&part->stamp, stamp, now) == stamp)) {
    0.00 :   ffff800010451c3c:       add     x6, x3, #0x340
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010451c40:       b       ffff800010451c68 <update_io_ticks+0x78>
    0.00 :   ffff800010451c44:       b       ffff800010451c68 <update_io_ticks+0x78>
         :                      __lse__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,  l, "memory")
         :                      __CMPXCHG_CASE(x,  , rel_, 64,  l, "memory")
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, al, "memory")
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, al, "memory")
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, al, "memory")
         :                      __CMPXCHG_CASE(x,  ,  mb_, 64, al, "memory")
    0.00 :   ffff800010451c48:       mov     x0, x6
    0.00 :   ffff800010451c4c:       mov     x1, x4
    0.00 :   ffff800010451c50:       mov     x2, x5
    0.00 :   ffff800010451c54:       mov     x9, x1
    0.00 :   ffff800010451c58:       casal   x9, x2, [x6]
    0.42 :   ffff800010451c5c:       mov     x0, x9
    0.00 :   ffff800010451c60:       b       ffff800010451c6c <update_io_ticks+0x7c>
         :                      update_io_ticks():
         :                      goto again;
         :                      }
         :                      }
    7.48 :   ffff800010451c64:       ret
         :                      __ll_sc__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  , rel_, 64,        ,  , l, "memory", L)
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  ,  mb_, 64, dmb ish,  , l, "memory", L)
    0.00 :   ffff800010451c68:       b       ffff8000104521d8 <bio_associate_blkg_from_page+0x1a0>
         :                      update_io_ticks():
         :                      if (likely(cmpxchg(&part->stamp, stamp, now) == stamp)) {
    0.00 :   ffff800010451c6c:       cmp     x4, x0
    0.00 :   ffff800010451c70:       b.ne    ffff800010451c18 <update_io_ticks+0x28>  // b.any
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800010451c74:       mrs     x2, tpidr_el1
         :                      update_io_ticks():
         :                      __part_stat_add(part, io_ticks, 1);
    0.00 :   ffff800010451c78:       mov     x1, x8
    0.00 :   ffff800010451c7c:       ldr     x0, [x3, #840]
    0.04 :   ffff800010451c80:       ldrsw   x1, [x1, x2]
    0.00 :   ffff800010451c84:       ldr     x1, [x7, x1, lsl #3]
    0.00 :   ffff800010451c88:       add     x0, x0, x1
    0.00 :   ffff800010451c8c:       ldr     x1, [x0, #128]
    0.00 :   ffff800010451c90:       add     x1, x1, #0x1
    0.00 :   ffff800010451c94:       str     x1, [x0, #128]
    0.00 :   ffff800010451c98:       b       ffff800010451c18 <update_io_ticks+0x28>
         :                      part_to_disk():
         :                      else
         :                      return dev_to_disk(part_to_dev(part));
         :                      }
         :                      return NULL;
    0.00 :   ffff800010451c9c:       mov     x3, #0x0                        // #0
         :                      update_io_ticks():
         :                      part = &part_to_disk(part)->part0;
    0.00 :   ffff800010451ca0:       add     x3, x3, #0x48
    0.00 :   ffff800010451ca4:       b       ffff800010451c30 <update_io_ticks+0x40>
 Percent |	Source code & Disassembly of vmlinux for cycles (3551 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010086ff0 <arch_cpu_idle>:
         :                      __cpu_do_idle():
         :
         :                      void (*arm_pm_restart)(enum reboot_mode reboot_mode, const char *cmd);
         :
         :                      static void __cpu_do_idle(void)
         :                      {
         :                      dsb(sy);
    0.00 :   ffff800010086ff0:       dsb     sy
         :                      wfi();
    0.00 :   ffff800010086ff4:       wfi
         :                      arch_local_irq_enable():
         :                      u32 pmr = read_sysreg_s(SYS_ICC_PMR_EL1);
         :
         :                      WARN_ON_ONCE(pmr != GIC_PRIO_IRQON && pmr != GIC_PRIO_IRQOFF);
         :                      }
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010086ff8:       mov     x0, #0xe0                       // #224
    0.00 :   ffff800010086ffc:       msr     daifclr, #0x2
         :                      arch_cpu_idle():
         :                      */
         :                      trace_cpu_idle_rcuidle(1, smp_processor_id());
         :                      cpu_do_idle();
         :                      local_irq_enable();
         :                      trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
         :                      }
  100.00 :   ffff800010087000:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (2159 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000107075f0 <arm_smmu_tlb_inv_page_nosync>:
         :                      arm_smmu_tlb_inv_page_nosync():
         :                      }
         :
         :                      static void arm_smmu_tlb_inv_page_nosync(struct iommu_iotlb_gather *gather,
         :                      unsigned long iova, size_t granule,
         :                      void *cookie)
         :                      {
    8.35 :   ffff8000107075f0:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff8000107075f4:       mov     x29, sp
    1.92 :   ffff8000107075f8:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000107075fc:       mov     x19, x0
    0.19 :   ffff800010707600:       stp     x21, x22, [sp, #32]
    0.05 :   ffff800010707604:       mov     x21, x1
         :                      iommu_iotlb_gather_add_page():
         :
         :                      static inline void iommu_iotlb_gather_add_page(struct iommu_domain *domain,
         :                      struct iommu_iotlb_gather *gather,
         :                      unsigned long iova, size_t size)
         :                      {
         :                      unsigned long start = iova, end = start + size;
    0.00 :   ffff800010707608:       add     x22, x1, x2
         :                      arm_smmu_tlb_inv_page_nosync():
    0.00 :   ffff80001070760c:       mov     x20, x2
         :                      iommu_iotlb_gather_add_page():
         :                      /*
         :                      * If the new page is disjoint from the current range or is mapped at
         :                      * a different granularity, then sync the TLB so that the gather
         :                      * structure can be rewritten.
         :                      */
         :                      if (gather->pgsize != size ||
    0.42 :   ffff800010707610:       ldr     x1, [x0, #16]
    0.00 :   ffff800010707614:       cmp     x2, x1
    0.00 :   ffff800010707618:       b.eq    ffff800010707684 <arm_smmu_tlb_inv_page_nosync+0x94>  // b.none
         :                      end < gather->start || start > gather->end) {
         :                      if (gather->pgsize)
    6.57 :   ffff80001070761c:       cbz     x1, ffff80001070767c <arm_smmu_tlb_inv_page_nosync+0x8c>
         :                      iommu_tlb_sync():
         :                      if (domain->ops->iotlb_sync)
    0.00 :   ffff800010707620:       add     x0, x3, #0x70
    0.00 :   ffff800010707624:       ldr     x1, [x0, #8]
    0.00 :   ffff800010707628:       ldr     x2, [x1, #72]
    0.00 :   ffff80001070762c:       cbz     x2, ffff800010707638 <arm_smmu_tlb_inv_page_nosync+0x48>
         :                      domain->ops->iotlb_sync(domain, iotlb_gather);
    0.00 :   ffff800010707630:       mov     x1, x19
    0.00 :   ffff800010707634:       blr     x2
         :                      iommu_iotlb_gather_init():
         :                      *gather = (struct iommu_iotlb_gather) {
    0.00 :   ffff800010707638:       mov     x1, #0xffffffffffffffff         // #-1
    0.00 :   ffff80001070763c:       stp     xzr, xzr, [x19]
    0.00 :   ffff800010707640:       mov     x0, x1
    0.00 :   ffff800010707644:       mov     x2, #0x0                        // #0
    0.00 :   ffff800010707648:       str     x1, [x19]
    0.00 :   ffff80001070764c:       str     xzr, [x19, #16]
         :                      iommu_iotlb_gather_add_page():
         :                      iommu_tlb_sync(domain, gather);
         :                      gather->pgsize = size;
    0.13 :   ffff800010707650:       str     x20, [x19, #16]
         :                      }
         :
         :                      if (gather->end < end)
    0.00 :   ffff800010707654:       cmp     x22, x2
    0.05 :   ffff800010707658:       b.ls    ffff800010707660 <arm_smmu_tlb_inv_page_nosync+0x70>  // b.plast
         :                      gather->end = end;
    1.44 :   ffff80001070765c:       str     x22, [x19, #8]
         :
         :                      if (gather->start > start)
    0.00 :   ffff800010707660:       cmp     x21, x0
    0.14 :   ffff800010707664:       b.cs    ffff80001070766c <arm_smmu_tlb_inv_page_nosync+0x7c>  // b.hs, b.nlast
         :                      gather->start = start;
    6.45 :   ffff800010707668:       str     x21, [x19]
         :                      arm_smmu_tlb_inv_page_nosync():
         :                      struct arm_smmu_domain *smmu_domain = cookie;
         :                      struct iommu_domain *domain = &smmu_domain->domain;
         :
         :                      iommu_iotlb_gather_add_page(domain, gather, iova, granule);
         :                      }
    2.52 :   ffff80001070766c:       ldp     x19, x20, [sp, #16]
   35.33 :   ffff800010707670:       ldp     x21, x22, [sp, #32]
   36.06 :   ffff800010707674:       ldp     x29, x30, [sp], #48
    0.09 :   ffff800010707678:       ret
    0.28 :   ffff80001070767c:       ldp     x0, x2, [x19]
    0.00 :   ffff800010707680:       b       ffff800010707650 <arm_smmu_tlb_inv_page_nosync+0x60>
         :                      iommu_iotlb_gather_add_page():
         :                      end < gather->start || start > gather->end) {
    0.00 :   ffff800010707684:       ldr     x0, [x0]
         :                      if (gather->pgsize != size ||
    0.00 :   ffff800010707688:       cmp     x22, x0
    0.00 :   ffff80001070768c:       b.cc    ffff80001070761c <arm_smmu_tlb_inv_page_nosync+0x2c>  // b.lo, b.ul, b.last
         :                      end < gather->start || start > gather->end) {
    0.00 :   ffff800010707690:       ldr     x2, [x19, #8]
    0.00 :   ffff800010707694:       cmp     x21, x2
    0.00 :   ffff800010707698:       b.ls    ffff800010707654 <arm_smmu_tlb_inv_page_nosync+0x64>  // b.plast
         :                      if (gather->pgsize)
    0.00 :   ffff80001070769c:       cbnz    x1, ffff800010707620 <arm_smmu_tlb_inv_page_nosync+0x30>
    0.00 :   ffff8000107076a0:       b       ffff80001070767c <arm_smmu_tlb_inv_page_nosync+0x8c>
 Percent |	Source code & Disassembly of vmlinux for cycles (3285 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001015a698 <__rcu_read_lock>:
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
   10.05 :   ffff80001015a698:       mrs     x1, sp_el0
         :                      __rcu_read_lock():
         :                      * Just increment ->rcu_read_lock_nesting, shared state will be updated
         :                      * if we block.
         :                      */
         :                      void __rcu_read_lock(void)
         :                      {
         :                      current->rcu_read_lock_nesting++;
   18.27 :   ffff80001015a69c:       ldr     w0, [x1, #776]
    0.37 :   ffff80001015a6a0:       add     w0, w0, #0x1
   71.28 :   ffff80001015a6a4:       str     w0, [x1, #776]
         :                      if (IS_ENABLED(CONFIG_PROVE_LOCKING))
         :                      WARN_ON_ONCE(current->rcu_read_lock_nesting > RCU_NEST_PMAX);
         :                      barrier();  /* critical section after entry code. */
         :                      }
    0.03 :   ffff80001015a6a8:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (1982 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001044e768 <bio_advance>:
         :                      bio_advance():
         :                      * be updated on the last bvec as well.
         :                      *
         :                      * @bio will then represent the remaining, uncompleted portion of the io.
         :                      */
         :                      void bio_advance(struct bio *bio, unsigned bytes)
         :                      {
    0.20 :   ffff80001044e768:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff80001044e76c:       mov     x29, sp
    2.15 :   ffff80001044e770:       stp     x19, x20, [sp, #16]
    0.05 :   ffff80001044e774:       mov     x20, x0
    0.30 :   ffff80001044e778:       str     x21, [sp, #32]
    0.41 :   ffff80001044e77c:       mov     w19, w1
         :                      if (bio_integrity(bio))
    5.48 :   ffff80001044e780:       ldr     w2, [x0, #16]
         :                      bio_integrity():
         :
         :                      #if defined(CONFIG_BLK_DEV_INTEGRITY)
         :
         :                      static inline struct bio_integrity_payload *bio_integrity(struct bio *bio)
         :                      {
         :                      if (bio->bi_opf & REQ_INTEGRITY)
    0.00 :   ffff80001044e784:       tbz     w2, #16, ffff80001044e798 <bio_advance+0x30>
         :                      bio_advance():
    0.00 :   ffff80001044e788:       ldr     x3, [x0, #88]
    0.00 :   ffff80001044e78c:       cbz     x3, ffff80001044e798 <bio_advance+0x30>
         :                      bio_integrity_advance(bio, bytes);
    0.00 :   ffff80001044e790:       bl      ffff80001047ac20 <bio_integrity_advance>
    0.00 :   ffff80001044e794:       ldr     w2, [x20, #16]
         :                      bio_advance_iter():
         :                      iter->bi_sector += bytes >> 9;
    1.67 :   ffff80001044e798:       ldr     x3, [x20, #32]
         :                      bio_no_advance_iter():
         :                      return bio_op(bio) == REQ_OP_DISCARD ||
    0.00 :   ffff80001044e79c:       and     w0, w2, #0xff
         :                      bio_advance_iter():
         :                      iter->bi_sector += bytes >> 9;
    0.96 :   ffff80001044e7a0:       add     x21, x20, #0x20
    0.00 :   ffff80001044e7a4:       lsr     w1, w19, #9
    3.03 :   ffff80001044e7a8:       add     x1, x3, x1
         :                      bio_no_advance_iter():
         :                      return bio_op(bio) == REQ_OP_DISCARD ||
    0.00 :   ffff80001044e7ac:       sub     w0, w0, #0x5
         :                      bio_advance_iter():
         :                      iter->bi_sector += bytes >> 9;
    0.61 :   ffff80001044e7b0:       str     x1, [x20, #32]
         :                      bio_no_advance_iter():
         :                      bio_op(bio) == REQ_OP_WRITE_SAME ||
    0.00 :   ffff80001044e7b4:       tst     w0, #0xfffffffb
         :                      return bio_op(bio) == REQ_OP_DISCARD ||
    0.05 :   ffff80001044e7b8:       mov     w1, #0xfb                       // #251
    0.00 :   ffff80001044e7bc:       and     w2, w2, w1
         :                      bio_op(bio) == REQ_OP_WRITE_SAME ||
    5.15 :   ffff80001044e7c0:       ccmp    w2, #0x3, #0x4, ne  // ne = any
    1.06 :   ffff80001044e7c4:       ldr     w1, [x21, #8]
    0.00 :   ffff80001044e7c8:       b.ne    ffff80001044e7e4 <bio_advance+0x7c>  // b.any
         :                      bio_advance_iter():
         :                      iter->bi_size -= bytes;
    0.00 :   ffff80001044e7cc:       sub     w1, w1, w19
    0.00 :   ffff80001044e7d0:       str     w1, [x21, #8]
         :                      bio_advance():
         :
         :                      bio_advance_iter(bio, &bio->bi_iter, bytes);
         :                      }
    0.00 :   ffff80001044e7d4:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001044e7d8:       ldr     x21, [sp, #32]
    0.00 :   ffff80001044e7dc:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001044e7e0:       ret
         :                      bvec_iter_advance():
         :                      static inline bool bvec_iter_advance(const struct bio_vec *bv,
         :                      struct bvec_iter *iter, unsigned bytes)
         :                      {
         :                      unsigned int idx = iter->bi_idx;
         :
         :                      if (WARN_ONCE(bytes > iter->bi_size,
    4.75 :   ffff80001044e7e4:       cmp     w19, w1
         :                      unsigned int idx = iter->bi_idx;
    6.04 :   ffff80001044e7e8:       ldr     w2, [x21, #12]
         :                      bio_advance_iter():
         :                      bvec_iter_advance(bio->bi_io_vec, iter, bytes);
    6.03 :   ffff80001044e7ec:       ldr     x3, [x20, #104]
         :                      bvec_iter_advance():
         :                      if (WARN_ONCE(bytes > iter->bi_size,
    0.00 :   ffff80001044e7f0:       b.hi    ffff80001044e840 <bio_advance+0xd8>  // b.pmore
         :                      "Attempted to advance past end of bvec iter\n")) {
         :                      iter->bi_size = 0;
         :                      return false;
         :                      }
         :
         :                      iter->bi_size -= bytes;
   11.61 :   ffff80001044e7f4:       sub     w1, w1, w19
         :                      bytes += iter->bi_bvec_done;
    1.21 :   ffff80001044e7f8:       ldr     w0, [x21, #16]
         :                      iter->bi_size -= bytes;
    2.11 :   ffff80001044e7fc:       str     w1, [x21, #8]
         :
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.05 :   ffff80001044e800:       adds    w1, w19, w0
    0.00 :   ffff80001044e804:       b.ne    ffff80001044e818 <bio_advance+0xb0>  // b.any
    0.00 :   ffff80001044e808:       b       ffff80001044e82c <bio_advance+0xc4>
   21.27 :   ffff80001044e80c:       subs    w1, w1, w0
         :                      bytes -= bv[idx].bv_len;
         :                      idx++;
    0.00 :   ffff80001044e810:       add     w2, w2, #0x1
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff80001044e814:       b.eq    ffff80001044e82c <bio_advance+0xc4>  // b.none
    2.87 :   ffff80001044e818:       ubfiz   x0, x2, #4, #32
    0.00 :   ffff80001044e81c:       add     x0, x3, x0
    3.09 :   ffff80001044e820:       ldr     w0, [x0, #8]
    0.05 :   ffff80001044e824:       cmp     w0, w1
    0.00 :   ffff80001044e828:       b.ls    ffff80001044e80c <bio_advance+0xa4>  // b.plast
         :                      }
         :
         :                      iter->bi_idx = idx;
         :                      iter->bi_bvec_done = bytes;
    6.39 :   ffff80001044e82c:       stp     w2, w1, [x21, #12]
         :                      bio_advance():
   10.42 :   ffff80001044e830:       ldp     x19, x20, [sp, #16]
    1.58 :   ffff80001044e834:       ldr     x21, [sp, #32]
    1.42 :   ffff80001044e838:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001044e83c:       ret
         :                      bvec_iter_advance():
         :                      if (WARN_ONCE(bytes > iter->bi_size,
    0.00 :   ffff80001044e840:       adrp    x1, ffff800011a5d000 <__print_once.28906>
    0.00 :   ffff80001044e844:       ldrb    w0, [x1, #69]
    0.00 :   ffff80001044e848:       cbz     w0, ffff80001044e854 <bio_advance+0xec>
         :                      iter->bi_size = 0;
    0.00 :   ffff80001044e84c:       str     wzr, [x21, #8]
    0.00 :   ffff80001044e850:       b       ffff80001044e7d4 <bio_advance+0x6c>
         :                      if (WARN_ONCE(bytes > iter->bi_size,
    0.00 :   ffff80001044e854:       mov     w2, #0x1                        // #1
    0.00 :   ffff80001044e858:       adrp    x0, ffff8000111aa000 <kallsyms_token_index+0x30330>
    0.00 :   ffff80001044e85c:       strb    w2, [x1, #69]
    0.00 :   ffff80001044e860:       add     x0, x0, #0xea8
    0.00 :   ffff80001044e864:       bl      ffff8000100e5288 <__warn_printk>
    0.00 :   ffff80001044e868:       brk     #0x800
         :                      iter->bi_size = 0;
    0.00 :   ffff80001044e86c:       str     wzr, [x21, #8]
    0.00 :   ffff80001044e870:       b       ffff80001044e7d4 <bio_advance+0x6c>
 Percent |	Source code & Disassembly of vmlinux for cycles (3292 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010700f80 <__arm_lpae_map>:
         :                      __arm_lpae_map():
         :                      }
         :
         :                      static int __arm_lpae_map(struct arm_lpae_io_pgtable *data, unsigned long iova,
         :                      phys_addr_t paddr, size_t size, arm_lpae_iopte prot,
         :                      int lvl, arm_lpae_iopte *ptep)
         :                      {
    1.94 :   ffff800010700f80:       stp     x29, x30, [sp, #-96]!
    0.00 :   ffff800010700f84:       mov     x29, sp
    1.09 :   ffff800010700f88:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010700f8c:       mov     x19, x2
    0.42 :   ffff800010700f90:       stp     x21, x22, [sp, #32]
    0.00 :   ffff800010700f94:       mov     w22, w5
    1.70 :   ffff800010700f98:       stp     x23, x24, [sp, #48]
    0.00 :   ffff800010700f9c:       mov     x21, x0
    0.33 :   ffff800010700fa0:       stp     x25, x26, [sp, #64]
         :                      arm_lpae_iopte *cptep, pte;
         :                      size_t block_size = ARM_LPAE_BLOCK_SIZE(lvl, data);
    0.00 :   ffff800010700fa4:       mov     w5, #0x4                        // #4
    0.00 :   ffff800010700fa8:       sub     w5, w5, w22
    0.00 :   ffff800010700fac:       mov     x23, #0x1                       // #1
    0.39 :   ffff800010700fb0:       ldr     w0, [x0, #128]
         :                      {
    0.00 :   ffff800010700fb4:       mov     x24, x1
         :                      size_t tblsz = ARM_LPAE_GRANULE(data);
         :                      struct io_pgtable_cfg *cfg = &data->iop.cfg;
         :
         :                      /* Find our entry at the current level */
         :                      ptep += ARM_LPAE_LVL_IDX(iova, lvl, data);
    0.45 :   ffff800010700fb8:       ldr     w7, [x21, #124]
         :                      {
    0.06 :   ffff800010700fbc:       mov     x25, x3
    0.00 :   ffff800010700fc0:       mov     x20, x4
         :                      ptep += ARM_LPAE_LVL_IDX(iova, lvl, data);
    0.00 :   ffff800010700fc4:       cmp     w7, w22
         :                      size_t block_size = ARM_LPAE_BLOCK_SIZE(lvl, data);
    1.34 :   ffff800010700fc8:       mul     w5, w5, w0
    0.00 :   ffff800010700fcc:       add     w5, w5, #0x3
    0.00 :   ffff800010700fd0:       lsl     x23, x23, x5
         :                      ptep += ARM_LPAE_LVL_IDX(iova, lvl, data);
    0.00 :   ffff800010700fd4:       lsr     x5, x1, x5
    0.00 :   ffff800010700fd8:       mov     w1, w0
    0.00 :   ffff800010700fdc:       b.ne    ffff800010700fe4 <__arm_lpae_map+0x64>  // b.any
    0.00 :   ffff800010700fe0:       ldr     w1, [x21, #120]
    0.00 :   ffff800010700fe4:       mov     w2, #0x1                        // #1
         :
         :                      /* If we can install a leaf entry at this level, then do so */
         :                      if (size == block_size)
    0.00 :   ffff800010700fe8:       cmp     x23, x25
         :                      ptep += ARM_LPAE_LVL_IDX(iova, lvl, data);
    0.00 :   ffff800010700fec:       lsl     w2, w2, w1
    1.22 :   ffff800010700ff0:       sub     w2, w2, #0x1
    0.37 :   ffff800010700ff4:       sxtw    x2, w2
    0.00 :   ffff800010700ff8:       and     x2, x2, x5
    0.00 :   ffff800010700ffc:       add     x26, x6, x2, lsl #3
         :                      if (size == block_size)
    0.00 :   ffff800010701000:       b.eq    ffff8000107010f8 <__arm_lpae_map+0x178>  // b.none
         :                      return arm_lpae_init_pte(data, iova, paddr, prot, lvl, ptep);
         :
         :                      /* We can't allocate tables at the final level */
         :                      if (WARN_ON(lvl >= ARM_LPAE_MAX_LEVELS - 1))
    0.88 :   ffff800010701004:       cmp     w22, #0x2
    0.00 :   ffff800010701008:       b.gt    ffff8000107011e4 <__arm_lpae_map+0x264>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    2.55 :   ffff80001070100c:       ldr     x23, [x6, x2, lsl #3]
         :                      __arm_lpae_map():
         :                      return -EINVAL;
         :
         :                      /* Grab a pointer to the next level */
         :                      pte = READ_ONCE(*ptep);
         :                      if (!pte) {
    0.00 :   ffff800010701010:       cbz     x23, ffff80001070104c <__arm_lpae_map+0xcc>
         :                      return -ENOMEM;
         :
         :                      pte = arm_lpae_install_table(cptep, ptep, 0, cfg);
         :                      if (pte)
         :                      __arm_lpae_free_pages(cptep, tblsz, cfg);
         :                      } else if (!cfg->coherent_walk && !(pte & ARM_LPAE_PTE_SW_SYNC)) {
   17.68 :   ffff800010701014:       ldrb    w0, [x21, #40]
    0.00 :   ffff800010701018:       cbnz    w0, ffff800010701020 <__arm_lpae_map+0xa0>
    0.00 :   ffff80001070101c:       tbz     x23, #55, ffff8000107010cc <__arm_lpae_map+0x14c>
         :                      iopte_leaf():
         :                      return iopte_type(pte, lvl) == ARM_LPAE_PTE_TYPE_BLOCK;
    1.36 :   ffff800010701020:       and     x0, x23, #0x3
         :                      __arm_lpae_map():
         :                      __arm_lpae_sync_pte(ptep, cfg);
         :                      }
         :
         :                      if (pte && !iopte_leaf(pte, lvl, data->iop.fmt)) {
    0.00 :   ffff800010701024:       cmp     x0, #0x1
    0.00 :   ffff800010701028:       b.ne    ffff800010701128 <__arm_lpae_map+0x1a8>  // b.any
         :                      cptep = iopte_deref(pte, data);
         :                      } else if (pte) {
         :                      /* We require an unmap first */
         :                      WARN_ON(!selftest_running);
    0.00 :   ffff80001070102c:       brk     #0x800
         :                      return -EEXIST;
    0.00 :   ffff800010701030:       mov     w0, #0xffffffef                 // #-17
         :                      }
         :
         :                      /* Rinse, repeat */
         :                      return __arm_lpae_map(data, iova, paddr, size, prot, lvl + 1, cptep);
         :                      }
    0.00 :   ffff800010701034:       ldp     x19, x20, [sp, #16]
    1.82 :   ffff800010701038:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff80001070103c:       ldp     x23, x24, [sp, #48]
    0.37 :   ffff800010701040:       ldp     x25, x26, [sp, #64]
    0.97 :   ffff800010701044:       ldp     x29, x30, [sp], #96
    0.00 :   ffff800010701048:       ret
    0.00 :   ffff80001070104c:       stp     x27, x28, [x29, #80]
         :                      size_t tblsz = ARM_LPAE_GRANULE(data);
    0.00 :   ffff800010701050:       mov     x28, #0x8                       // #8
    0.00 :   ffff800010701054:       lsl     x28, x28, x0
         :                      cptep = __arm_lpae_alloc_pages(tblsz, GFP_ATOMIC, cfg);
    0.00 :   ffff800010701058:       add     x2, x21, #0x28
    0.00 :   ffff80001070105c:       ldr     x3, [x21, #56]
    0.00 :   ffff800010701060:       mov     w1, #0xa20                      // #2592
    0.00 :   ffff800010701064:       mov     x0, x28
    0.00 :   ffff800010701068:       bl      ffff8000107000c8 <__arm_lpae_alloc_pages.isra.22>
    0.00 :   ffff80001070106c:       mov     x27, x0
         :                      if (!cptep)
    0.00 :   ffff800010701070:       cbz     x0, ffff8000107011f0 <__arm_lpae_map+0x270>
         :                      pte = arm_lpae_install_table(cptep, ptep, 0, cfg);
    0.00 :   ffff800010701074:       add     x3, x21, #0x10
    0.00 :   ffff800010701078:       mov     x2, #0x0                        // #0
    0.00 :   ffff80001070107c:       mov     x1, x26
    0.00 :   ffff800010701080:       bl      ffff8000106ffff0 <arm_lpae_install_table>
    0.00 :   ffff800010701084:       mov     x23, x0
         :                      if (pte)
    0.00 :   ffff800010701088:       cbnz    x0, ffff8000107010dc <__arm_lpae_map+0x15c>
    0.00 :   ffff80001070108c:       ldr     x28, [x29, #88]
         :                      return __arm_lpae_map(data, iova, paddr, size, prot, lvl + 1, cptep);
    0.21 :   ffff800010701090:       mov     x6, x27
    0.00 :   ffff800010701094:       add     w5, w22, #0x1
    0.00 :   ffff800010701098:       mov     x4, x20
    0.00 :   ffff80001070109c:       mov     x3, x25
    1.25 :   ffff8000107010a0:       mov     x2, x19
    0.00 :   ffff8000107010a4:       mov     x1, x24
    0.00 :   ffff8000107010a8:       mov     x0, x21
    0.00 :   ffff8000107010ac:       bl      ffff800010700f80 <__arm_lpae_map>
    1.37 :   ffff8000107010b0:       ldr     x27, [x29, #80]
         :                      }
    2.18 :   ffff8000107010b4:       ldp     x19, x20, [sp, #16]
    0.18 :   ffff8000107010b8:       ldp     x21, x22, [sp, #32]
    0.21 :   ffff8000107010bc:       ldp     x23, x24, [sp, #48]
    0.33 :   ffff8000107010c0:       ldp     x25, x26, [sp, #64]
    0.52 :   ffff8000107010c4:       ldp     x29, x30, [sp], #96
    0.00 :   ffff8000107010c8:       ret
         :                      __arm_lpae_sync_pte(ptep, cfg);
    0.00 :   ffff8000107010cc:       ldr     x1, [x21, #56]
    0.00 :   ffff8000107010d0:       mov     x0, x26
    0.00 :   ffff8000107010d4:       bl      ffff8000106ffef8 <__arm_lpae_sync_pte.isra.21>
    0.00 :   ffff8000107010d8:       b       ffff800010701020 <__arm_lpae_map+0xa0>
         :                      __arm_lpae_free_pages(cptep, tblsz, cfg);
    0.00 :   ffff8000107010dc:       ldrb    w2, [x21, #40]
    0.00 :   ffff8000107010e0:       mov     x1, x28
    0.00 :   ffff8000107010e4:       mov     x0, x27
    0.00 :   ffff8000107010e8:       add     x3, x21, #0x38
    0.00 :   ffff8000107010ec:       bl      ffff8000107008a0 <__arm_lpae_free_pages.isra.20>
    0.00 :   ffff8000107010f0:       ldp     x27, x28, [x29, #80]
    0.00 :   ffff8000107010f4:       b       ffff800010701020 <__arm_lpae_map+0xa0>
         :                      iopte_leaf():
         :                      if (lvl == (ARM_LPAE_MAX_LEVELS - 1) && fmt != ARM_MALI_LPAE)
    0.30 :   ffff8000107010f8:       ldr     w3, [x21]
    0.72 :   ffff8000107010fc:       ldr     x1, [x6, x2, lsl #3]
    0.12 :   ffff800010701100:       cmp     w3, #0x5
    0.00 :   ffff800010701104:       and     x1, x1, #0x3
    0.00 :   ffff800010701108:       ccmp    w22, #0x3, #0x0, ne  // ne = any
   53.50 :   ffff80001070110c:       b.ne    ffff800010701160 <__arm_lpae_map+0x1e0>  // b.any
         :                      return iopte_type(pte, lvl) == ARM_LPAE_PTE_TYPE_PAGE;
    0.00 :   ffff800010701110:       cmp     x1, #0x3
    0.00 :   ffff800010701114:       cset    w2, eq  // eq = none
         :                      arm_lpae_init_pte():
         :                      if (iopte_leaf(pte, lvl, data->iop.fmt)) {
    0.00 :   ffff800010701118:       cbz     w2, ffff80001070116c <__arm_lpae_map+0x1ec>
         :                      WARN_ON(!selftest_running);
    0.00 :   ffff80001070111c:       brk     #0x800
         :                      return -EEXIST;
    0.00 :   ffff800010701120:       mov     w0, #0xffffffef                 // #-17
    0.00 :   ffff800010701124:       b       ffff800010701034 <__arm_lpae_map+0xb4>
    0.00 :   ffff800010701128:       str     x27, [x29, #80]
         :                      iopte_to_paddr():
         :                      u64 paddr = pte & ARM_LPAE_PTE_ADDR_MASK;
    0.00 :   ffff80001070112c:       and     x23, x23, #0xfffffffff000
         :                      __arm_lpae_map():
         :                      cptep = iopte_deref(pte, data);
    0.00 :   ffff800010701130:       adrp    x2, ffff8000112ae000 <cpu_ops+0x248>
         :                      iopte_to_paddr():
         :                      if (ARM_LPAE_GRANULE(data) < SZ_64K)
    0.00 :   ffff800010701134:       mov     x1, #0x8                        // #8
    2.06 :   ffff800010701138:       ldr     w4, [x21, #128]
         :                      return (paddr | (paddr << (48 - 12))) & (ARM_LPAE_PTE_ADDR_MASK << 4);
    0.00 :   ffff80001070113c:       orr     x0, x23, x23, lsl #36
         :                      if (ARM_LPAE_GRANULE(data) < SZ_64K)
    0.00 :   ffff800010701140:       mov     x3, #0xffff                     // #65535
         :                      return (paddr | (paddr << (48 - 12))) & (ARM_LPAE_PTE_ADDR_MASK << 4);
    0.00 :   ffff800010701144:       and     x0, x0, #0xfffffffff0000
         :                      __arm_lpae_map():
         :                      cptep = iopte_deref(pte, data);
    0.00 :   ffff800010701148:       ldr     x27, [x2, #1872]
         :                      iopte_to_paddr():
         :                      if (ARM_LPAE_GRANULE(data) < SZ_64K)
    0.00 :   ffff80001070114c:       lsl     x1, x1, x4
         :                      return (paddr | (paddr << (48 - 12))) & (ARM_LPAE_PTE_ADDR_MASK << 4);
    0.00 :   ffff800010701150:       cmp     x1, x3
    0.00 :   ffff800010701154:       csel    x23, x0, x23, hi  // hi = pmore
         :                      __arm_lpae_map():
         :                      cptep = iopte_deref(pte, data);
    0.24 :   ffff800010701158:       sub     x27, x23, x27
    0.00 :   ffff80001070115c:       b       ffff800010701090 <__arm_lpae_map+0x110>
         :                      iopte_leaf():
         :                      return iopte_type(pte, lvl) == ARM_LPAE_PTE_TYPE_BLOCK;
    0.00 :   ffff800010701160:       cmp     x1, #0x1
    0.00 :   ffff800010701164:       cset    w2, eq  // eq = none
    0.00 :   ffff800010701168:       b       ffff800010701118 <__arm_lpae_map+0x198>
         :                      arm_lpae_init_pte():
         :                      } else if (iopte_type(pte, lvl) == ARM_LPAE_PTE_TYPE_TABLE) {
    0.00 :   ffff80001070116c:       cmp     x1, #0x3
    0.03 :   ffff800010701170:       b.ne    ffff8000107011c4 <__arm_lpae_map+0x244>  // b.any
         :                      tblp = ptep - ARM_LPAE_LVL_IDX(iova, lvl, data);
    0.00 :   ffff800010701174:       cmp     w7, w22
    0.00 :   ffff800010701178:       b.ne    ffff800010701180 <__arm_lpae_map+0x200>  // b.any
    0.00 :   ffff80001070117c:       ldr     w0, [x21, #120]
    0.00 :   ffff800010701180:       mov     w1, #0x1                        // #1
         :                      if (__arm_lpae_unmap(data, NULL, iova, sz, lvl, tblp) != sz) {
    0.00 :   ffff800010701184:       mov     w4, w22
         :                      tblp = ptep - ARM_LPAE_LVL_IDX(iova, lvl, data);
    0.00 :   ffff800010701188:       lsl     w0, w1, w0
    0.00 :   ffff80001070118c:       sub     w0, w0, #0x1
         :                      if (__arm_lpae_unmap(data, NULL, iova, sz, lvl, tblp) != sz) {
    0.00 :   ffff800010701190:       mov     x3, x23
    0.00 :   ffff800010701194:       mov     x2, x24
         :                      tblp = ptep - ARM_LPAE_LVL_IDX(iova, lvl, data);
    0.00 :   ffff800010701198:       sxtw    x0, w0
         :                      if (__arm_lpae_unmap(data, NULL, iova, sz, lvl, tblp) != sz) {
    0.00 :   ffff80001070119c:       mov     x1, #0x0                        // #0
         :                      tblp = ptep - ARM_LPAE_LVL_IDX(iova, lvl, data);
    0.00 :   ffff8000107011a0:       and     x5, x0, x5
         :                      if (__arm_lpae_unmap(data, NULL, iova, sz, lvl, tblp) != sz) {
    0.00 :   ffff8000107011a4:       mov     x0, x21
    0.00 :   ffff8000107011a8:       sub     x5, x26, x5, lsl #3
    0.00 :   ffff8000107011ac:       bl      ffff800010700aa0 <__arm_lpae_unmap>
    0.00 :   ffff8000107011b0:       cmp     x23, x0
    0.00 :   ffff8000107011b4:       b.eq    ffff8000107011c4 <__arm_lpae_map+0x244>  // b.none
         :                      WARN_ON(1);
    0.00 :   ffff8000107011b8:       brk     #0x800
         :                      return -EINVAL;
    0.00 :   ffff8000107011bc:       mov     w0, #0xffffffea                 // #-22
    0.00 :   ffff8000107011c0:       b       ffff800010701034 <__arm_lpae_map+0xb4>
         :                      __arm_lpae_init_pte(data, paddr, prot, lvl, ptep);
    1.70 :   ffff8000107011c4:       mov     x0, x21
    0.00 :   ffff8000107011c8:       mov     x4, x26
    0.00 :   ffff8000107011cc:       mov     w3, w22
    0.00 :   ffff8000107011d0:       mov     x2, x20
    0.09 :   ffff8000107011d4:       mov     x1, x19
    0.00 :   ffff8000107011d8:       bl      ffff8000106fff80 <__arm_lpae_init_pte>
         :                      return 0;
    0.03 :   ffff8000107011dc:       mov     w0, #0x0                        // #0
    0.00 :   ffff8000107011e0:       b       ffff800010701034 <__arm_lpae_map+0xb4>
         :                      __arm_lpae_map():
         :                      if (WARN_ON(lvl >= ARM_LPAE_MAX_LEVELS - 1))
    0.00 :   ffff8000107011e4:       brk     #0x800
         :                      return -EINVAL;
    0.00 :   ffff8000107011e8:       mov     w0, #0xffffffea                 // #-22
    0.00 :   ffff8000107011ec:       b       ffff800010701034 <__arm_lpae_map+0xb4>
         :                      return -ENOMEM;
    0.00 :   ffff8000107011f0:       mov     w0, #0xfffffff4                 // #-12
    0.00 :   ffff8000107011f4:       ldp     x27, x28, [x29, #80]
    0.00 :   ffff8000107011f8:       b       ffff800010701034 <__arm_lpae_map+0xb4>
 Percent |	Source code & Disassembly of vmlinux for cycles (2815 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102d66c8 <lookup_ioctx>:
         :                      lookup_ioctx():
         :                      req->ki_eventfd = NULL;
         :                      return req;
         :                      }
         :
         :                      static struct kioctx *lookup_ioctx(unsigned long ctx_id)
         :                      {
    1.07 :   ffff8000102d66c8:       stp     x29, x30, [sp, #-48]!
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.85 :   ffff8000102d66cc:       mrs     x1, sp_el0
         :                      lookup_ioctx():
    0.00 :   ffff8000102d66d0:       mov     x29, sp
    0.00 :   ffff8000102d66d4:       str     x20, [sp, #24]
    1.10 :   ffff8000102d66d8:       str     x22, [sp, #40]
         :                      __range_ok():
         :                      * Asynchronous I/O running in a kernel thread does not have the
         :                      * TIF_TAGGED_ADDR flag of the process owning the mm, so always untag
         :                      * the user address before checking.
         :                      */
         :                      if (IS_ENABLED(CONFIG_ARM64_TAGGED_ADDR_ABI) &&
         :                      (current->flags & PF_KTHREAD || test_thread_flag(TIF_TAGGED_ADDR)))
    0.07 :   ffff8000102d66dc:       ldr     w3, [x1, #44]
         :                      unsigned long ret, limit = current_thread_info()->addr_limit;
    3.24 :   ffff8000102d66e0:       ldr     x2, [x1, #8]
         :                      lookup_ioctx():
         :                      struct aio_ring __user *ring  = (void __user *)ctx_id;
         :                      struct mm_struct *mm = current->mm;
    0.39 :   ffff8000102d66e4:       ldr     x22, [x1, #952]
         :                      __range_ok():
         :                      if (IS_ENABLED(CONFIG_ARM64_TAGGED_ADDR_ABI) &&
    0.00 :   ffff8000102d66e8:       tbnz    w3, #21, ffff8000102d66fc <lookup_ioctx+0x34>
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    2.85 :   ffff8000102d66ec:       ldr     x3, [x1]
         :                      __range_ok():
    0.00 :   ffff8000102d66f0:       mov     x1, x0
    0.00 :   ffff8000102d66f4:       tst     w3, #0x4000000
    0.00 :   ffff8000102d66f8:       b.eq    ffff8000102d6704 <lookup_ioctx+0x3c>  // b.none
         :                      sign_extend64():
         :                      * @index: 0 based bit index (0<=index<64) to sign bit
         :                      */
         :                      static inline __s64 sign_extend64(__u64 value, int index)
         :                      {
         :                      __u8 shift = 63 - index;
         :                      return (__s64)(value << shift) >> shift;
    0.00 :   ffff8000102d66fc:       sbfx    x1, x0, #0, #56
         :                      __range_ok():
         :                      addr = untagged_addr(addr);
    0.00 :   ffff8000102d6700:       and     x1, x0, x1
         :
         :                      __chk_user_ptr(addr);
         :                      asm volatile(
    0.07 :   ffff8000102d6704:       adds    x1, x1, #0x4
    0.00 :   ffff8000102d6708:       csel    x2, xzr, x2, hi  // hi = pmore
    0.00 :   ffff8000102d670c:       csinv   x1, x1, xzr, cc  // cc = lo, ul, last
    0.00 :   ffff8000102d6710:       sbcs    xzr, x1, x2
    0.57 :   ffff8000102d6714:       cset    x1, ls  // ls = plast
         :                      lookup_ioctx():
         :                      struct kioctx *ctx, *ret = NULL;
         :                      struct kioctx_table *table;
         :                      unsigned id;
         :
         :                      if (get_user(id, &ring->id))
    0.00 :   ffff8000102d6718:       cbz     x1, ffff8000102d67cc <lookup_ioctx+0x104>
    0.04 :   ffff8000102d671c:       str     x21, [x29, #32]
         :                      sign_extend64():
    0.00 :   ffff8000102d6720:       sbfx    x1, x0, #0, #56
         :                      get_current():
    2.81 :   ffff8000102d6724:       mrs     x21, sp_el0
         :                      __uaccess_mask_ptr():
         :                      asm volatile(
         :                      "       bics    xzr, %3, %2\n"
         :                      "       csel    %0, %1, xzr, eq\n"
         :                      : "=&r" (safe_ptr)
         :                      : "r" (ptr), "r" (current_thread_info()->addr_limit),
         :                      "r" (untagged_addr(ptr))
    0.00 :   ffff8000102d6728:       and     x1, x0, x1
         :                      asm volatile(
    0.00 :   ffff8000102d672c:       ldr     x3, [x21, #8]
    0.00 :   ffff8000102d6730:       bics    xzr, x1, x3
    0.00 :   ffff8000102d6734:       csel    x2, x0, xzr, eq  // eq = none
         :                      : "cc");
         :
         :                      csdb();
    0.00 :   ffff8000102d6738:       csdb
         :                      uaccess_enable_not_uao():
         :                      __uaccess_enable(ARM64_ALT_PAN_NOT_UAO);
    0.11 :   ffff8000102d673c:       nop
         :                      lookup_ioctx():
    1.42 :   ffff8000102d6740:       mov     w1, #0x0                        // #0
    0.00 :   ffff8000102d6744:       ldr     w20, [x2]
         :                      uaccess_disable_not_uao():
         :                      __uaccess_disable(ARM64_ALT_PAN_NOT_UAO);
    0.00 :   ffff8000102d6748:       nop
         :                      lookup_ioctx():
   24.59 :   ffff8000102d674c:       cbnz    w1, ffff8000102d67c8 <lookup_ioctx+0x100>
    0.00 :   ffff8000102d6750:       str     x19, [x29, #16]
    0.00 :   ffff8000102d6754:       mov     x19, x0
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff8000102d6758:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d675c:       ldr     x0, [x22, #784]
         :                      lookup_ioctx():
         :                      return NULL;
         :
         :                      rcu_read_lock();
         :                      table = rcu_dereference(mm->ioctx_table);
         :
         :                      if (!table || id >= table->nr)
    0.00 :   ffff8000102d6760:       cbz     x0, ffff8000102d67a4 <lookup_ioctx+0xdc>
    0.36 :   ffff8000102d6764:       ldr     w1, [x0, #16]
    0.00 :   ffff8000102d6768:       cmp     w1, w20
    0.00 :   ffff8000102d676c:       b.ls    ffff8000102d67a4 <lookup_ioctx+0xdc>  // b.plast
         :                      array_index_mask_nospec():
         :                      static inline unsigned long array_index_mask_nospec(unsigned long idx,
         :                      unsigned long sz)
         :                      {
         :                      unsigned long mask;
         :
         :                      asm volatile(
    4.01 :   ffff8000102d6770:       and     x2, x20, #0xffffffff
         :                      lookup_ioctx():
         :                      goto out;
         :
         :                      id = array_index_nospec(id, table->nr);
    0.00 :   ffff8000102d6774:       mov     w1, w1
         :                      array_index_mask_nospec():
    0.00 :   ffff8000102d6778:       cmp     x2, x1
    0.00 :   ffff8000102d677c:       ngc     x1, xzr
         :                      "       sbc     %0, xzr, xzr\n"
         :                      : "=r" (mask)
         :                      : "r" (idx), "Ir" (sz)
         :                      : "cc");
         :
         :                      csdb();
    1.07 :   ffff8000102d6780:       csdb
         :                      lookup_ioctx():
         :                      ctx = rcu_dereference(table->table[id]);
    0.00 :   ffff8000102d6784:       and     w20, w20, w1
    0.00 :   ffff8000102d6788:       add     x20, x20, #0x3
         :                      __read_once_size():
    0.00 :   ffff8000102d678c:       ldr     x22, [x0, x20, lsl #3]
         :                      lookup_ioctx():
         :                      if (ctx && ctx->user_id == ctx_id) {
    0.00 :   ffff8000102d6790:       cbz     x22, ffff8000102d67a4 <lookup_ioctx+0xdc>
    9.28 :   ffff8000102d6794:       ldr     x0, [x22, #120]
         :                      ctx = rcu_dereference(table->table[id]);
    0.00 :   ffff8000102d6798:       mov     x20, x22
         :                      if (ctx && ctx->user_id == ctx_id) {
    0.00 :   ffff8000102d679c:       cmp     x0, x19
    0.07 :   ffff8000102d67a0:       b.eq    ffff8000102d67e4 <lookup_ioctx+0x11c>  // b.none
         :                      struct kioctx *ctx, *ret = NULL;
    0.00 :   ffff8000102d67a4:       mov     x20, #0x0                       // #0
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.75 :   ffff8000102d67a8:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      lookup_ioctx():
         :                      if (percpu_ref_tryget_live(&ctx->users))
         :                      ret = ctx;
         :                      }
         :                      out:
         :                      rcu_read_unlock();
         :                      return ret;
    0.21 :   ffff8000102d67ac:       ldr     x19, [x29, #16]
         :                      }
    0.00 :   ffff8000102d67b0:       mov     x0, x20
         :                      return ret;
    0.71 :   ffff8000102d67b4:       ldr     x21, [x29, #32]
         :                      }
    0.28 :   ffff8000102d67b8:       ldr     x20, [sp, #24]
    0.18 :   ffff8000102d67bc:       ldr     x22, [sp, #40]
    0.53 :   ffff8000102d67c0:       ldp     x29, x30, [sp], #48
    0.11 :   ffff8000102d67c4:       ret
    0.00 :   ffff8000102d67c8:       ldr     x21, [x29, #32]
         :                      return NULL;
    0.00 :   ffff8000102d67cc:       mov     x20, #0x0                       // #0
         :                      }
    0.00 :   ffff8000102d67d0:       ldr     x22, [sp, #40]
    0.00 :   ffff8000102d67d4:       mov     x0, x20
    0.00 :   ffff8000102d67d8:       ldr     x20, [sp, #24]
    0.00 :   ffff8000102d67dc:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000102d67e0:       ret
         :                      rcu_read_lock():
         :                      __rcu_read_lock();
   14.41 :   ffff8000102d67e4:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
    0.25 :   ffff8000102d67e8:       ldr     x0, [x22, #8]
         :                      __ref_is_percpu():
         :                      * Theoretically, the following could test just ATOMIC; however,
         :                      * then we'd have to mask off DEAD separately as DEAD may be
         :                      * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
         :                      * implies ATOMIC anyway.  Test them together.
         :                      */
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff8000102d67ec:       tst     x0, #0x3
    0.00 :   ffff8000102d67f0:       b.ne    ffff8000102d6848 <lookup_ioctx+0x180>  // b.any
         :                      __read_once_size():
    3.94 :   ffff8000102d67f4:       ldr     w1, [x21, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff8000102d67f8:       add     w1, w1, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.18 :   ffff8000102d67fc:       str     w1, [x21, #16]
         :                      __percpu_add_case_64():
         :
         :                      PERCPU_RW_OPS(8)
         :                      PERCPU_RW_OPS(16)
         :                      PERCPU_RW_OPS(32)
         :                      PERCPU_RW_OPS(64)
         :                      PERCPU_OP(add, add, stadd)
    0.07 :   ffff8000102d6800:       mov     x2, #0x1                        // #1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000102d6804:       mrs     x1, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.04 :   ffff8000102d6808:       add     x0, x0, x1
    0.78 :   ffff8000102d680c:       ldxr    x4, [x0]
   16.66 :   ffff8000102d6810:       add     x4, x4, x2
    0.00 :   ffff8000102d6814:       stxr    w3, x4, [x0]
    0.04 :   ffff8000102d6818:       cbnz    w3, ffff8000102d680c <lookup_ioctx+0x144>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.36 :   ffff8000102d681c:       ldr     x0, [x21, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.11 :   ffff8000102d6820:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    1.17 :   ffff8000102d6824:       str     w0, [x21, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102d6828:       cbz     x0, ffff8000102d6834 <lookup_ioctx+0x16c>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.18 :   ffff8000102d682c:       ldr     x0, [x21, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102d6830:       cbnz    x0, ffff8000102d6840 <lookup_ioctx+0x178>
         :                      percpu_ref_tryget_live():
         :                      bool ret = false;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count)) {
         :                      this_cpu_inc(*percpu_count);
    0.00 :   ffff8000102d6834:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      rcu_read_unlock():
         :                      __rcu_read_unlock();
    0.00 :   ffff8000102d6838:       bl      ffff80001015eee0 <__rcu_read_unlock>
    0.00 :   ffff8000102d683c:       b       ffff8000102d67a8 <lookup_ioctx+0xe0>
    4.12 :   ffff8000102d6840:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      lookup_ioctx():
    0.99 :   ffff8000102d6844:       b       ffff8000102d67a8 <lookup_ioctx+0xe0>
         :                      percpu_ref_tryget_live():
         :                      ret = true;
         :                      } else if (!(ref->percpu_count_ptr & __PERCPU_REF_DEAD)) {
    0.00 :   ffff8000102d6848:       ldr     x0, [x22, #8]
    0.00 :   ffff8000102d684c:       tbz     w0, #1, ffff8000102d6858 <lookup_ioctx+0x190>
         :                      rcu_read_unlock():
    0.00 :   ffff8000102d6850:       bl      ffff80001015eee0 <__rcu_read_unlock>
    0.00 :   ffff8000102d6854:       b       ffff8000102d67a4 <lookup_ioctx+0xdc>
         :                      __read_once_size():
    0.00 :   ffff8000102d6858:       ldr     x3, [x22]
         :                      atomic64_fetch_add_unless():
         :                      atomic64_fetch_add_unless(atomic64_t *v, s64 a, s64 u)
         :                      {
         :                      s64 c = atomic64_read(v);
         :
         :                      do {
         :                      if (unlikely(c == u))
    0.00 :   ffff8000102d685c:       cbz     x3, ffff8000102d6850 <lookup_ioctx+0x188>
         :                      break;
         :                      } while (!atomic64_try_cmpxchg(v, &c, c + a));
    0.00 :   ffff8000102d6860:       add     x2, x3, #0x1
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000102d6864:       b       ffff8000102d6898 <lookup_ioctx+0x1d0>
    0.00 :   ffff8000102d6868:       b       ffff8000102d6898 <lookup_ioctx+0x1d0>
         :                      __lse__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,  l, "memory")
         :                      __CMPXCHG_CASE(x,  , rel_, 64,  l, "memory")
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, al, "memory")
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, al, "memory")
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, al, "memory")
         :                      __CMPXCHG_CASE(x,  ,  mb_, 64, al, "memory")
    0.00 :   ffff8000102d686c:       mov     x0, x22
    0.00 :   ffff8000102d6870:       mov     x1, x3
    0.00 :   ffff8000102d6874:       mov     x4, x1
    0.00 :   ffff8000102d6878:       casal   x4, x2, [x22]
    0.00 :   ffff8000102d687c:       mov     x0, x4
         :                      atomic64_try_cmpxchg():
         :                      if (unlikely(r != o))
    0.00 :   ffff8000102d6880:       cmp     x0, x3
    0.00 :   ffff8000102d6884:       b.eq    ffff8000102d6840 <lookup_ioctx+0x178>  // b.none
    0.00 :   ffff8000102d6888:       mov     x3, x0
         :                      atomic64_fetch_add_unless():
         :                      if (unlikely(c == u))
    0.00 :   ffff8000102d688c:       cbnz    x0, ffff8000102d6860 <lookup_ioctx+0x198>
         :                      rcu_read_unlock():
    0.00 :   ffff8000102d6890:       bl      ffff80001015eee0 <__rcu_read_unlock>
    0.00 :   ffff8000102d6894:       b       ffff8000102d67a4 <lookup_ioctx+0xdc>
         :                      __ll_sc__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  , rel_, 64,        ,  , l, "memory", L)
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  ,  mb_, 64, dmb ish,  , l, "memory", L)
    0.00 :   ffff8000102d6898:       b       ffff8000102da50c <__arm64_compat_sys_io_pgetevents_time64+0x24c>
    0.00 :   ffff8000102d689c:       b       ffff8000102d6880 <lookup_ioctx+0x1b8>
 Percent |	Source code & Disassembly of vmlinux for cycles (2460 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010252178 <kmem_cache_alloc>:
         :                      kmem_cache_alloc():
         :                      {
         :                      return slab_alloc_node(s, gfpflags, NUMA_NO_NODE, addr);
         :                      }
         :
         :                      void *kmem_cache_alloc(struct kmem_cache *s, gfp_t gfpflags)
         :                      {
    7.14 :   ffff800010252178:       stp     x29, x30, [sp, #-64]!
         :                      slab_pre_alloc_hook():
         :                      }
         :
         :                      static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,
         :                      gfp_t flags)
         :                      {
         :                      flags &= gfp_allowed_mask;
    0.00 :   ffff80001025217c:       adrp    x2, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      kmem_cache_alloc():
    0.00 :   ffff800010252180:       mov     x29, sp
    0.74 :   ffff800010252184:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010252188:       mov     x20, x0
         :                      slab_pre_alloc_hook():
    0.81 :   ffff80001025218c:       ldr     w19, [x2, #1040]
         :                      kmem_cache_alloc():
    0.45 :   ffff800010252190:       stp     x21, x22, [sp, #32]
    0.00 :   ffff800010252194:       mov     w21, w1
         :                      slab_pre_alloc_hook():
    0.00 :   ffff800010252198:       and     w19, w1, w19
    0.00 :   ffff80001025219c:       mov     x22, x30
         :                      fs_reclaim_acquire(flags);
         :                      fs_reclaim_release(flags);
         :
         :                      might_sleep_if(gfpflags_allow_blocking(flags));
         :
         :                      if (should_failslab(s, flags))
    1.05 :   ffff8000102521a0:       mov     w1, w19
    0.00 :   ffff8000102521a4:       bl      ffff8000101fd338 <should_failslab>
    0.00 :   ffff8000102521a8:       cbnz    w0, ffff800010252390 <kmem_cache_alloc+0x218>
    0.08 :   ffff8000102521ac:       stp     x23, x24, [x29, #48]
         :                      arch_static_branch():
         :                      #define JUMP_LABEL_NOP_SIZE             AARCH64_INSN_SIZE
         :
         :                      static __always_inline bool arch_static_branch(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000102521b0:       nop
         :                      slab_alloc_node():
         :                      if (!s)
   14.24 :   ffff8000102521b4:       cbz     x20, ffff800010252388 <kmem_cache_alloc+0x210>
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.65 :   ffff8000102521b8:       mrs     x1, sp_el0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.33 :   ffff8000102521bc:       ldr     w0, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.04 :   ffff8000102521c0:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.04 :   ffff8000102521c4:       str     w0, [x1, #16]
         :                      slab_alloc_node():
         :                      tid = this_cpu_read(s->cpu_slab->tid);
    0.53 :   ffff8000102521c8:       ldr     x0, [x20]
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    1.63 :   ffff8000102521cc:       mrs     x2, tpidr_el1
         :                      slab_alloc_node():
    0.37 :   ffff8000102521d0:       add     x0, x0, #0x8
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102521d4:       ldr     x23, [x0, x2]
   16.17 :   ffff8000102521d8:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000102521dc:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.44 :   ffff8000102521e0:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102521e4:       cbz     x0, ffff8000102522d0 <kmem_cache_alloc+0x158>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.16 :   ffff8000102521e8:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102521ec:       cbz     x0, ffff8000102522d0 <kmem_cache_alloc+0x158>
         :                      __my_cpu_offset():
    4.52 :   ffff8000102521f0:       mrs     x1, tpidr_el1
         :                      slab_alloc_node():
         :                      c = raw_cpu_ptr(s->cpu_slab);
    1.35 :   ffff8000102521f4:       ldr     x0, [x20]
    0.00 :   ffff8000102521f8:       add     x2, x0, x1
         :                      __read_once_size():
    0.00 :   ffff8000102521fc:       ldr     x5, [x2, #8]
         :                      slab_alloc_node():
         :                      unlikely(tid != READ_ONCE(c->tid)));
    0.16 :   ffff800010252200:       cmp     x5, x23
    0.00 :   ffff800010252204:       b.ne    ffff8000102521b8 <kmem_cache_alloc+0x40>  // b.any
         :                      object = c->freelist;
    0.90 :   ffff800010252208:       ldr     x19, [x0, x1]
         :                      if (unlikely(!object || !node_match(page, node))) {
    0.00 :   ffff80001025220c:       cbz     x19, ffff80001025236c <kmem_cache_alloc+0x1f4>
         :                      get_freepointer():
         :                      return freelist_dereference(s, object + s->offset);
    1.02 :   ffff800010252210:       ldr     w2, [x20, #32]
         :                      get_current():
    0.57 :   ffff800010252214:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.00 :   ffff800010252218:       ldr     w0, [x1, #16]
         :                      freelist_dereference():
         :                      return freelist_ptr(s, (void *)*(unsigned long *)(ptr_addr),
    0.00 :   ffff80001025221c:       ldr     x24, [x19, x2]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff800010252220:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
   10.00 :   ffff800010252224:       str     w0, [x1, #16]
         :                      __my_cpu_offset():
    0.00 :   ffff800010252228:       mrs     x0, tpidr_el1
         :                      slab_alloc_node():
         :                      if (unlikely(!this_cpu_cmpxchg_double(
    0.00 :   ffff80001025222c:       ldr     x4, [x20]
         :                      next_tid():
         :                      return tid + TID_STEP;
    0.00 :   ffff800010252230:       add     x3, x23, #0x100
         :                      slab_alloc_node():
         :                      if (unlikely(!this_cpu_cmpxchg_double(
    0.00 :   ffff800010252234:       add     x4, x4, x0
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010252238:       b       ffff8000102522e0 <kmem_cache_alloc+0x168>
    0.00 :   ffff80001025223c:       b       ffff8000102522e0 <kmem_cache_alloc+0x168>
         :                      __lse__cmpxchg_double():
         :                      : cl);                                                          \
         :                      \
         :                      return x0;                                                      \
         :                      }
         :
         :                      __CMPXCHG_DBL(   ,   )
    1.06 :   ffff800010252240:       mov     x0, x19
    0.00 :   ffff800010252244:       mov     x1, x5
    0.00 :   ffff800010252248:       mov     x2, x24
    0.00 :   ffff80001025224c:       casp    x0, x1, x2, x3, [x4]
   27.43 :   ffff800010252250:       eor     x0, x0, x19
    0.00 :   ffff800010252254:       eor     x1, x1, x5
    0.00 :   ffff800010252258:       orr     x0, x0, x1
    0.00 :   ffff80001025225c:       mov     x23, x0
         :                      get_current():
    0.00 :   ffff800010252260:       mrs     x0, sp_el0
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.08 :   ffff800010252264:       ldr     x1, [x0, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800010252268:       sub     x1, x1, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.04 :   ffff80001025226c:       str     w1, [x0, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800010252270:       cbz     x1, ffff8000102522d8 <kmem_cache_alloc+0x160>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    1.26 :   ffff800010252274:       ldr     x0, [x0, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff800010252278:       cbz     x0, ffff8000102522d8 <kmem_cache_alloc+0x160>
         :                      slab_alloc_node():
    0.04 :   ffff80001025227c:       cbnz    x23, ffff8000102521b8 <kmem_cache_alloc+0x40>
         :                      prefetch_freepointer():
         :                      prefetch(object + s->offset);
    0.00 :   ffff800010252280:       ldr     w0, [x20, #32]
         :                      prefetch():
         :                      * Prefetching support
         :                      */
         :                      #define ARCH_HAS_PREFETCH
         :                      static inline void prefetch(const void *ptr)
         :                      {
         :                      asm volatile("prfm pldl1keep, %a0\n" : : "p" (ptr));
    2.63 :   ffff800010252284:       prfm    pldl1keep, [x24, x0]
         :                      arch_static_branch():
         :                      asm_volatile_goto(
    0.00 :   ffff800010252288:       nop
    0.00 :   ffff80001025228c:       nop
         :                      slab_want_init_on_alloc():
         :                      return false;
         :                      if (c->flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON))
         :                      return flags & __GFP_ZERO;
         :                      return true;
         :                      }
         :                      return flags & __GFP_ZERO;
    0.00 :   ffff800010252290:       ubfx    x21, x21, #8, #1
         :                      slab_alloc_node():
         :                      if (unlikely(slab_want_init_on_alloc(gfpflags, s)) && object)
    0.00 :   ffff800010252294:       cbnz    w21, ffff800010252354 <kmem_cache_alloc+0x1dc>
         :                      arch_static_branch():
    0.00 :   ffff800010252298:       nop
    0.00 :   ffff80001025229c:       ldp     x23, x24, [x29, #48]
         :                      kmem_cache_alloc():
         :
         :                      trace_kmem_cache_alloc(_RET_IP_, ret, s->object_size,
         :                      s->size, gfpflags);
         :
         :                      return ret;
         :                      }
    0.00 :   ffff8000102522a0:       mov     x0, x19
    0.00 :   ffff8000102522a4:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000102522a8:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000102522ac:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000102522b0:       ret
         :                      slab_pre_alloc_hook():
         :                      if (memcg_kmem_enabled() &&
    1.96 :   ffff8000102522b4:       tbnz    w19, #22, ffff8000102522c0 <kmem_cache_alloc+0x148>
         :                      ((flags & __GFP_ACCOUNT) || (s->flags & SLAB_ACCOUNT)))
    0.00 :   ffff8000102522b8:       ldr     w0, [x20, #8]
    0.00 :   ffff8000102522bc:       tbz     w0, #26, ffff8000102521b4 <kmem_cache_alloc+0x3c>
         :                      return memcg_kmem_get_cache(s);
    0.00 :   ffff8000102522c0:       mov     x0, x20
    0.00 :   ffff8000102522c4:       bl      ffff80001026be88 <memcg_kmem_get_cache>
    0.00 :   ffff8000102522c8:       mov     x20, x0
    0.00 :   ffff8000102522cc:       b       ffff8000102521b4 <kmem_cache_alloc+0x3c>
         :                      slab_alloc_node():
         :                      tid = this_cpu_read(s->cpu_slab->tid);
    0.00 :   ffff8000102522d0:       bl      ffff800010cad668 <preempt_schedule_notrace>
    0.00 :   ffff8000102522d4:       b       ffff8000102521f0 <kmem_cache_alloc+0x78>
         :                      if (unlikely(!this_cpu_cmpxchg_double(
    0.00 :   ffff8000102522d8:       bl      ffff800010cad668 <preempt_schedule_notrace>
    0.00 :   ffff8000102522dc:       b       ffff80001025227c <kmem_cache_alloc+0x104>
         :                      __ll_sc__cmpxchg_double():
         :                      : cl);                                                          \
         :                      \
         :                      return ret;                                                     \
         :                      }
         :
         :                      __CMPXCHG_DBL(   ,        ,  ,         )
    0.00 :   ffff8000102522e0:       b       ffff8000102544e4 <slabinfo_write+0x3f4>
    0.00 :   ffff8000102522e4:       b       ffff800010252260 <kmem_cache_alloc+0xe8>
         :                      slab_want_init_on_alloc():
         :                      if (c->ctor)
    0.00 :   ffff8000102522e8:       ldr     x0, [x20, #64]
    0.00 :   ffff8000102522ec:       cbnz    x0, ffff800010252298 <kmem_cache_alloc+0x120>
         :                      if (c->flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON))
    0.00 :   ffff8000102522f0:       ldr     w0, [x20, #8]
    0.00 :   ffff8000102522f4:       and     w0, w0, #0xff800
    0.00 :   ffff8000102522f8:       and     w0, w0, #0xfff80fff
    0.00 :   ffff8000102522fc:       cbz     w0, ffff800010252354 <kmem_cache_alloc+0x1dc>
         :                      return flags & __GFP_ZERO;
    0.00 :   ffff800010252300:       ubfx    x21, x21, #8, #1
         :                      slab_alloc_node():
         :                      if (unlikely(slab_want_init_on_alloc(gfpflags, s)) && object)
    0.00 :   ffff800010252304:       cbz     w21, ffff800010252298 <kmem_cache_alloc+0x120>
    0.00 :   ffff800010252308:       b       ffff800010252354 <kmem_cache_alloc+0x1dc>
         :                      slab_post_alloc_hook():
         :                      memcg_kmem_put_cache(s);
    1.06 :   ffff80001025230c:       mov     x0, x20
    0.00 :   ffff800010252310:       bl      ffff80001026bfb0 <memcg_kmem_put_cache>
         :                      kmem_cache_alloc():
         :                      }
    0.00 :   ffff800010252314:       mov     x0, x19
         :                      return ret;
    1.06 :   ffff800010252318:       ldp     x23, x24, [x29, #48]
         :                      }
    0.00 :   ffff80001025231c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010252320:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010252324:       ldp     x29, x30, [sp], #64
    0.00 :   ffff800010252328:       ret
         :                      slab_want_init_on_free():
         :                      }
         :
         :                      static inline bool slab_want_init_on_free(struct kmem_cache *c)
         :                      {
         :                      if (static_branch_unlikely(&init_on_free))
         :                      return !(c->ctor ||
    0.00 :   ffff80001025232c:       ldr     x0, [x20, #64]
    0.00 :   ffff800010252330:       cbnz    x0, ffff80001025228c <kmem_cache_alloc+0x114>
         :                      (c->flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON)));
    0.00 :   ffff800010252334:       ldr     w0, [x20, #8]
    0.00 :   ffff800010252338:       and     w0, w0, #0xff800
    0.00 :   ffff80001025233c:       and     w0, w0, #0xfff80fff
         :                      return !(c->ctor ||
    0.00 :   ffff800010252340:       cbnz    w0, ffff80001025228c <kmem_cache_alloc+0x114>
         :                      maybe_wipe_obj_freeptr():
         :                      if (unlikely(slab_want_init_on_free(s)) && obj)
    0.00 :   ffff800010252344:       cbz     x19, ffff80001025228c <kmem_cache_alloc+0x114>
         :                      memset((void *)((char *)obj + s->offset), 0, sizeof(void *));
    0.00 :   ffff800010252348:       ldr     w0, [x20, #32]
    0.00 :   ffff80001025234c:       str     xzr, [x19, x0]
    0.00 :   ffff800010252350:       b       ffff80001025228c <kmem_cache_alloc+0x114>
         :                      slab_alloc_node():
         :                      if (unlikely(slab_want_init_on_alloc(gfpflags, s)) && object)
    0.00 :   ffff800010252354:       cbz     x19, ffff800010252298 <kmem_cache_alloc+0x120>
         :                      memset(object, 0, s->object_size);
    0.00 :   ffff800010252358:       ldr     w2, [x20, #28]
    0.00 :   ffff80001025235c:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010252360:       mov     x0, x19
    0.00 :   ffff800010252364:       bl      ffff800010c92840 <__memset>
    0.00 :   ffff800010252368:       b       ffff800010252298 <kmem_cache_alloc+0x120>
         :                      object = __slab_alloc(s, gfpflags, node, addr, c);
    0.00 :   ffff80001025236c:       mov     x3, x22
    0.00 :   ffff800010252370:       mov     w2, #0xffffffff                 // #-1
    0.00 :   ffff800010252374:       mov     w1, w21
    0.00 :   ffff800010252378:       mov     x0, x20
    0.00 :   ffff80001025237c:       bl      ffff800010251700 <__slab_alloc.isra.96>
    0.00 :   ffff800010252380:       mov     x19, x0
    0.00 :   ffff800010252384:       b       ffff800010252288 <kmem_cache_alloc+0x110>
    0.00 :   ffff800010252388:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff80001025238c:       nop
         :                      return NULL;
    0.00 :   ffff800010252390:       mov     x19, #0x0                       // #0
    0.00 :   ffff800010252394:       b       ffff8000102522a0 <kmem_cache_alloc+0x128>
 Percent |	Source code & Disassembly of vmlinux for cycles (1436 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010124310 <find_busiest_group>:
         :                      find_busiest_group():
         :                      * @env: The load balancing environment.
         :                      *
         :                      * Return:       - The busiest group if imbalance exists.
         :                      */
         :                      static struct sched_group *find_busiest_group(struct lb_env *env)
         :                      {
    0.00 :   ffff800010124310:       stp     x29, x30, [sp, #-400]!
         :                      init_sd_lb_stats():
         :                      *sds = (struct sd_lb_stats){
    0.00 :   ffff800010124314:       mov     w2, #0xffffffff                 // #-1
         :                      find_busiest_group():
         :                      {
    0.00 :   ffff800010124318:       mov     x29, sp
         :                      init_sd_lb_stats():
         :                      *sds = (struct sd_lb_stats){
    0.00 :   ffff80001012431c:       add     x1, x29, #0x200
         :                      find_busiest_group():
         :                      {
    0.00 :   ffff800010124320:       stp     x19, x20, [sp, #16]
    0.07 :   ffff800010124324:       stp     x21, x22, [sp, #32]
    0.00 :   ffff800010124328:       stp     x23, x24, [sp, #48]
    0.00 :   ffff80001012432c:       stp     x25, x26, [sp, #64]
    0.00 :   ffff800010124330:       stp     x27, x28, [sp, #80]
    0.00 :   ffff800010124334:       mov     x28, x0
    0.00 :   ffff800010124338:       adrp    x0, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff80001012433c:       add     x0, x0, #0x8c8
         :                      init_sd_lb_stats():
         :                      *sds = (struct sd_lb_stats){
    0.00 :   ffff800010124340:       stp     xzr, xzr, [x1, #-232]
         :                      find_busiest_group():
         :                      {
    0.00 :   ffff800010124344:       ldr     x1, [x0]
    0.00 :   ffff800010124348:       str     x1, [x29, #392]
    0.00 :   ffff80001012434c:       mov     x1, #0x0                        // #0
         :                      init_sd_lb_stats():
         :                      *sds = (struct sd_lb_stats){
    0.00 :   ffff800010124350:       add     x1, x29, #0x200
         :                      update_sd_lb_stats():
         :                      struct sched_domain *child = env->sd->child;
    0.07 :   ffff800010124354:       ldr     x0, [x28]
         :                      init_sd_lb_stats():
         :                      *sds = (struct sd_lb_stats){
    0.00 :   ffff800010124358:       stp     xzr, xzr, [x29, #200]
    0.00 :   ffff80001012435c:       stp     xzr, xzr, [x29, #216]
    0.00 :   ffff800010124360:       stp     xzr, xzr, [x1, #-248]
    0.00 :   ffff800010124364:       add     x1, x29, #0x220
    0.00 :   ffff800010124368:       stp     xzr, xzr, [x29, #232]
    0.00 :   ffff80001012436c:       stp     xzr, xzr, [x29, #248]
    0.00 :   ffff800010124370:       stp     xzr, xzr, [x1, #-248]
    0.00 :   ffff800010124374:       stp     xzr, xzr, [x1, #-232]
    0.00 :   ffff800010124378:       stp     xzr, xzr, [x1, #-216]
    0.00 :   ffff80001012437c:       stp     xzr, xzr, [x1, #-200]
    0.43 :   ffff800010124380:       stp     xzr, xzr, [x1, #-184]
    0.00 :   ffff800010124384:       stp     xzr, xzr, [x1, #-168]
    0.00 :   ffff800010124388:       str     w2, [x29, #288]
         :                      update_sd_lb_stats():
         :                      if (env->idle == CPU_NEWLY_IDLE && READ_ONCE(nohz.has_blocked))
    0.21 :   ffff80001012438c:       ldr     w1, [x28, #44]
    0.00 :   ffff800010124390:       cmp     w1, #0x2
         :                      struct sched_domain *child = env->sd->child;
    0.00 :   ffff800010124394:       ldr     x1, [x0, #8]
    0.00 :   ffff800010124398:       str     x1, [x29, #104]
         :                      struct sched_group *sg = env->sd->groups;
    0.06 :   ffff80001012439c:       ldr     x23, [x0, #16]
         :                      if (env->idle == CPU_NEWLY_IDLE && READ_ONCE(nohz.has_blocked))
    0.00 :   ffff8000101243a0:       b.eq    ffff800010124ad4 <find_busiest_group+0x7c4>  // b.none
         :                      update_sg_lb_stats():
         :                      struct rq *rq = cpu_rq(i);
    0.00 :   ffff8000101243a4:       adrp    x19, ffff8000114d5000 <tegra_to+0x180>
    0.00 :   ffff8000101243a8:       add     x19, x19, #0xd80
         :                      find_busiest_group():
         :                      {
    0.00 :   ffff8000101243ac:       mov     w27, #0x0                       // #0
         :                      update_sd_lb_stats():
         :                      local_group = cpumask_test_cpu(env->dst_cpu, sched_group_span(sg));
    0.15 :   ffff8000101243b0:       ldr     w1, [x28, #20]
         :                      cpumask_test_cpu():
         :                      *
         :                      * Returns 1 if @cpu is set in @cpumask, else returns 0
         :                      */
         :                      static inline int cpumask_test_cpu(int cpu, const struct cpumask *cpumask)
         :                      {
         :                      return test_bit(cpumask_check(cpu), cpumask_bits((cpumask)));
    0.00 :   ffff8000101243b4:       add     x22, x23, #0x20
         :                      update_sd_lb_stats():
         :                      struct sg_lb_stats *sgs = &tmp_sgs;
    0.00 :   ffff8000101243b8:       add     x26, x29, #0x80
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff8000101243bc:       add     w2, w1, #0x3f
    0.00 :   ffff8000101243c0:       cmp     w1, #0x0
    0.19 :   ffff8000101243c4:       csel    w2, w2, w1, lt  // lt = tstop
    0.00 :   ffff8000101243c8:       asr     w2, w2, #6
    0.00 :   ffff8000101243cc:       sxtw    x2, w2
    0.14 :   ffff8000101243d0:       ldr     x2, [x22, x2, lsl #3]
    0.06 :   ffff8000101243d4:       lsr     x2, x2, x1
         :                      update_sd_lb_stats():
         :                      if (local_group) {
    0.62 :   ffff8000101243d8:       and     w3, w2, #0x1
   12.42 :   ffff8000101243dc:       str     w3, [x29, #120]
    0.00 :   ffff8000101243e0:       tbz     w2, #0, ffff80001012441c <find_busiest_group+0x10c>
         :                      if (env->idle != CPU_NEWLY_IDLE ||
    0.00 :   ffff8000101243e4:       ldr     w2, [x28, #44]
         :                      sds->local = sg;
    0.07 :   ffff8000101243e8:       str     x23, [x29, #208]
         :                      if (env->idle != CPU_NEWLY_IDLE ||
    0.00 :   ffff8000101243ec:       cmp     w2, #0x2
    0.00 :   ffff8000101243f0:       b.ne    ffff800010124410 <find_busiest_group+0x100>  // b.any
         :                      time_after_eq(jiffies, sg->sgc->next_update))
    0.00 :   ffff8000101243f4:       adrp    x2, ffff800011897000 <bit_wait_table+0xe80>
    0.00 :   ffff8000101243f8:       ldr     x5, [x23, #16]
         :                      sgs = local;
    0.00 :   ffff8000101243fc:       add     x26, x29, #0x140
         :                      time_after_eq(jiffies, sg->sgc->next_update))
    0.00 :   ffff800010124400:       ldr     x2, [x2, #2432]
    0.00 :   ffff800010124404:       ldr     x5, [x5, #32]
    0.00 :   ffff800010124408:       sub     x2, x2, x5
    0.00 :   ffff80001012440c:       tbnz    x2, #63, ffff80001012441c <find_busiest_group+0x10c>
         :                      update_group_capacity(env->sd, env->dst_cpu);
    0.00 :   ffff800010124410:       bl      ffff8000101240e8 <update_group_capacity>
         :                      sgs = local;
    0.00 :   ffff800010124414:       add     x26, x29, #0x140
    0.00 :   ffff800010124418:       ldr     w1, [x28, #20]
         :                      test_bit():
    0.21 :   ffff80001012441c:       add     w0, w1, #0x3f
    0.00 :   ffff800010124420:       cmp     w1, #0x0
    0.00 :   ffff800010124424:       csel    w0, w0, w1, lt  // lt = tstop
         :                      update_sg_lb_stats():
         :                      memset(sgs, 0, sizeof(*sgs));
    1.46 :   ffff800010124428:       stp     xzr, xzr, [x26]
    0.00 :   ffff80001012442c:       stp     xzr, xzr, [x26, #16]
         :                      struct rq *rq = cpu_rq(i);
    0.00 :   ffff800010124430:       adrp    x20, ffff800011899000 <page_wait_table+0x1500>
         :                      test_bit():
    0.00 :   ffff800010124434:       asr     w0, w0, #6
         :                      update_sg_lb_stats():
         :                      memset(sgs, 0, sizeof(*sgs));
    0.04 :   ffff800010124438:       stp     xzr, xzr, [x26, #32]
    0.08 :   ffff80001012443c:       stp     xzr, xzr, [x26, #48]
         :                      for_each_cpu_and(i, sched_group_span(group), env->cpus) {
    0.00 :   ffff800010124440:       adrp    x2, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      test_bit():
    0.15 :   ffff800010124444:       sxtw    x0, w0
         :                      update_sg_lb_stats():
         :                      memset(sgs, 0, sizeof(*sgs));
    0.48 :   ffff800010124448:       str     xzr, [x26, #64]
         :                      for_each_cpu_and(i, sched_group_span(group), env->cpus) {
    0.00 :   ffff80001012444c:       add     x24, x2, #0x2b4
         :                      struct rq *rq = cpu_rq(i);
    0.00 :   ffff800010124450:       add     x20, x20, #0x8e8
         :                      for_each_cpu_and(i, sched_group_span(group), env->cpus) {
    0.00 :   ffff800010124454:       mov     w25, #0xffffffff                // #-1
         :                      test_bit():
    0.00 :   ffff800010124458:       ldr     x21, [x22, x0, lsl #3]
    0.00 :   ffff80001012445c:       lsr     x1, x21, x1
    0.00 :   ffff800010124460:       and     w0, w1, #0x1
    0.47 :   ffff800010124464:       str     w0, [x29, #124]
         :                      update_sg_lb_stats():
    0.86 :   ffff800010124468:       ldr     x2, [x28, #56]
    0.06 :   ffff80001012446c:       mov     w0, w25
    0.00 :   ffff800010124470:       mov     x1, x22
    0.00 :   ffff800010124474:       bl      ffff800010c93b70 <cpumask_next_and>
    0.48 :   ffff800010124478:       mov     w25, w0
    0.28 :   ffff80001012447c:       ldr     w0, [x24]
    0.00 :   ffff800010124480:       cmp     w25, w0
    0.00 :   ffff800010124484:       b.cs    ffff8000101245b0 <find_busiest_group+0x2a0>  // b.hs, b.nlast
         :                      struct rq *rq = cpu_rq(i);
    0.29 :   ffff800010124488:       sxtw    x8, w25
         :                      if ((env->flags & LBF_NOHZ_STATS) && update_nohz_stats(rq, false))
    2.56 :   ffff80001012448c:       ldr     w0, [x28, #64]
         :                      struct rq *rq = cpu_rq(i);
    0.00 :   ffff800010124490:       mov     x2, x19
    0.00 :   ffff800010124494:       ldr     x5, [x20, x8, lsl #3]
    0.08 :   ffff800010124498:       add     x21, x2, x5
         :                      if ((env->flags & LBF_NOHZ_STATS) && update_nohz_stats(rq, false))
    0.15 :   ffff80001012449c:       tbnz    w0, #4, ffff800010124794 <find_busiest_group+0x484>
         :                      sgs->group_load += cpu_load(rq);
    4.84 :   ffff8000101244a0:       ldr     x8, [x21, #288]
         :                      cpu_util():
         :                      cfs_rq = &cpu_rq(cpu)->cfs;
    0.26 :   ffff8000101244a4:       mov     x0, x19
         :                      update_sg_lb_stats():
         :                      sgs->group_load += cpu_load(rq);
   20.40 :   ffff8000101244a8:       ldr     x1, [x26, #8]
         :                      *sg_status |= SG_OVERLOAD;
    0.00 :   ffff8000101244ac:       orr     w12, w27, #0x1
         :                      cpu_util():
         :                      cfs_rq = &cpu_rq(cpu)->cfs;
    0.00 :   ffff8000101244b0:       add     x0, x0, x5
         :                      update_sg_lb_stats():
         :                      sgs->group_load += cpu_load(rq);
    0.00 :   ffff8000101244b4:       add     x1, x1, x8
    0.00 :   ffff8000101244b8:       str     x1, [x26, #8]
         :                      sgs->group_util += cpu_util(i);
    0.08 :   ffff8000101244bc:       ldr     x13, [x26, #24]
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.04 :   ffff8000101244c0:       ldr     x1, [x0, #304]
    1.61 :   ffff8000101244c4:       ldr     w14, [x0, #312]
         :                      cpu_util():
         :                      return min_t(unsigned long, util, capacity_orig_of(cpu));
    0.46 :   ffff8000101244c8:       ldr     x5, [x0, #2488]
         :                      util = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));
    0.08 :   ffff8000101244cc:       cmp     w14, w1
         :                      update_sg_lb_stats():
         :                      sgs->sum_h_nr_running += rq->cfs.h_nr_running;
   10.14 :   ffff8000101244d0:       ldr     w8, [x26, #36]
         :                      cpu_util():
         :                      return min_t(unsigned long, util, capacity_orig_of(cpu));
    0.00 :   ffff8000101244d4:       csel    w1, w1, w14, ls  // ls = plast
         :                      update_sg_lb_stats():
         :                      sgs->sum_nr_running += nr_running;
    0.13 :   ffff8000101244d8:       ldr     w9, [x26, #32]
         :                      cpu_util():
         :                      return min_t(unsigned long, util, capacity_orig_of(cpu));
    0.00 :   ffff8000101244dc:       cmp     x5, x1
    0.00 :   ffff8000101244e0:       csel    x1, x5, x1, ls  // ls = plast
         :                      update_sg_lb_stats():
         :                      sgs->group_util += cpu_util(i);
    0.00 :   ffff8000101244e4:       add     x1, x13, x1
    0.13 :   ffff8000101244e8:       str     x1, [x26, #24]
         :                      sgs->nr_preferred_running += rq->nr_preferred_running;
    0.13 :   ffff8000101244ec:       ldp     w11, w10, [x26, #64]
         :                      sgs->sum_h_nr_running += rq->cfs.h_nr_running;
    0.29 :   ffff8000101244f0:       ldr     w1, [x21, #156]
    0.22 :   ffff8000101244f4:       add     w1, w8, w1
    1.41 :   ffff8000101244f8:       str     w1, [x26, #36]
         :                      nr_running = rq->nr_running;
    0.07 :   ffff8000101244fc:       ldr     w8, [x21, #4]
         :                      sgs->sum_nr_running += nr_running;
    0.00 :   ffff800010124500:       add     w1, w9, w8
    2.98 :   ffff800010124504:       str     w1, [x26, #32]
         :                      *sg_status |= SG_OVERLOAD;
    0.06 :   ffff800010124508:       cmp     w8, #0x1
         :                      __read_once_size():
    0.33 :   ffff80001012450c:       ldr     x5, [x0, #304]
         :                      update_sg_lb_stats():
    0.13 :   ffff800010124510:       csel    w27, w12, w27, gt
         :                      __read_once_size():
    1.00 :   ffff800010124514:       ldr     w9, [x0, #312]
         :                      cpu_util():
         :                      return min_t(unsigned long, util, capacity_orig_of(cpu));
    0.15 :   ffff800010124518:       ldr     x1, [x0, #2488]
         :                      update_sg_lb_stats():
         :                      sgs->nr_numa_running += rq->nr_numa_running;
    0.34 :   ffff80001012451c:       ldr     w12, [x21, #8]
         :                      cpu_util():
         :                      util = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));
    0.00 :   ffff800010124520:       cmp     w9, w5
         :                      return min_t(unsigned long, util, capacity_orig_of(cpu));
    0.06 :   ffff800010124524:       csel    w5, w5, w9, ls  // ls = plast
         :                      cpu_overutilized():
         :                      return !fits_capacity(cpu_util(cpu), capacity_of(cpu));
    0.74 :   ffff800010124528:       ldr     x9, [x0, #2480]
         :                      cpu_util():
         :                      return min_t(unsigned long, util, capacity_orig_of(cpu));
    0.00 :   ffff80001012452c:       cmp     x1, x5
         :                      update_sg_lb_stats():
         :                      sgs->nr_numa_running += rq->nr_numa_running;
    0.08 :   ffff800010124530:       add     w0, w11, w12
    0.22 :   ffff800010124534:       str     w0, [x26, #64]
         :                      cpu_util():
         :                      return min_t(unsigned long, util, capacity_orig_of(cpu));
    0.00 :   ffff800010124538:       csel    x0, x1, x5, ls  // ls = plast
         :                      cpu_overutilized():
         :                      return !fits_capacity(cpu_util(cpu), capacity_of(cpu));
    0.00 :   ffff80001012453c:       lsl     x1, x9, #10
         :                      update_sg_lb_stats():
         :                      *sg_status |= SG_OVERUTILIZED;
    0.00 :   ffff800010124540:       orr     w9, w27, #0x2
         :                      sgs->nr_preferred_running += rq->nr_preferred_running;
    0.07 :   ffff800010124544:       ldr     w5, [x21, #12]
         :                      cpu_overutilized():
         :                      return !fits_capacity(cpu_util(cpu), capacity_of(cpu));
    0.00 :   ffff800010124548:       add     x0, x0, x0, lsl #2
         :                      update_sg_lb_stats():
         :                      sgs->nr_preferred_running += rq->nr_preferred_running;
    0.00 :   ffff80001012454c:       add     w5, w10, w5
    3.01 :   ffff800010124550:       str     w5, [x26, #68]
         :                      *sg_status |= SG_OVERUTILIZED;
    0.05 :   ffff800010124554:       cmp     x1, x0, lsl #8
    0.07 :   ffff800010124558:       csel    w27, w9, w27, ls  // ls = plast
         :                      if (!nr_running && idle_cpu(i)) {
    0.00 :   ffff80001012455c:       cbz     w8, ffff800010124778 <find_busiest_group+0x468>
         :                      if (local_group)
    0.08 :   ffff800010124560:       ldr     w0, [x29, #124]
    0.00 :   ffff800010124564:       cbnz    w0, ffff800010124468 <find_busiest_group+0x158>
         :                      if (env->sd->flags & SD_ASYM_CPUCAPACITY &&
    0.00 :   ffff800010124568:       ldr     x0, [x28]
    0.00 :   ffff80001012456c:       ldr     w0, [x0, #56]
    0.00 :   ffff800010124570:       tbz     w0, #6, ffff800010124468 <find_busiest_group+0x158>
         :                      sgs->group_misfit_task_load < rq->misfit_task_load) {
    0.00 :   ffff800010124574:       ldr     x0, [x21, #2512]
         :                      if (env->sd->flags & SD_ASYM_CPUCAPACITY &&
    0.00 :   ffff800010124578:       ldr     x1, [x26, #56]
    0.00 :   ffff80001012457c:       cmp     x1, x0
    0.00 :   ffff800010124580:       b.cs    ffff800010124468 <find_busiest_group+0x158>  // b.hs, b.nlast
         :                      for_each_cpu_and(i, sched_group_span(group), env->cpus) {
    0.00 :   ffff800010124584:       ldr     x2, [x28, #56]
    0.00 :   ffff800010124588:       mov     x1, x22
         :                      sgs->group_misfit_task_load = rq->misfit_task_load;
    0.00 :   ffff80001012458c:       str     x0, [x26, #56]
         :                      for_each_cpu_and(i, sched_group_span(group), env->cpus) {
    0.00 :   ffff800010124590:       mov     w0, w25
         :                      *sg_status |= SG_OVERLOAD;
    0.00 :   ffff800010124594:       orr     w27, w27, #0x1
         :                      for_each_cpu_and(i, sched_group_span(group), env->cpus) {
    0.00 :   ffff800010124598:       bl      ffff800010c93b70 <cpumask_next_and>
    0.00 :   ffff80001012459c:       mov     w25, w0
    0.00 :   ffff8000101245a0:       ldr     w0, [x24]
    0.00 :   ffff8000101245a4:       cmp     w25, w0
    0.00 :   ffff8000101245a8:       b.cc    ffff800010124488 <find_busiest_group+0x178>  // b.lo, b.ul, b.last
    0.00 :   ffff8000101245ac:       nop
         :                      if (env->sd->flags & SD_ASYM_PACKING &&
    0.14 :   ffff8000101245b0:       ldr     x0, [x28]
    0.00 :   ffff8000101245b4:       mov     x10, x0
    0.22 :   ffff8000101245b8:       ldr     w1, [x0, #56]
    0.14 :   ffff8000101245bc:       tbz     w1, #11, ffff8000101245d4 <find_busiest_group+0x2c4>
    0.00 :   ffff8000101245c0:       ldr     w1, [x28, #44]
    0.00 :   ffff8000101245c4:       cmp     w1, #0x1
    0.00 :   ffff8000101245c8:       b.eq    ffff8000101245d4 <find_busiest_group+0x2c4>  // b.none
         :                      env->idle != CPU_NOT_IDLE &&
    0.00 :   ffff8000101245cc:       ldr     w1, [x26, #36]
    0.00 :   ffff8000101245d0:       cbnz    w1, ffff800010124940 <find_busiest_group+0x630>
         :                      sgs->group_capacity = group->sgc->capacity;
    0.94 :   ffff8000101245d4:       ldr     x1, [x23, #16]
         :                      group_is_overloaded():
         :                      if (sgs->sum_nr_running <= sgs->group_weight)
    0.34 :   ffff8000101245d8:       ldr     w6, [x26, #32]
         :                      update_sg_lb_stats():
         :                      sgs->group_capacity = group->sgc->capacity;
    0.27 :   ffff8000101245dc:       ldr     x1, [x1, #8]
   14.00 :   ffff8000101245e0:       str     x1, [x26, #16]
         :                      sgs->group_weight = group->group_weight;
    0.14 :   ffff8000101245e4:       ldr     w2, [x23, #12]
    0.15 :   ffff8000101245e8:       str     w2, [x26, #44]
         :                      group_is_overloaded():
         :                      if (sgs->sum_nr_running <= sgs->group_weight)
    0.00 :   ffff8000101245ec:       cmp     w2, w6
         :                      update_sg_lb_stats():
         :                      sgs->group_type = group_classify(env->sd->imbalance_pct, group, sgs);
    0.06 :   ffff8000101245f0:       ldr     w8, [x0, #44]
         :                      group_is_overloaded():
         :                      if (sgs->sum_nr_running <= sgs->group_weight)
    0.00 :   ffff8000101245f4:       b.cs    ffff800010124614 <find_busiest_group+0x304>  // b.hs, b.nlast
         :                      (sgs->group_util * imbalance_pct))
    0.00 :   ffff8000101245f8:       ldr     x11, [x26, #24]
    0.00 :   ffff8000101245fc:       mov     w5, w8
         :                      if ((sgs->group_capacity * 100) <
    0.00 :   ffff800010124600:       add     x9, x1, x1, lsl #1
    0.00 :   ffff800010124604:       add     x9, x1, x9, lsl #3
         :                      (sgs->group_util * imbalance_pct))
    0.00 :   ffff800010124608:       mul     x5, x5, x11
         :                      if ((sgs->group_capacity * 100) <
    0.00 :   ffff80001012460c:       cmp     x5, x9, lsl #2
    0.00 :   ffff800010124610:       b.hi    ffff8000101247c4 <find_busiest_group+0x4b4>  // b.pmore
         :                      sg_imbalanced():
         :                      return group->sgc->imbalance;
    0.54 :   ffff800010124614:       ldr     x5, [x23, #16]
         :                      group_classify():
         :                      if (sg_imbalanced(group))
    0.13 :   ffff800010124618:       ldr     w5, [x5, #40]
    0.14 :   ffff80001012461c:       cbnz    w5, ffff800010124834 <find_busiest_group+0x524>
         :                      if (sgs->group_asym_packing)
    2.33 :   ffff800010124620:       ldr     w5, [x26, #52]
    0.00 :   ffff800010124624:       cbnz    w5, ffff800010124870 <find_busiest_group+0x560>
         :                      if (sgs->group_misfit_task_load)
    0.84 :   ffff800010124628:       ldr     x5, [x26, #56]
    0.07 :   ffff80001012462c:       cbnz    x5, ffff8000101248d4 <find_busiest_group+0x5c4>
         :                      group_has_capacity():
         :                      if (sgs->sum_nr_running < sgs->group_weight)
    0.13 :   ffff800010124630:       cmp     w2, w6
    0.00 :   ffff800010124634:       b.hi    ffff800010124654 <find_busiest_group+0x344>  // b.pmore
         :                      (sgs->group_util * imbalance_pct))
    0.00 :   ffff800010124638:       ldr     x5, [x26, #24]
    0.00 :   ffff80001012463c:       mov     w8, w8
         :                      if ((sgs->group_capacity * 100) >
    0.00 :   ffff800010124640:       add     x2, x1, x1, lsl #1
    0.00 :   ffff800010124644:       add     x2, x1, x2, lsl #3
         :                      (sgs->group_util * imbalance_pct))
    0.00 :   ffff800010124648:       mul     x1, x8, x5
         :                      if ((sgs->group_capacity * 100) >
    0.00 :   ffff80001012464c:       cmp     x1, x2, lsl #2
    0.00 :   ffff800010124650:       b.cs    ffff800010124a8c <find_busiest_group+0x77c>  // b.hs, b.nlast
         :                      update_sd_lb_stats():
         :                      if (local_group)
    0.37 :   ffff800010124654:       ldr     w1, [x29, #120]
         :                      update_sg_lb_stats():
         :                      sgs->group_type = group_classify(env->sd->imbalance_pct, group, sgs);
    0.08 :   ffff800010124658:       str     wzr, [x26, #48]
         :                      update_sd_lb_stats():
         :                      if (local_group)
    0.00 :   ffff80001012465c:       cbz     w1, ffff800010124998 <find_busiest_group+0x688>
    0.15 :   ffff800010124660:       ldr     x5, [x26, #8]
         :                      sds->total_capacity += sgs->group_capacity;
    0.15 :   ffff800010124664:       ldp     x2, x1, [x29, #216]
         :                      sds->total_load += sgs->group_load;
    0.24 :   ffff800010124668:       add     x2, x2, x5
    1.73 :   ffff80001012466c:       str     x2, [x29, #216]
         :                      sds->total_capacity += sgs->group_capacity;
    0.19 :   ffff800010124670:       ldr     x2, [x26, #16]
    0.00 :   ffff800010124674:       add     x1, x1, x2
    0.30 :   ffff800010124678:       str     x1, [x29, #224]
         :                      } while (sg != env->sd->groups);
    0.15 :   ffff80001012467c:       ldr     x1, [x0, #16]
         :                      sg = sg->next;
    0.38 :   ffff800010124680:       ldr     x23, [x23]
         :                      } while (sg != env->sd->groups);
    0.00 :   ffff800010124684:       cmp     x23, x1
    0.00 :   ffff800010124688:       b.ne    ffff8000101243b0 <find_busiest_group+0xa0>  // b.any
         :                      sds->prefer_sibling = child && child->flags & SD_PREFER_SIBLING;
    0.00 :   ffff80001012468c:       ldr     x2, [x29, #104]
    0.00 :   ffff800010124690:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010124694:       cbz     x2, ffff8000101246a0 <find_busiest_group+0x390>
    0.00 :   ffff800010124698:       ldr     w1, [x2, #56]
    0.00 :   ffff80001012469c:       ubfx    x1, x1, #12, #1
         :                      if ((env->flags & LBF_NOHZ_AGAIN) &&
    0.44 :   ffff8000101246a0:       ldr     w2, [x28, #64]
         :                      sds->prefer_sibling = child && child->flags & SD_PREFER_SIBLING;
    0.08 :   ffff8000101246a4:       str     w1, [x29, #240]
         :                      if ((env->flags & LBF_NOHZ_AGAIN) &&
    0.00 :   ffff8000101246a8:       tbnz    w2, #5, ffff800010124af4 <find_busiest_group+0x7e4>
         :                      if (env->sd->flags & SD_NUMA)
    0.00 :   ffff8000101246ac:       ldr     w0, [x10, #56]
    0.00 :   ffff8000101246b0:       tbz     w0, #14, ffff8000101246cc <find_busiest_group+0x3bc>
         :                      env->fbq_type = fbq_classify_group(&sds->busiest_stat);
    0.00 :   ffff8000101246b4:       ldr     w1, [x29, #284]
         :                      fbq_classify_group():
         :                      return regular;
    0.00 :   ffff8000101246b8:       mov     w0, #0x0                        // #0
         :                      if (sgs->sum_h_nr_running > sgs->nr_numa_running)
    0.00 :   ffff8000101246bc:       ldr     w2, [x29, #312]
    0.00 :   ffff8000101246c0:       cmp     w1, w2
    0.00 :   ffff8000101246c4:       b.ls    ffff800010124a78 <find_busiest_group+0x768>  // b.plast
         :                      update_sd_lb_stats():
         :                      env->fbq_type = fbq_classify_group(&sds->busiest_stat);
    0.00 :   ffff8000101246c8:       str     w0, [x28, #80]
         :                      if (!env->sd->parent) {
    0.00 :   ffff8000101246cc:       ldr     x1, [x10]
    0.00 :   ffff8000101246d0:       and     w0, w27, #0x2
    0.00 :   ffff8000101246d4:       cbz     x1, ffff800010124b6c <find_busiest_group+0x85c>
         :                      } else if (sg_status & SG_OVERUTILIZED) {
    0.12 :   ffff8000101246d8:       cbz     w0, ffff8000101246ec <find_busiest_group+0x3dc>
         :                      struct root_domain *rd = env->dst_rq->rd;
    0.00 :   ffff8000101246dc:       ldr     x0, [x28, #24]
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000101246e0:       mov     w1, #0x2                        // #2
         :                      update_sd_lb_stats():
    0.00 :   ffff8000101246e4:       ldr     x0, [x0, #2464]
         :                      __write_once_size():
    0.07 :   ffff8000101246e8:       str     w1, [x0, #92]
         :                      arch_static_branch():
         :                      #define JUMP_LABEL_NOP_SIZE             AARCH64_INSN_SIZE
         :
         :                      static __always_inline bool arch_static_branch(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000101246ec:       nop
         :                      find_busiest_group():
         :
         :                      local = &sds.local_stat;
         :                      busiest = &sds.busiest_stat;
         :
         :                      /* There is no busy sibling group to pull tasks from */
         :                      if (!sds.busiest)
    0.00 :   ffff8000101246f0:       ldr     x0, [x29, #200]
    0.00 :   ffff8000101246f4:       mov     x3, x0
    0.00 :   ffff8000101246f8:       cbz     x0, ffff80001012498c <find_busiest_group+0x67c>
         :                      goto out_balanced;
         :
         :                      /* Misfit tasks should be dealt with regardless of the avg load */
         :                      if (busiest->group_type == group_misfit_task)
    0.00 :   ffff8000101246fc:       ldr     w1, [x29, #296]
    0.00 :   ffff800010124700:       cmp     w1, #0x2
    0.00 :   ffff800010124704:       b.eq    ffff800010124b28 <find_busiest_group+0x818>  // b.none
         :                      /*
         :                      * If the busiest group is imbalanced the below checks don't
         :                      * work because they assume all things are equal, which typically
         :                      * isn't true due to cpus_ptr constraints and the like.
         :                      */
         :                      if (busiest->group_type == group_imbalanced)
    0.00 :   ffff800010124708:       sub     w2, w1, #0x3
    0.00 :   ffff80001012470c:       cmp     w2, #0x1
    0.00 :   ffff800010124710:       b.hi    ffff8000101249f8 <find_busiest_group+0x6e8>  // b.pmore
         :                      calculate_imbalance():
         :                      if (busiest->group_type == group_asym_packing) {
    0.00 :   ffff800010124714:       cmp     w1, #0x3
    0.00 :   ffff800010124718:       b.eq    ffff800010124c3c <find_busiest_group+0x92c>  // b.none
         :                      if (busiest->group_type == group_imbalanced) {
    0.00 :   ffff80001012471c:       cmp     w1, #0x4
    0.00 :   ffff800010124720:       b.eq    ffff800010124a64 <find_busiest_group+0x754>  // b.none
    0.00 :   ffff800010124724:       ldr     w2, [x29, #368]
         :                      if (local->group_type == group_has_spare) {
    0.00 :   ffff800010124728:       cbz     w2, ffff800010124ba0 <find_busiest_group+0x890>
         :                      if (local->group_type < group_overloaded) {
    0.00 :   ffff80001012472c:       cmp     w2, #0x4
    0.00 :   ffff800010124730:       ldr     x4, [x29, #336]
    0.00 :   ffff800010124734:       b.ls    ffff800010124c98 <find_busiest_group+0x988>  // b.plast
    0.00 :   ffff800010124738:       ldr     x2, [x29, #232]
    0.00 :   ffff80001012473c:       ldr     x1, [x29, #320]
         :                      env->imbalance = min(
    0.00 :   ffff800010124740:       ldr     x0, [x29, #248]
    0.00 :   ffff800010124744:       sub     x1, x2, x1
    0.00 :   ffff800010124748:       ldr     x5, [x29, #264]
    0.00 :   ffff80001012474c:       sub     x0, x0, x2
         :                      env->migration_type = migrate_load;
    0.00 :   ffff800010124750:       str     wzr, [x28, #84]
         :                      env->imbalance = min(
    0.00 :   ffff800010124754:       mul     x1, x1, x4
    0.00 :   ffff800010124758:       mul     x0, x0, x5
    0.00 :   ffff80001012475c:       cmp     x0, x1
    0.00 :   ffff800010124760:       csel    x0, x0, x1, ls  // ls = plast
         :                      ) / SCHED_CAPACITY_SCALE;
    0.00 :   ffff800010124764:       lsr     x0, x0, #10
         :                      env->imbalance = min(
    0.00 :   ffff800010124768:       str     x0, [x28, #48]
    0.00 :   ffff80001012476c:       cmp     x0, #0x0
    0.00 :   ffff800010124770:       csel    x0, x3, xzr, ne  // ne = any
         :                      find_busiest_group():
    0.00 :   ffff800010124774:       b       ffff800010124b38 <find_busiest_group+0x828>
         :                      update_sg_lb_stats():
         :                      if (!nr_running && idle_cpu(i)) {
    0.29 :   ffff800010124778:       mov     w0, w25
    0.00 :   ffff80001012477c:       bl      ffff800010117488 <idle_cpu>
    0.66 :   ffff800010124780:       cbz     w0, ffff800010124560 <find_busiest_group+0x250>
         :                      sgs->idle_cpus++;
    0.14 :   ffff800010124784:       ldr     w0, [x26, #40]
    0.07 :   ffff800010124788:       add     w0, w0, #0x1
    1.06 :   ffff80001012478c:       str     w0, [x26, #40]
    0.07 :   ffff800010124790:       b       ffff800010124468 <find_busiest_group+0x158>
    0.00 :   ffff800010124794:       str     x8, [x29, #112]
         :                      if ((env->flags & LBF_NOHZ_STATS) && update_nohz_stats(rq, false))
    0.00 :   ffff800010124798:       mov     w1, #0x0                        // #0
    0.00 :   ffff80001012479c:       mov     x0, x21
    0.00 :   ffff8000101247a0:       bl      ffff800010121940 <update_nohz_stats>
    0.00 :   ffff8000101247a4:       ldr     x8, [x29, #112]
    0.00 :   ffff8000101247a8:       tst     w0, #0xff
    0.00 :   ffff8000101247ac:       b.eq    ffff8000101247bc <find_busiest_group+0x4ac>  // b.none
         :                      env->flags |= LBF_NOHZ_AGAIN;
    0.00 :   ffff8000101247b0:       ldr     w0, [x28, #64]
    0.00 :   ffff8000101247b4:       orr     w0, w0, #0x20
    0.00 :   ffff8000101247b8:       str     w0, [x28, #64]
    0.00 :   ffff8000101247bc:       ldr     x5, [x20, x8, lsl #3]
    0.00 :   ffff8000101247c0:       b       ffff8000101244a0 <find_busiest_group+0x190>
         :                      sgs->avg_load = (sgs->group_load * SCHED_CAPACITY_SCALE) /
    0.00 :   ffff8000101247c4:       ldr     x5, [x26, #8]
         :                      sgs->group_type = group_classify(env->sd->imbalance_pct, group, sgs);
    0.00 :   ffff8000101247c8:       mov     w2, #0x5                        // #5
    0.00 :   ffff8000101247cc:       str     w2, [x26, #48]
         :                      sgs->avg_load = (sgs->group_load * SCHED_CAPACITY_SCALE) /
    0.00 :   ffff8000101247d0:       mov     x6, x5
    0.00 :   ffff8000101247d4:       lsl     x2, x5, #10
    0.00 :   ffff8000101247d8:       udiv    x1, x2, x1
         :                      update_sd_lb_stats():
         :                      if (local_group)
    0.00 :   ffff8000101247dc:       ldr     w2, [x29, #120]
         :                      update_sg_lb_stats():
         :                      sgs->avg_load = (sgs->group_load * SCHED_CAPACITY_SCALE) /
    0.00 :   ffff8000101247e0:       str     x1, [x26]
         :                      update_sd_lb_stats():
         :                      if (local_group)
    0.00 :   ffff8000101247e4:       cbnz    w2, ffff800010124664 <find_busiest_group+0x354>
         :                      update_sd_pick_busiest():
         :                      if (!sgs->sum_h_nr_running)
    0.00 :   ffff8000101247e8:       ldr     w2, [x26, #36]
    0.00 :   ffff8000101247ec:       cbz     w2, ffff800010124664 <find_busiest_group+0x354>
         :                      if (sgs->group_type > busiest->group_type)
    0.00 :   ffff8000101247f0:       ldr     w2, [x29, #296]
    0.00 :   ffff8000101247f4:       cmp     w2, #0x4
    0.00 :   ffff8000101247f8:       b.hi    ffff800010124b84 <find_busiest_group+0x874>  // b.pmore
         :                      update_sd_lb_stats():
         :                      sds->busiest = sg;
    0.00 :   ffff8000101247fc:       str     x23, [x29, #200]
         :                      sds->busiest_stat = *sgs;
    0.00 :   ffff800010124800:       add     x1, x29, #0x200
    0.00 :   ffff800010124804:       ldp     x8, x9, [x26]
    0.00 :   ffff800010124808:       stp     x8, x9, [x29, #248]
    0.00 :   ffff80001012480c:       ldp     x8, x9, [x26, #16]
    0.00 :   ffff800010124810:       stp     x8, x9, [x1, #-248]
    0.00 :   ffff800010124814:       ldp     x8, x9, [x26, #32]
    0.00 :   ffff800010124818:       stp     x8, x9, [x1, #-232]
    0.00 :   ffff80001012481c:       add     x1, x29, #0x220
    0.00 :   ffff800010124820:       ldp     x8, x9, [x26, #48]
    0.00 :   ffff800010124824:       stp     x8, x9, [x1, #-248]
    0.00 :   ffff800010124828:       ldr     x1, [x26, #64]
    0.00 :   ffff80001012482c:       str     x1, [x29, #312]
    0.00 :   ffff800010124830:       b       ffff800010124660 <find_busiest_group+0x350>
         :                      update_sg_lb_stats():
         :                      sgs->group_type = group_classify(env->sd->imbalance_pct, group, sgs);
    0.00 :   ffff800010124834:       mov     w1, #0x4                        // #4
    0.00 :   ffff800010124838:       str     w1, [x26, #48]
         :                      update_sd_lb_stats():
         :                      if (local_group)
    0.00 :   ffff80001012483c:       ldr     w1, [x29, #120]
    0.00 :   ffff800010124840:       cbnz    w1, ffff800010124660 <find_busiest_group+0x350>
         :                      update_sd_pick_busiest():
         :                      if (!sgs->sum_h_nr_running)
    0.00 :   ffff800010124844:       ldr     w1, [x26, #36]
    0.00 :   ffff800010124848:       cbz     w1, ffff800010124660 <find_busiest_group+0x350>
         :                      if (sgs->group_type > busiest->group_type)
    0.00 :   ffff80001012484c:       ldr     w1, [x29, #296]
    0.00 :   ffff800010124850:       cmp     w1, #0x3
    0.00 :   ffff800010124854:       b.ls    ffff8000101247fc <find_busiest_group+0x4ec>  // b.plast
         :                      if (sgs->group_type < busiest->group_type)
    0.00 :   ffff800010124858:       cmp     w1, #0x4
    0.00 :   ffff80001012485c:       b.eq    ffff8000101248c8 <find_busiest_group+0x5b8>  // b.none
    0.07 :   ffff800010124860:       ldr     x6, [x26, #8]
    0.00 :   ffff800010124864:       mov     x5, x6
    0.00 :   ffff800010124868:       mov     x10, x0
    0.00 :   ffff80001012486c:       b       ffff800010124664 <find_busiest_group+0x354>
         :                      update_sg_lb_stats():
         :                      sgs->group_type = group_classify(env->sd->imbalance_pct, group, sgs);
    0.00 :   ffff800010124870:       mov     w1, #0x3                        // #3
    0.00 :   ffff800010124874:       str     w1, [x26, #48]
         :                      update_sd_lb_stats():
         :                      if (local_group)
    0.00 :   ffff800010124878:       ldr     w1, [x29, #120]
    0.00 :   ffff80001012487c:       cbnz    w1, ffff800010124660 <find_busiest_group+0x350>
         :                      update_sd_pick_busiest():
         :                      if (!sgs->sum_h_nr_running)
    0.00 :   ffff800010124880:       ldr     w1, [x26, #36]
    0.00 :   ffff800010124884:       cbz     w1, ffff800010124660 <find_busiest_group+0x350>
         :                      if (sgs->group_type > busiest->group_type)
    0.00 :   ffff800010124888:       ldr     w1, [x29, #296]
    0.00 :   ffff80001012488c:       cmp     w1, #0x2
    0.00 :   ffff800010124890:       b.ls    ffff8000101247fc <find_busiest_group+0x4ec>  // b.plast
         :                      if (sgs->group_type < busiest->group_type)
    0.00 :   ffff800010124894:       cmp     w1, #0x3
    0.00 :   ffff800010124898:       b.ne    ffff800010124860 <find_busiest_group+0x550>  // b.any
         :                      if (sched_asym_prefer(sg->asym_prefer_cpu, sds->busiest->asym_prefer_cpu))
    0.00 :   ffff80001012489c:       ldr     x1, [x29, #200]
         :                      sched_asym_prefer():
         :                      return scale_load_down(se->runnable_weight);
         :                      }
         :
         :                      static inline bool sched_asym_prefer(int a, int b)
         :                      {
         :                      return arch_asym_cpu_priority(a) > arch_asym_cpu_priority(b);
    0.00 :   ffff8000101248a0:       ldr     w0, [x23, #24]
         :                      update_sd_pick_busiest():
    0.00 :   ffff8000101248a4:       ldr     w21, [x1, #24]
         :                      sched_asym_prefer():
    0.00 :   ffff8000101248a8:       bl      ffff800010122780 <arch_asym_cpu_priority>
    0.00 :   ffff8000101248ac:       mov     w20, w0
    0.00 :   ffff8000101248b0:       mov     w0, w21
    0.00 :   ffff8000101248b4:       bl      ffff800010122780 <arch_asym_cpu_priority>
         :                      update_sd_pick_busiest():
    0.00 :   ffff8000101248b8:       cmp     w20, w0
    0.00 :   ffff8000101248bc:       ldr     x0, [x28]
    0.00 :   ffff8000101248c0:       mov     x10, x0
    0.00 :   ffff8000101248c4:       b.le    ffff8000101249bc <find_busiest_group+0x6ac>
    0.00 :   ffff8000101248c8:       mov     x10, x0
    0.00 :   ffff8000101248cc:       ldr     x5, [x26, #8]
    0.00 :   ffff8000101248d0:       b       ffff800010124664 <find_busiest_group+0x354>
         :                      update_sg_lb_stats():
         :                      sgs->group_type = group_classify(env->sd->imbalance_pct, group, sgs);
    0.00 :   ffff8000101248d4:       mov     w1, #0x2                        // #2
    0.00 :   ffff8000101248d8:       str     w1, [x26, #48]
         :                      update_sd_lb_stats():
         :                      if (local_group)
    0.00 :   ffff8000101248dc:       ldr     w1, [x29, #120]
    0.00 :   ffff8000101248e0:       cbnz    w1, ffff800010124660 <find_busiest_group+0x350>
         :                      update_sd_pick_busiest():
         :                      if (!sgs->sum_h_nr_running)
    0.00 :   ffff8000101248e4:       ldr     w1, [x26, #36]
    0.00 :   ffff8000101248e8:       cbz     w1, ffff800010124660 <find_busiest_group+0x350>
         :                      (!group_smaller_max_cpu_capacity(sg, sds->local) ||
    0.00 :   ffff8000101248ec:       ldr     x2, [x29, #208]
         :                      group_smaller_max_cpu_capacity():
         :                      return fits_capacity(sg->sgc->max_capacity, ref->sgc->max_capacity);
    0.00 :   ffff8000101248f0:       ldr     x1, [x23, #16]
    0.00 :   ffff8000101248f4:       ldr     x2, [x2, #16]
    0.00 :   ffff8000101248f8:       ldr     x1, [x1, #24]
    0.00 :   ffff8000101248fc:       ldr     x2, [x2, #24]
    0.00 :   ffff800010124900:       add     x1, x1, x1, lsl #2
    0.00 :   ffff800010124904:       lsl     x2, x2, #10
         :                      update_sd_pick_busiest():
         :                      if (sgs->group_type == group_misfit_task &&
    0.00 :   ffff800010124908:       cmp     x2, x1, lsl #8
    0.00 :   ffff80001012490c:       b.ls    ffff800010124660 <find_busiest_group+0x350>  // b.plast
         :                      (!group_smaller_max_cpu_capacity(sg, sds->local) ||
    0.00 :   ffff800010124910:       ldr     w1, [x29, #368]
    0.00 :   ffff800010124914:       cbnz    w1, ffff800010124660 <find_busiest_group+0x350>
         :                      if (sgs->group_type > busiest->group_type)
    0.00 :   ffff800010124918:       ldr     w1, [x29, #296]
    0.00 :   ffff80001012491c:       cmp     w1, #0x1
    0.00 :   ffff800010124920:       b.ls    ffff8000101247fc <find_busiest_group+0x4ec>  // b.plast
         :                      if (sgs->group_type < busiest->group_type)
    0.00 :   ffff800010124924:       cmp     w1, #0x2
    0.00 :   ffff800010124928:       b.ne    ffff800010124860 <find_busiest_group+0x550>  // b.any
         :                      if (sgs->group_misfit_task_load < busiest->group_misfit_task_load)
    0.00 :   ffff80001012492c:       ldr     x1, [x29, #304]
    0.00 :   ffff800010124930:       mov     x10, x0
    0.00 :   ffff800010124934:       cmp     x5, x1
    0.00 :   ffff800010124938:       b.cs    ffff8000101247fc <find_busiest_group+0x4ec>  // b.hs, b.nlast
    0.00 :   ffff80001012493c:       b       ffff800010124660 <find_busiest_group+0x350>
         :                      sched_asym_prefer():
    0.00 :   ffff800010124940:       ldr     w0, [x28, #20]
         :                      update_sg_lb_stats():
         :                      sched_asym_prefer(env->dst_cpu, group->asym_prefer_cpu)) {
    0.00 :   ffff800010124944:       ldr     w21, [x23, #24]
         :                      sched_asym_prefer():
    0.00 :   ffff800010124948:       bl      ffff800010122780 <arch_asym_cpu_priority>
    0.00 :   ffff80001012494c:       mov     w20, w0
    0.00 :   ffff800010124950:       mov     w0, w21
    0.00 :   ffff800010124954:       bl      ffff800010122780 <arch_asym_cpu_priority>
         :                      update_sg_lb_stats():
         :                      sgs->sum_h_nr_running &&
    0.00 :   ffff800010124958:       cmp     w20, w0
    0.00 :   ffff80001012495c:       ldr     x0, [x28]
    0.00 :   ffff800010124960:       b.le    ffff80001012496c <find_busiest_group+0x65c>
         :                      sgs->group_asym_packing = 1;
    0.00 :   ffff800010124964:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010124968:       str     w1, [x26, #52]
    0.00 :   ffff80001012496c:       mov     x10, x0
    0.00 :   ffff800010124970:       b       ffff8000101245d4 <find_busiest_group+0x2c4>
         :                      find_busiest_group():
         :                      struct root_domain *rd = env->dst_rq->rd;
    0.00 :   ffff800010124974:       ldr     x0, [x28, #24]
    0.00 :   ffff800010124978:       ldr     x0, [x0, #2464]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001012497c:       ldr     x1, [x0, #4384]
         :                      find_busiest_group():
         :                      if (rcu_dereference(rd->pd) && !READ_ONCE(rd->overutilized))
    0.00 :   ffff800010124980:       cbz     x1, ffff8000101246f0 <find_busiest_group+0x3e0>
         :                      __read_once_size():
    0.00 :   ffff800010124984:       ldr     w0, [x0, #92]
         :                      find_busiest_group():
    0.00 :   ffff800010124988:       cbnz    w0, ffff8000101246f0 <find_busiest_group+0x3e0>
         :                      calculate_imbalance(env, &sds);
         :                      return env->imbalance ? sds.busiest : NULL;
         :
         :                      out_balanced:
         :                      env->imbalance = 0;
         :                      return NULL;
    0.00 :   ffff80001012498c:       mov     x0, #0x0                        // #0
         :                      env->imbalance = 0;
    0.00 :   ffff800010124990:       str     xzr, [x28, #48]
         :                      return NULL;
    0.00 :   ffff800010124994:       b       ffff800010124b38 <find_busiest_group+0x828>
         :                      update_sd_pick_busiest():
         :                      if (!sgs->sum_h_nr_running)
    0.40 :   ffff800010124998:       ldr     w1, [x26, #36]
    0.00 :   ffff80001012499c:       cbz     w1, ffff800010124660 <find_busiest_group+0x350>
         :                      if (sgs->group_type < busiest->group_type)
    0.00 :   ffff8000101249a0:       ldr     w1, [x29, #296]
    0.00 :   ffff8000101249a4:       cbnz    w1, ffff800010124860 <find_busiest_group+0x550>
         :                      if (sgs->idle_cpus >= busiest->idle_cpus)
    0.15 :   ffff8000101249a8:       ldr     w2, [x26, #40]
    0.00 :   ffff8000101249ac:       mov     x10, x0
    0.00 :   ffff8000101249b0:       ldr     w1, [x29, #288]
    0.00 :   ffff8000101249b4:       cmp     w2, w1
    0.00 :   ffff8000101249b8:       b.cs    ffff800010124660 <find_busiest_group+0x350>  // b.hs, b.nlast
         :                      if ((env->sd->flags & SD_ASYM_CPUCAPACITY) &&
    0.00 :   ffff8000101249bc:       ldr     w1, [x0, #56]
    0.00 :   ffff8000101249c0:       tbz     w1, #6, ffff8000101247fc <find_busiest_group+0x4ec>
    0.00 :   ffff8000101249c4:       ldr     w1, [x26, #48]
    0.00 :   ffff8000101249c8:       cmp     w1, #0x1
    0.00 :   ffff8000101249cc:       b.hi    ffff8000101247fc <find_busiest_group+0x4ec>  // b.pmore
         :                      (group_smaller_min_cpu_capacity(sds->local, sg)))
    0.00 :   ffff8000101249d0:       ldr     x1, [x29, #208]
         :                      group_smaller_min_cpu_capacity():
         :                      return fits_capacity(sg->sgc->min_capacity, ref->sgc->min_capacity);
    0.00 :   ffff8000101249d4:       ldr     x2, [x23, #16]
    0.00 :   ffff8000101249d8:       ldr     x1, [x1, #16]
    0.00 :   ffff8000101249dc:       ldr     x2, [x2, #16]
    0.00 :   ffff8000101249e0:       ldr     x1, [x1, #16]
    0.00 :   ffff8000101249e4:       lsl     x2, x2, #10
    0.00 :   ffff8000101249e8:       add     x1, x1, x1, lsl #2
         :                      update_sd_pick_busiest():
         :                      (sgs->group_type <= group_fully_busy) &&
    0.00 :   ffff8000101249ec:       cmp     x2, x1, lsl #8
    0.00 :   ffff8000101249f0:       b.ls    ffff8000101247fc <find_busiest_group+0x4ec>  // b.plast
    0.00 :   ffff8000101249f4:       b       ffff800010124660 <find_busiest_group+0x350>
         :                      find_busiest_group():
         :                      if (local->group_type > busiest->group_type)
    0.00 :   ffff8000101249f8:       ldr     w2, [x29, #368]
    0.00 :   ffff8000101249fc:       cmp     w1, w2
    0.00 :   ffff800010124a00:       b.cc    ffff80001012498c <find_busiest_group+0x67c>  // b.lo, b.ul, b.last
         :                      if (local->group_type == group_overloaded) {
    0.00 :   ffff800010124a04:       cmp     w2, #0x5
    0.00 :   ffff800010124a08:       b.eq    ffff800010124c50 <find_busiest_group+0x940>  // b.none
         :                      if (sds.prefer_sibling && local->group_type == group_has_spare &&
    0.00 :   ffff800010124a0c:       ldr     w4, [x29, #240]
    0.00 :   ffff800010124a10:       cmp     w4, #0x0
    0.00 :   ffff800010124a14:       ccmp    w2, #0x0, #0x0, ne  // ne = any
    0.00 :   ffff800010124a18:       b.eq    ffff800010124c24 <find_busiest_group+0x914>  // b.none
         :                      if (busiest->group_type != group_overloaded) {
    0.00 :   ffff800010124a1c:       cmp     w1, #0x5
    0.00 :   ffff800010124a20:       b.eq    ffff800010124bdc <find_busiest_group+0x8cc>  // b.none
         :                      if (env->idle == CPU_NOT_IDLE)
    0.00 :   ffff800010124a24:       ldr     w2, [x28, #44]
    0.00 :   ffff800010124a28:       cmp     w2, #0x1
    0.00 :   ffff800010124a2c:       b.eq    ffff80001012498c <find_busiest_group+0x67c>  // b.none
         :                      if (busiest->group_weight > 1 &&
    0.00 :   ffff800010124a30:       ldr     w2, [x29, #292]
    0.00 :   ffff800010124a34:       cmp     w2, #0x1
    0.00 :   ffff800010124a38:       b.ls    ffff800010124a50 <find_busiest_group+0x740>  // b.plast
         :                      local->idle_cpus <= (busiest->idle_cpus + 1))
    0.00 :   ffff800010124a3c:       ldr     w2, [x29, #288]
         :                      if (busiest->group_weight > 1 &&
    0.00 :   ffff800010124a40:       ldr     w4, [x29, #360]
         :                      local->idle_cpus <= (busiest->idle_cpus + 1))
    0.00 :   ffff800010124a44:       add     w2, w2, #0x1
         :                      if (busiest->group_weight > 1 &&
    0.00 :   ffff800010124a48:       cmp     w4, w2
    0.00 :   ffff800010124a4c:       b.ls    ffff80001012498c <find_busiest_group+0x67c>  // b.plast
         :                      if (busiest->sum_h_nr_running == 1)
    0.00 :   ffff800010124a50:       ldr     w2, [x29, #284]
    0.00 :   ffff800010124a54:       cmp     w2, #0x1
    0.00 :   ffff800010124a58:       b.eq    ffff80001012498c <find_busiest_group+0x67c>  // b.none
         :                      calculate_imbalance():
         :                      if (busiest->group_type == group_imbalanced) {
    0.00 :   ffff800010124a5c:       cmp     w1, #0x4
    0.00 :   ffff800010124a60:       b.ne    ffff800010124724 <find_busiest_group+0x414>  // b.any
         :                      env->migration_type = migrate_task;
    0.00 :   ffff800010124a64:       mov     w1, #0x2                        // #2
         :                      env->imbalance = 1;
    0.00 :   ffff800010124a68:       mov     x2, #0x1                        // #1
    0.00 :   ffff800010124a6c:       str     x2, [x28, #48]
         :                      env->migration_type = migrate_task;
    0.00 :   ffff800010124a70:       str     w1, [x28, #84]
    0.00 :   ffff800010124a74:       b       ffff800010124b38 <find_busiest_group+0x828>
         :                      fbq_classify_group():
         :                      if (sgs->sum_h_nr_running > sgs->nr_preferred_running)
    0.00 :   ffff800010124a78:       ldr     w0, [x29, #316]
         :                      return remote;
    0.00 :   ffff800010124a7c:       cmp     w1, w0
    0.00 :   ffff800010124a80:       cset    w0, ls  // ls = plast
    0.00 :   ffff800010124a84:       add     w0, w0, #0x1
    0.00 :   ffff800010124a88:       b       ffff8000101246c8 <find_busiest_group+0x3b8>
         :                      update_sg_lb_stats():
         :                      sgs->group_type = group_classify(env->sd->imbalance_pct, group, sgs);
    0.00 :   ffff800010124a8c:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010124a90:       str     w1, [x26, #48]
         :                      update_sd_lb_stats():
         :                      if (local_group)
    0.00 :   ffff800010124a94:       ldr     w1, [x29, #120]
    0.00 :   ffff800010124a98:       cbnz    w1, ffff800010124660 <find_busiest_group+0x350>
         :                      update_sd_pick_busiest():
         :                      if (!sgs->sum_h_nr_running)
    0.00 :   ffff800010124a9c:       ldr     w1, [x26, #36]
    0.00 :   ffff800010124aa0:       cbz     w1, ffff800010124660 <find_busiest_group+0x350>
         :                      if (sgs->group_type > busiest->group_type)
    0.00 :   ffff800010124aa4:       ldr     w1, [x29, #296]
    0.00 :   ffff800010124aa8:       cbz     w1, ffff8000101247fc <find_busiest_group+0x4ec>
         :                      if (sgs->group_type < busiest->group_type)
    0.00 :   ffff800010124aac:       cmp     w1, #0x1
    0.00 :   ffff800010124ab0:       b.hi    ffff800010124860 <find_busiest_group+0x550>  // b.pmore
         :                      if (sgs->avg_load <= busiest->avg_load)
    0.00 :   ffff800010124ab4:       ldr     x2, [x26]
    0.00 :   ffff800010124ab8:       mov     x10, x0
    0.00 :   ffff800010124abc:       ldr     x1, [x29, #248]
    0.00 :   ffff800010124ac0:       cmp     x2, x1
    0.00 :   ffff800010124ac4:       b.ls    ffff800010124660 <find_busiest_group+0x350>  // b.plast
         :                      if ((env->sd->flags & SD_ASYM_CPUCAPACITY) &&
    0.00 :   ffff800010124ac8:       ldr     w1, [x0, #56]
    0.00 :   ffff800010124acc:       tbnz    w1, #6, ffff8000101249d0 <find_busiest_group+0x6c0>
    0.00 :   ffff800010124ad0:       b       ffff8000101247fc <find_busiest_group+0x4ec>
         :                      __read_once_size():
    0.00 :   ffff800010124ad4:       adrp    x1, ffff800011a7f000 <ucounts_hashtable+0x12f0>
    0.00 :   ffff800010124ad8:       add     x1, x1, #0xfc0
    0.00 :   ffff800010124adc:       ldr     w1, [x1, #36]
         :                      update_sd_lb_stats():
         :                      if (env->idle == CPU_NEWLY_IDLE && READ_ONCE(nohz.has_blocked))
    0.00 :   ffff800010124ae0:       cbz     w1, ffff8000101243a4 <find_busiest_group+0x94>
         :                      env->flags |= LBF_NOHZ_STATS;
    0.00 :   ffff800010124ae4:       ldr     w1, [x28, #64]
    0.00 :   ffff800010124ae8:       orr     w1, w1, #0x10
    0.00 :   ffff800010124aec:       str     w1, [x28, #64]
    0.00 :   ffff800010124af0:       b       ffff8000101243a4 <find_busiest_group+0x94>
         :                      bitmap_subset():
         :                      const unsigned long *src2, unsigned int nbits)
         :                      {
         :                      if (small_const_nbits(nbits))
         :                      return ! ((*src1 & ~(*src2)) & BITMAP_LAST_WORD_MASK(nbits));
         :                      else
         :                      return __bitmap_subset(src1, src2, nbits);
    0.00 :   ffff800010124af4:       adrp    x19, ffff800011a7f000 <ucounts_hashtable+0x12f0>
    0.00 :   ffff800010124af8:       add     x19, x19, #0xfc0
    0.00 :   ffff800010124afc:       add     x1, x0, #0x88
    0.00 :   ffff800010124b00:       mov     w2, #0x100                      // #256
    0.00 :   ffff800010124b04:       mov     x0, x19
    0.00 :   ffff800010124b08:       bl      ffff80001047f718 <__bitmap_subset>
         :                      update_sd_lb_stats():
         :                      if ((env->flags & LBF_NOHZ_AGAIN) &&
    0.00 :   ffff800010124b0c:       cbz     w0, ffff800010124b20 <find_busiest_group+0x810>
         :                      WRITE_ONCE(nohz.next_blocked,
    0.00 :   ffff800010124b10:       adrp    x0, ffff800011897000 <bit_wait_table+0xe80>
    0.00 :   ffff800010124b14:       ldr     x0, [x0, #2432]
    0.00 :   ffff800010124b18:       add     x0, x0, #0x8
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff800010124b1c:       str     x0, [x19, #48]
    0.00 :   ffff800010124b20:       ldr     x10, [x28]
    0.00 :   ffff800010124b24:       b       ffff8000101246ac <find_busiest_group+0x39c>
         :                      calculate_imbalance():
         :                      env->migration_type = migrate_misfit;
    0.00 :   ffff800010124b28:       mov     w1, #0x3                        // #3
         :                      env->imbalance = 1;
    0.00 :   ffff800010124b2c:       mov     x2, #0x1                        // #1
    0.00 :   ffff800010124b30:       str     x2, [x28, #48]
         :                      env->migration_type = migrate_misfit;
    0.00 :   ffff800010124b34:       str     w1, [x28, #84]
         :                      find_busiest_group():
         :                      }
    0.00 :   ffff800010124b38:       adrp    x1, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010124b3c:       add     x27, x1, #0x8c8
    0.00 :   ffff800010124b40:       ldr     x2, [x29, #392]
    0.08 :   ffff800010124b44:       ldr     x1, [x27]
    0.00 :   ffff800010124b48:       eor     x1, x2, x1
    0.00 :   ffff800010124b4c:       cbnz    x1, ffff800010124ce4 <find_busiest_group+0x9d4>
    0.05 :   ffff800010124b50:       ldp     x19, x20, [sp, #16]
    0.07 :   ffff800010124b54:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010124b58:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff800010124b5c:       ldp     x25, x26, [sp, #64]
    0.00 :   ffff800010124b60:       ldp     x27, x28, [sp, #80]
    0.06 :   ffff800010124b64:       ldp     x29, x30, [sp], #400
    0.00 :   ffff800010124b68:       ret
         :                      update_sd_lb_stats():
         :                      struct root_domain *rd = env->dst_rq->rd;
    0.00 :   ffff800010124b6c:       ldr     x1, [x28, #24]
         :                      WRITE_ONCE(rd->overload, sg_status & SG_OVERLOAD);
    0.00 :   ffff800010124b70:       and     w4, w27, #0x1
         :                      struct root_domain *rd = env->dst_rq->rd;
    0.00 :   ffff800010124b74:       ldr     x1, [x1, #2464]
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010124b78:       str     w4, [x1, #88]
    0.00 :   ffff800010124b7c:       str     w0, [x1, #92]
    0.00 :   ffff800010124b80:       b       ffff8000101246ec <find_busiest_group+0x3dc>
         :                      update_sd_pick_busiest():
         :                      if (sgs->group_type < busiest->group_type)
    0.00 :   ffff800010124b84:       cmp     w2, #0x5
    0.00 :   ffff800010124b88:       b.ne    ffff800010124864 <find_busiest_group+0x554>  // b.any
         :                      if (sgs->avg_load <= busiest->avg_load)
    0.00 :   ffff800010124b8c:       ldr     x2, [x29, #248]
    0.00 :   ffff800010124b90:       mov     x10, x0
    0.00 :   ffff800010124b94:       cmp     x1, x2
    0.00 :   ffff800010124b98:       b.hi    ffff8000101247fc <find_busiest_group+0x4ec>  // b.pmore
    0.00 :   ffff800010124b9c:       b       ffff800010124664 <find_busiest_group+0x354>
         :                      calculate_imbalance():
         :                      if (busiest->group_type > group_fully_busy) {
    0.00 :   ffff800010124ba0:       cmp     w1, #0x1
    0.00 :   ffff800010124ba4:       b.hi    ffff800010124be0 <find_busiest_group+0x8d0>  // b.pmore
         :                      if (busiest->group_weight == 1 || sds->prefer_sibling) {
    0.00 :   ffff800010124ba8:       ldr     w0, [x29, #292]
    0.00 :   ffff800010124bac:       cmp     w0, #0x1
    0.00 :   ffff800010124bb0:       b.eq    ffff800010124cbc <find_busiest_group+0x9ac>  // b.none
    0.00 :   ffff800010124bb4:       ldr     w0, [x29, #240]
    0.00 :   ffff800010124bb8:       cbnz    w0, ffff800010124cbc <find_busiest_group+0x9ac>
         :                      env->imbalance = max_t(long, 0, (local->idle_cpus -
    0.00 :   ffff800010124bbc:       ldr     w1, [x29, #288]
         :                      env->migration_type = migrate_task;
    0.00 :   ffff800010124bc0:       mov     w2, #0x2                        // #2
         :                      env->imbalance = max_t(long, 0, (local->idle_cpus -
    0.00 :   ffff800010124bc4:       ldr     w0, [x29, #360]
         :                      env->migration_type = migrate_task;
    0.00 :   ffff800010124bc8:       str     w2, [x28, #84]
         :                      env->imbalance = max_t(long, 0, (local->idle_cpus -
    0.00 :   ffff800010124bcc:       sub     w0, w0, w1
    0.00 :   ffff800010124bd0:       lsr     w0, w0, #1
    0.00 :   ffff800010124bd4:       str     x0, [x28, #48]
    0.00 :   ffff800010124bd8:       b       ffff80001012476c <find_busiest_group+0x45c>
         :                      if (local->group_type == group_has_spare) {
    0.00 :   ffff800010124bdc:       cbnz    w2, ffff80001012472c <find_busiest_group+0x41c>
         :                      env->imbalance = max(local->group_capacity, local->group_util) -
    0.00 :   ffff800010124be0:       ldp     x0, x1, [x29, #336]
         :                      env->migration_type = migrate_util;
    0.00 :   ffff800010124be4:       mov     w4, #0x1                        // #1
         :                      if (env->idle != CPU_NOT_IDLE && env->imbalance == 0) {
    0.00 :   ffff800010124be8:       ldr     w2, [x28, #44]
         :                      env->imbalance = max(local->group_capacity, local->group_util) -
    0.00 :   ffff800010124bec:       cmp     x0, x1
         :                      env->migration_type = migrate_util;
    0.00 :   ffff800010124bf0:       str     w4, [x28, #84]
         :                      env->imbalance = max(local->group_capacity, local->group_util) -
    0.00 :   ffff800010124bf4:       csel    x0, x0, x1, cs  // cs = hs, nlast
    0.00 :   ffff800010124bf8:       sub     x0, x0, x1
    0.00 :   ffff800010124bfc:       str     x0, [x28, #48]
         :                      if (env->idle != CPU_NOT_IDLE && env->imbalance == 0) {
    0.00 :   ffff800010124c00:       cmp     x0, #0x0
    0.00 :   ffff800010124c04:       ccmp    w2, w4, #0x4, eq  // eq = none
    0.00 :   ffff800010124c08:       b.eq    ffff80001012476c <find_busiest_group+0x45c>  // b.none
         :                      env->imbalance = 1;
    0.00 :   ffff800010124c0c:       mov     x0, #0x1                        // #1
         :                      env->migration_type = migrate_task;
    0.00 :   ffff800010124c10:       mov     w1, #0x2                        // #2
         :                      env->imbalance = 1;
    0.00 :   ffff800010124c14:       str     x0, [x28, #48]
    0.00 :   ffff800010124c18:       mov     x0, x3
         :                      env->migration_type = migrate_task;
    0.00 :   ffff800010124c1c:       str     w1, [x28, #84]
    0.00 :   ffff800010124c20:       b       ffff800010124b38 <find_busiest_group+0x828>
         :                      find_busiest_group():
         :                      busiest->sum_nr_running > local->sum_nr_running + 1)
    0.00 :   ffff800010124c24:       ldr     w4, [x29, #352]
         :                      if (sds.prefer_sibling && local->group_type == group_has_spare &&
    0.00 :   ffff800010124c28:       ldr     w5, [x29, #280]
         :                      busiest->sum_nr_running > local->sum_nr_running + 1)
    0.00 :   ffff800010124c2c:       add     w4, w4, #0x1
         :                      if (sds.prefer_sibling && local->group_type == group_has_spare &&
    0.00 :   ffff800010124c30:       cmp     w5, w4
    0.00 :   ffff800010124c34:       b.hi    ffff800010124714 <find_busiest_group+0x404>  // b.pmore
    0.00 :   ffff800010124c38:       b       ffff800010124a1c <find_busiest_group+0x70c>
         :                      calculate_imbalance():
         :                      env->imbalance = busiest->sum_h_nr_running;
    0.00 :   ffff800010124c3c:       ldr     w0, [x29, #284]
         :                      env->migration_type = migrate_task;
    0.00 :   ffff800010124c40:       mov     w1, #0x2                        // #2
         :                      env->imbalance = busiest->sum_h_nr_running;
    0.00 :   ffff800010124c44:       str     x0, [x28, #48]
         :                      env->migration_type = migrate_task;
    0.00 :   ffff800010124c48:       str     w1, [x28, #84]
    0.00 :   ffff800010124c4c:       b       ffff80001012476c <find_busiest_group+0x45c>
         :                      find_busiest_group():
         :                      if (local->avg_load >= busiest->avg_load)
    0.00 :   ffff800010124c50:       ldr     x6, [x29, #248]
    0.00 :   ffff800010124c54:       ldr     x5, [x29, #320]
    0.00 :   ffff800010124c58:       cmp     x5, x6
    0.00 :   ffff800010124c5c:       b.cs    ffff80001012498c <find_busiest_group+0x67c>  // b.hs, b.nlast
         :                      sds.avg_load = (sds.total_load * SCHED_CAPACITY_SCALE) /
    0.00 :   ffff800010124c60:       ldp     x4, x8, [x29, #216]
    0.00 :   ffff800010124c64:       lsl     x4, x4, #10
    0.00 :   ffff800010124c68:       udiv    x4, x4, x8
    0.00 :   ffff800010124c6c:       str     x4, [x29, #232]
         :                      if (local->avg_load >= sds.avg_load)
    0.00 :   ffff800010124c70:       cmp     x5, x4
    0.00 :   ffff800010124c74:       b.cs    ffff80001012498c <find_busiest_group+0x67c>  // b.hs, b.nlast
         :                      env->sd->imbalance_pct * local->avg_load)
    0.00 :   ffff800010124c78:       ldr     x8, [x28]
         :                      if (100 * busiest->avg_load <=
    0.00 :   ffff800010124c7c:       add     x4, x6, x6, lsl #1
    0.00 :   ffff800010124c80:       add     x6, x6, x4, lsl #3
         :                      env->sd->imbalance_pct * local->avg_load)
    0.00 :   ffff800010124c84:       ldr     w4, [x8, #44]
    0.00 :   ffff800010124c88:       mul     x5, x4, x5
         :                      if (100 * busiest->avg_load <=
    0.00 :   ffff800010124c8c:       cmp     x5, x6, lsl #2
    0.00 :   ffff800010124c90:       b.cc    ffff800010124a1c <find_busiest_group+0x70c>  // b.lo, b.ul, b.last
         :                      out_balanced:
    0.00 :   ffff800010124c94:       b       ffff80001012498c <find_busiest_group+0x67c>
         :                      calculate_imbalance():
         :                      sds->avg_load = (sds->total_load * SCHED_CAPACITY_SCALE) /
    0.00 :   ffff800010124c98:       ldp     x0, x2, [x29, #216]
         :                      local->avg_load = (local->group_load * SCHED_CAPACITY_SCALE) /
    0.00 :   ffff800010124c9c:       ldr     x1, [x29, #328]
         :                      sds->avg_load = (sds->total_load * SCHED_CAPACITY_SCALE) /
    0.00 :   ffff800010124ca0:       lsl     x0, x0, #10
         :                      local->avg_load = (local->group_load * SCHED_CAPACITY_SCALE) /
    0.00 :   ffff800010124ca4:       lsl     x1, x1, #10
         :                      sds->avg_load = (sds->total_load * SCHED_CAPACITY_SCALE) /
    0.00 :   ffff800010124ca8:       udiv    x2, x0, x2
         :                      local->avg_load = (local->group_load * SCHED_CAPACITY_SCALE) /
    0.00 :   ffff800010124cac:       udiv    x1, x1, x4
         :                      sds->avg_load = (sds->total_load * SCHED_CAPACITY_SCALE) /
    0.00 :   ffff800010124cb0:       str     x2, [x29, #232]
         :                      local->avg_load = (local->group_load * SCHED_CAPACITY_SCALE) /
    0.00 :   ffff800010124cb4:       str     x1, [x29, #320]
    0.00 :   ffff800010124cb8:       b       ffff800010124740 <find_busiest_group+0x430>
         :                      unsigned int nr_diff = busiest->sum_nr_running;
    0.00 :   ffff800010124cbc:       ldr     w0, [x29, #280]
         :                      env->migration_type = migrate_task;
    0.00 :   ffff800010124cc0:       mov     w2, #0x2                        // #2
         :                      lsub_positive(&nr_diff, local->sum_nr_running);
    0.00 :   ffff800010124cc4:       ldr     w1, [x29, #352]
         :                      env->migration_type = migrate_task;
    0.00 :   ffff800010124cc8:       str     w2, [x28, #84]
         :                      lsub_positive(&nr_diff, local->sum_nr_running);
    0.00 :   ffff800010124ccc:       cmp     w1, w0
    0.00 :   ffff800010124cd0:       csel    w1, w1, w0, ls  // ls = plast
    0.00 :   ffff800010124cd4:       sub     w0, w0, w1
         :                      env->imbalance = nr_diff >> 1;
    0.00 :   ffff800010124cd8:       lsr     w0, w0, #1
    0.00 :   ffff800010124cdc:       str     x0, [x28, #48]
    0.00 :   ffff800010124ce0:       b       ffff80001012476c <find_busiest_group+0x45c>
         :                      find_busiest_group():
         :                      }
    0.00 :   ffff800010124ce4:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (1165 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001045e968 <blk_mq_end_request>:
         :                      blk_mq_end_request():
         :                      }
         :                      }
         :                      EXPORT_SYMBOL(__blk_mq_end_request);
         :
         :                      void blk_mq_end_request(struct request *rq, blk_status_t error)
         :                      {
    5.34 :   ffff80001045e968:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff80001045e96c:       mov     x29, sp
    1.81 :   ffff80001045e970:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001045e974:       and     w20, w1, #0xff
    0.00 :   ffff80001045e978:       mov     x19, x0
         :                      if (blk_update_request(rq, error, blk_rq_bytes(rq)))
    0.00 :   ffff80001045e97c:       mov     w1, w20
    3.08 :   ffff80001045e980:       ldr     w2, [x0, #40]
    0.00 :   ffff80001045e984:       bl      ffff800010455308 <blk_update_request>
   10.01 :   ffff80001045e988:       str     x21, [x29, #32]
    0.00 :   ffff80001045e98c:       tst     w0, #0xff
    0.00 :   ffff80001045e990:       b.ne    ffff80001045ea9c <blk_mq_end_request+0x134>  // b.any
         :                      __blk_mq_end_request():
         :                      if (blk_mq_need_time_stamp(rq))
    3.26 :   ffff80001045e994:       ldr     w1, [x19, #28]
         :                      blk_mq_need_time_stamp():
         :                      return (rq->rq_flags & (RQF_IO_STAT | RQF_STATS)) || rq->q->elevator;
    0.00 :   ffff80001045e998:       and     w0, w1, #0x3e000
    0.00 :   ffff80001045e99c:       and     w0, w0, #0xfffe3fff
    0.00 :   ffff80001045e9a0:       cbz     w0, ffff80001045ea2c <blk_mq_end_request+0xc4>
         :                      ktime_get_ns():
         :                      return ktime_mono_to_any(mono, TK_OFFS_REAL);
         :                      }
         :
         :                      static inline u64 ktime_get_ns(void)
         :                      {
         :                      return ktime_to_ns(ktime_get());
   22.00 :   ffff80001045e9a4:       bl      ffff80001016ad10 <ktime_get>
    0.00 :   ffff80001045e9a8:       mov     x21, x0
    0.00 :   ffff80001045e9ac:       ldr     w1, [x19, #28]
         :                      __blk_mq_end_request():
         :                      if (rq->rq_flags & RQF_STATS) {
    0.00 :   ffff80001045e9b0:       tbnz    w1, #17, ffff80001045ea40 <blk_mq_end_request+0xd8>
         :                      if (rq->internal_tag != -1)
    0.69 :   ffff80001045e9b4:       ldr     w0, [x19, #36]
    0.00 :   ffff80001045e9b8:       cmn     w0, #0x1
    0.00 :   ffff80001045e9bc:       b.eq    ffff80001045e9e4 <blk_mq_end_request+0x7c>  // b.none
         :                      blk_mq_sched_completed_request():
         :                      return true;
         :                      }
         :
         :                      static inline void blk_mq_sched_completed_request(struct request *rq, u64 now)
         :                      {
         :                      struct elevator_queue *e = rq->q->elevator;
    0.00 :   ffff80001045e9c0:       ldr     x0, [x19]
    0.00 :   ffff80001045e9c4:       ldr     x0, [x0, #8]
         :
         :                      if (e && e->type->ops.completed_request)
    0.00 :   ffff80001045e9c8:       cbz     x0, ffff80001045e9e4 <blk_mq_end_request+0x7c>
    0.00 :   ffff80001045e9cc:       ldr     x0, [x0]
    0.00 :   ffff80001045e9d0:       ldr     x2, [x0, #136]
    0.00 :   ffff80001045e9d4:       cbz     x2, ffff80001045e9e4 <blk_mq_end_request+0x7c>
         :                      e->type->ops.completed_request(rq, now);
    0.00 :   ffff80001045e9d8:       mov     x1, x21
    0.00 :   ffff80001045e9dc:       mov     x0, x19
    0.00 :   ffff80001045e9e0:       blr     x2
         :                      __blk_mq_end_request():
         :                      blk_account_io_done(rq, now);
   12.78 :   ffff80001045e9e4:       mov     x1, x21
    0.00 :   ffff80001045e9e8:       mov     x0, x19
    0.00 :   ffff80001045e9ec:       bl      ffff8000104556a8 <blk_account_io_done>
         :                      if (rq->end_io) {
    0.60 :   ffff80001045e9f0:       ldr     x2, [x19, #264]
    0.00 :   ffff80001045e9f4:       cbz     x2, ffff80001045ea6c <blk_mq_end_request+0x104>
         :                      rq_qos_done(rq->q, rq);
    0.00 :   ffff80001045e9f8:       ldr     x0, [x19]
    0.00 :   ffff80001045e9fc:       ldr     x0, [x0, #24]
         :                      rq_qos_done():
         :                      __rq_qos_cleanup(q->rq_qos, bio);
         :                      }
         :
         :                      static inline void rq_qos_done(struct request_queue *q, struct request *rq)
         :                      {
         :                      if (q->rq_qos)
    0.00 :   ffff80001045ea00:       cbz     x0, ffff80001045ea10 <blk_mq_end_request+0xa8>
         :                      __rq_qos_done(q->rq_qos, rq);
    0.00 :   ffff80001045ea04:       mov     x1, x19
    0.00 :   ffff80001045ea08:       bl      ffff80001046f040 <__rq_qos_done>
    0.00 :   ffff80001045ea0c:       ldr     x2, [x19, #264]
         :                      __blk_mq_end_request():
         :                      rq->end_io(rq, error);
    0.00 :   ffff80001045ea10:       mov     w1, w20
    0.00 :   ffff80001045ea14:       mov     x0, x19
    0.00 :   ffff80001045ea18:       blr     x2
    0.00 :   ffff80001045ea1c:       ldr     x21, [x29, #32]
         :                      blk_mq_end_request():
         :                      BUG();
         :                      __blk_mq_end_request(rq, error);
         :                      }
    0.00 :   ffff80001045ea20:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001045ea24:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001045ea28:       ret
         :                      blk_mq_need_time_stamp():
         :                      return (rq->rq_flags & (RQF_IO_STAT | RQF_STATS)) || rq->q->elevator;
    0.00 :   ffff80001045ea2c:       ldr     x0, [x19]
    0.00 :   ffff80001045ea30:       ldr     x0, [x0, #8]
    0.00 :   ffff80001045ea34:       cbnz    x0, ffff80001045e9a4 <blk_mq_end_request+0x3c>
         :                      __blk_mq_end_request():
         :                      u64 now = 0;
    0.00 :   ffff80001045ea38:       mov     x21, #0x0                       // #0
         :                      if (rq->rq_flags & RQF_STATS) {
    0.00 :   ffff80001045ea3c:       tbz     w1, #17, ffff80001045e9b4 <blk_mq_end_request+0x4c>
         :                      blk_mq_poll_stats_start(rq->q);
    0.00 :   ffff80001045ea40:       ldr     x0, [x19]
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff80001045ea44:       ldr     x1, [x0, #104]
         :                      blk_mq_poll_stats_start():
         :                      {
         :                      /*
         :                      * We don't arm the callback if polling stats are not enabled or the
         :                      * callback is already active.
         :                      */
         :                      if (!test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
    0.00 :   ffff80001045ea48:       tst     w1, #0x200000
    0.00 :   ffff80001045ea4c:       b.eq    ffff80001045ea5c <blk_mq_end_request+0xf4>  // b.none
         :                      blk_stat_is_active(q->poll_cb))
    0.00 :   ffff80001045ea50:       ldr     x0, [x0, #272]
         :                      if (!test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
    0.00 :   ffff80001045ea54:       ldr     x1, [x0, #24]
    0.00 :   ffff80001045ea58:       cbz     x1, ffff80001045ea84 <blk_mq_end_request+0x11c>
         :                      __blk_mq_end_request():
         :                      blk_stat_add(rq, now);
    0.00 :   ffff80001045ea5c:       mov     x1, x21
    0.00 :   ffff80001045ea60:       mov     x0, x19
    0.00 :   ffff80001045ea64:       bl      ffff800010464810 <blk_stat_add>
    0.00 :   ffff80001045ea68:       b       ffff80001045e9b4 <blk_mq_end_request+0x4c>
         :                      blk_mq_free_request(rq);
    1.53 :   ffff80001045ea6c:       mov     x0, x19
    0.00 :   ffff80001045ea70:       bl      ffff80001045e6c8 <blk_mq_free_request>
    6.42 :   ffff80001045ea74:       ldr     x21, [x29, #32]
         :                      blk_mq_end_request():
         :                      }
   31.53 :   ffff80001045ea78:       ldp     x19, x20, [sp, #16]
    0.86 :   ffff80001045ea7c:       ldp     x29, x30, [sp], #48
    0.09 :   ffff80001045ea80:       ret
         :                      blk_stat_activate_msecs():
         :                      * The timer callback will be called when the window expires.
         :                      */
         :                      static inline void blk_stat_activate_msecs(struct blk_stat_callback *cb,
         :                      unsigned int msecs)
         :                      {
         :                      mod_timer(&cb->timer, jiffies + msecs_to_jiffies(msecs));
    0.00 :   ffff80001045ea84:       adrp    x1, ffff800011897000 <bit_wait_table+0xe80>
    0.00 :   ffff80001045ea88:       add     x0, x0, #0x10
    0.00 :   ffff80001045ea8c:       ldr     x1, [x1, #2432]
    0.00 :   ffff80001045ea90:       add     x1, x1, #0x19
    0.00 :   ffff80001045ea94:       bl      ffff800010167fe0 <mod_timer>
    0.00 :   ffff80001045ea98:       b       ffff80001045ea5c <blk_mq_end_request+0xf4>
         :                      blk_mq_end_request():
         :                      BUG();
    0.00 :   ffff80001045ea9c:       brk     #0x800
 Percent |	Source code & Disassembly of vmlinux for cycles (2011 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102d6130 <aio_read_events>:
         :                      aio_read_events():
         :                      return ret;
         :                      }
         :
         :                      static bool aio_read_events(struct kioctx *ctx, long min_nr, long nr,
         :                      struct io_event __user *event, long *i)
         :                      {
    0.00 :   ffff8000102d6130:       stp     x29, x30, [sp, #-128]!
    0.00 :   ffff8000102d6134:       mov     x29, sp
    1.89 :   ffff8000102d6138:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000102d613c:       mov     x20, x4
    0.50 :   ffff8000102d6140:       stp     x21, x22, [sp, #32]
         :                      aio_read_events_ring():
         :                      mutex_lock(&ctx->ring_lock);
    0.00 :   ffff8000102d6144:       add     x22, x0, #0x180
         :                      aio_read_events():
         :                      {
    0.90 :   ffff8000102d6148:       stp     x23, x24, [sp, #48]
    0.00 :   ffff8000102d614c:       mov     x21, x3
    0.65 :   ffff8000102d6150:       str     x26, [sp, #72]
    0.00 :   ffff8000102d6154:       mov     x23, x2
    0.40 :   ffff8000102d6158:       str     x28, [sp, #88]
    0.00 :   ffff8000102d615c:       mov     x28, x0
    0.45 :   ffff8000102d6160:       str     x1, [x29, #104]
         :                      aio_read_events_ring():
         :                      mutex_lock(&ctx->ring_lock);
    0.00 :   ffff8000102d6164:       mov     x0, x22
         :                      aio_read_events():
         :                      long ret = aio_read_events_ring(ctx, event + *i, nr - *i);
    0.54 :   ffff8000102d6168:       ldr     x19, [x4]
         :                      aio_read_events_ring():
         :                      mutex_lock(&ctx->ring_lock);
    0.00 :   ffff8000102d616c:       bl      ffff800010caf310 <mutex_lock>
         :                      ring = kmap_atomic(ctx->ring_pages[0]);
    3.03 :   ffff8000102d6170:       ldr     x1, [x28, #168]
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.45 :   ffff8000102d6174:       mrs     x0, sp_el0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d6178:       ldr     w2, [x0, #16]
         :                      aio_read_events_ring():
    1.30 :   ffff8000102d617c:       ldr     x1, [x1]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    1.10 :   ffff8000102d6180:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    2.15 :   ffff8000102d6184:       str     w2, [x0, #16]
         :                      pagefault_disabled_inc():
         :                      }
         :                      #endif
         :
         :                      static __always_inline void pagefault_disabled_inc(void)
         :                      {
         :                      current->pagefault_disabled++;
    0.00 :   ffff8000102d6188:       ldr     w2, [x0, #2448]
    0.00 :   ffff8000102d618c:       add     w2, w2, #0x1
    0.59 :   ffff8000102d6190:       str     w2, [x0, #2448]
         :                      lowmem_page_address():
         :                      */
         :                      #include <linux/vmstat.h>
         :
         :                      static __always_inline void *lowmem_page_address(const struct page *page)
         :                      {
         :                      return page_to_virt(page);
    0.00 :   ffff8000102d6194:       mov     x2, #0x200000                   // #2097152
    0.00 :   ffff8000102d6198:       movk    x2, #0x200, lsl #32
    0.00 :   ffff8000102d619c:       add     x1, x1, x2
    0.25 :   ffff8000102d61a0:       mov     x2, #0xffff000000000000         // #-281474976710656
    0.00 :   ffff8000102d61a4:       lsr     x1, x1, #6
    0.00 :   ffff8000102d61a8:       add     x1, x2, x1, lsl #12
         :                      aio_read_events_ring():
         :                      tail = ring->tail;
    0.35 :   ffff8000102d61ac:       ldp     w26, w24, [x1, #8]
         :                      pagefault_disabled_dec():
         :                      }
         :
         :                      static __always_inline void pagefault_disabled_dec(void)
         :                      {
         :                      current->pagefault_disabled--;
   29.08 :   ffff8000102d61b0:       ldr     w1, [x0, #2448]
    0.00 :   ffff8000102d61b4:       sub     w1, w1, #0x1
    0.75 :   ffff8000102d61b8:       str     w1, [x0, #2448]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.25 :   ffff8000102d61bc:       ldr     x1, [x0, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000102d61c0:       sub     x1, x1, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.85 :   ffff8000102d61c4:       str     w1, [x0, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102d61c8:       cbz     x1, ffff8000102d6438 <aio_read_events+0x308>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    1.39 :   ffff8000102d61cc:       ldr     x0, [x0, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102d61d0:       cbz     x0, ffff8000102d6438 <aio_read_events+0x308>
         :                      aio_read_events_ring():
         :                      smp_rmb();
    1.30 :   ffff8000102d61d4:       dmb     ishld
         :                      if (head == tail)
   18.93 :   ffff8000102d61d8:       cmp     w26, w24
    0.00 :   ffff8000102d61dc:       b.eq    ffff8000102d6448 <aio_read_events+0x318>  // b.none
    0.00 :   ffff8000102d61e0:       str     x27, [x29, #80]
         :                      aio_read_events():
         :                      long ret = aio_read_events_ring(ctx, event + *i, nr - *i);
    0.00 :   ffff8000102d61e4:       sub     x5, x23, x19
         :                      aio_read_events_ring():
         :                      while (ret < nr) {
    0.00 :   ffff8000102d61e8:       cmp     x5, #0x0
         :                      head %= ctx->nr_events;
    0.05 :   ffff8000102d61ec:       ldr     w0, [x28, #144]
    0.00 :   ffff8000102d61f0:       udiv    w1, w26, w0
         :                      tail %= ctx->nr_events;
    0.00 :   ffff8000102d61f4:       udiv    w6, w24, w0
         :                      head %= ctx->nr_events;
    0.00 :   ffff8000102d61f8:       msub    w26, w1, w0, w26
         :                      tail %= ctx->nr_events;
    0.00 :   ffff8000102d61fc:       msub    w6, w6, w0, w24
         :                      while (ret < nr) {
    0.70 :   ffff8000102d6200:       b.le    ffff8000102d64a4 <aio_read_events+0x374>
         :                      aio_read_events():
         :                      long ret = aio_read_events_ring(ctx, event + *i, nr - *i);
    0.10 :   ffff8000102d6204:       lsl     x24, x19, #5
    0.00 :   ffff8000102d6208:       str     x25, [x29, #64]
         :                      aio_read_events_ring():
         :                      long ret = 0;
    0.00 :   ffff8000102d620c:       mov     x27, #0x0                       // #0
         :                      avail = min_t(long, avail, AIO_EVENTS_PER_PAGE - pos);
    0.00 :   ffff8000102d6210:       mov     x23, #0x80                      // #128
         :                      check_copy_size():
         :                      __bad_copy_from();
         :                      else
         :                      __bad_copy_to();
         :                      return false;
         :                      }
         :                      if (WARN_ON_ONCE(bytes > INT_MAX))
    0.00 :   ffff8000102d6214:       mov     x25, #0x7fffffff                // #2147483647
         :                      aio_read_events_ring():
         :                      avail = (head <= tail ?  tail : ctx->nr_events) - head;
    0.00 :   ffff8000102d6218:       sub     w0, w0, w26
    0.00 :   ffff8000102d621c:       cmp     w6, w26
    0.00 :   ffff8000102d6220:       b.cc    ffff8000102d622c <aio_read_events+0xfc>  // b.lo, b.ul, b.last
    0.50 :   ffff8000102d6224:       sub     w0, w6, w26
         :                      if (head == tail)
    0.00 :   ffff8000102d6228:       b.eq    ffff8000102d62bc <aio_read_events+0x18c>  // b.none
         :                      pos = head + AIO_EVENTS_OFFSET;
    0.80 :   ffff8000102d622c:       add     w1, w26, #0x1
         :                      avail = min(avail, nr - ret);
    0.00 :   ffff8000102d6230:       sub     x19, x5, x27
         :                      avail = min_t(long, avail, AIO_EVENTS_PER_PAGE - pos);
    0.00 :   ffff8000102d6234:       and     x10, x1, #0x7f
         :                      copy_ret = copy_to_user(event + ret, ev + pos,
    0.00 :   ffff8000102d6238:       add     x4, x24, x27, lsl #5
         :                      avail = min_t(long, avail, AIO_EVENTS_PER_PAGE - pos);
    0.50 :   ffff8000102d623c:       sub     x2, x23, x10
         :                      copy_ret = copy_to_user(event + ret, ev + pos,
    0.00 :   ffff8000102d6240:       add     x4, x21, x4
         :                      avail = min_t(long, avail, AIO_EVENTS_PER_PAGE - pos);
    0.00 :   ffff8000102d6244:       cmp     x19, x2
    0.00 :   ffff8000102d6248:       csel    x19, x19, x2, le
    0.00 :   ffff8000102d624c:       cmp     x19, x0
    0.00 :   ffff8000102d6250:       csel    x19, x19, x0, le
         :                      copy_ret = copy_to_user(event + ret, ev + pos,
    0.00 :   ffff8000102d6254:       lsl     x2, x19, #5
         :                      check_copy_size():
    0.00 :   ffff8000102d6258:       cmp     x2, x25
    0.25 :   ffff8000102d625c:       b.hi    ffff8000102d6460 <aio_read_events+0x330>  // b.pmore
         :                      get_current():
    0.10 :   ffff8000102d6260:       mrs     x3, sp_el0
         :                      __range_ok():
         :                      * Asynchronous I/O running in a kernel thread does not have the
         :                      * TIF_TAGGED_ADDR flag of the process owning the mm, so always untag
         :                      * the user address before checking.
         :                      */
         :                      if (IS_ENABLED(CONFIG_ARM64_TAGGED_ADDR_ABI) &&
         :                      (current->flags & PF_KTHREAD || test_thread_flag(TIF_TAGGED_ADDR)))
    1.44 :   ffff8000102d6264:       ldr     w11, [x3, #44]
         :                      unsigned long ret, limit = current_thread_info()->addr_limit;
    0.00 :   ffff8000102d6268:       ldr     x0, [x3, #8]
         :                      if (IS_ENABLED(CONFIG_ARM64_TAGGED_ADDR_ABI) &&
    0.00 :   ffff8000102d626c:       tbnz    w11, #21, ffff8000102d627c <aio_read_events+0x14c>
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.50 :   ffff8000102d6270:       ldr     x11, [x3]
         :                      __range_ok():
    0.00 :   ffff8000102d6274:       mov     x3, x4
    0.00 :   ffff8000102d6278:       tbz     w11, #26, ffff8000102d6284 <aio_read_events+0x154>
         :                      sign_extend64():
         :                      * @index: 0 based bit index (0<=index<64) to sign bit
         :                      */
         :                      static inline __s64 sign_extend64(__u64 value, int index)
         :                      {
         :                      __u8 shift = 63 - index;
         :                      return (__s64)(value << shift) >> shift;
    0.00 :   ffff8000102d627c:       sbfx    x3, x4, #0, #56
         :                      __range_ok():
         :                      addr = untagged_addr(addr);
    0.00 :   ffff8000102d6280:       and     x3, x4, x3
         :
         :                      __chk_user_ptr(addr);
         :                      asm volatile(
    0.00 :   ffff8000102d6284:       adds    x3, x3, x2
    0.00 :   ffff8000102d6288:       csel    x0, xzr, x0, hi  // hi = pmore
    0.00 :   ffff8000102d628c:       csinv   x3, x3, xzr, cc  // cc = lo, ul, last
    0.00 :   ffff8000102d6290:       sbcs    xzr, x3, x0
    0.50 :   ffff8000102d6294:       cset    x3, ls  // ls = plast
         :                      _copy_to_user():
         :                      if (access_ok(to, n)) {
    0.00 :   ffff8000102d6298:       cbnz    x3, ffff8000102d6388 <aio_read_events+0x258>
         :                      aio_read_events_ring():
         :                      if (unlikely(copy_ret)) {
    0.00 :   ffff8000102d629c:       cbnz    w2, ffff8000102d63f0 <aio_read_events+0x2c0>
         :                      head %= ctx->nr_events;
    0.20 :   ffff8000102d62a0:       ldr     w0, [x28, #144]
         :                      head += avail;
    0.05 :   ffff8000102d62a4:       add     w26, w26, w19
         :                      ret += avail;
    0.00 :   ffff8000102d62a8:       add     x27, x27, x19
         :                      while (ret < nr) {
    0.10 :   ffff8000102d62ac:       cmp     x5, x27
         :                      head %= ctx->nr_events;
    1.15 :   ffff8000102d62b0:       udiv    w1, w26, w0
    0.00 :   ffff8000102d62b4:       msub    w26, w1, w0, w26
         :                      while (ret < nr) {
    0.00 :   ffff8000102d62b8:       b.gt    ffff8000102d6218 <aio_read_events+0xe8>
    0.00 :   ffff8000102d62bc:       ldr     x25, [x29, #64]
         :                      ring = kmap_atomic(ctx->ring_pages[0]);
    0.40 :   ffff8000102d62c0:       ldr     x1, [x28, #168]
         :                      get_current():
    0.64 :   ffff8000102d62c4:       mrs     x0, sp_el0
         :                      __read_once_size():
    0.00 :   ffff8000102d62c8:       ldr     w2, [x0, #16]
         :                      aio_read_events_ring():
    0.65 :   ffff8000102d62cc:       ldr     x1, [x1]
         :                      __preempt_count_add():
         :                      pc += val;
    0.10 :   ffff8000102d62d0:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.69 :   ffff8000102d62d4:       str     w2, [x0, #16]
         :                      pagefault_disabled_inc():
         :                      current->pagefault_disabled++;
    0.25 :   ffff8000102d62d8:       ldr     w2, [x0, #2448]
    0.05 :   ffff8000102d62dc:       add     w2, w2, #0x1
    0.20 :   ffff8000102d62e0:       str     w2, [x0, #2448]
         :                      lowmem_page_address():
    0.15 :   ffff8000102d62e4:       mov     x2, #0x200000                   // #2097152
    0.00 :   ffff8000102d62e8:       movk    x2, #0x200, lsl #32
    0.10 :   ffff8000102d62ec:       add     x1, x1, x2
         :                      aio_read_events_ring():
         :                      ring->head = head;
    0.10 :   ffff8000102d62f0:       mov     x2, #0xffff000000000000         // #-281474976710656
         :                      lowmem_page_address():
    0.00 :   ffff8000102d62f4:       lsr     x1, x1, #6
         :                      aio_read_events_ring():
    0.00 :   ffff8000102d62f8:       add     x1, x2, x1, lsl #12
    0.30 :   ffff8000102d62fc:       str     w26, [x1, #8]
         :                      pagefault_disabled_dec():
         :                      current->pagefault_disabled--;
    2.58 :   ffff8000102d6300:       ldr     w1, [x0, #2448]
    0.00 :   ffff8000102d6304:       sub     w1, w1, #0x1
    0.85 :   ffff8000102d6308:       str     w1, [x0, #2448]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.90 :   ffff8000102d630c:       ldr     x1, [x0, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000102d6310:       sub     x1, x1, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.79 :   ffff8000102d6314:       str     w1, [x0, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102d6318:       cbnz    x1, ffff8000102d6468 <aio_read_events+0x338>
         :                      __kunmap_atomic():
         :                      #define kmap_atomic_prot(page, prot)    kmap_atomic(page)
         :
         :                      static inline void __kunmap_atomic(void *addr)
         :                      {
         :                      pagefault_enable();
         :                      preempt_enable();
    0.00 :   ffff8000102d631c:       bl      ffff800010cad640 <preempt_schedule>
         :                      aio_read_events_ring():
         :                      flush_dcache_page(ctx->ring_pages[0]);
    0.99 :   ffff8000102d6320:       ldr     x0, [x28, #168]
    0.10 :   ffff8000102d6324:       ldr     x0, [x0]
    0.00 :   ffff8000102d6328:       bl      ffff8000100a2608 <flush_dcache_page>
         :                      mutex_unlock(&ctx->ring_lock);
    0.69 :   ffff8000102d632c:       mov     x0, x22
    0.00 :   ffff8000102d6330:       bl      ffff800010caec10 <mutex_unlock>
         :                      aio_read_events():
         :
         :                      if (ret > 0)
    1.04 :   ffff8000102d6334:       cmp     x27, #0x0
         :                      *i += ret;
    0.00 :   ffff8000102d6338:       ldr     x0, [x20]
         :                      if (ret > 0)
    0.00 :   ffff8000102d633c:       b.le    ffff8000102d6348 <aio_read_events+0x218>
         :                      *i += ret;
    0.00 :   ffff8000102d6340:       add     x0, x27, x0
    0.00 :   ffff8000102d6344:       str     x0, [x20]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.70 :   ffff8000102d6348:       ldr     w1, [x28, #56]
         :                      aio_read_events():
         :
         :                      if (unlikely(atomic_read(&ctx->dead)))
    0.00 :   ffff8000102d634c:       cbnz    w1, ffff8000102d648c <aio_read_events+0x35c>
         :                      ret = -EINVAL;
         :
         :                      if (!*i)
    0.10 :   ffff8000102d6350:       cbz     x0, ffff8000102d6474 <aio_read_events+0x344>
         :                      *i = ret;
         :
         :                      return ret < 0 || *i >= min_nr;
    0.00 :   ffff8000102d6354:       tbnz    x27, #63, ffff8000102d6480 <aio_read_events+0x350>
    0.45 :   ffff8000102d6358:       ldr     x0, [x20]
    0.90 :   ffff8000102d635c:       ldr     x27, [x29, #80]
    0.15 :   ffff8000102d6360:       ldr     x1, [x29, #104]
    0.00 :   ffff8000102d6364:       cmp     x1, x0
    0.00 :   ffff8000102d6368:       cset    w0, le
         :                      }
    0.00 :   ffff8000102d636c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000102d6370:       ldp     x21, x22, [sp, #32]
    0.75 :   ffff8000102d6374:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff8000102d6378:       ldr     x26, [sp, #72]
    0.00 :   ffff8000102d637c:       ldr     x28, [sp, #88]
    0.05 :   ffff8000102d6380:       ldp     x29, x30, [sp], #128
    0.00 :   ffff8000102d6384:       ret
         :                      aio_read_events_ring():
         :                      page = ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE];
    0.00 :   ffff8000102d6388:       ldr     x0, [x28, #168]
    0.00 :   ffff8000102d638c:       lsr     w1, w1, #7
    1.29 :   ffff8000102d6390:       str     w6, [x29, #116]
    0.00 :   ffff8000102d6394:       str     x5, [x29, #120]
    0.00 :   ffff8000102d6398:       ldr     x1, [x0, x1, lsl #3]
         :                      uaccess_enable_not_uao():
         :                      __uaccess_disable(ARM64_ALT_PAN_NOT_UAO);
         :                      }
         :
         :                      static inline void uaccess_enable_not_uao(void)
         :                      {
         :                      __uaccess_enable(ARM64_ALT_PAN_NOT_UAO);
    0.00 :   ffff8000102d639c:       nop
         :                      sign_extend64():
    0.05 :   ffff8000102d63a0:       sbfx    x3, x4, #0, #56
         :                      get_current():
    0.75 :   ffff8000102d63a4:       mrs     x0, sp_el0
         :                      __uaccess_mask_ptr():
         :                      asm volatile(
         :                      "       bics    xzr, %3, %2\n"
         :                      "       csel    %0, %1, xzr, eq\n"
         :                      : "=&r" (safe_ptr)
         :                      : "r" (ptr), "r" (current_thread_info()->addr_limit),
         :                      "r" (untagged_addr(ptr))
    0.15 :   ffff8000102d63a8:       and     x3, x4, x3
         :                      asm volatile(
    0.00 :   ffff8000102d63ac:       ldr     x11, [x0, #8]
    0.00 :   ffff8000102d63b0:       bics    xzr, x3, x11
    0.00 :   ffff8000102d63b4:       csel    x0, x4, xzr, eq  // eq = none
         :                      : "cc");
         :
         :                      csdb();
    0.00 :   ffff8000102d63b8:       csdb
         :                      lowmem_page_address():
    0.00 :   ffff8000102d63bc:       mov     x3, #0x200000                   // #2097152
    0.10 :   ffff8000102d63c0:       movk    x3, #0x200, lsl #32
    0.00 :   ffff8000102d63c4:       add     x1, x1, x3
    0.00 :   ffff8000102d63c8:       mov     x3, #0xffff000000000000         // #-281474976710656
    0.25 :   ffff8000102d63cc:       lsr     x1, x1, #6
    0.00 :   ffff8000102d63d0:       add     x1, x3, x1, lsl #12
         :                      _copy_to_user():
         :                      n = raw_copy_to_user(to, from, n);
    0.00 :   ffff8000102d63d4:       add     x1, x1, x10, lsl #5
    0.00 :   ffff8000102d63d8:       bl      ffff800010c92000 <__arch_copy_to_user>
    0.40 :   ffff8000102d63dc:       mov     x2, x0
         :                      uaccess_disable_not_uao():
         :                      __uaccess_disable(ARM64_ALT_PAN_NOT_UAO);
    0.00 :   ffff8000102d63e0:       nop
    0.25 :   ffff8000102d63e4:       ldr     w6, [x29, #116]
    1.54 :   ffff8000102d63e8:       ldr     x5, [x29, #120]
         :                      aio_read_events_ring():
         :                      if (unlikely(copy_ret)) {
    0.00 :   ffff8000102d63ec:       cbz     w2, ffff8000102d62a0 <aio_read_events+0x170>
         :                      mutex_unlock(&ctx->ring_lock);
    0.00 :   ffff8000102d63f0:       mov     x0, x22
    0.00 :   ffff8000102d63f4:       bl      ffff800010caec10 <mutex_unlock>
         :                      __read_once_size():
    0.00 :   ffff8000102d63f8:       ldr     w0, [x28, #56]
         :                      aio_read_events():
         :                      if (unlikely(atomic_read(&ctx->dead)))
    0.00 :   ffff8000102d63fc:       cbnz    w0, ffff8000102d64ac <aio_read_events+0x37c>
         :                      if (!*i)
    0.00 :   ffff8000102d6400:       ldr     x0, [x20]
    0.00 :   ffff8000102d6404:       cbnz    x0, ffff8000102d647c <aio_read_events+0x34c>
         :                      *i = ret;
    0.00 :   ffff8000102d6408:       mov     x0, #0xfffffffffffffff2         // #-14
    0.00 :   ffff8000102d640c:       str     x0, [x20]
    0.00 :   ffff8000102d6410:       ldr     x25, [x29, #64]
    0.00 :   ffff8000102d6414:       ldr     x27, [x29, #80]
    0.00 :   ffff8000102d6418:       mov     w0, #0x1                        // #1
         :                      }
    0.00 :   ffff8000102d641c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000102d6420:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000102d6424:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff8000102d6428:       ldr     x26, [sp, #72]
    0.00 :   ffff8000102d642c:       ldr     x28, [sp, #88]
    0.00 :   ffff8000102d6430:       ldp     x29, x30, [sp], #128
    0.00 :   ffff8000102d6434:       ret
         :                      __kunmap_atomic():
    0.00 :   ffff8000102d6438:       bl      ffff800010cad640 <preempt_schedule>
         :                      aio_read_events_ring():
         :                      smp_rmb();
    0.00 :   ffff8000102d643c:       dmb     ishld
         :                      if (head == tail)
    0.00 :   ffff8000102d6440:       cmp     w26, w24
    0.00 :   ffff8000102d6444:       b.ne    ffff8000102d61e0 <aio_read_events+0xb0>  // b.any
         :                      mutex_unlock(&ctx->ring_lock);
    0.00 :   ffff8000102d6448:       mov     x0, x22
    0.00 :   ffff8000102d644c:       bl      ffff800010caec10 <mutex_unlock>
         :                      __read_once_size():
    0.00 :   ffff8000102d6450:       ldr     w0, [x28, #56]
         :                      aio_read_events():
         :                      if (unlikely(atomic_read(&ctx->dead)))
    0.00 :   ffff8000102d6454:       cbnz    w0, ffff8000102d64bc <aio_read_events+0x38c>
         :                      if (!*i)
    0.00 :   ffff8000102d6458:       ldr     x0, [x20]
    0.00 :   ffff8000102d645c:       b       ffff8000102d6360 <aio_read_events+0x230>
         :                      check_copy_size():
    0.00 :   ffff8000102d6460:       brk     #0x800
    0.00 :   ffff8000102d6464:       b       ffff8000102d629c <aio_read_events+0x16c>
         :                      __read_once_size():
    0.05 :   ffff8000102d6468:       ldr     x0, [x0, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102d646c:       cbz     x0, ffff8000102d631c <aio_read_events+0x1ec>
    6.54 :   ffff8000102d6470:       b       ffff8000102d6320 <aio_read_events+0x1f0>
         :                      aio_read_events():
         :                      *i = ret;
    0.00 :   ffff8000102d6474:       str     x27, [x20]
    0.00 :   ffff8000102d6478:       b       ffff8000102d6354 <aio_read_events+0x224>
    0.00 :   ffff8000102d647c:       ldr     x25, [x29, #64]
    0.00 :   ffff8000102d6480:       ldr     x27, [x29, #80]
    0.00 :   ffff8000102d6484:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000102d6488:       b       ffff8000102d641c <aio_read_events+0x2ec>
    0.00 :   ffff8000102d648c:       ldr     x27, [x29, #80]
         :                      if (!*i)
    0.00 :   ffff8000102d6490:       cbnz    x0, ffff8000102d6418 <aio_read_events+0x2e8>
         :                      *i = ret;
    0.00 :   ffff8000102d6494:       mov     x1, #0xffffffffffffffea         // #-22
    0.00 :   ffff8000102d6498:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000102d649c:       str     x1, [x20]
         :                      return ret < 0 || *i >= min_nr;
    0.00 :   ffff8000102d64a0:       b       ffff8000102d636c <aio_read_events+0x23c>
         :                      aio_read_events_ring():
         :                      long ret = 0;
    0.00 :   ffff8000102d64a4:       mov     x27, #0x0                       // #0
    0.00 :   ffff8000102d64a8:       b       ffff8000102d62c0 <aio_read_events+0x190>
    0.00 :   ffff8000102d64ac:       ldr     x0, [x20]
    0.00 :   ffff8000102d64b0:       ldr     x25, [x29, #64]
    0.00 :   ffff8000102d64b4:       ldr     x27, [x29, #80]
    0.00 :   ffff8000102d64b8:       b       ffff8000102d6490 <aio_read_events+0x360>
    0.00 :   ffff8000102d64bc:       ldr     x0, [x20]
    0.00 :   ffff8000102d64c0:       b       ffff8000102d6490 <aio_read_events+0x360>
 Percent |	Source code & Disassembly of vmlinux for cycles (1016 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101163d8 <cpus_share_cache>:
         :                      cpus_share_cache():
         :                      rcu_read_unlock();
         :                      }
         :
         :                      bool cpus_share_cache(int this_cpu, int that_cpu)
         :                      {
         :                      return per_cpu(sd_llc_id, this_cpu) == per_cpu(sd_llc_id, that_cpu);
    0.69 :   ffff8000101163d8:       adrp    x3, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000101163dc:       add     x3, x3, #0x8e8
    0.00 :   ffff8000101163e0:       adrp    x2, ffff8000114cc000 <kernel_cpustat+0x48>
    0.10 :   ffff8000101163e4:       add     x2, x2, #0x11c
    0.40 :   ffff8000101163e8:       ldr     x4, [x3, w0, sxtw #3]
    6.33 :   ffff8000101163ec:       ldr     x0, [x3, w1, sxtw #3]
   25.93 :   ffff8000101163f0:       ldr     w1, [x4, x2]
    2.17 :   ffff8000101163f4:       ldr     w0, [x0, x2]
    4.62 :   ffff8000101163f8:       cmp     w1, w0
         :                      }
   38.65 :   ffff8000101163fc:       cset    w0, eq  // eq = none
   21.12 :   ffff800010116400:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (1855 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102d7f50 <io_submit_one>:
         :                      io_submit_one():
         :                      }
         :                      }
         :
         :                      static int io_submit_one(struct kioctx *ctx, struct iocb __user *user_iocb,
         :                      bool compat)
         :                      {
    0.16 :   ffff8000102d7f50:       stp     x29, x30, [sp, #-208]!
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.32 :   ffff8000102d7f54:       mrs     x3, sp_el0
         :                      io_submit_one():
    0.27 :   ffff8000102d7f58:       mov     x29, sp
    0.00 :   ffff8000102d7f5c:       stp     x20, x21, [sp, #24]
    0.05 :   ffff8000102d7f60:       adrp    x21, ffff800011899000 <page_wait_table+0x1500>
    0.05 :   ffff8000102d7f64:       stp     x22, x23, [sp, #40]
    0.00 :   ffff8000102d7f68:       add     x5, x21, #0x8c8
    0.00 :   ffff8000102d7f6c:       mov     x22, x0
    0.05 :   ffff8000102d7f70:       mov     x20, x1
         :                      __range_ok():
         :                      * Asynchronous I/O running in a kernel thread does not have the
         :                      * TIF_TAGGED_ADDR flag of the process owning the mm, so always untag
         :                      * the user address before checking.
         :                      */
         :                      if (IS_ENABLED(CONFIG_ARM64_TAGGED_ADDR_ABI) &&
         :                      (current->flags & PF_KTHREAD || test_thread_flag(TIF_TAGGED_ADDR)))
    0.22 :   ffff8000102d7f74:       ldr     w4, [x3, #44]
         :                      io_submit_one():
    0.11 :   ffff8000102d7f78:       ldr     x0, [x5]
    0.05 :   ffff8000102d7f7c:       str     x0, [x29, #200]
    1.52 :   ffff8000102d7f80:       mov     x0, #0x0                        // #0
    0.00 :   ffff8000102d7f84:       and     w23, w2, #0xff
         :                      __range_ok():
         :                      unsigned long ret, limit = current_thread_info()->addr_limit;
    0.05 :   ffff8000102d7f88:       ldr     x1, [x3, #8]
         :                      if (IS_ENABLED(CONFIG_ARM64_TAGGED_ADDR_ABI) &&
    0.00 :   ffff8000102d7f8c:       tbnz    w4, #21, ffff8000102d7fa0 <io_submit_one+0x50>
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.21 :   ffff8000102d7f90:       ldr     x2, [x3]
         :                      __range_ok():
    0.00 :   ffff8000102d7f94:       mov     x0, x20
    0.00 :   ffff8000102d7f98:       tst     w2, #0x4000000
    0.00 :   ffff8000102d7f9c:       b.eq    ffff8000102d7fa8 <io_submit_one+0x58>  // b.none
         :                      sign_extend64():
         :                      * @index: 0 based bit index (0<=index<64) to sign bit
         :                      */
         :                      static inline __s64 sign_extend64(__u64 value, int index)
         :                      {
         :                      __u8 shift = 63 - index;
         :                      return (__s64)(value << shift) >> shift;
    0.00 :   ffff8000102d7fa0:       sbfx    x0, x20, #0, #56
         :                      __range_ok():
         :                      addr = untagged_addr(addr);
    0.00 :   ffff8000102d7fa4:       and     x0, x20, x0
         :
         :                      __chk_user_ptr(addr);
         :                      asm volatile(
    0.16 :   ffff8000102d7fa8:       adds    x0, x0, #0x40
    0.00 :   ffff8000102d7fac:       csel    x1, xzr, x1, hi  // hi = pmore
    0.00 :   ffff8000102d7fb0:       csinv   x0, x0, xzr, cc  // cc = lo, ul, last
    0.00 :   ffff8000102d7fb4:       sbcs    xzr, x0, x1
    0.48 :   ffff8000102d7fb8:       cset    x0, ls  // ls = plast
         :                      _copy_from_user():
         :                      static inline __must_check unsigned long
         :                      _copy_from_user(void *to, const void __user *from, unsigned long n)
         :                      {
         :                      unsigned long res = n;
         :                      might_fault();
         :                      if (likely(access_ok(from, n))) {
    0.00 :   ffff8000102d7fbc:       cbz     x0, ffff8000102d8698 <io_submit_one+0x748>
    0.16 :   ffff8000102d7fc0:       str     x25, [x29, #64]
         :                      uaccess_enable_not_uao():
         :                      __uaccess_disable(ARM64_ALT_PAN_NOT_UAO);
         :                      }
         :
         :                      static inline void uaccess_enable_not_uao(void)
         :                      {
         :                      __uaccess_enable(ARM64_ALT_PAN_NOT_UAO);
    0.00 :   ffff8000102d7fc4:       nop
         :                      sign_extend64():
    4.74 :   ffff8000102d7fc8:       sbfx    x0, x20, #0, #56
         :                      get_current():
    0.16 :   ffff8000102d7fcc:       mrs     x25, sp_el0
         :                      __uaccess_mask_ptr():
         :                      asm volatile(
         :                      "       bics    xzr, %3, %2\n"
         :                      "       csel    %0, %1, xzr, eq\n"
         :                      : "=&r" (safe_ptr)
         :                      : "r" (ptr), "r" (current_thread_info()->addr_limit),
         :                      "r" (untagged_addr(ptr))
    0.16 :   ffff8000102d7fd0:       and     x0, x20, x0
         :                      asm volatile(
    0.00 :   ffff8000102d7fd4:       ldr     x2, [x25, #8]
    0.16 :   ffff8000102d7fd8:       bics    xzr, x0, x2
    0.00 :   ffff8000102d7fdc:       csel    x1, x20, xzr, eq  // eq = none
         :                      : "cc");
         :
         :                      csdb();
    0.00 :   ffff8000102d7fe0:       csdb
         :                      _copy_from_user():
         :                      kasan_check_write(to, n);
         :                      res = raw_copy_from_user(to, from, n);
    0.59 :   ffff8000102d7fe4:       mov     x2, #0x40                       // #64
    0.00 :   ffff8000102d7fe8:       add     x0, x29, #0x88
    0.00 :   ffff8000102d7fec:       bl      ffff800010c91a40 <__arch_copy_from_user>
         :                      uaccess_disable_not_uao():
         :                      __uaccess_disable(ARM64_ALT_PAN_NOT_UAO);
    0.32 :   ffff8000102d7ff0:       nop
         :                      _copy_from_user():
         :                      }
         :                      if (unlikely(res))
    0.70 :   ffff8000102d7ff4:       cbnz    x0, ffff8000102d8690 <io_submit_one+0x740>
         :                      io_submit_one():
         :
         :                      if (unlikely(copy_from_user(&iocb, user_iocb, sizeof(iocb))))
         :                      return -EFAULT;
         :
         :                      /* enforce forwards compatibility on users */
         :                      if (unlikely(iocb.aio_reserved2)) {
    2.97 :   ffff8000102d7ff8:       ldr     x0, [x29, #184]
    0.00 :   ffff8000102d7ffc:       cbnz    x0, ffff8000102d8684 <io_submit_one+0x734>
         :                      pr_debug("EINVAL: reserve field set\n");
         :                      return -EINVAL;
         :                      }
         :
         :                      /* prevent overflows */
         :                      if (unlikely(
    6.09 :   ffff8000102d8000:       ldr     x0, [x29, #168]
    0.00 :   ffff8000102d8004:       tbnz    x0, #63, ffff8000102d8684 <io_submit_one+0x734>
    1.72 :   ffff8000102d8008:       str     x24, [x29, #56]
         :                      aio_get_req():
         :                      req = kmem_cache_alloc(kiocb_cachep, GFP_KERNEL);
    0.00 :   ffff8000102d800c:       adrp    x24, ffff800011ab4000 <in_lookup_hashtable+0x1c08>
    0.00 :   ffff8000102d8010:       add     x0, x24, #0xf58
    0.32 :   ffff8000102d8014:       str     x19, [x29, #16]
    0.00 :   ffff8000102d8018:       mov     w1, #0xcc0                      // #3264
    0.00 :   ffff8000102d801c:       ldr     x0, [x0, #8]
    0.16 :   ffff8000102d8020:       bl      ffff800010252178 <kmem_cache_alloc>
    0.92 :   ffff8000102d8024:       mov     x19, x0
         :                      if (unlikely(!req))
    0.00 :   ffff8000102d8028:       cbz     x0, ffff8000102d86a0 <io_submit_one+0x750>
         :                      get_reqs_available():
         :                      if (__get_reqs_available(ctx))
    0.00 :   ffff8000102d802c:       mov     x0, x22
    0.00 :   ffff8000102d8030:       str     x26, [x29, #72]
    0.00 :   ffff8000102d8034:       bl      ffff8000102d5ff0 <__get_reqs_available>
    0.92 :   ffff8000102d8038:       tst     w0, #0xff
    0.00 :   ffff8000102d803c:       b.eq    ffff8000102d8194 <io_submit_one+0x244>  // b.none
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    5.44 :   ffff8000102d8040:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d8044:       ldr     x0, [x22, #72]
         :                      __ref_is_percpu():
         :                      * Theoretically, the following could test just ATOMIC; however,
         :                      * then we'd have to mask off DEAD separately as DEAD may be
         :                      * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
         :                      * implies ATOMIC anyway.  Test them together.
         :                      */
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff8000102d8048:       tst     x0, #0x3
    0.00 :   ffff8000102d804c:       b.ne    ffff8000102d87d8 <io_submit_one+0x888>  // b.any
         :                      get_current():
    0.21 :   ffff8000102d8050:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.16 :   ffff8000102d8054:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff8000102d8058:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    1.51 :   ffff8000102d805c:       str     w2, [x1, #16]
         :                      __percpu_add_case_64():
         :
         :                      PERCPU_RW_OPS(8)
         :                      PERCPU_RW_OPS(16)
         :                      PERCPU_RW_OPS(32)
         :                      PERCPU_RW_OPS(64)
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff8000102d8060:       mov     x3, #0x1                        // #1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000102d8064:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff8000102d8068:       add     x0, x0, x2
    0.00 :   ffff8000102d806c:       ldxr    x5, [x0]
    8.68 :   ffff8000102d8070:       add     x5, x5, x3
    0.00 :   ffff8000102d8074:       stxr    w4, x5, [x0]
    0.00 :   ffff8000102d8078:       cbnz    w4, ffff8000102d806c <io_submit_one+0x11c>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d807c:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.32 :   ffff8000102d8080:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.54 :   ffff8000102d8084:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102d8088:       cbnz    x0, ffff8000102d8188 <io_submit_one+0x238>
         :                      percpu_ref_get_many():
         :                      unsigned long __percpu *percpu_count;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count))
         :                      this_cpu_add(*percpu_count, nr);
    0.05 :   ffff8000102d808c:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.97 :   ffff8000102d8090:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      __io_submit_one():
         :                      req->ki_filp = fget(iocb->aio_fildes);
    0.00 :   ffff8000102d8094:       mov     x25, x19
    1.08 :   ffff8000102d8098:       ldr     w0, [x29, #156]
         :                      __write_once_size():
    0.00 :   ffff8000102d809c:       mov     w1, #0x2                        // #2
         :                      aio_get_req():
         :                      INIT_LIST_HEAD(&req->ki_list);
    0.00 :   ffff8000102d80a0:       add     x26, x19, #0x90
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff8000102d80a4:       str     x26, [x19, #144]
         :                      aio_get_req():
         :                      req->ki_ctx = ctx;
    0.00 :   ffff8000102d80a8:       str     x22, [x19, #96]
         :                      INIT_LIST_HEAD():
         :                      struct list_head name = LIST_HEAD_INIT(name)
         :
         :                      static inline void INIT_LIST_HEAD(struct list_head *list)
         :                      {
         :                      WRITE_ONCE(list->next, list);
         :                      list->prev = list;
    0.05 :   ffff8000102d80ac:       str     x26, [x19, #152]
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.70 :   ffff8000102d80b0:       str     w1, [x19, #160]
         :                      aio_get_req():
         :                      req->ki_eventfd = NULL;
    0.05 :   ffff8000102d80b4:       str     xzr, [x19, #168]
         :                      __io_submit_one():
         :                      req->ki_filp = fget(iocb->aio_fildes);
    0.00 :   ffff8000102d80b8:       bl      ffff80001029ecc8 <fget>
    0.16 :   ffff8000102d80bc:       str     x0, [x25], #160
         :                      if (unlikely(!req->ki_filp))
    0.49 :   ffff8000102d80c0:       cbz     x0, ffff8000102d86b0 <io_submit_one+0x760>
         :                      if (iocb->aio_flags & IOCB_FLAG_RESFD) {
    0.70 :   ffff8000102d80c4:       ldr     w0, [x29, #192]
    0.00 :   ffff8000102d80c8:       tbz     w0, #0, ffff8000102d80e0 <io_submit_one+0x190>
         :                      eventfd = eventfd_ctx_fdget(iocb->aio_resfd);
    0.00 :   ffff8000102d80cc:       ldr     w0, [x29, #196]
    0.00 :   ffff8000102d80d0:       bl      ffff8000102d5098 <eventfd_ctx_fdget>
         :                      if (IS_ERR(eventfd))
    0.00 :   ffff8000102d80d4:       cmn     x0, #0x1, lsl #12
    0.00 :   ffff8000102d80d8:       b.hi    ffff8000102d848c <io_submit_one+0x53c>  // b.pmore
         :                      req->ki_eventfd = eventfd;
    0.00 :   ffff8000102d80dc:       str     x0, [x19, #168]
         :                      get_current():
    0.11 :   ffff8000102d80e0:       mrs     x0, sp_el0
         :                      __range_ok():
         :                      (current->flags & PF_KTHREAD || test_thread_flag(TIF_TAGGED_ADDR)))
    0.00 :   ffff8000102d80e4:       ldr     w3, [x0, #44]
         :                      __io_submit_one():
         :                      if (unlikely(put_user(KIOCB_KEY, &user_iocb->aio_key))) {
    0.00 :   ffff8000102d80e8:       add     x2, x20, #0x8
         :                      __range_ok():
         :                      unsigned long ret, limit = current_thread_info()->addr_limit;
    0.27 :   ffff8000102d80ec:       ldr     x1, [x0, #8]
         :                      if (IS_ENABLED(CONFIG_ARM64_TAGGED_ADDR_ABI) &&
    0.00 :   ffff8000102d80f0:       tbnz    w3, #21, ffff8000102d8104 <io_submit_one+0x1b4>
         :                      test_bit():
    0.54 :   ffff8000102d80f4:       ldr     x3, [x0]
         :                      __range_ok():
    0.00 :   ffff8000102d80f8:       mov     x0, x2
    0.00 :   ffff8000102d80fc:       tst     w3, #0x4000000
    0.00 :   ffff8000102d8100:       b.eq    ffff8000102d810c <io_submit_one+0x1bc>  // b.none
         :                      sign_extend64():
    0.00 :   ffff8000102d8104:       sbfx    x0, x2, #0, #56
         :                      __range_ok():
         :                      addr = untagged_addr(addr);
    0.00 :   ffff8000102d8108:       and     x0, x2, x0
         :                      asm volatile(
    0.05 :   ffff8000102d810c:       adds    x0, x0, #0x4
    0.00 :   ffff8000102d8110:       csel    x1, xzr, x1, hi  // hi = pmore
    0.00 :   ffff8000102d8114:       csinv   x0, x0, xzr, cc  // cc = lo, ul, last
    0.00 :   ffff8000102d8118:       sbcs    xzr, x0, x1
    0.21 :   ffff8000102d811c:       cset    x0, ls  // ls = plast
         :                      __io_submit_one():
    0.05 :   ffff8000102d8120:       cbnz    x0, ffff8000102d8420 <io_submit_one+0x4d0>
         :                      return -EFAULT;
    0.00 :   ffff8000102d8124:       mov     w20, #0xfffffff2                // #-14
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.75 :   ffff8000102d8128:       b       ffff8000102d81e4 <io_submit_one+0x294>
    0.27 :   ffff8000102d812c:       b       ffff8000102d81e4 <io_submit_one+0x294>
         :                      __lse_atomic_fetch_sub_release():
         :                      return i;                                                       \
         :                      }
         :
         :                      ATOMIC_FETCH_OP_SUB(_relaxed,   )
         :                      ATOMIC_FETCH_OP_SUB(_acquire,  a, "memory")
         :                      ATOMIC_FETCH_OP_SUB(_release,  l, "memory")
    0.00 :   ffff8000102d8130:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000102d8134:       add     x0, x19, #0xa0
    0.00 :   ffff8000102d8138:       neg     w1, w1
    0.11 :   ffff8000102d813c:       ldaddl  w1, w1, [x0]
         :                      refcount_sub_and_test():
         :                      */
         :                      static inline __must_check bool refcount_sub_and_test(int i, refcount_t *r)
         :                      {
         :                      int old = atomic_fetch_sub_release(i, &r->refs);
         :
         :                      if (old == i) {
   36.25 :   ffff8000102d8140:       cmp     w1, #0x1
    0.00 :   ffff8000102d8144:       b.eq    ffff8000102d81f8 <io_submit_one+0x2a8>  // b.none
         :                      smp_acquire__after_ctrl_dep();
         :                      return true;
         :                      }
         :
         :                      if (unlikely(old < 0 || old - i < 0))
    0.05 :   ffff8000102d8148:       cmp     w1, #0x0
    0.00 :   ffff8000102d814c:       b.le    ffff8000102d8520 <io_submit_one+0x5d0>
         :                      io_submit_one():
         :                      /*
         :                      * If err is 0, we'd either done aio_complete() ourselves or have
         :                      * arranged for that to be done asynchronously.  Anything non-zero
         :                      * means that we need to destroy req ourselves.
         :                      */
         :                      if (unlikely(err)) {
    0.81 :   ffff8000102d8150:       cbnz    w20, ffff8000102d86b8 <io_submit_one+0x768>
    1.08 :   ffff8000102d8154:       ldr     x19, [x29, #16]
    0.11 :   ffff8000102d8158:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff8000102d815c:       ldr     x26, [x29, #72]
         :                      iocb_destroy(req);
         :                      put_reqs_available(ctx, 1);
         :                      }
         :                      return err;
         :                      }
    0.00 :   ffff8000102d8160:       add     x21, x21, #0x8c8
    0.00 :   ffff8000102d8164:       mov     w0, w20
    0.11 :   ffff8000102d8168:       ldr     x2, [x29, #200]
    0.87 :   ffff8000102d816c:       ldr     x1, [x21]
    0.00 :   ffff8000102d8170:       eor     x1, x2, x1
    0.00 :   ffff8000102d8174:       cbnz    x1, ffff8000102d8bc0 <io_submit_one+0xc70>
    0.00 :   ffff8000102d8178:       ldp     x20, x21, [sp, #24]
    0.16 :   ffff8000102d817c:       ldp     x22, x23, [sp, #40]
    0.00 :   ffff8000102d8180:       ldp     x29, x30, [sp], #208
    0.00 :   ffff8000102d8184:       ret
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.11 :   ffff8000102d8188:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102d818c:       cbz     x0, ffff8000102d808c <io_submit_one+0x13c>
    2.41 :   ffff8000102d8190:       b       ffff8000102d8090 <io_submit_one+0x140>
         :                      spin_lock_irq():
         :                      raw_spin_lock_nest_lock(spinlock_check(lock), nest_lock);       \
         :                      } while (0)
         :
         :                      static __always_inline void spin_lock_irq(spinlock_t *lock)
         :                      {
         :                      raw_spin_lock_irq(&lock->rlock);
    0.00 :   ffff8000102d8194:       add     x26, x22, #0x1c8
    0.00 :   ffff8000102d8198:       mov     x0, x26
    0.00 :   ffff8000102d819c:       bl      ffff800010cb2cd0 <_raw_spin_lock_irq>
         :                      user_refill_reqs_available():
         :                      if (ctx->completed_events) {
    0.00 :   ffff8000102d81a0:       ldr     w0, [x22, #452]
    0.00 :   ffff8000102d81a4:       cbnz    w0, ffff8000102d84a0 <io_submit_one+0x550>
         :                      spin_unlock_irq():
         :                      raw_spin_unlock_bh(&lock->rlock);
         :                      }
         :
         :                      static __always_inline void spin_unlock_irq(spinlock_t *lock)
         :                      {
         :                      raw_spin_unlock_irq(&lock->rlock);
    0.00 :   ffff8000102d81a8:       mov     x0, x26
    0.00 :   ffff8000102d81ac:       bl      ffff800010cb2bf0 <_raw_spin_unlock_irq>
         :                      get_reqs_available():
         :                      return __get_reqs_available(ctx);
    0.00 :   ffff8000102d81b0:       mov     x0, x22
    0.00 :   ffff8000102d81b4:       bl      ffff8000102d5ff0 <__get_reqs_available>
         :                      aio_get_req():
         :                      if (unlikely(!get_reqs_available(ctx))) {
    0.00 :   ffff8000102d81b8:       tst     w0, #0xff
    0.00 :   ffff8000102d81bc:       b.ne    ffff8000102d8040 <io_submit_one+0xf0>  // b.any
         :                      kmem_cache_free(kiocb_cachep, req);
    0.00 :   ffff8000102d81c0:       add     x24, x24, #0xf58
    0.00 :   ffff8000102d81c4:       mov     x1, x19
         :                      io_submit_one():
         :                      return -EAGAIN;
    0.00 :   ffff8000102d81c8:       mov     w20, #0xfffffff5                // #-11
         :                      aio_get_req():
         :                      kmem_cache_free(kiocb_cachep, req);
    0.00 :   ffff8000102d81cc:       ldr     x0, [x24, #8]
    0.00 :   ffff8000102d81d0:       bl      ffff800010250100 <kmem_cache_free>
    0.00 :   ffff8000102d81d4:       ldr     x19, [x29, #16]
    0.00 :   ffff8000102d81d8:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff8000102d81dc:       ldr     x26, [x29, #72]
    0.00 :   ffff8000102d81e0:       b       ffff8000102d8160 <io_submit_one+0x210>
         :                      __ll_sc_atomic_fetch_sub_release():
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff8000102d81e4:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000102d81e8:       add     x4, x19, #0xa0
    0.00 :   ffff8000102d81ec:       b       ffff8000102da668 <__arm64_compat_sys_io_pgetevents_time64+0x3a8>
         :                      refcount_sub_and_test():
         :                      if (old == i) {
    0.00 :   ffff8000102d81f0:       cmp     w1, #0x1
    0.00 :   ffff8000102d81f4:       b.ne    ffff8000102d8148 <io_submit_one+0x1f8>  // b.any
    0.32 :   ffff8000102d81f8:       stp     x27, x28, [x29, #80]
         :                      smp_acquire__after_ctrl_dep();
    0.00 :   ffff8000102d81fc:       dmb     ishld
         :                      aio_complete():
         :                      struct kioctx   *ctx = iocb->ki_ctx;
    0.21 :   ffff8000102d8200:       ldr     x23, [x19, #96]
         :                      spinlock_check():
         :                      return &lock->rlock;
    0.00 :   ffff8000102d8204:       add     x26, x23, #0x1c8
         :                      aio_complete():
         :                      spin_lock_irqsave(&ctx->completion_lock, flags);
    0.00 :   ffff8000102d8208:       mov     x0, x26
    0.00 :   ffff8000102d820c:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
         :                      pos = tail + AIO_EVENTS_OFFSET;
    0.00 :   ffff8000102d8210:       ldr     w2, [x23, #448]
         :                      spin_lock_irqsave(&ctx->completion_lock, flags);
    0.00 :   ffff8000102d8214:       mov     x27, x0
         :                      if (++tail >= ctx->nr_events)
    0.00 :   ffff8000102d8218:       ldr     w4, [x23, #144]
         :                      pos = tail + AIO_EVENTS_OFFSET;
    0.00 :   ffff8000102d821c:       add     w2, w2, #0x1
         :                      ev_page = kmap_atomic(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);
    0.00 :   ffff8000102d8220:       ldr     x0, [x23, #168]
         :                      get_current():
    0.00 :   ffff8000102d8224:       mrs     x1, sp_el0
         :                      aio_complete():
    0.00 :   ffff8000102d8228:       lsr     w28, w2, #7
         :                      __read_once_size():
    0.00 :   ffff8000102d822c:       ldr     w3, [x1, #16]
         :                      aio_complete():
         :                      if (++tail >= ctx->nr_events)
    0.00 :   ffff8000102d8230:       cmp     w2, w4
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff8000102d8234:       add     w3, w3, #0x1
         :                      aio_complete():
    0.00 :   ffff8000102d8238:       csel    w25, w2, wzr, cc  // cc = lo, ul, last
         :                      ev_page = kmap_atomic(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);
    0.00 :   ffff8000102d823c:       ldr     x0, [x0, x28, lsl #3]
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d8240:       str     w3, [x1, #16]
         :                      pagefault_disabled_inc():
         :                      }
         :                      #endif
         :
         :                      static __always_inline void pagefault_disabled_inc(void)
         :                      {
         :                      current->pagefault_disabled++;
    0.00 :   ffff8000102d8244:       ldr     w3, [x1, #2448]
    0.00 :   ffff8000102d8248:       add     w3, w3, #0x1
    0.00 :   ffff8000102d824c:       str     w3, [x1, #2448]
         :                      lowmem_page_address():
         :                      */
         :                      #include <linux/vmstat.h>
         :
         :                      static __always_inline void *lowmem_page_address(const struct page *page)
         :                      {
         :                      return page_to_virt(page);
    0.00 :   ffff8000102d8250:       mov     x3, #0x200000                   // #2097152
         :                      aio_complete():
         :                      event = ev_page + pos % AIO_EVENTS_PER_PAGE;
    0.00 :   ffff8000102d8254:       ubfiz   x4, x2, #5, #7
         :                      lowmem_page_address():
    0.00 :   ffff8000102d8258:       movk    x3, #0x200, lsl #32
    0.00 :   ffff8000102d825c:       add     x0, x0, x3
         :                      aio_complete():
         :                      *event = iocb->ki_res;
    0.00 :   ffff8000102d8260:       mov     x5, #0xffff000000000000         // #-281474976710656
         :                      lowmem_page_address():
    0.00 :   ffff8000102d8264:       lsr     x0, x0, #6
         :                      aio_complete():
    0.00 :   ffff8000102d8268:       ldp     x2, x3, [x19, #112]
    0.00 :   ffff8000102d826c:       add     x0, x4, x0, lsl #12
    0.00 :   ffff8000102d8270:       add     x0, x0, x5
    0.00 :   ffff8000102d8274:       stp     x2, x3, [x0]
    0.00 :   ffff8000102d8278:       ldp     x2, x3, [x19, #128]
    0.00 :   ffff8000102d827c:       stp     x2, x3, [x0, #16]
         :                      pagefault_disabled_dec():
         :                      }
         :
         :                      static __always_inline void pagefault_disabled_dec(void)
         :                      {
         :                      current->pagefault_disabled--;
    0.00 :   ffff8000102d8280:       ldr     w0, [x1, #2448]
    0.00 :   ffff8000102d8284:       sub     w0, w0, #0x1
    0.00 :   ffff8000102d8288:       str     w0, [x1, #2448]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d828c:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000102d8290:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d8294:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102d8298:       cbnz    x0, ffff8000102d8494 <io_submit_one+0x544>
         :                      __kunmap_atomic():
         :                      #define kmap_atomic_prot(page, prot)    kmap_atomic(page)
         :
         :                      static inline void __kunmap_atomic(void *addr)
         :                      {
         :                      pagefault_enable();
         :                      preempt_enable();
    0.00 :   ffff8000102d829c:       bl      ffff800010cad640 <preempt_schedule>
         :                      aio_complete():
         :                      flush_dcache_page(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);
    0.00 :   ffff8000102d82a0:       ldr     x0, [x23, #168]
    0.00 :   ffff8000102d82a4:       ldr     x0, [x0, x28, lsl #3]
    0.00 :   ffff8000102d82a8:       bl      ffff8000100a2608 <flush_dcache_page>
         :                      smp_wmb();      /* make event visible before updating tail */
    0.00 :   ffff8000102d82ac:       dmb     ishst
         :                      ctx->tail = tail;
    0.00 :   ffff8000102d82b0:       str     w25, [x23, #448]
         :                      ring = kmap_atomic(ctx->ring_pages[0]);
    0.00 :   ffff8000102d82b4:       ldr     x1, [x23, #168]
         :                      get_current():
    0.00 :   ffff8000102d82b8:       mrs     x0, sp_el0
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d82bc:       ldr     w2, [x0, #16]
         :                      aio_complete():
    0.00 :   ffff8000102d82c0:       ldr     x1, [x1]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff8000102d82c4:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d82c8:       str     w2, [x0, #16]
         :                      pagefault_disabled_inc():
         :                      current->pagefault_disabled++;
    0.00 :   ffff8000102d82cc:       ldr     w2, [x0, #2448]
    0.00 :   ffff8000102d82d0:       add     w2, w2, #0x1
    0.00 :   ffff8000102d82d4:       str     w2, [x0, #2448]
         :                      lowmem_page_address():
    0.00 :   ffff8000102d82d8:       mov     x2, #0x200000                   // #2097152
    0.00 :   ffff8000102d82dc:       movk    x2, #0x200, lsl #32
    0.00 :   ffff8000102d82e0:       add     x1, x1, x2
    0.00 :   ffff8000102d82e4:       mov     x2, #0xffff000000000000         // #-281474976710656
    0.00 :   ffff8000102d82e8:       lsr     x1, x1, #6
    0.00 :   ffff8000102d82ec:       add     x1, x2, x1, lsl #12
         :                      aio_complete():
         :                      head = ring->head;
    0.00 :   ffff8000102d82f0:       ldr     w28, [x1, #8]
         :                      ring->tail = tail;
    0.00 :   ffff8000102d82f4:       str     w25, [x1, #12]
         :                      pagefault_disabled_dec():
         :                      current->pagefault_disabled--;
    0.00 :   ffff8000102d82f8:       ldr     w1, [x0, #2448]
    0.00 :   ffff8000102d82fc:       sub     w1, w1, #0x1
    0.00 :   ffff8000102d8300:       str     w1, [x0, #2448]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d8304:       ldr     x1, [x0, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000102d8308:       sub     x1, x1, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d830c:       str     w1, [x0, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102d8310:       cbz     x1, ffff8000102d8518 <io_submit_one+0x5c8>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d8314:       ldr     x0, [x0, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102d8318:       cbz     x0, ffff8000102d8518 <io_submit_one+0x5c8>
         :                      aio_complete():
         :                      flush_dcache_page(ctx->ring_pages[0]);
    0.00 :   ffff8000102d831c:       ldr     x0, [x23, #168]
    0.00 :   ffff8000102d8320:       ldr     x0, [x0]
    0.00 :   ffff8000102d8324:       bl      ffff8000100a2608 <flush_dcache_page>
         :                      ctx->completed_events++;
    0.00 :   ffff8000102d8328:       ldr     w0, [x23, #452]
    0.00 :   ffff8000102d832c:       add     w0, w0, #0x1
    0.00 :   ffff8000102d8330:       str     w0, [x23, #452]
         :                      if (ctx->completed_events > 1)
    0.00 :   ffff8000102d8334:       cmp     w0, #0x1
    0.00 :   ffff8000102d8338:       b.ls    ffff8000102d834c <io_submit_one+0x3fc>  // b.plast
         :                      refill_reqs_available(ctx, head, tail);
    0.00 :   ffff8000102d833c:       mov     w2, w25
    0.00 :   ffff8000102d8340:       mov     w1, w28
    0.00 :   ffff8000102d8344:       mov     x0, x23
    0.00 :   ffff8000102d8348:       bl      ffff8000102d5fa0 <refill_reqs_available>
         :                      spin_unlock_irqrestore():
         :                      }
         :
         :                      static __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
         :                      {
         :                      raw_spin_unlock_irqrestore(&lock->rlock, flags);
    0.00 :   ffff8000102d834c:       mov     x0, x26
    0.00 :   ffff8000102d8350:       mov     x1, x27
    0.00 :   ffff8000102d8354:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      aio_complete():
         :                      if (iocb->ki_eventfd)
    0.00 :   ffff8000102d8358:       ldr     x0, [x19, #168]
    0.00 :   ffff8000102d835c:       cbz     x0, ffff8000102d8368 <io_submit_one+0x418>
         :                      eventfd_signal(iocb->ki_eventfd, 1);
    0.00 :   ffff8000102d8360:       mov     x1, #0x1                        // #1
    0.00 :   ffff8000102d8364:       bl      ffff8000102d49a0 <eventfd_signal>
         :                      smp_mb();
    0.00 :   ffff8000102d8368:       dmb     ish
         :                      __read_once_size():
    0.21 :   ffff8000102d836c:       ldr     x1, [x23, #424]
         :                      waitqueue_active():
         :                      * Also note that this 'optimization' trades a spin_lock() for an smp_mb(),
         :                      * which (when the lock is uncontended) are of roughly equal cost.
         :                      */
         :                      static inline int waitqueue_active(struct wait_queue_head *wq_head)
         :                      {
         :                      return !list_empty(&wq_head->head);
    0.00 :   ffff8000102d8370:       add     x2, x23, #0x1a8
    0.00 :   ffff8000102d8374:       add     x0, x23, #0x1a0
         :                      aio_complete():
         :                      if (waitqueue_active(&ctx->wait))
    0.00 :   ffff8000102d8378:       cmp     x2, x1
    0.00 :   ffff8000102d837c:       b.eq    ffff8000102d8390 <io_submit_one+0x440>  // b.none
         :                      wake_up(&ctx->wait);
    0.00 :   ffff8000102d8380:       mov     x3, #0x0                        // #0
    0.00 :   ffff8000102d8384:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000102d8388:       mov     w1, #0x3                        // #3
    0.00 :   ffff8000102d838c:       bl      ffff80001012e430 <__wake_up>
         :                      iocb_destroy():
         :                      if (iocb->ki_eventfd)
    0.00 :   ffff8000102d8390:       ldr     x0, [x19, #168]
    0.00 :   ffff8000102d8394:       cbz     x0, ffff8000102d839c <io_submit_one+0x44c>
         :                      eventfd_ctx_put(iocb->ki_eventfd);
    0.00 :   ffff8000102d8398:       bl      ffff8000102d4f38 <eventfd_ctx_put>
         :                      if (iocb->ki_filp)
    0.05 :   ffff8000102d839c:       ldr     x0, [x19]
    0.00 :   ffff8000102d83a0:       cbz     x0, ffff8000102d83a8 <io_submit_one+0x458>
         :                      fput(iocb->ki_filp);
    0.00 :   ffff8000102d83a4:       bl      ffff80001027bc88 <fput>
         :                      percpu_ref_put(&iocb->ki_ctx->reqs);
    0.00 :   ffff8000102d83a8:       ldr     x23, [x19, #96]
         :                      rcu_read_lock():
         :                      __rcu_read_lock();
    0.00 :   ffff8000102d83ac:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
    0.00 :   ffff8000102d83b0:       ldr     x0, [x23, #72]
         :                      __ref_is_percpu():
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff8000102d83b4:       tst     x0, #0x3
    0.00 :   ffff8000102d83b8:       b.ne    ffff8000102d8888 <io_submit_one+0x938>  // b.any
         :                      get_current():
    0.00 :   ffff8000102d83bc:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.00 :   ffff8000102d83c0:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff8000102d83c4:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d83c8:       str     w2, [x1, #16]
         :                      __percpu_add_case_64():
    0.00 :   ffff8000102d83cc:       mov     x3, #0xffffffffffffffff         // #-1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000102d83d0:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff8000102d83d4:       add     x0, x0, x2
    0.00 :   ffff8000102d83d8:       ldxr    x5, [x0]
    0.38 :   ffff8000102d83dc:       add     x5, x5, x3
    0.00 :   ffff8000102d83e0:       stxr    w4, x5, [x0]
    0.00 :   ffff8000102d83e4:       cbnz    w4, ffff8000102d83d8 <io_submit_one+0x488>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d83e8:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000102d83ec:       add     x0, x0, x3
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d83f0:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102d83f4:       cbz     x0, ffff8000102d8400 <io_submit_one+0x4b0>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d83f8:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102d83fc:       cbnz    x0, ffff8000102d8404 <io_submit_one+0x4b4>
         :                      percpu_ref_put_many():
         :                      unsigned long __percpu *percpu_count;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count))
         :                      this_cpu_sub(*percpu_count, nr);
    0.00 :   ffff8000102d8400:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      rcu_read_unlock():
         :                      __rcu_read_unlock();
    0.11 :   ffff8000102d8404:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      iocb_destroy():
         :                      kmem_cache_free(kiocb_cachep, iocb);
    0.00 :   ffff8000102d8408:       add     x0, x24, #0xf58
    0.00 :   ffff8000102d840c:       mov     x1, x19
    0.00 :   ffff8000102d8410:       ldr     x0, [x0, #8]
    0.00 :   ffff8000102d8414:       bl      ffff800010250100 <kmem_cache_free>
    0.00 :   ffff8000102d8418:       ldp     x27, x28, [x29, #80]
    0.00 :   ffff8000102d841c:       b       ffff8000102d8150 <io_submit_one+0x200>
         :                      sign_extend64():
    0.70 :   ffff8000102d8420:       sbfx    x0, x2, #0, #56
         :                      get_current():
    0.43 :   ffff8000102d8424:       mrs     x1, sp_el0
         :                      __uaccess_mask_ptr():
         :                      "r" (untagged_addr(ptr))
    0.00 :   ffff8000102d8428:       and     x0, x2, x0
         :                      asm volatile(
    0.00 :   ffff8000102d842c:       ldr     x3, [x1, #8]
    0.00 :   ffff8000102d8430:       bics    xzr, x0, x3
    0.00 :   ffff8000102d8434:       csel    x1, x2, xzr, eq  // eq = none
         :                      csdb();
    0.00 :   ffff8000102d8438:       csdb
         :                      uaccess_enable_not_uao():
         :                      __uaccess_enable(ARM64_ALT_PAN_NOT_UAO);
    0.59 :   ffff8000102d843c:       nop
         :                      __io_submit_one():
         :                      if (unlikely(put_user(KIOCB_KEY, &user_iocb->aio_key))) {
    0.81 :   ffff8000102d8440:       mov     w0, #0x0                        // #0
    0.05 :   ffff8000102d8444:       str     w0, [x1]
         :                      uaccess_disable_not_uao():
         :                      __uaccess_disable(ARM64_ALT_PAN_NOT_UAO);
    0.00 :   ffff8000102d8448:       nop
         :                      __io_submit_one():
    0.38 :   ffff8000102d844c:       cbnz    w0, ffff8000102d8124 <io_submit_one+0x1d4>
         :                      req->ki_res.data = iocb->aio_data;
    0.05 :   ffff8000102d8450:       ldr     x1, [x29, #136]
         :                      switch (iocb->aio_lio_opcode) {
    0.87 :   ffff8000102d8454:       ldrh    w0, [x29, #152]
         :                      req->ki_res.obj = (u64)(unsigned long)user_iocb;
    0.05 :   ffff8000102d8458:       stp     x1, x20, [x19, #112]
         :                      req->ki_res.res2 = 0;
    0.21 :   ffff8000102d845c:       stp     xzr, xzr, [x19, #128]
         :                      switch (iocb->aio_lio_opcode) {
    0.00 :   ffff8000102d8460:       cmp     w0, #0x3
    0.00 :   ffff8000102d8464:       b.eq    ffff8000102d87c0 <io_submit_one+0x870>  // b.none
    0.16 :   ffff8000102d8468:       b.hi    ffff8000102d8530 <io_submit_one+0x5e0>  // b.pmore
    0.96 :   ffff8000102d846c:       cmp     w0, #0x1
    0.00 :   ffff8000102d8470:       b.eq    ffff8000102d87a4 <io_submit_one+0x854>  // b.none
    0.00 :   ffff8000102d8474:       b.hi    ffff8000102d878c <io_submit_one+0x83c>  // b.pmore
         :                      return aio_read(&req->rw, iocb, false, compat);
    0.16 :   ffff8000102d8478:       mov     w3, w23
    0.00 :   ffff8000102d847c:       mov     w2, #0x0                        // #0
    0.16 :   ffff8000102d8480:       add     x1, x29, #0x88
    0.00 :   ffff8000102d8484:       mov     x0, x19
    0.00 :   ffff8000102d8488:       bl      ffff8000102d5980 <aio_read>
    3.83 :   ffff8000102d848c:       mov     w20, w0
    0.00 :   ffff8000102d8490:       b       ffff8000102d8128 <io_submit_one+0x1d8>
         :                      __read_once_size():
    0.00 :   ffff8000102d8494:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102d8498:       cbz     x0, ffff8000102d829c <io_submit_one+0x34c>
    0.00 :   ffff8000102d849c:       b       ffff8000102d82a0 <io_submit_one+0x350>
    0.00 :   ffff8000102d84a0:       str     x27, [x29, #80]
         :                      user_refill_reqs_available():
         :                      ring = kmap_atomic(ctx->ring_pages[0]);
    0.00 :   ffff8000102d84a4:       ldr     x0, [x22, #168]
         :                      __read_once_size():
    0.00 :   ffff8000102d84a8:       ldr     w1, [x25, #16]
         :                      user_refill_reqs_available():
    0.00 :   ffff8000102d84ac:       ldr     x0, [x0]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff8000102d84b0:       add     w1, w1, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d84b4:       str     w1, [x25, #16]
         :                      pagefault_disabled_inc():
         :                      current->pagefault_disabled++;
    0.00 :   ffff8000102d84b8:       ldr     w1, [x25, #2448]
    0.00 :   ffff8000102d84bc:       add     w1, w1, #0x1
    0.00 :   ffff8000102d84c0:       str     w1, [x25, #2448]
         :                      lowmem_page_address():
    0.00 :   ffff8000102d84c4:       mov     x1, #0x200000                   // #2097152
    0.00 :   ffff8000102d84c8:       movk    x1, #0x200, lsl #32
    0.00 :   ffff8000102d84cc:       add     x0, x0, x1
         :                      user_refill_reqs_available():
         :                      head = ring->head;
    0.00 :   ffff8000102d84d0:       mov     x1, #0xffff000000000000         // #-281474976710656
         :                      lowmem_page_address():
    0.00 :   ffff8000102d84d4:       lsr     x0, x0, #6
         :                      user_refill_reqs_available():
    0.00 :   ffff8000102d84d8:       add     x0, x1, x0, lsl #12
    0.00 :   ffff8000102d84dc:       ldr     w27, [x0, #8]
         :                      pagefault_disabled_dec():
         :                      current->pagefault_disabled--;
    0.00 :   ffff8000102d84e0:       ldr     w0, [x25, #2448]
    0.00 :   ffff8000102d84e4:       sub     w0, w0, #0x1
    0.00 :   ffff8000102d84e8:       str     w0, [x25, #2448]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d84ec:       ldr     x0, [x25, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000102d84f0:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d84f4:       str     w0, [x25, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102d84f8:       cbnz    x0, ffff8000102d87fc <io_submit_one+0x8ac>
         :                      __kunmap_atomic():
    0.00 :   ffff8000102d84fc:       bl      ffff800010cad640 <preempt_schedule>
         :                      user_refill_reqs_available():
         :                      refill_reqs_available(ctx, head, ctx->tail);
    0.00 :   ffff8000102d8500:       ldr     w2, [x22, #448]
    0.00 :   ffff8000102d8504:       mov     w1, w27
    0.00 :   ffff8000102d8508:       mov     x0, x22
    0.00 :   ffff8000102d850c:       bl      ffff8000102d5fa0 <refill_reqs_available>
    0.00 :   ffff8000102d8510:       ldr     x27, [x29, #80]
    0.00 :   ffff8000102d8514:       b       ffff8000102d81a8 <io_submit_one+0x258>
         :                      __kunmap_atomic():
    0.00 :   ffff8000102d8518:       bl      ffff800010cad640 <preempt_schedule>
    0.00 :   ffff8000102d851c:       b       ffff8000102d831c <io_submit_one+0x3cc>
         :                      refcount_sub_and_test():
         :                      refcount_warn_saturate(r, REFCOUNT_SUB_UAF);
    0.00 :   ffff8000102d8520:       mov     w1, #0x3                        // #3
    0.00 :   ffff8000102d8524:       mov     x0, x25
    0.00 :   ffff8000102d8528:       bl      ffff80001048ba28 <refcount_warn_saturate>
    0.00 :   ffff8000102d852c:       b       ffff8000102d8150 <io_submit_one+0x200>
         :                      __io_submit_one():
         :                      switch (iocb->aio_lio_opcode) {
    0.00 :   ffff8000102d8530:       cmp     w0, #0x7
    0.00 :   ffff8000102d8534:       b.eq    ffff8000102d8770 <io_submit_one+0x820>  // b.none
    0.00 :   ffff8000102d8538:       cmp     w0, #0x8
    0.00 :   ffff8000102d853c:       b.eq    ffff8000102d8754 <io_submit_one+0x804>  // b.none
         :                      return -EINVAL;
    0.00 :   ffff8000102d8540:       mov     w20, #0xffffffea                // #-22
         :                      switch (iocb->aio_lio_opcode) {
    0.00 :   ffff8000102d8544:       cmp     w0, #0x5
    0.00 :   ffff8000102d8548:       b.ne    ffff8000102d8128 <io_submit_one+0x1d8>  // b.any
         :                      aio_poll():
         :                      if ((u16)iocb->aio_buf != iocb->aio_buf)
    0.00 :   ffff8000102d854c:       ldr     x2, [x29, #160]
    0.00 :   ffff8000102d8550:       stp     x27, x28, [x29, #80]
         :                      struct kioctx *ctx = aiocb->ki_ctx;
    0.00 :   ffff8000102d8554:       ldr     x20, [x19, #96]
         :                      if ((u16)iocb->aio_buf != iocb->aio_buf)
    0.00 :   ffff8000102d8558:       tst     x2, #0xffffffffffff0000
    0.00 :   ffff8000102d855c:       b.ne    ffff8000102d8b44 <io_submit_one+0xbf4>  // b.any
         :                      if (iocb->aio_offset || iocb->aio_nbytes || iocb->aio_rw_flags)
    0.00 :   ffff8000102d8560:       ldr     x0, [x29, #176]
    0.00 :   ffff8000102d8564:       cbnz    x0, ffff8000102d8b44 <io_submit_one+0xbf4>
    0.00 :   ffff8000102d8568:       ldr     x0, [x29, #168]
    0.00 :   ffff8000102d856c:       cbnz    x0, ffff8000102d8b44 <io_submit_one+0xbf4>
    0.00 :   ffff8000102d8570:       ldr     w0, [x29, #148]
    0.00 :   ffff8000102d8574:       cbnz    w0, ffff8000102d8b44 <io_submit_one+0xbf4>
         :                      INIT_WORK(&req->work, aio_poll_complete_work);
    0.00 :   ffff8000102d8578:       add     x3, x19, #0x48
         :                      req->events = demangle_poll(iocb->aio_buf) | EPOLLERR | EPOLLHUP;
    0.00 :   ffff8000102d857c:       mov     w1, #0x27ff                     // #10239
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff8000102d8580:       str     x3, [x19, #72]
         :                      aio_poll():
    0.00 :   ffff8000102d8584:       and     w1, w1, w2
         :                      mask = vfs_poll(req->file, &apt.pt) & req->events;
    0.00 :   ffff8000102d8588:       ldr     x0, [x19]
         :                      INIT_WORK(&req->work, aio_poll_complete_work);
    0.00 :   ffff8000102d858c:       adrp    x2, ffff8000102d7000 <aio_fsync_work+0x290>
    0.00 :   ffff8000102d8590:       add     x2, x2, #0x468
         :                      req->head = NULL;
    0.00 :   ffff8000102d8594:       str     xzr, [x19, #8]
         :                      req->done = false;
    0.00 :   ffff8000102d8598:       strb    wzr, [x19, #20]
         :                      req->events = demangle_poll(iocb->aio_buf) | EPOLLERR | EPOLLHUP;
    0.00 :   ffff8000102d859c:       orr     w1, w1, #0x18
         :                      INIT_LIST_HEAD(&req->wait.entry);
    0.00 :   ffff8000102d85a0:       add     x23, x19, #0x30
         :                      req->events = demangle_poll(iocb->aio_buf) | EPOLLERR | EPOLLHUP;
    0.00 :   ffff8000102d85a4:       str     w1, [x19, #16]
         :                      req->cancelled = false;
    0.00 :   ffff8000102d85a8:       strb    wzr, [x19, #21]
         :                      init_waitqueue_func_entry():
         :                      wq_entry->flags         = 0;
    0.00 :   ffff8000102d85ac:       str     wzr, [x19, #24]
         :                      wq_entry->private       = NULL;
    0.00 :   ffff8000102d85b0:       str     xzr, [x19, #32]
         :                      __write_once_size():
    0.00 :   ffff8000102d85b4:       str     x23, [x19, #48]
         :                      INIT_LIST_HEAD():
    0.00 :   ffff8000102d85b8:       str     x23, [x19, #56]
         :                      aio_poll():
         :                      INIT_WORK(&req->work, aio_poll_complete_work);
    0.00 :   ffff8000102d85bc:       stp     x3, x2, [x19, #80]
    0.00 :   ffff8000102d85c0:       mov     x3, #0xfffffffe0                // #68719476704
         :                      init_waitqueue_func_entry():
         :                      wq_entry->func          = func;
    0.00 :   ffff8000102d85c4:       adrp    x2, ffff8000102d8000 <io_submit_one+0xb0>
         :                      aio_poll():
    0.00 :   ffff8000102d85c8:       str     x3, [x19, #64]
         :                      init_waitqueue_func_entry():
    0.00 :   ffff8000102d85cc:       add     x2, x2, #0xbd8
    0.00 :   ffff8000102d85d0:       str     x2, [x19, #40]
         :                      aio_poll():
         :                      apt.pt._qproc = aio_poll_queue_proc;
    0.00 :   ffff8000102d85d4:       adrp    x2, ffff8000102d5000 <eventfd_ctx_fileget.part.7+0x20>
    0.00 :   ffff8000102d85d8:       add     x2, x2, #0x6c8
    0.00 :   ffff8000102d85dc:       str     x2, [x29, #104]
         :                      apt.error = -EINVAL; /* same as no support for IOCB_CMD_POLL */
    0.00 :   ffff8000102d85e0:       mov     w3, #0xffffffea                 // #-22
         :                      vfs_poll():
         :                      return file->f_op->poll;
         :                      }
         :
         :                      static inline __poll_t vfs_poll(struct file *file, struct poll_table_struct *pt)
         :                      {
         :                      if (unlikely(!file->f_op->poll))
    0.00 :   ffff8000102d85e4:       ldr     x2, [x0, #40]
         :                      aio_poll():
         :                      apt.pt._key = req->events;
    0.00 :   ffff8000102d85e8:       str     w1, [x29, #112]
         :                      apt.iocb = aiocb;
    0.00 :   ffff8000102d85ec:       str     x19, [x29, #120]
         :                      vfs_poll():
    0.00 :   ffff8000102d85f0:       ldr     x2, [x2, #72]
         :                      aio_poll():
         :                      apt.error = -EINVAL; /* same as no support for IOCB_CMD_POLL */
    0.00 :   ffff8000102d85f4:       str     w3, [x29, #128]
         :                      vfs_poll():
    0.00 :   ffff8000102d85f8:       cbz     x2, ffff8000102d8b50 <io_submit_one+0xc00>
         :                      return DEFAULT_POLLMASK;
         :                      return file->f_op->poll(file, pt);
    0.00 :   ffff8000102d85fc:       add     x1, x29, #0x68
    0.00 :   ffff8000102d8600:       blr     x2
    0.00 :   ffff8000102d8604:       ldr     w1, [x19, #16]
         :                      spin_lock_irq():
         :                      raw_spin_lock_irq(&lock->rlock);
    0.00 :   ffff8000102d8608:       add     x27, x20, #0x140
         :                      aio_poll():
         :                      mask = vfs_poll(req->file, &apt.pt) & req->events;
    0.00 :   ffff8000102d860c:       and     w28, w0, w1
         :                      spin_lock_irq():
    0.00 :   ffff8000102d8610:       mov     x0, x27
    0.00 :   ffff8000102d8614:       bl      ffff800010cb2cd0 <_raw_spin_lock_irq>
         :                      aio_poll():
         :                      if (likely(req->head)) {
    0.00 :   ffff8000102d8618:       ldr     x0, [x19, #8]
    0.00 :   ffff8000102d861c:       cbz     x0, ffff8000102d8830 <io_submit_one+0x8e0>
         :                      spin_lock():
         :                      raw_spin_lock(&lock->rlock);
    0.00 :   ffff8000102d8620:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d8624:       ldr     x0, [x19, #48]
         :                      aio_poll():
         :                      if (unlikely(list_empty(&req->wait.entry))) {
    0.00 :   ffff8000102d8628:       cmp     x23, x0
    0.00 :   ffff8000102d862c:       b.eq    ffff8000102d8b58 <io_submit_one+0xc08>  // b.none
         :                      if (mask || apt.error) {
    0.00 :   ffff8000102d8630:       cbnz    w28, ffff8000102d8814 <io_submit_one+0x8c4>
    0.00 :   ffff8000102d8634:       ldr     w0, [x29, #128]
    0.00 :   ffff8000102d8638:       cbnz    w0, ffff8000102d8814 <io_submit_one+0x8c4>
         :                      } else if (!req->done) { /* actually waiting for an event */
    0.00 :   ffff8000102d863c:       ldrb    w0, [x19, #20]
    0.00 :   ffff8000102d8640:       cbnz    w0, ffff8000102d8b78 <io_submit_one+0xc28>
         :                      list_add_tail():
         :                      * Insert a new entry before the specified head.
         :                      * This is useful for implementing queues.
         :                      */
         :                      static inline void list_add_tail(struct list_head *new, struct list_head *head)
         :                      {
         :                      __list_add(new, head->prev, head);
    0.00 :   ffff8000102d8644:       add     x2, x20, #0x200
    0.00 :   ffff8000102d8648:       ldr     x1, [x20, #336]
         :                      aio_poll():
         :                      list_add_tail(&aiocb->ki_list, &ctx->active_reqs);
    0.00 :   ffff8000102d864c:       add     x20, x20, #0x148
         :                      aiocb->ki_cancel = aio_poll_cancel;
    0.00 :   ffff8000102d8650:       adrp    x0, ffff8000102d5000 <eventfd_ctx_fileget.part.7+0x20>
    0.00 :   ffff8000102d8654:       add     x0, x0, #0x658
         :                      __list_add():
         :                      next->prev = new;
    0.00 :   ffff8000102d8658:       stur    x26, [x2, #-176]
         :                      new->prev = prev;
    0.00 :   ffff8000102d865c:       stp     x20, x1, [x19, #144]
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff8000102d8660:       str     x26, [x1]
         :                      aio_poll():
    0.00 :   ffff8000102d8664:       str     x0, [x19, #104]
         :                      spin_unlock():
         :                      raw_spin_unlock(&lock->rlock);
    0.00 :   ffff8000102d8668:       ldr     x0, [x19, #8]
    0.00 :   ffff8000102d866c:       bl      ffff800010cb2408 <_raw_spin_unlock>
         :                      spin_unlock_irq():
         :                      raw_spin_unlock_irq(&lock->rlock);
    0.00 :   ffff8000102d8670:       mov     x0, x27
    0.00 :   ffff8000102d8674:       bl      ffff800010cb2bf0 <_raw_spin_unlock_irq>
    0.00 :   ffff8000102d8678:       ldp     x27, x28, [x29, #80]
         :                      aio_poll():
         :                      return apt.error;
    0.00 :   ffff8000102d867c:       ldr     w20, [x29, #128]
    0.00 :   ffff8000102d8680:       b       ffff8000102d8128 <io_submit_one+0x1d8>
         :                      io_submit_one():
         :                      return -EINVAL;
    0.00 :   ffff8000102d8684:       mov     w20, #0xffffffea                // #-22
    0.00 :   ffff8000102d8688:       ldr     x25, [x29, #64]
    0.00 :   ffff8000102d868c:       b       ffff8000102d8160 <io_submit_one+0x210>
    0.00 :   ffff8000102d8690:       ldr     x25, [x29, #64]
    0.00 :   ffff8000102d8694:       nop
         :                      return -EFAULT;
    0.00 :   ffff8000102d8698:       mov     w20, #0xfffffff2                // #-14
    0.00 :   ffff8000102d869c:       b       ffff8000102d8160 <io_submit_one+0x210>
         :                      return -EAGAIN;
    0.00 :   ffff8000102d86a0:       mov     w20, #0xfffffff5                // #-11
    0.00 :   ffff8000102d86a4:       ldr     x19, [x29, #16]
    0.00 :   ffff8000102d86a8:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff8000102d86ac:       b       ffff8000102d8160 <io_submit_one+0x210>
         :                      __io_submit_one():
         :                      return -EBADF;
    0.00 :   ffff8000102d86b0:       mov     w20, #0xfffffff7                // #-9
    0.00 :   ffff8000102d86b4:       b       ffff8000102d8128 <io_submit_one+0x1d8>
         :                      iocb_destroy():
         :                      if (iocb->ki_eventfd)
    0.00 :   ffff8000102d86b8:       ldr     x0, [x19, #168]
    0.00 :   ffff8000102d86bc:       cbz     x0, ffff8000102d86c4 <io_submit_one+0x774>
         :                      eventfd_ctx_put(iocb->ki_eventfd);
    0.00 :   ffff8000102d86c0:       bl      ffff8000102d4f38 <eventfd_ctx_put>
         :                      if (iocb->ki_filp)
    0.00 :   ffff8000102d86c4:       ldr     x0, [x19]
    0.00 :   ffff8000102d86c8:       cbz     x0, ffff8000102d86d0 <io_submit_one+0x780>
         :                      fput(iocb->ki_filp);
    0.00 :   ffff8000102d86cc:       bl      ffff80001027bc88 <fput>
         :                      percpu_ref_put(&iocb->ki_ctx->reqs);
    0.00 :   ffff8000102d86d0:       ldr     x23, [x19, #96]
         :                      rcu_read_lock():
         :                      __rcu_read_lock();
    0.00 :   ffff8000102d86d4:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d86d8:       ldr     x0, [x23, #72]
         :                      __ref_is_percpu():
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff8000102d86dc:       tst     x0, #0x3
    0.00 :   ffff8000102d86e0:       b.ne    ffff8000102d8b08 <io_submit_one+0xbb8>  // b.any
         :                      get_current():
    0.00 :   ffff8000102d86e4:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.00 :   ffff8000102d86e8:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff8000102d86ec:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d86f0:       str     w2, [x1, #16]
         :                      __percpu_add_case_64():
    0.00 :   ffff8000102d86f4:       mov     x3, #0xffffffffffffffff         // #-1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000102d86f8:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff8000102d86fc:       add     x0, x0, x2
    0.00 :   ffff8000102d8700:       ldxr    x5, [x0]
    0.00 :   ffff8000102d8704:       add     x5, x5, x3
    0.00 :   ffff8000102d8708:       stxr    w4, x5, [x0]
    0.00 :   ffff8000102d870c:       cbnz    w4, ffff8000102d8700 <io_submit_one+0x7b0>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d8710:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000102d8714:       add     x0, x0, x3
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d8718:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102d871c:       cbnz    x0, ffff8000102d8808 <io_submit_one+0x8b8>
         :                      percpu_ref_put_many():
         :                      this_cpu_sub(*percpu_count, nr);
    0.00 :   ffff8000102d8720:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      iocb_destroy():
         :                      kmem_cache_free(kiocb_cachep, iocb);
    0.00 :   ffff8000102d8724:       add     x24, x24, #0xf58
         :                      rcu_read_unlock():
         :                      __rcu_read_unlock();
    0.00 :   ffff8000102d8728:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      iocb_destroy():
    0.00 :   ffff8000102d872c:       mov     x1, x19
    0.00 :   ffff8000102d8730:       ldr     x0, [x24, #8]
    0.00 :   ffff8000102d8734:       bl      ffff800010250100 <kmem_cache_free>
         :                      io_submit_one():
         :                      put_reqs_available(ctx, 1);
    0.00 :   ffff8000102d8738:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000102d873c:       mov     x0, x22
    0.00 :   ffff8000102d8740:       bl      ffff8000102d5f18 <put_reqs_available>
    0.00 :   ffff8000102d8744:       ldr     x19, [x29, #16]
    0.00 :   ffff8000102d8748:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff8000102d874c:       ldr     x26, [x29, #72]
    0.00 :   ffff8000102d8750:       b       ffff8000102d8160 <io_submit_one+0x210>
         :                      __io_submit_one():
         :                      return aio_write(&req->rw, iocb, true, compat);
    0.00 :   ffff8000102d8754:       mov     w3, w23
    0.00 :   ffff8000102d8758:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000102d875c:       add     x1, x29, #0x88
    0.00 :   ffff8000102d8760:       mov     x0, x19
    0.00 :   ffff8000102d8764:       bl      ffff8000102d5af0 <aio_write>
    0.00 :   ffff8000102d8768:       mov     w20, w0
    0.00 :   ffff8000102d876c:       b       ffff8000102d8128 <io_submit_one+0x1d8>
         :                      return aio_read(&req->rw, iocb, true, compat);
    0.00 :   ffff8000102d8770:       mov     w3, w23
    0.00 :   ffff8000102d8774:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000102d8778:       add     x1, x29, #0x88
    0.00 :   ffff8000102d877c:       mov     x0, x19
    0.00 :   ffff8000102d8780:       bl      ffff8000102d5980 <aio_read>
    0.00 :   ffff8000102d8784:       mov     w20, w0
    0.00 :   ffff8000102d8788:       b       ffff8000102d8128 <io_submit_one+0x1d8>
         :                      return aio_fsync(&req->fsync, iocb, false);
    0.00 :   ffff8000102d878c:       mov     w2, #0x0                        // #0
    0.00 :   ffff8000102d8790:       add     x1, x29, #0x88
    0.00 :   ffff8000102d8794:       mov     x0, x19
    0.00 :   ffff8000102d8798:       bl      ffff8000102d5c88 <aio_fsync>
    0.00 :   ffff8000102d879c:       mov     w20, w0
    0.00 :   ffff8000102d87a0:       b       ffff8000102d8128 <io_submit_one+0x1d8>
         :                      return aio_write(&req->rw, iocb, false, compat);
    0.00 :   ffff8000102d87a4:       mov     w3, w23
    0.00 :   ffff8000102d87a8:       mov     w2, #0x0                        // #0
    0.00 :   ffff8000102d87ac:       add     x1, x29, #0x88
    0.00 :   ffff8000102d87b0:       mov     x0, x19
    0.00 :   ffff8000102d87b4:       bl      ffff8000102d5af0 <aio_write>
    0.00 :   ffff8000102d87b8:       mov     w20, w0
    0.00 :   ffff8000102d87bc:       b       ffff8000102d8128 <io_submit_one+0x1d8>
         :                      return aio_fsync(&req->fsync, iocb, true);
    0.00 :   ffff8000102d87c0:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000102d87c4:       add     x1, x29, #0x88
    0.00 :   ffff8000102d87c8:       mov     x0, x19
    0.00 :   ffff8000102d87cc:       bl      ffff8000102d5c88 <aio_fsync>
    0.00 :   ffff8000102d87d0:       mov     w20, w0
    0.00 :   ffff8000102d87d4:       b       ffff8000102d8128 <io_submit_one+0x1d8>
         :                      percpu_ref_get_many():
         :                      atomic_long_add(nr, &ref->count);
    0.00 :   ffff8000102d87d8:       add     x1, x22, #0x40
         :                      arch_static_branch_jump():
    0.00 :   ffff8000102d87dc:       b       ffff8000102d87f0 <io_submit_one+0x8a0>
    0.00 :   ffff8000102d87e0:       b       ffff8000102d87f0 <io_submit_one+0x8a0>
         :                      __lse_atomic64_add():
         :                      }
         :
         :                      ATOMIC64_OP(andnot, stclr)
         :                      ATOMIC64_OP(or, stset)
         :                      ATOMIC64_OP(xor, steor)
         :                      ATOMIC64_OP(add, stadd)
    0.00 :   ffff8000102d87e4:       mov     x0, #0x1                        // #1
    0.00 :   ffff8000102d87e8:       stadd   x0, [x1]
    0.00 :   ffff8000102d87ec:       b       ffff8000102d8090 <io_submit_one+0x140>
         :                      __ll_sc_atomic64_add():
         :                      ATOMIC64_FETCH_OP (, dmb ish,  , l, "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(add, add, I)
    0.00 :   ffff8000102d87f0:       add     x2, x22, #0x40
    0.00 :   ffff8000102d87f4:       b       ffff8000102da680 <__arm64_compat_sys_io_pgetevents_time64+0x3c0>
    0.00 :   ffff8000102d87f8:       b       ffff8000102d8090 <io_submit_one+0x140>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d87fc:       ldr     x0, [x25, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102d8800:       cbz     x0, ffff8000102d84fc <io_submit_one+0x5ac>
    0.00 :   ffff8000102d8804:       b       ffff8000102d8500 <io_submit_one+0x5b0>
         :                      __read_once_size():
    0.00 :   ffff8000102d8808:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102d880c:       cbz     x0, ffff8000102d8720 <io_submit_one+0x7d0>
    0.00 :   ffff8000102d8810:       b       ffff8000102d8724 <io_submit_one+0x7d4>
         :                      __list_del_entry():
         :                      static inline void __list_del_entry(struct list_head *entry)
         :                      {
         :                      if (!__list_del_entry_valid(entry))
         :                      return;
         :
         :                      __list_del(entry->prev, entry->next);
    0.00 :   ffff8000102d8814:       ldp     x1, x0, [x19, #48]
         :                      __list_del():
         :                      next->prev = prev;
    0.00 :   ffff8000102d8818:       str     x0, [x1, #8]
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff8000102d881c:       str     x1, [x0]
    0.00 :   ffff8000102d8820:       str     x23, [x19, #48]
         :                      spin_unlock():
         :                      raw_spin_unlock(&lock->rlock);
    0.00 :   ffff8000102d8824:       ldr     x0, [x19, #8]
         :                      INIT_LIST_HEAD():
         :                      list->prev = list;
    0.00 :   ffff8000102d8828:       str     x23, [x19, #56]
         :                      spin_unlock():
    0.00 :   ffff8000102d882c:       bl      ffff800010cb2408 <_raw_spin_unlock>
         :                      aio_poll():
         :                      if (mask) { /* no async, we'd stolen it */
    0.00 :   ffff8000102d8830:       cbz     w28, ffff8000102d8670 <io_submit_one+0x720>
         :                      mangle_poll():
         :                      {
         :                      __u16 v = (__force __u16)val;
         :                      #define M(X) __MAP(v, (__force __u16)EPOLL##X, POLL##X)
         :                      return M(IN) | M(OUT) | M(PRI) | M(ERR) | M(NVAL) |
         :                      M(RDNORM) | M(RDBAND) | M(WRNORM) | M(WRBAND) |
         :                      M(HUP) | M(RDHUP) | M(MSG);
    0.00 :   ffff8000102d8834:       mov     w0, #0x27ff                     // #10239
    0.00 :   ffff8000102d8838:       and     w0, w28, w0
         :                      aio_poll():
         :                      aiocb->ki_res.res = mangle_poll(mask);
    0.00 :   ffff8000102d883c:       and     x0, x0, #0xffff
         :                      apt.error = 0;
    0.00 :   ffff8000102d8840:       str     wzr, [x29, #128]
         :                      aiocb->ki_res.res = mangle_poll(mask);
    0.00 :   ffff8000102d8844:       str     x0, [x19, #128]
         :                      spin_unlock_irq():
         :                      raw_spin_unlock_irq(&lock->rlock);
    0.00 :   ffff8000102d8848:       mov     x0, x27
    0.00 :   ffff8000102d884c:       bl      ffff800010cb2bf0 <_raw_spin_unlock_irq>
         :                      arch_static_branch_jump():
    0.00 :   ffff8000102d8850:       b       ffff8000102d88c4 <io_submit_one+0x974>
    0.00 :   ffff8000102d8854:       b       ffff8000102d88c4 <io_submit_one+0x974>
         :                      __lse_atomic_fetch_sub_release():
         :                      ATOMIC_FETCH_OP_SUB(_release,  l, "memory")
    0.00 :   ffff8000102d8858:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000102d885c:       add     x1, x19, #0xa0
    0.00 :   ffff8000102d8860:       neg     w0, w0
    0.00 :   ffff8000102d8864:       ldaddl  w0, w0, [x1]
         :                      refcount_sub_and_test():
         :                      if (old == i) {
    0.00 :   ffff8000102d8868:       cmp     w0, #0x1
    0.00 :   ffff8000102d886c:       b.eq    ffff8000102d88d4 <io_submit_one+0x984>  // b.none
         :                      if (unlikely(old < 0 || old - i < 0))
    0.00 :   ffff8000102d8870:       cmp     w0, #0x0
    0.00 :   ffff8000102d8874:       b.gt    ffff8000102d8678 <io_submit_one+0x728>
         :                      refcount_warn_saturate(r, REFCOUNT_SUB_UAF);
    0.00 :   ffff8000102d8878:       mov     w1, #0x3                        // #3
    0.00 :   ffff8000102d887c:       mov     x0, x25
    0.00 :   ffff8000102d8880:       bl      ffff80001048ba28 <refcount_warn_saturate>
    0.00 :   ffff8000102d8884:       b       ffff8000102d8678 <io_submit_one+0x728>
         :                      iocb_destroy():
         :                      percpu_ref_put(&iocb->ki_ctx->reqs);
    0.00 :   ffff8000102d8888:       add     x0, x23, #0x40
         :                      arch_static_branch_jump():
    0.00 :   ffff8000102d888c:       b       ffff8000102d88b4 <io_submit_one+0x964>
    0.00 :   ffff8000102d8890:       b       ffff8000102d88b4 <io_submit_one+0x964>
         :                      __lse_atomic64_sub_return():
         :                      }
         :
         :                      ATOMIC64_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC64_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff8000102d8894:       mov     x1, #0x1                        // #1
    0.00 :   ffff8000102d8898:       neg     x1, x1
    0.00 :   ffff8000102d889c:       ldaddal x1, x2, [x0]
    0.00 :   ffff8000102d88a0:       add     x1, x1, x2
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff8000102d88a4:       cbnz    x1, ffff8000102d8404 <io_submit_one+0x4b4>
         :                      ref->release(ref);
    0.00 :   ffff8000102d88a8:       ldr     x1, [x0, #16]
    0.00 :   ffff8000102d88ac:       blr     x1
    0.00 :   ffff8000102d88b0:       b       ffff8000102d8404 <io_submit_one+0x4b4>
         :                      __ll_sc_atomic64_sub_return():
         :                      ATOMIC64_OPS(sub, sub, J)
    0.00 :   ffff8000102d88b4:       mov     x2, #0x1                        // #1
    0.00 :   ffff8000102d88b8:       add     x4, x23, #0x40
    0.00 :   ffff8000102d88bc:       b       ffff8000102da698 <__arm64_compat_sys_io_pgetevents_time64+0x3d8>
    0.00 :   ffff8000102d88c0:       b       ffff8000102d88a4 <io_submit_one+0x954>
         :                      __ll_sc_atomic_fetch_sub_release():
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff8000102d88c4:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000102d88c8:       add     x4, x19, #0xa0
    0.00 :   ffff8000102d88cc:       b       ffff8000102da6b4 <__arm64_compat_sys_io_pgetevents_time64+0x3f4>
    0.00 :   ffff8000102d88d0:       b       ffff8000102d8868 <io_submit_one+0x918>
         :                      refcount_sub_and_test():
         :                      smp_acquire__after_ctrl_dep();
    0.00 :   ffff8000102d88d4:       dmb     ishld
         :                      aio_complete():
         :                      struct kioctx   *ctx = iocb->ki_ctx;
    0.00 :   ffff8000102d88d8:       ldr     x20, [x19, #96]
         :                      spinlock_check():
         :                      return &lock->rlock;
    0.00 :   ffff8000102d88dc:       add     x26, x20, #0x1c8
         :                      aio_complete():
         :                      spin_lock_irqsave(&ctx->completion_lock, flags);
    0.00 :   ffff8000102d88e0:       mov     x0, x26
    0.00 :   ffff8000102d88e4:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
         :                      pos = tail + AIO_EVENTS_OFFSET;
    0.00 :   ffff8000102d88e8:       ldr     w2, [x20, #448]
         :                      spin_lock_irqsave(&ctx->completion_lock, flags);
    0.00 :   ffff8000102d88ec:       mov     x27, x0
         :                      if (++tail >= ctx->nr_events)
    0.00 :   ffff8000102d88f0:       ldr     w4, [x20, #144]
         :                      pos = tail + AIO_EVENTS_OFFSET;
    0.00 :   ffff8000102d88f4:       add     w2, w2, #0x1
         :                      ev_page = kmap_atomic(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);
    0.00 :   ffff8000102d88f8:       ldr     x0, [x20, #168]
         :                      get_current():
    0.00 :   ffff8000102d88fc:       mrs     x1, sp_el0
         :                      aio_complete():
    0.00 :   ffff8000102d8900:       lsr     w28, w2, #7
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d8904:       ldr     w3, [x1, #16]
         :                      aio_complete():
         :                      if (++tail >= ctx->nr_events)
    0.00 :   ffff8000102d8908:       cmp     w2, w4
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff8000102d890c:       add     w3, w3, #0x1
         :                      aio_complete():
    0.00 :   ffff8000102d8910:       csel    w23, w2, wzr, cc  // cc = lo, ul, last
         :                      ev_page = kmap_atomic(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);
    0.00 :   ffff8000102d8914:       ldr     x0, [x0, x28, lsl #3]
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d8918:       str     w3, [x1, #16]
         :                      pagefault_disabled_inc():
         :                      current->pagefault_disabled++;
    0.00 :   ffff8000102d891c:       ldr     w3, [x1, #2448]
    0.00 :   ffff8000102d8920:       add     w3, w3, #0x1
    0.00 :   ffff8000102d8924:       str     w3, [x1, #2448]
         :                      lowmem_page_address():
    0.00 :   ffff8000102d8928:       mov     x3, #0x200000                   // #2097152
         :                      aio_complete():
         :                      event = ev_page + pos % AIO_EVENTS_PER_PAGE;
    0.00 :   ffff8000102d892c:       ubfiz   x4, x2, #5, #7
         :                      lowmem_page_address():
    0.00 :   ffff8000102d8930:       movk    x3, #0x200, lsl #32
    0.00 :   ffff8000102d8934:       add     x0, x0, x3
         :                      aio_complete():
         :                      *event = iocb->ki_res;
    0.00 :   ffff8000102d8938:       mov     x5, #0xffff000000000000         // #-281474976710656
         :                      lowmem_page_address():
    0.00 :   ffff8000102d893c:       lsr     x0, x0, #6
         :                      aio_complete():
    0.00 :   ffff8000102d8940:       ldp     x2, x3, [x19, #112]
    0.00 :   ffff8000102d8944:       add     x0, x4, x0, lsl #12
    0.00 :   ffff8000102d8948:       add     x0, x0, x5
    0.00 :   ffff8000102d894c:       stp     x2, x3, [x0]
    0.00 :   ffff8000102d8950:       ldp     x2, x3, [x19, #128]
    0.00 :   ffff8000102d8954:       stp     x2, x3, [x0, #16]
         :                      pagefault_disabled_dec():
         :                      current->pagefault_disabled--;
    0.00 :   ffff8000102d8958:       ldr     w0, [x1, #2448]
    0.00 :   ffff8000102d895c:       sub     w0, w0, #0x1
    0.00 :   ffff8000102d8960:       str     w0, [x1, #2448]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d8964:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000102d8968:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d896c:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102d8970:       cbz     x0, ffff8000102d8b00 <io_submit_one+0xbb0>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d8974:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102d8978:       cbz     x0, ffff8000102d8b00 <io_submit_one+0xbb0>
         :                      aio_complete():
         :                      flush_dcache_page(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);
    0.00 :   ffff8000102d897c:       ldr     x0, [x20, #168]
    0.00 :   ffff8000102d8980:       ldr     x0, [x0, x28, lsl #3]
    0.00 :   ffff8000102d8984:       bl      ffff8000100a2608 <flush_dcache_page>
         :                      smp_wmb();      /* make event visible before updating tail */
    0.00 :   ffff8000102d8988:       dmb     ishst
         :                      ctx->tail = tail;
    0.00 :   ffff8000102d898c:       str     w23, [x20, #448]
         :                      ring = kmap_atomic(ctx->ring_pages[0]);
    0.00 :   ffff8000102d8990:       ldr     x1, [x20, #168]
         :                      get_current():
    0.00 :   ffff8000102d8994:       mrs     x0, sp_el0
         :                      __read_once_size():
    0.00 :   ffff8000102d8998:       ldr     w2, [x0, #16]
         :                      aio_complete():
    0.00 :   ffff8000102d899c:       ldr     x1, [x1]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff8000102d89a0:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d89a4:       str     w2, [x0, #16]
         :                      pagefault_disabled_inc():
         :                      current->pagefault_disabled++;
    0.00 :   ffff8000102d89a8:       ldr     w2, [x0, #2448]
    0.00 :   ffff8000102d89ac:       add     w2, w2, #0x1
    0.00 :   ffff8000102d89b0:       str     w2, [x0, #2448]
         :                      lowmem_page_address():
    0.00 :   ffff8000102d89b4:       mov     x2, #0x200000                   // #2097152
    0.00 :   ffff8000102d89b8:       movk    x2, #0x200, lsl #32
    0.00 :   ffff8000102d89bc:       add     x1, x1, x2
    0.00 :   ffff8000102d89c0:       mov     x2, #0xffff000000000000         // #-281474976710656
    0.00 :   ffff8000102d89c4:       lsr     x1, x1, #6
    0.00 :   ffff8000102d89c8:       add     x1, x2, x1, lsl #12
         :                      aio_complete():
         :                      head = ring->head;
    0.00 :   ffff8000102d89cc:       ldr     w28, [x1, #8]
         :                      ring->tail = tail;
    0.00 :   ffff8000102d89d0:       str     w23, [x1, #12]
         :                      pagefault_disabled_dec():
         :                      current->pagefault_disabled--;
    0.00 :   ffff8000102d89d4:       ldr     w1, [x0, #2448]
    0.00 :   ffff8000102d89d8:       sub     w1, w1, #0x1
    0.00 :   ffff8000102d89dc:       str     w1, [x0, #2448]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d89e0:       ldr     x1, [x0, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000102d89e4:       sub     x1, x1, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d89e8:       str     w1, [x0, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102d89ec:       cbz     x1, ffff8000102d8af8 <io_submit_one+0xba8>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d89f0:       ldr     x0, [x0, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102d89f4:       cbz     x0, ffff8000102d8af8 <io_submit_one+0xba8>
         :                      aio_complete():
         :                      flush_dcache_page(ctx->ring_pages[0]);
    0.00 :   ffff8000102d89f8:       ldr     x0, [x20, #168]
    0.00 :   ffff8000102d89fc:       ldr     x0, [x0]
    0.00 :   ffff8000102d8a00:       bl      ffff8000100a2608 <flush_dcache_page>
         :                      ctx->completed_events++;
    0.00 :   ffff8000102d8a04:       ldr     w0, [x20, #452]
    0.00 :   ffff8000102d8a08:       add     w0, w0, #0x1
    0.00 :   ffff8000102d8a0c:       str     w0, [x20, #452]
         :                      if (ctx->completed_events > 1)
    0.00 :   ffff8000102d8a10:       cmp     w0, #0x1
    0.00 :   ffff8000102d8a14:       b.ls    ffff8000102d8a28 <io_submit_one+0xad8>  // b.plast
         :                      refill_reqs_available(ctx, head, tail);
    0.00 :   ffff8000102d8a18:       mov     w2, w23
    0.00 :   ffff8000102d8a1c:       mov     w1, w28
    0.00 :   ffff8000102d8a20:       mov     x0, x20
    0.00 :   ffff8000102d8a24:       bl      ffff8000102d5fa0 <refill_reqs_available>
         :                      spin_unlock_irqrestore():
         :                      raw_spin_unlock_irqrestore(&lock->rlock, flags);
    0.00 :   ffff8000102d8a28:       mov     x0, x26
    0.00 :   ffff8000102d8a2c:       mov     x1, x27
    0.00 :   ffff8000102d8a30:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      aio_complete():
         :                      if (iocb->ki_eventfd)
    0.00 :   ffff8000102d8a34:       ldr     x0, [x19, #168]
    0.00 :   ffff8000102d8a38:       cbz     x0, ffff8000102d8a44 <io_submit_one+0xaf4>
         :                      eventfd_signal(iocb->ki_eventfd, 1);
    0.00 :   ffff8000102d8a3c:       mov     x1, #0x1                        // #1
    0.00 :   ffff8000102d8a40:       bl      ffff8000102d49a0 <eventfd_signal>
         :                      smp_mb();
    0.00 :   ffff8000102d8a44:       dmb     ish
         :                      __read_once_size():
    0.00 :   ffff8000102d8a48:       ldr     x1, [x20, #424]
         :                      waitqueue_active():
         :                      return !list_empty(&wq_head->head);
    0.00 :   ffff8000102d8a4c:       add     x2, x20, #0x1a8
    0.00 :   ffff8000102d8a50:       add     x0, x20, #0x1a0
         :                      aio_complete():
         :                      if (waitqueue_active(&ctx->wait))
    0.00 :   ffff8000102d8a54:       cmp     x2, x1
    0.00 :   ffff8000102d8a58:       b.eq    ffff8000102d8a6c <io_submit_one+0xb1c>  // b.none
         :                      wake_up(&ctx->wait);
    0.00 :   ffff8000102d8a5c:       mov     x3, #0x0                        // #0
    0.00 :   ffff8000102d8a60:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000102d8a64:       mov     w1, #0x3                        // #3
    0.00 :   ffff8000102d8a68:       bl      ffff80001012e430 <__wake_up>
         :                      iocb_destroy():
         :                      if (iocb->ki_eventfd)
    0.00 :   ffff8000102d8a6c:       ldr     x0, [x19, #168]
    0.00 :   ffff8000102d8a70:       cbz     x0, ffff8000102d8a78 <io_submit_one+0xb28>
         :                      eventfd_ctx_put(iocb->ki_eventfd);
    0.00 :   ffff8000102d8a74:       bl      ffff8000102d4f38 <eventfd_ctx_put>
         :                      if (iocb->ki_filp)
    0.00 :   ffff8000102d8a78:       ldr     x0, [x19]
    0.00 :   ffff8000102d8a7c:       cbz     x0, ffff8000102d8a84 <io_submit_one+0xb34>
         :                      fput(iocb->ki_filp);
    0.00 :   ffff8000102d8a80:       bl      ffff80001027bc88 <fput>
         :                      percpu_ref_put(&iocb->ki_ctx->reqs);
    0.00 :   ffff8000102d8a84:       ldr     x20, [x19, #96]
         :                      rcu_read_lock():
         :                      __rcu_read_lock();
    0.00 :   ffff8000102d8a88:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
    0.00 :   ffff8000102d8a8c:       ldr     x0, [x20, #72]
         :                      __ref_is_percpu():
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff8000102d8a90:       tst     x0, #0x3
    0.00 :   ffff8000102d8a94:       b.ne    ffff8000102d8b84 <io_submit_one+0xc34>  // b.any
         :                      get_current():
    0.00 :   ffff8000102d8a98:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.00 :   ffff8000102d8a9c:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff8000102d8aa0:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d8aa4:       str     w2, [x1, #16]
         :                      __percpu_add_case_64():
    0.00 :   ffff8000102d8aa8:       mov     x3, #0xffffffffffffffff         // #-1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000102d8aac:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff8000102d8ab0:       add     x0, x0, x2
    0.00 :   ffff8000102d8ab4:       ldxr    x5, [x0]
    0.00 :   ffff8000102d8ab8:       add     x5, x5, x3
    0.00 :   ffff8000102d8abc:       stxr    w4, x5, [x0]
    0.00 :   ffff8000102d8ac0:       cbnz    w4, ffff8000102d8ab4 <io_submit_one+0xb64>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d8ac4:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000102d8ac8:       add     x0, x0, x3
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102d8acc:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102d8ad0:       cbz     x0, ffff8000102d8adc <io_submit_one+0xb8c>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d8ad4:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102d8ad8:       cbnz    x0, ffff8000102d8ae0 <io_submit_one+0xb90>
         :                      percpu_ref_put_many():
         :                      this_cpu_sub(*percpu_count, nr);
    0.00 :   ffff8000102d8adc:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      rcu_read_unlock():
         :                      __rcu_read_unlock();
    0.00 :   ffff8000102d8ae0:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      iocb_destroy():
         :                      kmem_cache_free(kiocb_cachep, iocb);
    0.00 :   ffff8000102d8ae4:       add     x0, x24, #0xf58
    0.00 :   ffff8000102d8ae8:       mov     x1, x19
    0.00 :   ffff8000102d8aec:       ldr     x0, [x0, #8]
    0.00 :   ffff8000102d8af0:       bl      ffff800010250100 <kmem_cache_free>
    0.00 :   ffff8000102d8af4:       b       ffff8000102d8678 <io_submit_one+0x728>
         :                      __kunmap_atomic():
    0.00 :   ffff8000102d8af8:       bl      ffff800010cad640 <preempt_schedule>
    0.00 :   ffff8000102d8afc:       b       ffff8000102d89f8 <io_submit_one+0xaa8>
    0.00 :   ffff8000102d8b00:       bl      ffff800010cad640 <preempt_schedule>
    0.00 :   ffff8000102d8b04:       b       ffff8000102d897c <io_submit_one+0xa2c>
         :                      iocb_destroy():
         :                      percpu_ref_put(&iocb->ki_ctx->reqs);
    0.00 :   ffff8000102d8b08:       add     x0, x23, #0x40
         :                      arch_static_branch_jump():
    0.00 :   ffff8000102d8b0c:       b       ffff8000102d8b34 <io_submit_one+0xbe4>
    0.00 :   ffff8000102d8b10:       b       ffff8000102d8b34 <io_submit_one+0xbe4>
         :                      __lse_atomic64_sub_return():
    0.00 :   ffff8000102d8b14:       mov     x1, #0x1                        // #1
    0.00 :   ffff8000102d8b18:       neg     x1, x1
    0.00 :   ffff8000102d8b1c:       ldaddal x1, x2, [x0]
    0.00 :   ffff8000102d8b20:       add     x1, x1, x2
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff8000102d8b24:       cbnz    x1, ffff8000102d8724 <io_submit_one+0x7d4>
         :                      ref->release(ref);
    0.00 :   ffff8000102d8b28:       ldr     x1, [x0, #16]
    0.00 :   ffff8000102d8b2c:       blr     x1
    0.00 :   ffff8000102d8b30:       b       ffff8000102d8724 <io_submit_one+0x7d4>
         :                      __ll_sc_atomic64_sub_return():
         :                      ATOMIC64_OPS(sub, sub, J)
    0.00 :   ffff8000102d8b34:       mov     x2, #0x1                        // #1
    0.00 :   ffff8000102d8b38:       add     x4, x23, #0x40
    0.00 :   ffff8000102d8b3c:       b       ffff8000102da6cc <__arm64_compat_sys_io_pgetevents_time64+0x40c>
    0.00 :   ffff8000102d8b40:       b       ffff8000102d8b24 <io_submit_one+0xbd4>
         :                      aio_poll():
         :                      return -EINVAL;
    0.00 :   ffff8000102d8b44:       mov     w20, #0xffffffea                // #-22
    0.00 :   ffff8000102d8b48:       ldp     x27, x28, [x29, #80]
    0.00 :   ffff8000102d8b4c:       b       ffff8000102d8128 <io_submit_one+0x1d8>
         :                      vfs_poll():
         :                      return DEFAULT_POLLMASK;
    0.00 :   ffff8000102d8b50:       mov     w0, #0x145                      // #325
    0.00 :   ffff8000102d8b54:       b       ffff8000102d8608 <io_submit_one+0x6b8>
         :                      aio_poll():
         :                      if (apt.error)
    0.00 :   ffff8000102d8b58:       ldr     w0, [x29, #128]
    0.00 :   ffff8000102d8b5c:       cbz     w0, ffff8000102d863c <io_submit_one+0x6ec>
         :                      spin_unlock():
         :                      raw_spin_unlock(&lock->rlock);
    0.00 :   ffff8000102d8b60:       ldr     x0, [x19, #8]
         :                      __write_once_size():
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
    0.00 :   ffff8000102d8b64:       mov     w1, #0x1                        // #1
         :                      aio_poll():
         :                      apt.error = 0;
    0.00 :   ffff8000102d8b68:       str     wzr, [x29, #128]
         :                      __write_once_size():
    0.00 :   ffff8000102d8b6c:       strb    w1, [x19, #21]
         :                      spin_unlock():
    0.00 :   ffff8000102d8b70:       bl      ffff800010cb2408 <_raw_spin_unlock>
    0.00 :   ffff8000102d8b74:       b       ffff8000102d8670 <io_submit_one+0x720>
    0.00 :   ffff8000102d8b78:       ldr     x0, [x19, #8]
    0.00 :   ffff8000102d8b7c:       bl      ffff800010cb2408 <_raw_spin_unlock>
    0.00 :   ffff8000102d8b80:       b       ffff8000102d8670 <io_submit_one+0x720>
         :                      iocb_destroy():
         :                      percpu_ref_put(&iocb->ki_ctx->reqs);
    0.00 :   ffff8000102d8b84:       add     x0, x20, #0x40
         :                      arch_static_branch_jump():
    0.00 :   ffff8000102d8b88:       b       ffff8000102d8bb0 <io_submit_one+0xc60>
    0.00 :   ffff8000102d8b8c:       b       ffff8000102d8bb0 <io_submit_one+0xc60>
         :                      __lse_atomic64_sub_return():
    0.00 :   ffff8000102d8b90:       mov     x1, #0x1                        // #1
    0.00 :   ffff8000102d8b94:       neg     x1, x1
    0.00 :   ffff8000102d8b98:       ldaddal x1, x2, [x0]
    0.00 :   ffff8000102d8b9c:       add     x1, x1, x2
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff8000102d8ba0:       cbnz    x1, ffff8000102d8ae0 <io_submit_one+0xb90>
         :                      ref->release(ref);
    0.00 :   ffff8000102d8ba4:       ldr     x1, [x0, #16]
    0.00 :   ffff8000102d8ba8:       blr     x1
    0.00 :   ffff8000102d8bac:       b       ffff8000102d8ae0 <io_submit_one+0xb90>
         :                      __ll_sc_atomic64_sub_return():
    0.00 :   ffff8000102d8bb0:       mov     x2, #0x1                        // #1
    0.00 :   ffff8000102d8bb4:       add     x4, x20, #0x40
    0.00 :   ffff8000102d8bb8:       b       ffff8000102da6e8 <__arm64_compat_sys_io_pgetevents_time64+0x428>
    0.00 :   ffff8000102d8bbc:       b       ffff8000102d8ba0 <io_submit_one+0xc50>
    0.00 :   ffff8000102d8bc0:       str     x19, [x29, #16]
    0.00 :   ffff8000102d8bc4:       stp     x24, x25, [x29, #56]
    0.00 :   ffff8000102d8bc8:       stp     x26, x27, [x29, #72]
    0.00 :   ffff8000102d8bcc:       str     x28, [x29, #88]
         :                      io_submit_one():
         :                      }
    0.00 :   ffff8000102d8bd0:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (925 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001014c3d8 <irq_thread_fn>:
         :                      irq_thread_fn():
         :                      * preemtible - many of them need to sleep and wait for slow busses to
         :                      * complete.
         :                      */
         :                      static irqreturn_t irq_thread_fn(struct irq_desc *desc,
         :                      struct irqaction *action)
         :                      {
    3.05 :   ffff80001014c3d8:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff80001014c3dc:       mov     x29, sp
   17.57 :   ffff80001014c3e0:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001014c3e4:       mov     x19, x1
    4.42 :   ffff80001014c3e8:       str     x21, [sp, #32]
    0.00 :   ffff80001014c3ec:       mov     x20, x0
         :                      irqreturn_t ret;
         :
         :                      ret = action->thread_fn(action->irq, action->dev_id);
    1.82 :   ffff80001014c3f0:       ldr     x2, [x19, #32]
    1.84 :   ffff80001014c3f4:       ldr     w0, [x1, #56]
    1.86 :   ffff80001014c3f8:       ldr     x1, [x1, #8]
    0.00 :   ffff80001014c3fc:       blr     x2
    2.15 :   ffff80001014c400:       mov     w21, w0
         :                      if (ret == IRQ_HANDLED)
    0.00 :   ffff80001014c404:       cmp     w0, #0x1
    0.00 :   ffff80001014c408:       b.eq    ffff80001014c448 <irq_thread_fn+0x70>  // b.none
         :                      irq_finalize_oneshot():
         :                      if (!(desc->istate & IRQS_ONESHOT) ||
    0.00 :   ffff80001014c40c:       ldr     w0, [x20, #172]
    0.00 :   ffff80001014c410:       tbz     w0, #5, ffff80001014c434 <irq_thread_fn+0x5c>
    0.00 :   ffff80001014c414:       ldr     x1, [x19]
    0.00 :   ffff80001014c418:       adrp    x0, ffff80001014b000 <irq_set_vcpu_affinity+0x88>
    0.00 :   ffff80001014c41c:       add     x0, x0, #0x4e0
    0.00 :   ffff80001014c420:       cmp     x1, x0
    0.00 :   ffff80001014c424:       b.eq    ffff80001014c434 <irq_thread_fn+0x5c>  // b.none
    0.00 :   ffff80001014c428:       mov     x1, x19
    0.00 :   ffff80001014c42c:       mov     x0, x20
    0.00 :   ffff80001014c430:       bl      ffff80001014b860 <irq_finalize_oneshot.part.50>
         :                      irq_thread_fn():
         :                      atomic_inc(&desc->threads_handled);
         :
         :                      irq_finalize_oneshot(desc, action);
         :                      return ret;
         :                      }
    7.85 :   ffff80001014c434:       mov     w0, w21
    1.60 :   ffff80001014c438:       ldr     x21, [sp, #32]
    2.49 :   ffff80001014c43c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001014c440:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001014c444:       ret
         :                      atomic_inc(&desc->threads_handled);
    2.04 :   ffff80001014c448:       add     x1, x20, #0xcc
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff80001014c44c:       b       ffff80001014c460 <irq_thread_fn+0x88>
    0.11 :   ffff80001014c450:       b       ffff80001014c460 <irq_thread_fn+0x88>
         :                      __lse_atomic_add():
         :                      }
         :
         :                      ATOMIC_OP(andnot, stclr)
         :                      ATOMIC_OP(or, stset)
         :                      ATOMIC_OP(xor, steor)
         :                      ATOMIC_OP(add, stadd)
    0.53 :   ffff80001014c454:       mov     w0, #0x1                        // #1
    0.96 :   ffff80001014c458:       stadd   w0, [x1]
   51.72 :   ffff80001014c45c:       b       ffff80001014c40c <irq_thread_fn+0x34>
         :                      __ll_sc_atomic_add():
         :                      ATOMIC_FETCH_OP (        , dmb ish,  , l, "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
    0.00 :   ffff80001014c460:       add     x2, x20, #0xcc
    0.00 :   ffff80001014c464:       b       ffff80001014e630 <__irq_get_irqchip_state+0x120>
    0.00 :   ffff80001014c468:       b       ffff80001014c40c <irq_thread_fn+0x34>
 Percent |	Source code & Disassembly of vmlinux for cycles (1765 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104542f0 <generic_make_request_checks>:
         :                      generic_make_request_checks():
         :                      return ret;
         :                      }
         :
         :                      static noinline_for_stack bool
         :                      generic_make_request_checks(struct bio *bio)
         :                      {
    0.45 :   ffff8000104542f0:       stp     x29, x30, [sp, #-112]!
    0.00 :   ffff8000104542f4:       mov     x29, sp
    0.85 :   ffff8000104542f8:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000104542fc:       adrp    x20, ffff800011899000 <page_wait_table+0x1500>
    0.62 :   ffff800010454300:       stp     x21, x22, [sp, #32]
    0.00 :   ffff800010454304:       add     x1, x20, #0x8c8
    0.00 :   ffff800010454308:       mov     x19, x0
         :                      blk_status_t status = BLK_STS_IOERR;
         :                      char b[BDEVNAME_SIZE];
         :
         :                      might_sleep();
         :
         :                      q = bio->bi_disk->queue;
    0.85 :   ffff80001045430c:       ldr     x22, [x0, #8]
         :                      {
    0.17 :   ffff800010454310:       ldr     x2, [x1]
    1.54 :   ffff800010454314:       str     x2, [x29, #104]
    0.00 :   ffff800010454318:       mov     x2, #0x0                        // #0
         :                      q = bio->bi_disk->queue;
    0.11 :   ffff80001045431c:       ldr     x21, [x22, #1040]
         :                      if (unlikely(!q)) {
    0.00 :   ffff800010454320:       cbz     x21, ffff80001045467c <generic_make_request_checks+0x38c>
         :                      /*
         :                      * Non-mq queues do not honor REQ_NOWAIT, so complete a bio
         :                      * with BLK_STS_AGAIN status in order to catch -EAGAIN and
         :                      * to give a chance to the caller to repeat request gracefully.
         :                      */
         :                      if ((bio->bi_opf & REQ_NOWAIT) && !queue_is_mq(q)) {
    1.92 :   ffff800010454324:       ldr     w1, [x0, #16]
    0.00 :   ffff800010454328:       tbz     w1, #21, ffff800010454334 <generic_make_request_checks+0x44>
    0.00 :   ffff80001045432c:       ldr     x0, [x21, #48]
    0.00 :   ffff800010454330:       cbz     x0, ffff800010454638 <generic_make_request_checks+0x348>
         :                      status = BLK_STS_AGAIN;
         :                      goto end_io;
         :                      }
         :
         :                      if (should_fail_bio(bio))
    0.63 :   ffff800010454334:       bl      ffff800010453da8 <should_fail_bio.isra.39>
    1.35 :   ffff800010454338:       cbz     w0, ffff800010454374 <generic_make_request_checks+0x84>
         :                      blk_status_t status = BLK_STS_IOERR;
    0.00 :   ffff80001045433c:       mov     w0, #0xa                        // #10
         :                      return true;
         :
         :                      not_supported:
         :                      status = BLK_STS_NOTSUPP;
         :                      end_io:
         :                      bio->bi_status = status;
    0.00 :   ffff800010454340:       strb    w0, [x19, #26]
         :                      bio_endio(bio);
    0.00 :   ffff800010454344:       mov     x0, x19
    0.00 :   ffff800010454348:       bl      ffff800010450080 <bio_endio>
         :                      return false;
    0.00 :   ffff80001045434c:       mov     w0, #0x0                        // #0
         :                      }
    0.00 :   ffff800010454350:       add     x20, x20, #0x8c8
    0.00 :   ffff800010454354:       ldr     x2, [x29, #104]
    0.62 :   ffff800010454358:       ldr     x1, [x20]
    0.00 :   ffff80001045435c:       eor     x1, x2, x1
    0.00 :   ffff800010454360:       cbnz    x1, ffff8000104547d0 <generic_make_request_checks+0x4e0>
    0.06 :   ffff800010454364:       ldp     x19, x20, [sp, #16]
    0.51 :   ffff800010454368:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff80001045436c:       ldp     x29, x30, [sp], #112
    0.00 :   ffff800010454370:       ret
    2.26 :   ffff800010454374:       str     x23, [x29, #48]
         :                      if (bio->bi_partno) {
    9.00 :   ffff800010454378:       ldrb    w0, [x19, #27]
         :                      int nr_sectors = bio_sectors(bio);
    0.06 :   ffff80001045437c:       ldr     w23, [x19, #40]
    1.08 :   ffff800010454380:       lsr     w23, w23, #9
         :                      if (bio->bi_partno) {
    0.00 :   ffff800010454384:       cbz     w0, ffff800010454574 <generic_make_request_checks+0x284>
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff800010454388:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      blk_partition_remap():
         :                      p = __disk_get_part(bio->bi_disk, bio->bi_partno);
    0.00 :   ffff80001045438c:       ldrb    w1, [x19, #27]
    0.00 :   ffff800010454390:       ldr     x0, [x19, #8]
    0.00 :   ffff800010454394:       bl      ffff800010469e10 <__disk_get_part>
    0.00 :   ffff800010454398:       mov     x22, x0
         :                      if (unlikely(!p))
    0.00 :   ffff80001045439c:       cbz     x0, ffff800010454754 <generic_make_request_checks+0x464>
         :                      bio_check_ro():
         :                      if (part->policy && op_is_write(op)) {
    0.00 :   ffff8000104543a0:       ldr     w0, [x0, #816]
    0.00 :   ffff8000104543a4:       cbz     w0, ffff8000104543f8 <generic_make_request_checks+0x108>
         :                      const int op = bio_op(bio);
    0.00 :   ffff8000104543a8:       ldr     w0, [x19, #16]
         :                      if (part->policy && op_is_write(op)) {
    0.00 :   ffff8000104543ac:       tbz     w0, #0, ffff8000104543f8 <generic_make_request_checks+0x108>
         :                      if (op_is_flush(bio->bi_opf) && !bio_sectors(bio))
    0.00 :   ffff8000104543b0:       tst     w0, #0x60000
    0.00 :   ffff8000104543b4:       b.ne    ffff8000104546f4 <generic_make_request_checks+0x404>  // b.any
         :                      WARN_ONCE(1,
    0.00 :   ffff8000104543b8:       adrp    x2, ffff800011a5d000 <__print_once.28906>
    0.00 :   ffff8000104543bc:       add     x2, x2, #0x46
    0.00 :   ffff8000104543c0:       ldrb    w0, [x2, #1]
    0.00 :   ffff8000104543c4:       cbnz    w0, ffff8000104543f8 <generic_make_request_checks+0x108>
    0.00 :   ffff8000104543c8:       mov     w3, #0x1                        // #1
    0.00 :   ffff8000104543cc:       add     x1, x29, #0x48
    0.00 :   ffff8000104543d0:       strb    w3, [x2, #1]
    0.00 :   ffff8000104543d4:       mov     x0, x19
    0.00 :   ffff8000104543d8:       bl      ffff80001046b940 <bio_devname>
    0.00 :   ffff8000104543dc:       mov     x1, x0
    0.00 :   ffff8000104543e0:       ldr     w2, [x22, #820]
    0.00 :   ffff8000104543e4:       adrp    x3, ffff8000111ab000 <kallsyms_token_index+0x31330>
    0.00 :   ffff8000104543e8:       add     x0, x3, #0x290
    0.00 :   ffff8000104543ec:       bl      ffff8000100e5288 <__warn_printk>
    0.00 :   ffff8000104543f0:       brk     #0x800
    0.00 :   ffff8000104543f4:       nop
    0.00 :   ffff8000104543f8:       ldr     w0, [x19, #40]
    0.00 :   ffff8000104543fc:       lsr     w0, w0, #9
         :                      blk_partition_remap():
         :                      if (bio_sectors(bio)) {
    0.00 :   ffff800010454400:       cbnz    w0, ffff8000104546a8 <generic_make_request_checks+0x3b8>
         :                      bio->bi_partno = 0;
    0.00 :   ffff800010454404:       strb    wzr, [x19, #27]
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.00 :   ffff800010454408:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      generic_make_request_checks():
         :                      if (op_is_flush(bio->bi_opf) &&
    0.00 :   ffff80001045440c:       ldr     w0, [x19, #16]
    0.00 :   ffff800010454410:       tst     w0, #0x60000
    0.00 :   ffff800010454414:       b.ne    ffff800010454614 <generic_make_request_checks+0x324>  // b.any
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    1.25 :   ffff800010454418:       ldr     x1, [x21, #104]
         :                      generic_make_request_checks():
         :                      if (!test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
    0.00 :   ffff80001045441c:       tst     w1, #0x10000
    0.96 :   ffff800010454420:       b.ne    ffff80001045442c <generic_make_request_checks+0x13c>  // b.any
         :                      bio->bi_opf &= ~REQ_HIPRI;
    5.27 :   ffff800010454424:       and     w0, w0, #0xfdffffff
    0.68 :   ffff800010454428:       str     w0, [x19, #16]
         :                      switch (bio_op(bio)) {
    0.00 :   ffff80001045442c:       and     w0, w0, #0xff
    0.00 :   ffff800010454430:       cmp     w0, #0x7
    0.00 :   ffff800010454434:       b.eq    ffff800010454748 <generic_make_request_checks+0x458>  // b.none
    0.06 :   ffff800010454438:       b.hi    ffff800010454648 <generic_make_request_checks+0x358>  // b.pmore
    0.00 :   ffff80001045443c:       cmp     w0, #0x5
    0.00 :   ffff800010454440:       b.eq    ffff800010454738 <generic_make_request_checks+0x448>  // b.none
    0.17 :   ffff800010454444:       b.hi    ffff8000104546e0 <generic_make_request_checks+0x3f0>  // b.pmore
    0.46 :   ffff800010454448:       cmp     w0, #0x3
    0.00 :   ffff80001045444c:       b.ne    ffff800010454460 <generic_make_request_checks+0x170>  // b.any
         :                      test_bit():
    0.00 :   ffff800010454450:       ldr     x0, [x21, #104]
         :                      generic_make_request_checks():
         :                      if (!blk_queue_discard(q))
    0.00 :   ffff800010454454:       tst     w0, #0x100
    0.00 :   ffff800010454458:       b.eq    ffff800010454670 <generic_make_request_checks+0x380>  // b.none
    0.00 :   ffff80001045445c:       nop
         :                      create_io_context(GFP_ATOMIC, q->node);
    0.12 :   ffff800010454460:       ldr     w2, [x21, #1156]
         :                      arch_local_save_flags():
         :                      */
         :                      static inline unsigned long arch_local_save_flags(void)
         :                      {
         :                      unsigned long flags;
         :
         :                      asm volatile(ALTERNATIVE(
    0.51 :   ffff800010454464:       mrs     x1, daif
         :                      arch_irqs_disabled_flags():
         :
         :                      static inline int arch_irqs_disabled_flags(unsigned long flags)
         :                      {
         :                      int res;
         :
         :                      asm volatile(ALTERNATIVE(
    0.06 :   ffff800010454468:       and     w0, w1, #0x80
         :                      create_io_context():
         :                      * Note that this function can't be called with IRQ disabled because
         :                      * task_lock which protects %current->io_context is IRQ-unsafe.
         :                      */
         :                      static inline struct io_context *create_io_context(gfp_t gfp_mask, int node)
         :                      {
         :                      WARN_ON_ONCE(irqs_disabled());
    0.00 :   ffff80001045446c:       cbnz    w0, ffff800010454770 <generic_make_request_checks+0x480>
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.28 :   ffff800010454470:       mrs     x0, sp_el0
         :                      create_io_context():
         :                      if (unlikely(!current->io_context))
    0.46 :   ffff800010454474:       ldr     x1, [x0, #1864]
    0.00 :   ffff800010454478:       cbz     x1, ffff800010454764 <generic_make_request_checks+0x474>
         :                      rcu_read_lock():
         :                      __rcu_read_lock();
    1.41 :   ffff80001045447c:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      blkcg_bio_issue_check():
         :                      struct blkcg_gq *blkg;
         :                      bool throtl = false;
         :
         :                      rcu_read_lock();
         :
         :                      if (!bio->bi_blkg) {
    0.00 :   ffff800010454480:       ldr     x3, [x19, #72]
    0.00 :   ffff800010454484:       cbz     x3, ffff80001045470c <generic_make_request_checks+0x41c>
         :
         :                      if (!throtl) {
         :                      struct blkg_iostat_set *bis;
         :                      int rwd, cpu;
         :
         :                      if (op_is_discard(bio->bi_opf))
    4.58 :   ffff800010454488:       ldr     w0, [x19, #16]
         :                      op_is_write():
         :                      bio->bi_opf = op | op_flags;
         :                      }
         :
         :                      static inline bool op_is_write(unsigned int op)
         :                      {
         :                      return (op & 1);
    0.00 :   ffff80001045448c:       mov     w4, #0x2                        // #2
         :                      get_current():
    0.96 :   ffff800010454490:       mrs     x2, sp_el0
         :                      op_is_discard():
         :                      (op & (REQ_SYNC | REQ_FUA | REQ_PREFLUSH));
         :                      }
         :
         :                      static inline bool op_is_discard(unsigned int op)
         :                      {
         :                      return (op & REQ_OP_MASK) == REQ_OP_DISCARD;
    0.00 :   ffff800010454494:       and     w5, w0, #0xff
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010454498:       ldr     w1, [x2, #16]
         :                      op_is_write():
         :                      return (op & 1);
    0.00 :   ffff80001045449c:       cmp     w5, #0x3
    0.11 :   ffff8000104544a0:       and     w0, w0, #0x1
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff8000104544a4:       add     w1, w1, #0x1
         :                      op_is_write():
    2.67 :   ffff8000104544a8:       csel    w0, w0, w4, ne  // ne = any
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000104544ac:       str     w1, [x2, #16]
         :                      blkcg_bio_issue_check():
         :                      else if (op_is_write(bio->bi_opf))
         :                      rwd = BLKG_IOSTAT_WRITE;
         :                      else
         :                      rwd = BLKG_IOSTAT_READ;
         :
         :                      cpu = get_cpu();
    0.00 :   ffff8000104544b0:       adrp    x1, ffff8000114ca000 <bp_hardening_data>
    0.00 :   ffff8000104544b4:       add     x1, x1, #0x18
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000104544b8:       mrs     x2, tpidr_el1
         :                      blkcg_bio_issue_check():
    0.06 :   ffff8000104544bc:       ldr     w1, [x1, x2]
         :                      bis = per_cpu_ptr(blkg->iostat_cpu, cpu);
    2.89 :   ffff8000104544c0:       adrp    x4, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000104544c4:       add     x4, x4, #0x8e8
         :                      bio_flagged():
         :                      atomic_set(&bio->__bi_cnt, count);
         :                      }
         :
         :                      static inline bool bio_flagged(struct bio *bio, unsigned int bit)
         :                      {
         :                      return (bio->bi_flags & (1U << bit)) != 0;
   13.04 :   ffff8000104544c8:       ldrh    w5, [x19, #20]
         :                      blkcg_bio_issue_check():
    0.24 :   ffff8000104544cc:       ldr     x2, [x3, #128]
    0.00 :   ffff8000104544d0:       sxtw    x0, w0
    0.57 :   ffff8000104544d4:       ldr     x4, [x4, w1, sxtw #3]
    0.00 :   ffff8000104544d8:       add     x2, x2, x4
         :                      /*
         :                      * If the bio is flagged with BIO_QUEUE_ENTERED it means this
         :                      * is a split bio and we would have already accounted for the
         :                      * size of the bio.
         :                      */
         :                      if (!bio_flagged(bio, BIO_QUEUE_ENTERED))
    0.00 :   ffff8000104544dc:       tbnz    w5, #11, ffff8000104544f0 <generic_make_request_checks+0x200>
         :                      bis->cur.bytes[rwd] += bio->bi_iter.bi_size;
    8.73 :   ffff8000104544e0:       ldr     x4, [x2, x0, lsl #3]
    4.37 :   ffff8000104544e4:       ldr     w5, [x19, #40]
    0.00 :   ffff8000104544e8:       add     x4, x4, x5
    0.00 :   ffff8000104544ec:       str     x4, [x2, x0, lsl #3]
    0.00 :   ffff8000104544f0:       add     x0, x2, x0, lsl #3
         :                      bis->cur.ios[rwd]++;
    0.86 :   ffff8000104544f4:       ldr     x2, [x0, #24]
    0.00 :   ffff8000104544f8:       add     x2, x2, #0x1
    0.85 :   ffff8000104544fc:       str     x2, [x0, #24]
         :                      arch_static_branch():
         :                      #define JUMP_LABEL_NOP_SIZE             AARCH64_INSN_SIZE
         :
         :                      static __always_inline bool arch_static_branch(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010454500:       nop
         :                      blkcg_bio_issue_check():
         :
         :                      u64_stats_update_end(&bis->sync);
         :                      if (cgroup_subsys_on_dfl(io_cgrp_subsys))
         :                      cgroup_rstat_updated(blkg->blkcg->css.cgroup, cpu);
    0.00 :   ffff800010454504:       ldr     x0, [x3, #40]
    0.00 :   ffff800010454508:       ldr     x0, [x0]
    0.00 :   ffff80001045450c:       bl      ffff8000101957a8 <cgroup_rstat_updated>
         :                      get_current():
    0.06 :   ffff800010454510:       mrs     x1, sp_el0
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010454514:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800010454518:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001045451c:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800010454520:       cbz     x0, ffff800010454640 <generic_make_request_checks+0x350>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    1.08 :   ffff800010454524:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff800010454528:       cbz     x0, ffff800010454640 <generic_make_request_checks+0x350>
         :                      blkcg_bio_issue_init():
         :                      bio_issue_init(&bio->bi_issue, bio_sectors(bio));
    0.73 :   ffff80001045452c:       ldr     w21, [x19, #40]
         :                      bio_issue_init():
         :                      issue->value = ((issue->value & BIO_ISSUE_RES_MASK) |
    0.23 :   ffff800010454530:       ldr     x22, [x19, #80]
         :                      ktime_get_ns():
         :                      return ktime_mono_to_any(mono, TK_OFFS_REAL);
         :                      }
         :
         :                      static inline u64 ktime_get_ns(void)
         :                      {
         :                      return ktime_to_ns(ktime_get());
    0.00 :   ffff800010454534:       bl      ffff80001016ad10 <ktime_get>
         :                      bio_issue_init():
         :                      (ktime_get_ns() & BIO_ISSUE_TIME_MASK) |
    0.06 :   ffff800010454538:       and     x0, x0, #0x7ffffffffffff
         :                      blkcg_bio_issue_init():
    0.00 :   ffff80001045453c:       lsr     w21, w21, #9
         :                      bio_issue_init():
         :                      issue->value = ((issue->value & BIO_ISSUE_RES_MASK) |
    0.00 :   ffff800010454540:       and     x22, x22, #0x8000000000000000
         :                      ((u64)size << BIO_ISSUE_SIZE_SHIFT));
    0.00 :   ffff800010454544:       ubfiz   x21, x21, #51, #12
         :                      (ktime_get_ns() & BIO_ISSUE_TIME_MASK) |
    0.00 :   ffff800010454548:       orr     x21, x21, x22
    0.00 :   ffff80001045454c:       orr     x0, x21, x0
         :                      issue->value = ((issue->value & BIO_ISSUE_RES_MASK) |
    0.00 :   ffff800010454550:       str     x0, [x19, #80]
         :                      rcu_read_unlock():
         :                      __rcu_read_unlock();
    0.00 :   ffff800010454554:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      generic_make_request_checks():
         :                      if (!bio_flagged(bio, BIO_TRACE_COMPLETION)) {
    0.00 :   ffff800010454558:       ldrh    w1, [x19, #20]
         :                      return true;
    0.00 :   ffff80001045455c:       mov     w0, #0x1                        // #1
         :                      if (!bio_flagged(bio, BIO_TRACE_COMPLETION)) {
    0.00 :   ffff800010454560:       tbnz    w1, #10, ffff8000104546a0 <generic_make_request_checks+0x3b0>
         :                      bio_set_flag():
         :                      }
         :
         :                      static inline void bio_set_flag(struct bio *bio, unsigned int bit)
         :                      {
         :                      bio->bi_flags |= (1U << bit);
    0.06 :   ffff800010454564:       orr     w1, w1, #0x400
    0.85 :   ffff800010454568:       strh    w1, [x19, #20]
    0.00 :   ffff80001045456c:       ldr     x23, [x29, #48]
    0.00 :   ffff800010454570:       b       ffff800010454350 <generic_make_request_checks+0x60>
         :                      bio_check_ro():
         :                      if (part->policy && op_is_write(op)) {
    0.62 :   ffff800010454574:       ldr     w2, [x22, #888]
         :                      generic_make_request_checks():
         :                      int nr_sectors = bio_sectors(bio);
    0.00 :   ffff800010454578:       mov     w0, w23
         :                      bio_check_ro():
         :                      if (part->policy && op_is_write(op)) {
    0.00 :   ffff80001045457c:       cbz     w2, ffff8000104545e0 <generic_make_request_checks+0x2f0>
    0.00 :   ffff800010454580:       tbz     w1, #0, ffff8000104545e0 <generic_make_request_checks+0x2f0>
         :                      if (op_is_flush(bio->bi_opf) && !bio_sectors(bio))
    0.00 :   ffff800010454584:       tst     w1, #0x60000
    0.00 :   ffff800010454588:       mov     x0, #0x0                        // #0
    0.00 :   ffff80001045458c:       ccmp    w23, #0x0, #0x0, ne  // ne = any
    0.00 :   ffff800010454590:       b.eq    ffff8000104545e0 <generic_make_request_checks+0x2f0>  // b.none
         :                      WARN_ONCE(1,
    0.00 :   ffff800010454594:       adrp    x2, ffff800011a5d000 <__print_once.28906>
    0.00 :   ffff800010454598:       add     x2, x2, #0x46
         :                      generic_make_request_checks():
         :                      int nr_sectors = bio_sectors(bio);
    0.00 :   ffff80001045459c:       mov     w0, w23
         :                      bio_check_ro():
         :                      WARN_ONCE(1,
    0.00 :   ffff8000104545a0:       ldrb    w1, [x2, #1]
    0.00 :   ffff8000104545a4:       cbnz    w1, ffff8000104545e0 <generic_make_request_checks+0x2f0>
    0.00 :   ffff8000104545a8:       mov     w3, #0x1                        // #1
    0.00 :   ffff8000104545ac:       add     x1, x29, #0x48
    0.00 :   ffff8000104545b0:       strb    w3, [x2, #1]
    0.00 :   ffff8000104545b4:       mov     x0, x19
    0.00 :   ffff8000104545b8:       bl      ffff80001046b940 <bio_devname>
    0.00 :   ffff8000104545bc:       mov     x1, x0
    0.00 :   ffff8000104545c0:       ldr     w2, [x22, #892]
    0.00 :   ffff8000104545c4:       adrp    x3, ffff8000111ab000 <kallsyms_token_index+0x31330>
    0.00 :   ffff8000104545c8:       add     x0, x3, #0x290
    0.00 :   ffff8000104545cc:       bl      ffff8000100e5288 <__warn_printk>
    0.00 :   ffff8000104545d0:       brk     #0x800
    0.00 :   ffff8000104545d4:       ldr     w0, [x19, #40]
    0.00 :   ffff8000104545d8:       ldr     x22, [x19, #8]
    0.00 :   ffff8000104545dc:       lsr     w0, w0, #9
         :                      generic_make_request_checks():
         :                      if (unlikely(bio_check_eod(bio, get_capacity(bio->bi_disk))))
    6.07 :   ffff8000104545e0:       ldr     x1, [x22, #80]
         :                      bio_check_eod():
         :                      if (nr_sectors && maxsector &&
    0.00 :   ffff8000104545e4:       cmp     w0, #0x0
    0.00 :   ffff8000104545e8:       ccmp    x1, #0x0, #0x4, ne  // ne = any
    0.00 :   ffff8000104545ec:       b.eq    ffff800010454608 <generic_make_request_checks+0x318>  // b.none
   14.81 :   ffff8000104545f0:       cmp     x1, x0
    0.00 :   ffff8000104545f4:       b.cc    ffff8000104547bc <generic_make_request_checks+0x4cc>  // b.lo, b.ul, b.last
         :                      (nr_sectors > maxsector ||
    0.91 :   ffff8000104545f8:       ldr     x2, [x19, #32]
         :                      bio->bi_iter.bi_sector > maxsector - nr_sectors)) {
    0.00 :   ffff8000104545fc:       sub     x0, x1, x0
         :                      (nr_sectors > maxsector ||
    0.06 :   ffff800010454600:       cmp     x2, x0
    0.00 :   ffff800010454604:       b.hi    ffff8000104547bc <generic_make_request_checks+0x4cc>  // b.pmore
         :                      generic_make_request_checks():
         :                      if (op_is_flush(bio->bi_opf) &&
    0.79 :   ffff800010454608:       ldr     w0, [x19, #16]
    0.00 :   ffff80001045460c:       tst     w0, #0x60000
    0.00 :   ffff800010454610:       b.eq    ffff800010454418 <generic_make_request_checks+0x128>  // b.none
         :                      test_bit():
    0.00 :   ffff800010454614:       ldr     x1, [x21, #104]
         :                      generic_make_request_checks():
    0.00 :   ffff800010454618:       tst     w1, #0x20000
    0.00 :   ffff80001045461c:       b.ne    ffff800010454418 <generic_make_request_checks+0x128>  // b.any
         :                      bio->bi_opf &= ~(REQ_PREFLUSH | REQ_FUA);
    0.00 :   ffff800010454620:       and     w0, w0, #0xfff9ffff
    0.00 :   ffff800010454624:       str     w0, [x19, #16]
         :                      if (!nr_sectors) {
    0.00 :   ffff800010454628:       cbnz    w23, ffff800010454418 <generic_make_request_checks+0x128>
         :                      status = BLK_STS_OK;
    0.00 :   ffff80001045462c:       mov     w0, #0x0                        // #0
    0.00 :   ffff800010454630:       ldr     x23, [x29, #48]
    0.00 :   ffff800010454634:       b       ffff800010454340 <generic_make_request_checks+0x50>
         :                      status = BLK_STS_AGAIN;
    0.00 :   ffff800010454638:       mov     w0, #0xc                        // #12
    0.00 :   ffff80001045463c:       b       ffff800010454340 <generic_make_request_checks+0x50>
         :                      blkcg_bio_issue_check():
         :                      put_cpu();
    0.00 :   ffff800010454640:       bl      ffff800010cad640 <preempt_schedule>
    0.00 :   ffff800010454644:       b       ffff80001045452c <generic_make_request_checks+0x23c>
         :                      generic_make_request_checks():
         :                      switch (bio_op(bio)) {
    0.00 :   ffff800010454648:       cmp     w0, #0x9
    0.00 :   ffff80001045464c:       b.eq    ffff80001045472c <generic_make_request_checks+0x43c>  // b.none
    0.00 :   ffff800010454650:       b.cs    ffff8000104546d8 <generic_make_request_checks+0x3e8>  // b.hs, b.nlast
         :                      blk_queue_is_zoned():
         :                      return q->limits.zoned;
         :                      }
         :
         :                      static inline bool blk_queue_is_zoned(struct request_queue *q)
         :                      {
         :                      switch (blk_queue_zoned_model(q)) {
    0.00 :   ffff800010454654:       ldr     w0, [x21, #1140]
    0.00 :   ffff800010454658:       sub     w0, w0, #0x1
    0.00 :   ffff80001045465c:       cmp     w0, #0x1
    0.00 :   ffff800010454660:       b.hi    ffff800010454670 <generic_make_request_checks+0x380>  // b.pmore
         :                      test_bit():
    0.00 :   ffff800010454664:       ldr     x0, [x21, #104]
         :                      generic_make_request_checks():
         :                      if (!blk_queue_is_zoned(q) || !blk_queue_zone_resetall(q))
    0.00 :   ffff800010454668:       tst     w0, #0x4000000
    0.00 :   ffff80001045466c:       b.ne    ffff800010454460 <generic_make_request_checks+0x170>  // b.any
         :                      status = BLK_STS_NOTSUPP;
    0.00 :   ffff800010454670:       mov     w0, #0x1                        // #1
    0.00 :   ffff800010454674:       ldr     x23, [x29, #48]
    0.00 :   ffff800010454678:       b       ffff800010454340 <generic_make_request_checks+0x50>
         :                      printk(KERN_ERR
    0.00 :   ffff80001045467c:       add     x1, x29, #0x48
    0.00 :   ffff800010454680:       bl      ffff80001046b940 <bio_devname>
    0.00 :   ffff800010454684:       ldr     x2, [x19, #32]
    0.00 :   ffff800010454688:       mov     x1, x0
    0.00 :   ffff80001045468c:       adrp    x0, ffff8000111ab000 <kallsyms_token_index+0x31330>
    0.00 :   ffff800010454690:       add     x0, x0, #0x240
    0.00 :   ffff800010454694:       bl      ffff800010148e94 <printk>
         :                      blk_status_t status = BLK_STS_IOERR;
    0.00 :   ffff800010454698:       mov     w0, #0xa                        // #10
         :                      goto end_io;
    0.00 :   ffff80001045469c:       b       ffff800010454340 <generic_make_request_checks+0x50>
    0.00 :   ffff8000104546a0:       ldr     x23, [x29, #48]
    0.00 :   ffff8000104546a4:       b       ffff800010454350 <generic_make_request_checks+0x60>
         :                      blk_partition_remap():
         :                      if (bio_check_eod(bio, part_nr_sects_read(p)))
    0.00 :   ffff8000104546a8:       ldr     x1, [x22, #8]
         :                      bio_check_eod():
         :                      if (nr_sectors && maxsector &&
    0.00 :   ffff8000104546ac:       cbz     x1, ffff800010454704 <generic_make_request_checks+0x414>
    0.00 :   ffff8000104546b0:       cmp     x1, x0
    0.00 :   ffff8000104546b4:       b.cc    ffff8000104547a4 <generic_make_request_checks+0x4b4>  // b.lo, b.ul, b.last
         :                      bio->bi_iter.bi_sector > maxsector - nr_sectors)) {
    0.00 :   ffff8000104546b8:       ldr     x2, [x19, #32]
    0.00 :   ffff8000104546bc:       sub     x0, x1, x0
         :                      (nr_sectors > maxsector ||
    0.00 :   ffff8000104546c0:       cmp     x2, x0
    0.00 :   ffff8000104546c4:       b.hi    ffff8000104547a4 <generic_make_request_checks+0x4b4>  // b.pmore
         :                      blk_partition_remap():
         :                      bio->bi_iter.bi_sector += p->start_sect;
    0.00 :   ffff8000104546c8:       ldr     x0, [x22]
    0.00 :   ffff8000104546cc:       add     x0, x0, x2
    0.00 :   ffff8000104546d0:       str     x0, [x19, #32]
    0.00 :   ffff8000104546d4:       b       ffff800010454404 <generic_make_request_checks+0x114>
         :                      generic_make_request_checks():
         :                      switch (bio_op(bio)) {
    0.00 :   ffff8000104546d8:       cmp     w0, #0xc
    0.00 :   ffff8000104546dc:       b.hi    ffff800010454460 <generic_make_request_checks+0x170>  // b.pmore
         :                      blk_queue_is_zoned():
    0.00 :   ffff8000104546e0:       ldr     w0, [x21, #1140]
    0.00 :   ffff8000104546e4:       sub     w0, w0, #0x1
    0.00 :   ffff8000104546e8:       cmp     w0, #0x1
    0.00 :   ffff8000104546ec:       b.hi    ffff800010454670 <generic_make_request_checks+0x380>  // b.pmore
    0.00 :   ffff8000104546f0:       b       ffff800010454460 <generic_make_request_checks+0x170>
         :                      bio_check_ro():
         :                      if (op_is_flush(bio->bi_opf) && !bio_sectors(bio))
    0.00 :   ffff8000104546f4:       ldr     w0, [x19, #40]
    0.00 :   ffff8000104546f8:       negs    wzr, w0, lsr #9
    0.00 :   ffff8000104546fc:       b.eq    ffff800010454404 <generic_make_request_checks+0x114>  // b.none
    0.00 :   ffff800010454700:       b       ffff8000104543b8 <generic_make_request_checks+0xc8>
    0.00 :   ffff800010454704:       ldr     x2, [x19, #32]
    0.00 :   ffff800010454708:       b       ffff8000104546c8 <generic_make_request_checks+0x3d8>
         :                      blkcg_bio_issue_check():
         :                      WARN_ONCE(1,
    0.00 :   ffff80001045470c:       adrp    x2, ffff800011a5d000 <__print_once.28906>
    0.00 :   ffff800010454710:       add     x2, x2, #0x46
    0.00 :   ffff800010454714:       ldrb    w0, [x2, #2]
    0.00 :   ffff800010454718:       cbz     w0, ffff800010454778 <generic_make_request_checks+0x488>
         :                      bio_associate_blkg(bio);
    0.00 :   ffff80001045471c:       mov     x0, x19
    0.00 :   ffff800010454720:       bl      ffff80001044fed8 <bio_associate_blkg>
    0.00 :   ffff800010454724:       ldr     x3, [x19, #72]
    0.00 :   ffff800010454728:       b       ffff800010454488 <generic_make_request_checks+0x198>
         :                      generic_make_request_checks():
         :                      if (!q->limits.max_write_zeroes_sectors)
    0.00 :   ffff80001045472c:       ldr     w0, [x21, #1116]
    0.00 :   ffff800010454730:       cbz     w0, ffff800010454670 <generic_make_request_checks+0x380>
    0.00 :   ffff800010454734:       b       ffff800010454460 <generic_make_request_checks+0x170>
         :                      test_bit():
    0.00 :   ffff800010454738:       ldr     x0, [x21, #104]
         :                      generic_make_request_checks():
         :                      if (!blk_queue_secure_erase(q))
    0.00 :   ffff80001045473c:       tst     w0, #0x800
    0.00 :   ffff800010454740:       b.eq    ffff800010454670 <generic_make_request_checks+0x380>  // b.none
    0.00 :   ffff800010454744:       b       ffff800010454460 <generic_make_request_checks+0x170>
         :                      if (!q->limits.max_write_same_sectors)
    0.00 :   ffff800010454748:       ldr     w0, [x21, #1112]
    0.00 :   ffff80001045474c:       cbz     w0, ffff800010454670 <generic_make_request_checks+0x380>
    0.00 :   ffff800010454750:       b       ffff800010454460 <generic_make_request_checks+0x170>
         :                      rcu_read_unlock():
    0.00 :   ffff800010454754:       bl      ffff80001015eee0 <__rcu_read_unlock>
    0.00 :   ffff800010454758:       ldr     x23, [x29, #48]
         :                      generic_make_request_checks():
         :                      blk_status_t status = BLK_STS_IOERR;
    0.00 :   ffff80001045475c:       mov     w0, #0xa                        // #10
    0.00 :   ffff800010454760:       b       ffff800010454340 <generic_make_request_checks+0x50>
         :                      create_io_context():
         :                      create_task_io_context(current, gfp_mask, node);
    0.00 :   ffff800010454764:       mov     w1, #0xa20                      // #2592
    0.00 :   ffff800010454768:       bl      ffff800010459748 <create_task_io_context>
    0.00 :   ffff80001045476c:       b       ffff80001045447c <generic_make_request_checks+0x18c>
         :                      WARN_ON_ONCE(irqs_disabled());
    0.00 :   ffff800010454770:       brk     #0x800
    0.00 :   ffff800010454774:       b       ffff800010454470 <generic_make_request_checks+0x180>
         :                      blkcg_bio_issue_check():
         :                      WARN_ONCE(1,
    0.00 :   ffff800010454778:       mov     w3, #0x1                        // #1
    0.00 :   ffff80001045477c:       add     x1, x29, #0x48
    0.00 :   ffff800010454780:       strb    w3, [x2, #2]
    0.00 :   ffff800010454784:       mov     x0, x19
    0.00 :   ffff800010454788:       bl      ffff80001046b940 <bio_devname>
    0.00 :   ffff80001045478c:       mov     x1, x0
    0.00 :   ffff800010454790:       adrp    x2, ffff8000111ab000 <kallsyms_token_index+0x31330>
    0.00 :   ffff800010454794:       add     x0, x2, #0x2e0
    0.00 :   ffff800010454798:       bl      ffff8000100e5288 <__warn_printk>
    0.00 :   ffff80001045479c:       brk     #0x800
    0.00 :   ffff8000104547a0:       b       ffff80001045471c <generic_make_request_checks+0x42c>
         :                      bio_check_eod():
         :                      handle_bad_sector(bio, maxsector);
    0.00 :   ffff8000104547a4:       mov     x0, x19
    0.00 :   ffff8000104547a8:       bl      ffff8000104539b8 <handle_bad_sector>
         :                      rcu_read_unlock():
    0.00 :   ffff8000104547ac:       bl      ffff80001015eee0 <__rcu_read_unlock>
    0.00 :   ffff8000104547b0:       ldr     x23, [x29, #48]
         :                      generic_make_request_checks():
         :                      blk_status_t status = BLK_STS_IOERR;
    0.00 :   ffff8000104547b4:       mov     w0, #0xa                        // #10
    0.00 :   ffff8000104547b8:       b       ffff800010454340 <generic_make_request_checks+0x50>
         :                      bio_check_eod():
         :                      handle_bad_sector(bio, maxsector);
    0.00 :   ffff8000104547bc:       mov     x0, x19
    0.00 :   ffff8000104547c0:       bl      ffff8000104539b8 <handle_bad_sector>
         :                      generic_make_request_checks():
         :                      blk_status_t status = BLK_STS_IOERR;
    0.00 :   ffff8000104547c4:       mov     w0, #0xa                        // #10
    0.00 :   ffff8000104547c8:       ldr     x23, [x29, #48]
    0.00 :   ffff8000104547cc:       b       ffff800010454340 <generic_make_request_checks+0x50>
    0.00 :   ffff8000104547d0:       str     x23, [x29, #48]
         :                      }
    0.00 :   ffff8000104547d4:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (1690 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000107bcb50 <nvme_submit_cmd>:
         :                      nvme_submit_cmd():
         :                      * @cmd: The command to send
         :                      * @write_sq: whether to write to the SQ doorbell
         :                      */
         :                      static void nvme_submit_cmd(struct nvme_queue *nvmeq, struct nvme_command *cmd,
         :                      bool write_sq)
         :                      {
    0.30 :   ffff8000107bcb50:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff8000107bcb54:       mov     x29, sp
    2.61 :   ffff8000107bcb58:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000107bcb5c:       mov     x19, x0
    0.35 :   ffff8000107bcb60:       stp     x21, x22, [sp, #32]
         :                      spin_lock():
         :                      raw_spin_lock_init(&(_lock)->rlock);            \
         :                      } while (0)
         :
         :                      static __always_inline void spin_lock(spinlock_t *lock)
         :                      {
         :                      raw_spin_lock(&lock->rlock);
    0.00 :   ffff8000107bcb64:       add     x21, x0, #0x8
         :                      nvme_submit_cmd():
    0.00 :   ffff8000107bcb68:       mov     x20, x1
    0.00 :   ffff8000107bcb6c:       and     w22, w2, #0xff
         :                      spin_lock():
    0.41 :   ffff8000107bcb70:       mov     x0, x21
    0.00 :   ffff8000107bcb74:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      nvme_submit_cmd():
         :                      spin_lock(&nvmeq->sq_lock);
         :                      memcpy(nvmeq->sq_cmds + (nvmeq->sq_tail << nvmeq->sqes),
    0.89 :   ffff8000107bcb78:       ldrh    w0, [x19, #116]
    0.18 :   ffff8000107bcb7c:       ldrb    w2, [x19, #125]
    3.78 :   ffff8000107bcb80:       ldr     x1, [x19, #16]
   17.50 :   ffff8000107bcb84:       ldp     x4, x5, [x20]
    0.00 :   ffff8000107bcb88:       lsl     w0, w0, w2
    0.00 :   ffff8000107bcb8c:       add     x0, x1, w0, sxtw
    0.77 :   ffff8000107bcb90:       stp     x4, x5, [x0]
    9.76 :   ffff8000107bcb94:       ldp     x2, x3, [x20, #16]
    0.18 :   ffff8000107bcb98:       stp     x2, x3, [x0, #16]
    2.01 :   ffff8000107bcb9c:       ldp     x2, x3, [x20, #32]
    0.12 :   ffff8000107bcba0:       stp     x2, x3, [x0, #32]
    0.65 :   ffff8000107bcba4:       ldp     x2, x3, [x20, #48]
    1.31 :   ffff8000107bcba8:       stp     x2, x3, [x0, #48]
         :                      cmd, sizeof(*cmd));
         :                      if (++nvmeq->sq_tail == nvmeq->q_depth)
    0.59 :   ffff8000107bcbac:       ldrh    w0, [x19, #116]
    0.00 :   ffff8000107bcbb0:       ldrh    w3, [x19, #112]
    0.00 :   ffff8000107bcbb4:       add     w0, w0, #0x1
    0.00 :   ffff8000107bcbb8:       and     w0, w0, #0xffff
    0.18 :   ffff8000107bcbbc:       strh    w0, [x19, #116]
    0.00 :   ffff8000107bcbc0:       cmp     w0, w3
    0.00 :   ffff8000107bcbc4:       b.eq    ffff8000107bcc58 <nvme_submit_cmd+0x108>  // b.none
         :                      nvme_write_sq_db():
         :                      if (!write_sq) {
    0.83 :   ffff8000107bcbc8:       cbnz    w22, ffff8000107bcc00 <nvme_submit_cmd+0xb0>
         :                      u16 next_tail = nvmeq->sq_tail + 1;
    0.00 :   ffff8000107bcbcc:       add     w1, w0, #0x1
         :                      if (next_tail != nvmeq->last_sq_tail)
    0.00 :   ffff8000107bcbd0:       ldrh    w2, [x19, #118]
         :                      u16 next_tail = nvmeq->sq_tail + 1;
    0.00 :   ffff8000107bcbd4:       and     w1, w1, #0xffff
         :                      next_tail = 0;
    0.00 :   ffff8000107bcbd8:       cmp     w3, w1
    0.00 :   ffff8000107bcbdc:       csel    w1, w1, wzr, ne  // ne = any
         :                      if (next_tail != nvmeq->last_sq_tail)
    0.00 :   ffff8000107bcbe0:       cmp     w2, w1
    0.00 :   ffff8000107bcbe4:       b.eq    ffff8000107bcc00 <nvme_submit_cmd+0xb0>  // b.none
         :                      spin_unlock():
         :                      raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
         :                      } while (0)
         :
         :                      static __always_inline void spin_unlock(spinlock_t *lock)
         :                      {
         :                      raw_spin_unlock(&lock->rlock);
    0.00 :   ffff8000107bcbe8:       mov     x0, x21
    0.00 :   ffff8000107bcbec:       bl      ffff800010cb2408 <_raw_spin_unlock>
         :                      nvme_submit_cmd():
         :                      nvmeq->sq_tail = 0;
         :                      nvme_write_sq_db(nvmeq, write_sq);
         :                      spin_unlock(&nvmeq->sq_lock);
         :                      }
    0.00 :   ffff8000107bcbf0:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000107bcbf4:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000107bcbf8:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000107bcbfc:       ret
         :                      nvme_write_sq_db():
         :                      if (nvme_dbbuf_update_and_check_event(nvmeq->sq_tail,
    0.18 :   ffff8000107bcc00:       ldr     x2, [x19, #136]
         :                      nvme_dbbuf_update_and_check_event():
         :                      if (dbbuf_db) {
    0.00 :   ffff8000107bcc04:       cbz     x2, ffff8000107bcc64 <nvme_submit_cmd+0x114>
         :                      nvme_write_sq_db():
         :                      nvmeq->dbbuf_sq_db, nvmeq->dbbuf_sq_ei))
    0.00 :   ffff8000107bcc08:       ldr     x3, [x19, #152]
         :                      nvme_dbbuf_update_and_check_event():
         :                      wmb();
    0.00 :   ffff8000107bcc0c:       dsb     st
         :                      old_value = *dbbuf_db;
    0.00 :   ffff8000107bcc10:       ldrh    w1, [x2]
         :                      *dbbuf_db = value;
    0.00 :   ffff8000107bcc14:       str     w0, [x2]
         :                      mb();
    0.00 :   ffff8000107bcc18:       dsb     sy
         :                      if (!nvme_dbbuf_need_event(*dbbuf_ei, value, old_value))
    0.00 :   ffff8000107bcc1c:       ldr     w2, [x3]
         :                      nvme_dbbuf_need_event():
         :                      return (u16)(new_idx - event_idx - 1) < (u16)(new_idx - old);
    0.00 :   ffff8000107bcc20:       sub     w1, w0, w1
         :                      nvme_dbbuf_update_and_check_event():
         :                      if (!nvme_dbbuf_need_event(*dbbuf_ei, value, old_value))
    0.00 :   ffff8000107bcc24:       and     w1, w1, #0xffff
         :                      nvme_dbbuf_need_event():
         :                      return (u16)(new_idx - event_idx - 1) < (u16)(new_idx - old);
    0.00 :   ffff8000107bcc28:       mvn     w2, w2
    0.00 :   ffff8000107bcc2c:       add     w0, w0, w2
         :                      nvme_dbbuf_update_and_check_event():
         :                      if (!nvme_dbbuf_need_event(*dbbuf_ei, value, old_value))
    0.00 :   ffff8000107bcc30:       cmp     w1, w0, uxth
    0.00 :   ffff8000107bcc34:       b.hi    ffff8000107bcc64 <nvme_submit_cmd+0x114>  // b.pmore
    0.00 :   ffff8000107bcc38:       ldrh    w0, [x19, #116]
         :                      nvme_write_sq_db():
         :                      nvmeq->last_sq_tail = nvmeq->sq_tail;
    0.00 :   ffff8000107bcc3c:       strh    w0, [x19, #118]
         :                      spin_unlock():
    0.76 :   ffff8000107bcc40:       mov     x0, x21
    0.00 :   ffff8000107bcc44:       bl      ffff800010cb2408 <_raw_spin_unlock>
         :                      nvme_submit_cmd():
         :                      }
    3.19 :   ffff8000107bcc48:       ldp     x19, x20, [sp, #16]
    0.24 :   ffff8000107bcc4c:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000107bcc50:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000107bcc54:       ret
         :                      nvmeq->sq_tail = 0;
    0.00 :   ffff8000107bcc58:       strh    wzr, [x19, #116]
    0.00 :   ffff8000107bcc5c:       mov     w0, #0x0                        // #0
    0.00 :   ffff8000107bcc60:       b       ffff8000107bcbc8 <nvme_submit_cmd+0x78>
         :                      nvme_write_sq_db():
         :                      writel(nvmeq->sq_tail, nvmeq->q_db);
    0.06 :   ffff8000107bcc64:       dmb     oshst
   53.02 :   ffff8000107bcc68:       ldrh    w0, [x19, #116]
         :                      __raw_writel():
         :                      }
         :
         :                      #define __raw_writel __raw_writel
         :                      static inline void __raw_writel(u32 val, volatile void __iomem *addr)
         :                      {
         :                      asm volatile("str %w0, [%1]" : : "rZ" (val), "r" (addr));
    0.00 :   ffff8000107bcc6c:       ldr     x1, [x19, #104]
    0.00 :   ffff8000107bcc70:       str     w0, [x1]
         :                      nvme_write_sq_db():
         :                      nvmeq->last_sq_tail = nvmeq->sq_tail;
    0.12 :   ffff8000107bcc74:       strh    w0, [x19, #118]
    0.00 :   ffff8000107bcc78:       b       ffff8000107bcc40 <nvme_submit_cmd+0xf0>
 Percent |	Source code & Disassembly of vmlinux for cycles (845 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001012e368 <__wake_up_common_lock>:
         :                      __wake_up_common_lock():
         :                      return nr_exclusive;
         :                      }
         :
         :                      static void __wake_up_common_lock(struct wait_queue_head *wq_head, unsigned int mode,
         :                      int nr_exclusive, int wake_flags, void *key)
         :                      {
    0.59 :   ffff80001012e368:       stp     x29, x30, [sp, #-128]!
    0.00 :   ffff80001012e36c:       mov     x29, sp
    7.58 :   ffff80001012e370:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001012e374:       mov     w20, w2
    2.00 :   ffff80001012e378:       stp     x21, x22, [sp, #32]
    0.00 :   ffff80001012e37c:       mov     x22, x4
   13.06 :   ffff80001012e380:       stp     x23, x24, [sp, #48]
    0.00 :   ffff80001012e384:       mov     w24, w1
    1.18 :   ffff80001012e388:       str     x25, [sp, #64]
    0.00 :   ffff80001012e38c:       adrp    x25, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff80001012e390:       add     x5, x25, #0x8c8
    0.00 :   ffff80001012e394:       mov     w23, w3
    1.29 :   ffff80001012e398:       ldr     x6, [x5]
    2.87 :   ffff80001012e39c:       str     x6, [x29, #120]
    0.00 :   ffff80001012e3a0:       mov     x6, #0x0                        // #0
    0.00 :   ffff80001012e3a4:       mov     x19, x0
         :                      unsigned long flags;
         :                      wait_queue_entry_t bookmark;
         :
         :                      bookmark.flags = 0;
   12.78 :   ffff80001012e3a8:       str     wzr, [x29, #80]
         :                      INIT_LIST_HEAD():
         :                      #define LIST_HEAD(name) \
         :                      struct list_head name = LIST_HEAD_INIT(name)
         :
         :                      static inline void INIT_LIST_HEAD(struct list_head *list)
         :                      {
         :                      WRITE_ONCE(list->next, list);
    0.00 :   ffff80001012e3ac:       add     x0, x29, #0x68
         :                      __wake_up_common_lock():
         :                      bookmark.private = NULL;
         :                      bookmark.func = NULL;
    0.80 :   ffff80001012e3b0:       stp     xzr, xzr, [x29, #88]
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.70 :   ffff80001012e3b4:       str     x0, [x29, #104]
         :                      INIT_LIST_HEAD():
         :                      list->prev = list;
    0.83 :   ffff80001012e3b8:       str     x0, [x29, #112]
    0.00 :   ffff80001012e3bc:       nop
         :                      __wake_up_common_lock():
         :                      INIT_LIST_HEAD(&bookmark.entry);
         :
         :                      do {
         :                      spin_lock_irqsave(&wq_head->lock, flags);
   12.19 :   ffff80001012e3c0:       mov     x0, x19
    0.00 :   ffff80001012e3c4:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
         :                      nr_exclusive = __wake_up_common(wq_head, mode, nr_exclusive,
    0.00 :   ffff80001012e3c8:       mov     w2, w20
    0.00 :   ffff80001012e3cc:       add     x5, x29, #0x50
    0.00 :   ffff80001012e3d0:       mov     x4, x22
    0.00 :   ffff80001012e3d4:       mov     w3, w23
         :                      spin_lock_irqsave(&wq_head->lock, flags);
    0.00 :   ffff80001012e3d8:       mov     x21, x0
         :                      nr_exclusive = __wake_up_common(wq_head, mode, nr_exclusive,
    0.00 :   ffff80001012e3dc:       mov     w1, w24
    0.00 :   ffff80001012e3e0:       mov     x0, x19
    0.00 :   ffff80001012e3e4:       bl      ffff80001012e218 <__wake_up_common>
         :                      spin_unlock_irqrestore():
         :                      raw_spin_unlock_irq(&lock->rlock);
         :                      }
         :
         :                      static __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
         :                      {
         :                      raw_spin_unlock_irqrestore(&lock->rlock, flags);
    0.00 :   ffff80001012e3e8:       mov     x1, x21
         :                      __wake_up_common_lock():
    0.00 :   ffff80001012e3ec:       mov     w20, w0
         :                      spin_unlock_irqrestore():
    0.00 :   ffff80001012e3f0:       mov     x0, x19
    0.00 :   ffff80001012e3f4:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      __wake_up_common_lock():
         :                      wake_flags, key, &bookmark);
         :                      spin_unlock_irqrestore(&wq_head->lock, flags);
         :                      } while (bookmark.flags & WQ_FLAG_BOOKMARK);
   12.25 :   ffff80001012e3f8:       ldr     w0, [x29, #80]
    0.00 :   ffff80001012e3fc:       tbnz    w0, #2, ffff80001012e3c0 <__wake_up_common_lock+0x58>
         :                      }
    5.45 :   ffff80001012e400:       add     x25, x25, #0x8c8
    1.30 :   ffff80001012e404:       ldr     x1, [x29, #120]
    0.47 :   ffff80001012e408:       ldr     x0, [x25]
    0.00 :   ffff80001012e40c:       eor     x0, x1, x0
    0.00 :   ffff80001012e410:       cbnz    x0, ffff80001012e42c <__wake_up_common_lock+0xc4>
   13.16 :   ffff80001012e414:       ldp     x19, x20, [sp, #16]
    0.71 :   ffff80001012e418:       ldp     x21, x22, [sp, #32]
    0.11 :   ffff80001012e41c:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff80001012e420:       ldr     x25, [sp, #64]
   10.67 :   ffff80001012e424:       ldp     x29, x30, [sp], #128
    0.00 :   ffff80001012e428:       ret
    0.00 :   ffff80001012e42c:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (822 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001045d810 <__blk_mq_free_request>:
         :                      __blk_mq_free_request():
         :                      return rq;
         :                      }
         :                      EXPORT_SYMBOL_GPL(blk_mq_alloc_request_hctx);
         :
         :                      static void __blk_mq_free_request(struct request *rq)
         :                      {
    0.00 :   ffff80001045d810:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff80001045d814:       mov     x29, sp
    0.12 :   ffff80001045d818:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001045d81c:       mov     x19, x0
    0.00 :   ffff80001045d820:       stp     x21, x22, [sp, #32]
   18.67 :   ffff80001045d824:       stp     x23, x24, [sp, #48]
         :                      struct request_queue *q = rq->q;
         :                      struct blk_mq_ctx *ctx = rq->mq_ctx;
    0.12 :   ffff80001045d828:       ldp     x21, x24, [x0]
         :                      struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
         :                      const int sched_tag = rq->internal_tag;
    0.97 :   ffff80001045d82c:       ldr     w22, [x0, #36]
         :                      struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
    0.36 :   ffff80001045d830:       ldr     x20, [x0, #16]
         :                      blk_pm_mark_last_busy():
         :                      pm_request_resume(q->dev);
         :                      }
         :
         :                      static inline void blk_pm_mark_last_busy(struct request *rq)
         :                      {
         :                      if (rq->q->dev && !(rq->rq_flags & RQF_PM))
   19.83 :   ffff80001045d834:       ldr     x23, [x21, #216]
    0.00 :   ffff80001045d838:       cbz     x23, ffff80001045d844 <__blk_mq_free_request+0x34>
    0.00 :   ffff80001045d83c:       ldr     w0, [x0, #28]
    0.00 :   ffff80001045d840:       tbz     w0, #15, ffff80001045d8a4 <__blk_mq_free_request+0x94>
         :                      __blk_mq_free_request():
         :
         :                      blk_pm_mark_last_busy(rq);
         :                      rq->mq_hctx = NULL;
         :                      if (rq->tag != -1)
    0.97 :   ffff80001045d844:       ldr     w3, [x19, #32]
         :                      rq->mq_hctx = NULL;
    0.97 :   ffff80001045d848:       str     xzr, [x19, #16]
         :                      if (rq->tag != -1)
    0.00 :   ffff80001045d84c:       cmn     w3, #0x1
    0.00 :   ffff80001045d850:       b.eq    ffff80001045d864 <__blk_mq_free_request+0x54>  // b.none
         :                      blk_mq_put_tag(hctx, hctx->tags, ctx, rq->tag);
    0.24 :   ffff80001045d854:       ldr     x1, [x20, #336]
    0.00 :   ffff80001045d858:       mov     x2, x24
    0.00 :   ffff80001045d85c:       mov     x0, x20
    0.37 :   ffff80001045d860:       bl      ffff800010463e98 <blk_mq_put_tag>
         :                      if (sched_tag != -1)
   19.75 :   ffff80001045d864:       cmn     w22, #0x1
    0.00 :   ffff80001045d868:       b.eq    ffff80001045d880 <__blk_mq_free_request+0x70>  // b.none
         :                      blk_mq_put_tag(hctx, hctx->sched_tags, ctx, sched_tag);
    0.00 :   ffff80001045d86c:       ldr     x1, [x20, #344]
    0.00 :   ffff80001045d870:       mov     w3, w22
    0.00 :   ffff80001045d874:       mov     x2, x24
    0.00 :   ffff80001045d878:       mov     x0, x20
    0.00 :   ffff80001045d87c:       bl      ffff800010463e98 <blk_mq_put_tag>
         :                      blk_mq_sched_restart(hctx);
    0.23 :   ffff80001045d880:       mov     x0, x20
    0.00 :   ffff80001045d884:       bl      ffff800010465f40 <blk_mq_sched_restart>
         :                      blk_queue_exit(q);
   14.98 :   ffff80001045d888:       mov     x0, x21
    0.00 :   ffff80001045d88c:       bl      ffff800010455110 <blk_queue_exit>
         :                      }
   16.55 :   ffff80001045d890:       ldp     x19, x20, [sp, #16]
    1.94 :   ffff80001045d894:       ldp     x21, x22, [sp, #32]
    0.13 :   ffff80001045d898:       ldp     x23, x24, [sp, #48]
    3.56 :   ffff80001045d89c:       ldp     x29, x30, [sp], #64
    0.24 :   ffff80001045d8a0:       ret
         :                      pm_runtime_mark_last_busy():
         :                      return !dev->power.no_callbacks;
         :                      }
         :
         :                      static inline void pm_runtime_mark_last_busy(struct device *dev)
         :                      {
         :                      WRITE_ONCE(dev->power.last_busy, ktime_get_mono_fast_ns());
    0.00 :   ffff80001045d8a4:       bl      ffff80001016bfb8 <ktime_get_mono_fast_ns>
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff80001045d8a8:       str     x0, [x23, #480]
    0.00 :   ffff80001045d8ac:       b       ffff80001045d844 <__blk_mq_free_request+0x34>
 Percent |	Source code & Disassembly of vmlinux for cycles (1598 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010cb2b08 <_raw_spin_lock>:
         :                      _raw_spin_lock():
         :                      EXPORT_SYMBOL(_raw_spin_trylock_bh);
         :                      #endif
         :
         :                      #ifndef CONFIG_INLINE_SPIN_LOCK
         :                      void __lockfunc _raw_spin_lock(raw_spinlock_t *lock)
         :                      {
    0.00 :   ffff800010cb2b08:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff800010cb2b0c:       mov     x3, x0
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    1.07 :   ffff800010cb2b10:       mrs     x2, sp_el0
         :                      _raw_spin_lock():
    0.06 :   ffff800010cb2b14:       mov     x29, sp
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.25 :   ffff800010cb2b18:       ldr     w1, [x2, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff800010cb2b1c:       add     w1, w1, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    3.32 :   ffff800010cb2b20:       str     w1, [x2, #16]
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010cb2b24:       b       ffff800010cb2b4c <_raw_spin_lock+0x44>
    0.00 :   ffff800010cb2b28:       b       ffff800010cb2b4c <_raw_spin_lock+0x44>
         :                      __lse__cmpxchg_case_acq_32():
         :                      __CMPXCHG_CASE(w, h,     , 16,   )
         :                      __CMPXCHG_CASE(w,  ,     , 32,   )
         :                      __CMPXCHG_CASE(x,  ,     , 64,   )
         :                      __CMPXCHG_CASE(w, b, acq_,  8,  a, "memory")
         :                      __CMPXCHG_CASE(w, h, acq_, 16,  a, "memory")
         :                      __CMPXCHG_CASE(w,  , acq_, 32,  a, "memory")
    0.19 :   ffff800010cb2b2c:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010cb2b30:       mov     w2, #0x1                        // #1
    0.00 :   ffff800010cb2b34:       mov     w4, w1
    0.19 :   ffff800010cb2b38:       casa    w4, w2, [x0]
   94.92 :   ffff800010cb2b3c:       mov     w0, w4
         :                      atomic_try_cmpxchg_acquire():
         :                      static inline bool
         :                      atomic_try_cmpxchg_acquire(atomic_t *v, int *old, int new)
         :                      {
         :                      int r, o = *old;
         :                      r = atomic_cmpxchg_acquire(v, o, new);
         :                      if (unlikely(r != o))
    0.00 :   ffff800010cb2b40:       cbnz    w0, ffff800010cb2b5c <_raw_spin_lock+0x54>
         :                      _raw_spin_lock():
         :                      __raw_spin_lock(lock);
         :                      }
    0.00 :   ffff800010cb2b44:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010cb2b48:       ret
         :                      __ll_sc__cmpxchg_case_acq_32():
         :                      __CMPXCHG_CASE(w, h,     , 16,        ,  ,  ,         , K)
         :                      __CMPXCHG_CASE(w,  ,     , 32,        ,  ,  ,         , K)
         :                      __CMPXCHG_CASE( ,  ,     , 64,        ,  ,  ,         , L)
         :                      __CMPXCHG_CASE(w, b, acq_,  8,        , a,  , "memory", K)
         :                      __CMPXCHG_CASE(w, h, acq_, 16,        , a,  , "memory", K)
         :                      __CMPXCHG_CASE(w,  , acq_, 32,        , a,  , "memory", K)
    0.00 :   ffff800010cb2b4c:       mov     x1, #0x0                        // #0
    0.00 :   ffff800010cb2b50:       mov     w2, #0x1                        // #1
    0.00 :   ffff800010cb2b54:       b       ffff800010cb3060 <_raw_write_lock_irqsave+0x210>
         :                      atomic_try_cmpxchg_acquire():
    0.00 :   ffff800010cb2b58:       cbz     w0, ffff800010cb2b44 <_raw_spin_lock+0x3c>
         :                      queued_spin_lock():
         :                      u32 val = 0;
         :
         :                      if (likely(atomic_try_cmpxchg_acquire(&lock->val, &val, _Q_LOCKED_VAL)))
         :                      return;
         :
         :                      queued_spin_lock_slowpath(lock, val);
    0.00 :   ffff800010cb2b5c:       mov     w1, w0
    0.00 :   ffff800010cb2b60:       mov     x0, x3
    0.00 :   ffff800010cb2b64:       bl      ffff8000101387b0 <queued_spin_lock_slowpath>
         :                      _raw_spin_lock():
    0.00 :   ffff800010cb2b68:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010cb2b6c:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (1450 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001029ebd8 <__fget>:
         :                      __fget():
         :                      }
         :                      spin_unlock(&files->file_lock);
         :                      }
         :
         :                      static struct file *__fget(unsigned int fd, fmode_t mask, unsigned int refs)
         :                      {
    0.97 :   ffff80001029ebd8:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff80001029ebdc:       mov     x29, sp
    0.62 :   ffff80001029ebe0:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001029ebe4:       mov     w20, w0
    0.21 :   ffff80001029ebe8:       stp     x21, x22, [sp, #32]
    0.00 :   ffff80001029ebec:       mov     w19, w2
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80001029ebf0:       mrs     x0, sp_el0
         :                      __fget():
    1.25 :   ffff80001029ebf4:       mov     w22, w1
         :                      struct files_struct *files = current->files;
    0.00 :   ffff80001029ebf8:       ldr     x21, [x0, #1624]
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff80001029ebfc:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __fcheck_files():
         :                      static inline struct file *__fcheck_files(struct files_struct *files, unsigned int fd)
         :                      {
         :                      struct fdtable *fdt = rcu_dereference_raw(files->fdt);
         :
         :                      if (fd < fdt->max_fds) {
         :                      fd = array_index_nospec(fd, fdt->max_fds);
    0.00 :   ffff80001029ec00:       mov     w6, w20
         :                      __fget():
         :                      * dup2() atomicity guarantee is the reason
         :                      * we loop to catch the new file (or NULL pointer)
         :                      */
         :                      if (file->f_mode & mask)
         :                      file = NULL;
         :                      else if (!get_file_rcu_many(file, refs))
    0.00 :   ffff80001029ec04:       mov     w5, w19
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001029ec08:       ldr     x1, [x21, #32]
         :                      __fcheck_files():
         :                      if (fd < fdt->max_fds) {
   14.54 :   ffff80001029ec0c:       ldr     w0, [x1]
    0.00 :   ffff80001029ec10:       cmp     w20, w0
    0.00 :   ffff80001029ec14:       b.cs    ffff80001029eca8 <__fget+0xd0>  // b.hs, b.nlast
         :                      fd = array_index_nospec(fd, fdt->max_fds);
    6.06 :   ffff80001029ec18:       mov     w0, w0
         :                      array_index_mask_nospec():
         :                      static inline unsigned long array_index_mask_nospec(unsigned long idx,
         :                      unsigned long sz)
         :                      {
         :                      unsigned long mask;
         :
         :                      asm volatile(
    0.00 :   ffff80001029ec1c:       cmp     x6, x0
    0.00 :   ffff80001029ec20:       ngc     x0, xzr
         :                      "       sbc     %0, xzr, xzr\n"
         :                      : "=r" (mask)
         :                      : "r" (idx), "Ir" (sz)
         :                      : "cc");
         :
         :                      csdb();
    0.00 :   ffff80001029ec24:       csdb
         :                      __fcheck_files():
         :                      return rcu_dereference_raw(fdt->fd[fd]);
    0.89 :   ffff80001029ec28:       and     w0, w20, w0
    0.00 :   ffff80001029ec2c:       ldr     x1, [x1, #8]
         :                      __read_once_size():
    1.24 :   ffff80001029ec30:       ldr     x19, [x1, x0, lsl #3]
         :                      __fget():
         :                      if (file) {
    0.00 :   ffff80001029ec34:       cbz     x19, ffff80001029eca8 <__fget+0xd0>
         :                      if (file->f_mode & mask)
   11.72 :   ffff80001029ec38:       ldr     w0, [x19, #68]
    0.00 :   ffff80001029ec3c:       tst     w22, w0
    2.68 :   ffff80001029ec40:       b.ne    ffff80001029eca8 <__fget+0xd0>  // b.any
         :                      __read_once_size():
   13.23 :   ffff80001029ec44:       ldr     x3, [x19, #56]
    0.00 :   ffff80001029ec48:       add     x4, x19, #0x38
         :                      atomic64_fetch_add_unless():
         :                      atomic64_fetch_add_unless(atomic64_t *v, s64 a, s64 u)
         :                      {
         :                      s64 c = atomic64_read(v);
         :
         :                      do {
         :                      if (unlikely(c == u))
    0.00 :   ffff80001029ec4c:       cbz     x3, ffff80001029ec08 <__fget+0x30>
         :                      break;
         :                      } while (!atomic64_try_cmpxchg(v, &c, c + a));
    4.08 :   ffff80001029ec50:       add     x2, x5, x3
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff80001029ec54:       b       ffff80001029ec90 <__fget+0xb8>
    0.83 :   ffff80001029ec58:       b       ffff80001029ec90 <__fget+0xb8>
         :                      __lse__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,  l, "memory")
         :                      __CMPXCHG_CASE(x,  , rel_, 64,  l, "memory")
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, al, "memory")
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, al, "memory")
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, al, "memory")
         :                      __CMPXCHG_CASE(x,  ,  mb_, 64, al, "memory")
    0.00 :   ffff80001029ec5c:       mov     x0, x4
    0.00 :   ffff80001029ec60:       mov     x1, x3
    0.00 :   ffff80001029ec64:       mov     x7, x1
    0.00 :   ffff80001029ec68:       casal   x7, x2, [x4]
   39.68 :   ffff80001029ec6c:       mov     x0, x7
         :                      atomic64_try_cmpxchg():
         :                      if (unlikely(r != o))
    0.00 :   ffff80001029ec70:       cmp     x0, x3
    0.00 :   ffff80001029ec74:       b.ne    ffff80001029ec9c <__fget+0xc4>  // b.any
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.00 :   ffff80001029ec78:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      __fget():
         :                      goto loop;
         :                      }
         :                      rcu_read_unlock();
         :
         :                      return file;
         :                      }
    0.41 :   ffff80001029ec7c:       mov     x0, x19
    0.00 :   ffff80001029ec80:       ldp     x19, x20, [sp, #16]
    1.52 :   ffff80001029ec84:       ldp     x21, x22, [sp, #32]
    0.07 :   ffff80001029ec88:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001029ec8c:       ret
         :                      __ll_sc__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  , rel_, 64,        ,  , l, "memory", L)
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  ,  mb_, 64, dmb ish,  , l, "memory", L)
    0.00 :   ffff80001029ec90:       b       ffff8000102a0150 <f_dupfd+0xa8>
         :                      atomic64_try_cmpxchg():
    0.00 :   ffff80001029ec94:       cmp     x0, x3
    0.00 :   ffff80001029ec98:       b.eq    ffff80001029ec78 <__fget+0xa0>  // b.none
    0.00 :   ffff80001029ec9c:       mov     x3, x0
         :                      atomic64_fetch_add_unless():
         :                      if (unlikely(c == u))
    0.00 :   ffff80001029eca0:       cbnz    x0, ffff80001029ec50 <__fget+0x78>
    0.00 :   ffff80001029eca4:       b       ffff80001029ec08 <__fget+0x30>
         :                      rcu_read_unlock():
    0.00 :   ffff80001029eca8:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      __fcheck_files():
         :                      }
         :                      return NULL;
    0.00 :   ffff80001029ecac:       mov     x19, #0x0                       // #0
         :                      __fget():
    0.00 :   ffff80001029ecb0:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff80001029ecb4:       mov     x0, x19
    0.00 :   ffff80001029ecb8:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001029ecbc:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001029ecc0:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (1502 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102d5ff0 <__get_reqs_available>:
         :                      __get_reqs_available():
         :
         :                      local_irq_restore(flags);
         :                      }
         :
         :                      static bool __get_reqs_available(struct kioctx *ctx)
         :                      {
    0.07 :   ffff8000102d5ff0:       mov     x6, x0
         :                      arch_local_save_flags():
         :                      */
         :                      static inline unsigned long arch_local_save_flags(void)
         :                      {
         :                      unsigned long flags;
         :
         :                      asm volatile(ALTERNATIVE(
    0.73 :   ffff8000102d5ff4:       mrs     x8, daif
         :                      arch_irqs_disabled_flags():
         :
         :                      static inline int arch_irqs_disabled_flags(unsigned long flags)
         :                      {
         :                      int res;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff8000102d5ff8:       and     w0, w8, #0x80
         :                      arch_local_irq_save():
         :
         :                      /*
         :                      * There are too many states with IRQs disabled, just keep the current
         :                      * state if interrupts are already disabled/masked.
         :                      */
         :                      if (!arch_irqs_disabled_flags(flags))
    0.00 :   ffff8000102d5ffc:       cbnz    w0, ffff8000102d6008 <__get_reqs_available+0x18>
         :                      arch_local_irq_disable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff8000102d6000:       mov     x0, #0x60                       // #96
    0.00 :   ffff8000102d6004:       msr     daifset, #0x2
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000102d6008:       mrs     x9, tpidr_el1
         :                      __get_reqs_available():
         :                      struct kioctx_cpu *kcpu;
         :                      bool ret = false;
         :                      unsigned long flags;
         :
         :                      local_irq_save(flags);
         :                      kcpu = this_cpu_ptr(ctx->cpu);
    0.00 :   ffff8000102d600c:       ldr     x5, [x6, #128]
         :                      if (!kcpu->reqs_available) {
    0.00 :   ffff8000102d6010:       ldr     w0, [x5, x9]
    0.00 :   ffff8000102d6014:       cbnz    w0, ffff8000102d6084 <__get_reqs_available+0x94>
         :                      atomic_cmpxchg():
         :                      #if !defined(arch_atomic_cmpxchg_relaxed) || defined(arch_atomic_cmpxchg)
         :                      static inline int
         :                      atomic_cmpxchg(atomic_t *v, int old, int new)
         :                      {
         :                      kasan_check_write(v, sizeof(*v));
         :                      return arch_atomic_cmpxchg(v, old, new);
    0.00 :   ffff8000102d6018:       add     x3, x6, #0x100
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d601c:       ldr     w4, [x6, #256]
    0.00 :   ffff8000102d6020:       b       ffff8000102d604c <__get_reqs_available+0x5c>
         :                      __get_reqs_available():
         :                      if (avail < ctx->req_batch)
         :                      goto out;
         :
         :                      old = avail;
         :                      avail = atomic_cmpxchg(&ctx->reqs_available,
         :                      avail, avail - ctx->req_batch);
    0.00 :   ffff8000102d6024:       sub     w2, w4, w2
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000102d6028:       b       ffff8000102d6064 <__get_reqs_available+0x74>
    0.00 :   ffff8000102d602c:       b       ffff8000102d6064 <__get_reqs_available+0x74>
         :                      __lse__cmpxchg_case_mb_32():
         :                      __CMPXCHG_CASE(w, h, rel_, 16,  l, "memory")
         :                      __CMPXCHG_CASE(w,  , rel_, 32,  l, "memory")
         :                      __CMPXCHG_CASE(x,  , rel_, 64,  l, "memory")
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, al, "memory")
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, al, "memory")
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, al, "memory")
    0.00 :   ffff8000102d6030:       mov     x0, x3
    0.00 :   ffff8000102d6034:       mov     w7, w1
    0.00 :   ffff8000102d6038:       casal   w7, w2, [x3]
    0.00 :   ffff8000102d603c:       mov     w0, w7
         :                      __get_reqs_available():
         :                      } while (avail != old);
    0.00 :   ffff8000102d6040:       cmp     w4, w0
    0.00 :   ffff8000102d6044:       mov     w4, w0
    0.00 :   ffff8000102d6048:       b.eq    ffff8000102d6078 <__get_reqs_available+0x88>  // b.none
         :                      if (avail < ctx->req_batch)
    0.00 :   ffff8000102d604c:       ldr     w2, [x6, #136]
         :                      atomic_cmpxchg():
    0.00 :   ffff8000102d6050:       sxtw    x1, w4
         :                      __get_reqs_available():
    0.00 :   ffff8000102d6054:       cmp     w2, w4
    0.00 :   ffff8000102d6058:       b.ls    ffff8000102d6024 <__get_reqs_available+0x34>  // b.plast
         :                      bool ret = false;
    0.00 :   ffff8000102d605c:       mov     w0, #0x0                        // #0
    0.00 :   ffff8000102d6060:       b       ffff8000102d6090 <__get_reqs_available+0xa0>
         :                      __ll_sc__cmpxchg_case_mb_32():
         :                      __CMPXCHG_CASE(w, h, rel_, 16,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w,  , rel_, 32,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  , rel_, 64,        ,  , l, "memory", L)
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, dmb ish,  , l, "memory", K)
    0.00 :   ffff8000102d6064:       and     x1, x1, #0xffffffff
    0.00 :   ffff8000102d6068:       b       ffff8000102da4b8 <__arm64_compat_sys_io_pgetevents_time64+0x1f8>
         :                      __get_reqs_available():
         :                      } while (avail != old);
    0.00 :   ffff8000102d606c:       cmp     w4, w0
    0.00 :   ffff8000102d6070:       mov     w4, w0
    0.00 :   ffff8000102d6074:       b.ne    ffff8000102d604c <__get_reqs_available+0x5c>  // b.any
         :
         :                      kcpu->reqs_available += ctx->req_batch;
    0.00 :   ffff8000102d6078:       ldr     w0, [x5, x9]
    0.00 :   ffff8000102d607c:       ldr     w1, [x6, #136]
    0.00 :   ffff8000102d6080:       add     w0, w0, w1
         :                      }
         :
         :                      ret = true;
         :                      kcpu->reqs_available--;
    0.00 :   ffff8000102d6084:       sub     w1, w0, #0x1
         :                      ret = true;
    0.00 :   ffff8000102d6088:       mov     w0, #0x1                        // #1
         :                      kcpu->reqs_available--;
    0.00 :   ffff8000102d608c:       str     w1, [x5, x9]
         :                      arch_local_irq_restore():
         :                      /*
         :                      * restore saved IRQ state
         :                      */
         :                      static inline void arch_local_irq_restore(unsigned long flags)
         :                      {
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff8000102d6090:       msr     daif, x8
         :                      __get_reqs_available():
         :                      out:
         :                      local_irq_restore(flags);
         :                      return ret;
         :                      }
   99.20 :   ffff8000102d6094:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (791 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010469598 <part_dec_in_flight>:
         :                      part_dec_in_flight():
         :                      part_stat_local_inc(&part_to_disk(part)->part0, in_flight[rw]);
         :                      }
         :
         :                      void part_dec_in_flight(struct request_queue *q, struct hd_struct *part, int rw)
         :                      {
         :                      if (queue_is_mq(q))
    6.49 :   ffff800010469598:       ldr     x0, [x0, #48]
    0.00 :   ffff80001046959c:       cbz     x0, ffff8000104695a4 <part_dec_in_flight+0xc>
         :                      return;
         :
         :                      part_stat_local_dec(part, in_flight[rw]);
         :                      if (part->partno)
         :                      part_stat_local_dec(&part_to_disk(part)->part0, in_flight[rw]);
         :                      }
   93.51 :   ffff8000104695a0:       ret
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000104695a4:       mrs     x4, tpidr_el1
         :                      part_dec_in_flight():
         :                      part_stat_local_dec(part, in_flight[rw]);
    0.00 :   ffff8000104695a8:       adrp    x3, ffff8000114ca000 <bp_hardening_data>
    0.00 :   ffff8000104695ac:       add     x0, x3, #0x18
    0.00 :   ffff8000104695b0:       ldrsw   x6, [x0, x4]
    0.00 :   ffff8000104695b4:       adrp    x5, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000104695b8:       add     x4, x5, #0x8e8
    0.00 :   ffff8000104695bc:       sxtw    x2, w2
    0.00 :   ffff8000104695c0:       add     x2, x2, #0x12
    0.00 :   ffff8000104695c4:       ldr     x0, [x1, #840]
    0.00 :   ffff8000104695c8:       ldr     x4, [x4, x6, lsl #3]
    0.00 :   ffff8000104695cc:       lsl     x2, x2, #3
    0.00 :   ffff8000104695d0:       add     x0, x0, x4
    0.00 :   ffff8000104695d4:       add     x0, x0, x2
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000104695d8:       b       ffff800010469644 <part_dec_in_flight+0xac>
    0.00 :   ffff8000104695dc:       b       ffff800010469644 <part_dec_in_flight+0xac>
         :                      __lse_atomic64_sub():
         :
         :                      #undef ATOMIC64_FETCH_OP_AND
         :
         :                      static inline void __lse_atomic64_sub(s64 i, atomic64_t *v)
         :                      {
         :                      asm volatile(
    0.00 :   ffff8000104695e0:       mov     x4, #0x1                        // #1
    0.00 :   ffff8000104695e4:       neg     x4, x4
    0.00 :   ffff8000104695e8:       stadd   x4, [x0]
         :                      part_dec_in_flight():
         :                      if (part->partno)
    0.00 :   ffff8000104695ec:       ldr     w0, [x1, #820]
    0.00 :   ffff8000104695f0:       cbz     w0, ffff8000104695a0 <part_dec_in_flight+0x8>
         :                      part_to_disk():
         :                      struct lockdep_map lockdep_map;
         :                      };
         :
         :                      static inline struct gendisk *part_to_disk(struct hd_struct *part)
         :                      {
         :                      if (likely(part)) {
    0.00 :   ffff8000104695f4:       cbz     x1, ffff800010469650 <part_dec_in_flight+0xb8>
         :                      if (part->partno)
         :                      return dev_to_disk(part_to_dev(part)->parent);
    0.00 :   ffff8000104695f8:       ldr     x0, [x1, #104]
    0.00 :   ffff8000104695fc:       sub     x0, x0, #0x70
         :                      part_dec_in_flight():
         :                      part_stat_local_dec(&part_to_disk(part)->part0, in_flight[rw]);
    0.00 :   ffff800010469600:       add     x3, x3, #0x18
    0.00 :   ffff800010469604:       add     x5, x5, #0x8e8
         :                      __my_cpu_offset():
    0.00 :   ffff800010469608:       mrs     x1, tpidr_el1
         :                      part_dec_in_flight():
    0.00 :   ffff80001046960c:       ldrsw   x1, [x3, x1]
    0.00 :   ffff800010469610:       ldr     x0, [x0, #912]
    0.00 :   ffff800010469614:       ldr     x1, [x5, x1, lsl #3]
    0.00 :   ffff800010469618:       add     x0, x0, x1
    0.00 :   ffff80001046961c:       add     x2, x0, x2
         :                      arch_static_branch_jump():
    0.00 :   ffff800010469620:       b       ffff800010469638 <part_dec_in_flight+0xa0>
    0.00 :   ffff800010469624:       b       ffff800010469638 <part_dec_in_flight+0xa0>
         :                      __lse_atomic64_sub():
    0.00 :   ffff800010469628:       mov     x0, #0x1                        // #1
    0.00 :   ffff80001046962c:       neg     x0, x0
    0.00 :   ffff800010469630:       stadd   x0, [x2]
         :                      part_dec_in_flight():
         :                      }
    0.00 :   ffff800010469634:       ret
         :                      __ll_sc_atomic64_sub():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(add, add, I)
         :                      ATOMIC64_OPS(sub, sub, J)
    0.00 :   ffff800010469638:       mov     x0, #0x1                        // #1
    0.00 :   ffff80001046963c:       b       ffff80001046ae04 <disk_clear_events+0x164>
         :                      part_dec_in_flight():
    0.00 :   ffff800010469640:       ret
         :                      __ll_sc_atomic64_sub():
    0.00 :   ffff800010469644:       mov     x4, #0x1                        // #1
    0.00 :   ffff800010469648:       b       ffff80001046ae1c <disk_clear_events+0x17c>
    0.00 :   ffff80001046964c:       b       ffff8000104695ec <part_dec_in_flight+0x54>
         :                      part_to_disk():
         :                      else
         :                      return dev_to_disk(part_to_dev(part));
         :                      }
         :                      return NULL;
    0.00 :   ffff800010469650:       mov     x0, #0x0                        // #0
    0.00 :   ffff800010469654:       b       ffff800010469600 <part_dec_in_flight+0x68>
 Percent |	Source code & Disassembly of vmlinux for cycles (1559 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102c4718 <blkdev_direct_IO>:
         :                      blkdev_direct_IO():
         :                      return ret;
         :                      }
         :
         :                      static ssize_t
         :                      blkdev_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
         :                      {
    1.09 :   ffff8000102c4718:       stp     x29, x30, [sp, #-176]!
    0.00 :   ffff8000102c471c:       mov     x29, sp
    1.15 :   ffff8000102c4720:       str     x20, [sp, #24]
    0.00 :   ffff8000102c4724:       mov     x20, x0
    0.32 :   ffff8000102c4728:       str     x23, [sp, #48]
    0.00 :   ffff8000102c472c:       mov     x23, x1
    1.28 :   ffff8000102c4730:       str     x25, [sp, #64]
    0.00 :   ffff8000102c4734:       adrp    x1, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000102c4738:       add     x1, x1, #0x8c8
    0.13 :   ffff8000102c473c:       ldr     x0, [x1]
    0.38 :   ffff8000102c4740:       str     x0, [x29, #168]
    0.00 :   ffff8000102c4744:       mov     x0, #0x0                        // #0
         :                      int nr_pages;
         :
         :                      nr_pages = iov_iter_npages(iter, BIO_MAX_PAGES + 1);
    0.00 :   ffff8000102c4748:       mov     w1, #0x101                      // #257
    0.00 :   ffff8000102c474c:       mov     x0, x23
    0.45 :   ffff8000102c4750:       bl      ffff800010483cf8 <iov_iter_npages>
    0.00 :   ffff8000102c4754:       mov     w2, w0
         :                      if (!nr_pages)
         :                      return 0;
    0.00 :   ffff8000102c4758:       mov     x0, #0x0                        // #0
         :                      if (!nr_pages)
    0.00 :   ffff8000102c475c:       cbz     w2, ffff8000102c4a00 <blkdev_direct_IO+0x2e8>
    0.00 :   ffff8000102c4760:       str     x24, [x29, #56]
         :                      if (is_sync_kiocb(iocb) && nr_pages <= BIO_MAX_PAGES)
    0.00 :   ffff8000102c4764:       ldr     x0, [x20, #16]
    0.00 :   ffff8000102c4768:       cmp     x0, #0x0
    0.00 :   ffff8000102c476c:       cset    w0, eq  // eq = none
    0.00 :   ffff8000102c4770:       cmp     w2, #0x100
    3.47 :   ffff8000102c4774:       cset    w24, le
    0.00 :   ffff8000102c4778:       ands    w24, w0, w24
    0.00 :   ffff8000102c477c:       b.ne    ffff8000102c4acc <blkdev_direct_IO+0x3b4>  // b.any
    0.00 :   ffff8000102c4780:       str     x19, [x29, #16]
         :                      return __blkdev_direct_IO_simple(iocb, iter, nr_pages);
         :
         :                      return __blkdev_direct_IO(iocb, iter, min(nr_pages, BIO_MAX_PAGES));
    0.00 :   ffff8000102c4784:       cmp     w2, #0x100
    2.70 :   ffff8000102c4788:       str     x22, [x29, #40]
    0.00 :   ffff8000102c478c:       mov     w19, #0x100                     // #256
    0.00 :   ffff8000102c4790:       str     x26, [x29, #72]
    0.00 :   ffff8000102c4794:       csel    w19, w2, w19, le
    0.00 :   ffff8000102c4798:       str     x28, [x29, #88]
         :                      __blkdev_direct_IO():
         :                      loff_t pos = iocb->ki_pos;
    0.00 :   ffff8000102c479c:       ldp     x0, x22, [x20]
         :                      bool is_poll = (iocb->ki_flags & IOCB_HIPRI) != 0;
    0.71 :   ffff8000102c47a0:       ldr     w4, [x20, #32]
         :                      iov_iter_rw():
         :                      return iov_iter_type(i) == ITER_DISCARD;
         :                      }
         :
         :                      static inline unsigned char iov_iter_rw(const struct iov_iter *i)
         :                      {
         :                      return i->type & (READ | WRITE);
    0.00 :   ffff8000102c47a4:       ldr     w26, [x23]
         :                      bdev_file_inode():
         :                      return file->f_mapping->host;
    0.00 :   ffff8000102c47a8:       ldr     x3, [x0, #240]
         :                      __blkdev_direct_IO():
         :                      bool is_poll = (iocb->ki_flags & IOCB_HIPRI) != 0;
    0.00 :   ffff8000102c47ac:       and     w0, w4, #0x8
    0.06 :   ffff8000102c47b0:       str     w0, [x29, #120]
         :                      iov_iter_rw():
    0.00 :   ffff8000102c47b4:       and     w0, w26, #0x1
    0.71 :   ffff8000102c47b8:       str     w0, [x29, #124]
         :                      __blkdev_direct_IO():
         :                      if ((pos | iov_iter_alignment(iter)) &
    0.00 :   ffff8000102c47bc:       mov     x0, x23
         :                      bdev_file_inode():
         :                      return file->f_mapping->host;
    0.00 :   ffff8000102c47c0:       ldr     x28, [x3]
         :                      __blkdev_direct_IO():
         :                      if ((pos | iov_iter_alignment(iter)) &
    0.00 :   ffff8000102c47c4:       bl      ffff8000104834c8 <iov_iter_alignment>
    0.00 :   ffff8000102c47c8:       orr     x0, x22, x0
         :                      queue_logical_block_size():
         :
         :                      static inline unsigned queue_logical_block_size(const struct request_queue *q)
         :                      {
         :                      int retval = 512;
         :
         :                      if (q && q->limits.logical_block_size)
    0.00 :   ffff8000102c47cc:       mov     x1, #0x1ff                      // #511
         :                      bdev_get_queue():
         :                      return bdev->bd_disk->queue;    /* this is never NULL */
    0.06 :   ffff8000102c47d0:       ldur    x2, [x28, #-88]
    1.41 :   ffff8000102c47d4:       ldr     x2, [x2, #1040]
         :                      queue_logical_block_size():
         :                      if (q && q->limits.logical_block_size)
    0.00 :   ffff8000102c47d8:       cbz     x2, ffff8000102c47ec <blkdev_direct_IO+0xd4>
    8.34 :   ffff8000102c47dc:       ldr     w2, [x2, #1088]
    0.00 :   ffff8000102c47e0:       sub     w3, w2, #0x1
    0.00 :   ffff8000102c47e4:       cmp     w2, #0x0
    0.00 :   ffff8000102c47e8:       csel    x1, x3, x1, ne  // ne = any
         :                      __blkdev_direct_IO():
   10.86 :   ffff8000102c47ec:       tst     x0, x1
    0.00 :   ffff8000102c47f0:       b.ne    ffff8000102c4bb4 <blkdev_direct_IO+0x49c>  // b.any
         :                      bio = bio_alloc_bioset(GFP_KERNEL, nr_pages, &blkdev_dio_pool);
    0.00 :   ffff8000102c47f4:       mov     w1, w19
    3.01 :   ffff8000102c47f8:       str     x21, [x29, #32]
    0.00 :   ffff8000102c47fc:       adrp    x2, ffff800011ab4000 <in_lookup_hashtable+0x1c08>
    0.00 :   ffff8000102c4800:       mov     w0, #0xcc0                      // #3264
    0.00 :   ffff8000102c4804:       add     x2, x2, #0x5e0
    0.00 :   ffff8000102c4808:       str     x27, [x29, #80]
    0.00 :   ffff8000102c480c:       bl      ffff8000104504e0 <bio_alloc_bioset>
    0.58 :   ffff8000102c4810:       mov     x21, x0
         :                      dio = container_of(bio, struct blkdev_dio, bio);
    0.06 :   ffff8000102c4814:       sub     x19, x0, #0x18
         :                      dio->is_sync = is_sync = is_sync_kiocb(iocb);
    0.26 :   ffff8000102c4818:       ldr     x0, [x20, #16]
    0.00 :   ffff8000102c481c:       mov     x1, x0
    0.06 :   ffff8000102c4820:       ldurb   w0, [x21, #-4]
         :                      is_sync_kiocb():
         :                      randomized_struct_fields_end
         :                      };
         :
         :                      static inline bool is_sync_kiocb(struct kiocb *kiocb)
         :                      {
         :                      return kiocb->ki_complete == NULL;
    0.00 :   ffff8000102c4824:       cmp     x1, #0x0
         :                      __blkdev_direct_IO():
    0.06 :   ffff8000102c4828:       str     x1, [x29, #104]
         :                      is_sync_kiocb():
    0.00 :   ffff8000102c482c:       cset    w1, eq  // eq = none
         :                      __blkdev_direct_IO():
    0.00 :   ffff8000102c4830:       bfi     w0, w1, #2, #1
    0.45 :   ffff8000102c4834:       sturb   w0, [x21, #-4]
         :                      if (dio->is_sync) {
    0.00 :   ffff8000102c4838:       tbnz    w0, #2, ffff8000102c4b28 <blkdev_direct_IO+0x410>
         :                      dio->iocb = iocb;
    0.06 :   ffff8000102c483c:       stur    x20, [x21, #-24]
         :                      dio->multi_bio = false;
    3.72 :   ffff8000102c4840:       ldurb   w0, [x21, #-4]
         :                      dio->size = 0;
    2.96 :   ffff8000102c4844:       stur    xzr, [x21, #-16]
         :                      dio->multi_bio = false;
    0.00 :   ffff8000102c4848:       and     w0, w0, #0xfffffffe
    0.71 :   ffff8000102c484c:       sturb   w0, [x21, #-4]
         :                      dio->should_dirty = is_read && iter_is_iovec(iter);
    4.37 :   ffff8000102c4850:       ldr     w0, [x29, #124]
    0.00 :   ffff8000102c4854:       cbnz    w0, ffff8000102c4868 <blkdev_direct_IO+0x150>
         :                      iov_iter_type():
         :                      return i->type & ~(READ | WRITE);
    0.00 :   ffff8000102c4858:       ldr     w0, [x23]
    0.00 :   ffff8000102c485c:       and     w0, w0, #0xfffffffe
         :                      __blkdev_direct_IO():
    0.00 :   ffff8000102c4860:       cmp     w0, #0x4
    0.00 :   ffff8000102c4864:       cset    w24, eq  // eq = none
    5.71 :   ffff8000102c4868:       ldurb   w0, [x21, #-4]
    0.00 :   ffff8000102c486c:       bfi     w0, w24, #1, #1
    7.57 :   ffff8000102c4870:       sturb   w0, [x21, #-4]
         :                      if (!is_poll)
    0.00 :   ffff8000102c4874:       ldr     w0, [x29, #120]
    0.00 :   ffff8000102c4878:       cbz     w0, ffff8000102c4b1c <blkdev_direct_IO+0x404>
    0.19 :   ffff8000102c487c:       adrp    x27, ffff8000102c4000 <__blkdev_get+0x28>
         :                      bio_alloc():
         :
         :                      extern struct bio_set fs_bio_set;
         :
         :                      static inline struct bio *bio_alloc(gfp_t gfp_mask, unsigned int nr_iovecs)
         :                      {
         :                      return bio_alloc_bioset(gfp_mask, nr_iovecs, &fs_bio_set);
    0.00 :   ffff8000102c4880:       adrp    x0, ffff800011abc000 <drbg_algs+0x2b80>
         :                      queue_logical_block_size():
    0.00 :   ffff8000102c4884:       mov     x26, x21
         :                      bio_alloc():
    0.00 :   ffff8000102c4888:       add     x0, x0, #0xb8
    0.32 :   ffff8000102c488c:       sub     x24, x28, #0xd8
         :                      __blkdev_direct_IO():
         :                      bio->bi_end_io = blkdev_bio_end_io;
    0.00 :   ffff8000102c4890:       add     x27, x27, #0xcd8
         :                      bio_alloc():
    0.00 :   ffff8000102c4894:       str     x0, [x29, #112]
    0.00 :   ffff8000102c4898:       b       ffff8000102c4938 <blkdev_direct_IO+0x220>
         :                      __blkdev_direct_IO():
         :                      bio->bi_opf = REQ_OP_READ;
    1.74 :   ffff8000102c489c:       str     wzr, [x26, #16]
         :                      if (dio->should_dirty)
    1.09 :   ffff8000102c48a0:       ldrb    w0, [x19, #20]
    0.00 :   ffff8000102c48a4:       tbnz    w0, #1, ffff8000102c4a58 <blkdev_direct_IO+0x340>
         :                      dio->size += bio->bi_iter.bi_size;
    0.65 :   ffff8000102c48a8:       ldr     w4, [x26, #40]
         :                      nr_pages = iov_iter_npages(iter, BIO_MAX_PAGES);
    0.00 :   ffff8000102c48ac:       mov     w1, #0x100                      // #256
         :                      dio->size += bio->bi_iter.bi_size;
    0.00 :   ffff8000102c48b0:       ldr     x3, [x19, #8]
         :                      nr_pages = iov_iter_npages(iter, BIO_MAX_PAGES);
    0.00 :   ffff8000102c48b4:       mov     x0, x23
         :                      dio->size += bio->bi_iter.bi_size;
    0.00 :   ffff8000102c48b8:       add     x3, x3, x4
    0.00 :   ffff8000102c48bc:       str     x3, [x19, #8]
         :                      pos += bio->bi_iter.bi_size;
    0.38 :   ffff8000102c48c0:       ldr     w3, [x26, #40]
    0.00 :   ffff8000102c48c4:       add     x22, x22, x3
         :                      nr_pages = iov_iter_npages(iter, BIO_MAX_PAGES);
    0.00 :   ffff8000102c48c8:       bl      ffff800010483cf8 <iov_iter_npages>
    0.00 :   ffff8000102c48cc:       mov     w25, w0
         :                      if (!nr_pages) {
    0.00 :   ffff8000102c48d0:       cbz     w0, ffff8000102c4ae0 <blkdev_direct_IO+0x3c8>
         :                      if (!dio->multi_bio) {
    0.00 :   ffff8000102c48d4:       ldrb    w0, [x19, #20]
    0.00 :   ffff8000102c48d8:       tbnz    w0, #0, ffff8000102c4a34 <blkdev_direct_IO+0x31c>
         :                      if (!is_sync)
    0.00 :   ffff8000102c48dc:       ldr     x0, [x29, #104]
    0.00 :   ffff8000102c48e0:       cbz     x0, ffff8000102c4908 <blkdev_direct_IO+0x1f0>
         :                      bio_get():
         :                      bio->bi_flags |= (1 << BIO_REFFED);
    0.00 :   ffff8000102c48e4:       ldrh    w0, [x26, #20]
    0.00 :   ffff8000102c48e8:       orr     w0, w0, #0x100
    0.00 :   ffff8000102c48ec:       strh    w0, [x26, #20]
         :                      smp_mb__before_atomic();
    0.00 :   ffff8000102c48f0:       dmb     ish
         :                      atomic_inc(&bio->__bi_cnt);
    0.00 :   ffff8000102c48f4:       add     x0, x26, #0x64
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000102c48f8:       b       ffff8000102c4a2c <blkdev_direct_IO+0x314>
    0.00 :   ffff8000102c48fc:       b       ffff8000102c4a2c <blkdev_direct_IO+0x314>
         :                      __lse_atomic_add():
         :                      }
         :
         :                      ATOMIC_OP(andnot, stclr)
         :                      ATOMIC_OP(or, stset)
         :                      ATOMIC_OP(xor, steor)
         :                      ATOMIC_OP(add, stadd)
    0.00 :   ffff8000102c4900:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000102c4904:       stadd   w1, [x0]
         :                      __blkdev_direct_IO():
         :                      dio->multi_bio = true;
    0.00 :   ffff8000102c4908:       ldrb    w0, [x19, #20]
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102c490c:       mov     w1, #0x2                        // #2
         :                      __blkdev_direct_IO():
    0.00 :   ffff8000102c4910:       orr     w0, w0, #0x1
    0.00 :   ffff8000102c4914:       strb    w0, [x19, #20]
         :                      __write_once_size():
    0.00 :   ffff8000102c4918:       stur    w1, [x21, #-8]
         :                      __blkdev_direct_IO():
         :                      submit_bio(bio);
    0.00 :   ffff8000102c491c:       mov     x0, x26
    0.00 :   ffff8000102c4920:       bl      ffff800010454dc0 <submit_bio>
         :                      bio_alloc():
         :                      return bio_alloc_bioset(gfp_mask, nr_iovecs, &fs_bio_set);
    0.00 :   ffff8000102c4924:       ldr     x2, [x29, #112]
    0.00 :   ffff8000102c4928:       mov     w1, w25
    0.00 :   ffff8000102c492c:       mov     w0, #0xcc0                      // #3264
    0.00 :   ffff8000102c4930:       bl      ffff8000104504e0 <bio_alloc_bioset>
    0.00 :   ffff8000102c4934:       mov     x26, x0
         :                      __blkdev_direct_IO():
         :                      bio_set_dev(bio, bdev);
    0.51 :   ffff8000102c4938:       ldr     x0, [x24, #128]
    0.06 :   ffff8000102c493c:       ldr     x1, [x26, #8]
    0.00 :   ffff8000102c4940:       cmp     x1, x0
    0.00 :   ffff8000102c4944:       b.eq    ffff8000102c4958 <blkdev_direct_IO+0x240>  // b.none
         :                      bio_clear_flag():
         :                      bio->bi_flags &= ~(1U << bit);
    0.64 :   ffff8000102c4948:       ldrh    w0, [x26, #20]
    0.00 :   ffff8000102c494c:       and     w0, w0, #0xfffffdff
    0.52 :   ffff8000102c4950:       strh    w0, [x26, #20]
    0.00 :   ffff8000102c4954:       ldr     x0, [x24, #128]
         :                      __blkdev_direct_IO():
    0.13 :   ffff8000102c4958:       str     x0, [x26, #8]
    0.00 :   ffff8000102c495c:       mov     x0, x26
    1.16 :   ffff8000102c4960:       ldrb    w1, [x24, #108]
    2.43 :   ffff8000102c4964:       strb    w1, [x26, #27]
    0.00 :   ffff8000102c4968:       bl      ffff80001044fed8 <bio_associate_blkg>
         :                      bio->bi_iter.bi_sector = pos >> 9;
    0.06 :   ffff8000102c496c:       asr     x0, x22, #9
    0.26 :   ffff8000102c4970:       str     x0, [x26, #32]
         :                      ret = bio_iov_iter_get_pages(bio, iter);
    0.00 :   ffff8000102c4974:       mov     x1, x23
    0.00 :   ffff8000102c4978:       mov     x0, x26
         :                      bio->bi_write_hint = iocb->ki_hint;
    0.00 :   ffff8000102c497c:       ldrh    w3, [x20, #36]
    0.39 :   ffff8000102c4980:       strh    w3, [x26, #24]
         :                      bio->bi_private = dio;
    1.21 :   ffff8000102c4984:       stp     x27, x19, [x26, #56]
         :                      bio->bi_ioprio = iocb->ki_ioprio;
    0.52 :   ffff8000102c4988:       ldrh    w3, [x20, #38]
    0.26 :   ffff8000102c498c:       strh    w3, [x26, #22]
         :                      ret = bio_iov_iter_get_pages(bio, iter);
    0.00 :   ffff8000102c4990:       bl      ffff800010450aa8 <bio_iov_iter_get_pages>
    0.06 :   ffff8000102c4994:       mov     w25, w0
         :                      if (unlikely(ret)) {
    0.00 :   ffff8000102c4998:       cbnz    w0, ffff8000102c4a64 <blkdev_direct_IO+0x34c>
         :                      if (is_read) {
    1.15 :   ffff8000102c499c:       ldr     w0, [x29, #124]
    0.00 :   ffff8000102c49a0:       cbz     w0, ffff8000102c489c <blkdev_direct_IO+0x184>
         :                      dio_bio_write_op():
         :                      if (iocb->ki_flags & IOCB_DSYNC)
    0.00 :   ffff8000102c49a4:       ldr     w0, [x20, #32]
         :                      unsigned int op = REQ_OP_WRITE | REQ_SYNC | REQ_IDLE;
    0.00 :   ffff8000102c49a8:       mov     w2, #0x8801                     // #34817
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff8000102c49ac:       mrs     x1, sp_el0
         :                      __blkdev_direct_IO():
         :                      task_io_account_write(bio->bi_iter.bi_size);
    0.00 :   ffff8000102c49b0:       ldr     w3, [x26, #40]
         :                      dio_bio_write_op():
         :                      unsigned int op = REQ_OP_WRITE | REQ_SYNC | REQ_IDLE;
    0.00 :   ffff8000102c49b4:       tst     x0, #0x10
    0.00 :   ffff8000102c49b8:       mov     w0, #0x8801                     // #34817
    0.00 :   ffff8000102c49bc:       movk    w0, #0x2, lsl #16
    0.00 :   ffff8000102c49c0:       csel    w0, w0, w2, ne  // ne = any
         :                      __blkdev_direct_IO():
         :                      bio->bi_opf = dio_bio_write_op(iocb);
    0.00 :   ffff8000102c49c4:       str     w0, [x26, #16]
         :                      task_io_account_write():
         :                      return p->ioac.read_bytes >> 9;
         :                      }
         :
         :                      static inline void task_io_account_write(size_t bytes)
         :                      {
         :                      current->ioac.write_bytes += bytes;
    0.00 :   ffff8000102c49c8:       ldr     x0, [x1, #1936]
    0.00 :   ffff8000102c49cc:       add     x0, x0, x3
    0.00 :   ffff8000102c49d0:       str     x0, [x1, #1936]
    0.00 :   ffff8000102c49d4:       b       ffff8000102c48a8 <blkdev_direct_IO+0x190>
         :                      __blkdev_direct_IO():
         :                      __set_current_state(TASK_RUNNING);
    0.00 :   ffff8000102c49d8:       str     xzr, [x0, #24]
         :                      if (!ret)
    0.00 :   ffff8000102c49dc:       cbz     w25, ffff8000102c4b64 <blkdev_direct_IO+0x44c>
         :                      bio_put(&dio->bio);
    0.00 :   ffff8000102c49e0:       mov     x0, x21
    0.00 :   ffff8000102c49e4:       bl      ffff800010450020 <bio_put>
         :                      return ret;
    0.00 :   ffff8000102c49e8:       sxtw    x0, w25
         :                      blkdev_direct_IO():
         :                      return __blkdev_direct_IO(iocb, iter, min(nr_pages, BIO_MAX_PAGES));
    0.77 :   ffff8000102c49ec:       ldr     x19, [x29, #16]
    1.16 :   ffff8000102c49f0:       ldp     x21, x22, [x29, #32]
    0.13 :   ffff8000102c49f4:       ldr     x24, [x29, #56]
    0.77 :   ffff8000102c49f8:       ldp     x26, x27, [x29, #72]
    6.22 :   ffff8000102c49fc:       ldr     x28, [x29, #88]
         :                      }
    0.00 :   ffff8000102c4a00:       adrp    x1, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000102c4a04:       add     x25, x1, #0x8c8
    0.00 :   ffff8000102c4a08:       ldr     x2, [x29, #168]
    0.13 :   ffff8000102c4a0c:       ldr     x1, [x25]
    0.00 :   ffff8000102c4a10:       eor     x1, x2, x1
    0.00 :   ffff8000102c4a14:       cbnz    x1, ffff8000102c4bd0 <blkdev_direct_IO+0x4b8>
    0.89 :   ffff8000102c4a18:       ldr     x20, [sp, #24]
    1.09 :   ffff8000102c4a1c:       ldr     x23, [sp, #48]
    0.00 :   ffff8000102c4a20:       ldr     x25, [sp, #64]
    0.00 :   ffff8000102c4a24:       ldp     x29, x30, [sp], #176
    0.00 :   ffff8000102c4a28:       ret
         :                      __ll_sc_atomic_add():
         :                      ATOMIC_FETCH_OP (        , dmb ish,  , l, "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
    0.00 :   ffff8000102c4a2c:       b       ffff8000102c55ec <iterate_bdevs+0x184>
    0.00 :   ffff8000102c4a30:       b       ffff8000102c4908 <blkdev_direct_IO+0x1f0>
         :                      __blkdev_direct_IO():
         :                      atomic_inc(&dio->ref);
    0.00 :   ffff8000102c4a34:       add     x1, x19, #0x10
         :                      arch_static_branch_jump():
    0.00 :   ffff8000102c4a38:       b       ffff8000102c4a4c <blkdev_direct_IO+0x334>
    0.00 :   ffff8000102c4a3c:       b       ffff8000102c4a4c <blkdev_direct_IO+0x334>
         :                      __lse_atomic_add():
    0.00 :   ffff8000102c4a40:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000102c4a44:       stadd   w0, [x1]
    0.00 :   ffff8000102c4a48:       b       ffff8000102c491c <blkdev_direct_IO+0x204>
         :                      __ll_sc_atomic_add():
    0.00 :   ffff8000102c4a4c:       add     x2, x19, #0x10
    0.00 :   ffff8000102c4a50:       b       ffff8000102c5604 <iterate_bdevs+0x19c>
    0.00 :   ffff8000102c4a54:       b       ffff8000102c491c <blkdev_direct_IO+0x204>
         :                      __blkdev_direct_IO():
         :                      bio_set_pages_dirty(bio);
    0.06 :   ffff8000102c4a58:       mov     x0, x26
    0.00 :   ffff8000102c4a5c:       bl      ffff800010451a00 <bio_set_pages_dirty>
    0.38 :   ffff8000102c4a60:       b       ffff8000102c48a8 <blkdev_direct_IO+0x190>
         :                      bio->bi_status = BLK_STS_IOERR;
    0.00 :   ffff8000102c4a64:       mov     w0, #0xa                        // #10
    0.00 :   ffff8000102c4a68:       strb    w0, [x26, #26]
         :                      blk_qc_t qc = BLK_QC_T_NONE;
    0.00 :   ffff8000102c4a6c:       mov     w22, #0xffffffff                // #-1
         :                      bio_endio(bio);
    0.00 :   ffff8000102c4a70:       mov     x0, x26
    0.00 :   ffff8000102c4a74:       bl      ffff800010450080 <bio_endio>
         :                      if (!is_poll)
    0.00 :   ffff8000102c4a78:       ldr     w0, [x29, #120]
    0.00 :   ffff8000102c4a7c:       cbz     w0, ffff8000102c4b58 <blkdev_direct_IO+0x440>
         :                      if (!is_sync)
    0.96 :   ffff8000102c4a80:       ldr     x0, [x29, #104]
    0.00 :   ffff8000102c4a84:       cbnz    x0, ffff8000102c4bac <blkdev_direct_IO+0x494>
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff8000102c4a88:       mov     x23, #0x2                       // #2
    0.00 :   ffff8000102c4a8c:       nop
         :                      get_current():
    0.00 :   ffff8000102c4a90:       mrs     x0, sp_el0
         :                      __write_once_size():
    0.00 :   ffff8000102c4a94:       str     x23, [x0, #24]
         :                      __blkdev_direct_IO():
         :                      set_current_state(TASK_UNINTERRUPTIBLE);
    0.00 :   ffff8000102c4a98:       dmb     ish
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102c4a9c:       ldur    x1, [x21, #-24]
         :                      __blkdev_direct_IO():
         :                      if (!READ_ONCE(dio->waiter))
    0.00 :   ffff8000102c4aa0:       cbz     x1, ffff8000102c49d8 <blkdev_direct_IO+0x2c0>
         :                      if (!(iocb->ki_flags & IOCB_HIPRI) ||
    0.00 :   ffff8000102c4aa4:       ldr     w0, [x20, #32]
    0.00 :   ffff8000102c4aa8:       tbz     w0, #3, ffff8000102c4ac4 <blkdev_direct_IO+0x3ac>
         :                      bdev_get_queue():
         :                      return bdev->bd_disk->queue;    /* this is never NULL */
    0.00 :   ffff8000102c4aac:       ldur    x0, [x28, #-88]
         :                      __blkdev_direct_IO():
         :                      !blk_poll(bdev_get_queue(bdev), qc, true))
    0.00 :   ffff8000102c4ab0:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000102c4ab4:       mov     w1, w22
    0.00 :   ffff8000102c4ab8:       ldr     x0, [x0, #1040]
    0.00 :   ffff8000102c4abc:       bl      ffff80001045dec8 <blk_poll>
         :                      if (!(iocb->ki_flags & IOCB_HIPRI) ||
    0.00 :   ffff8000102c4ac0:       cbnz    w0, ffff8000102c4a90 <blkdev_direct_IO+0x378>
         :                      io_schedule();
    0.00 :   ffff8000102c4ac4:       bl      ffff800010cad7f0 <io_schedule>
    0.00 :   ffff8000102c4ac8:       b       ffff8000102c4a90 <blkdev_direct_IO+0x378>
         :                      blkdev_direct_IO():
         :                      return __blkdev_direct_IO_simple(iocb, iter, nr_pages);
    0.00 :   ffff8000102c4acc:       mov     x1, x23
    0.00 :   ffff8000102c4ad0:       mov     x0, x20
    0.00 :   ffff8000102c4ad4:       bl      ffff8000102c3af8 <__blkdev_direct_IO_simple>
    0.00 :   ffff8000102c4ad8:       ldr     x24, [x29, #56]
    0.00 :   ffff8000102c4adc:       b       ffff8000102c4a00 <blkdev_direct_IO+0x2e8>
         :                      __blkdev_direct_IO():
         :                      if (iocb->ki_flags & IOCB_HIPRI) {
    0.70 :   ffff8000102c4ae0:       ldr     w0, [x20, #32]
    0.00 :   ffff8000102c4ae4:       tbz     w0, #3, ffff8000102c4b88 <blkdev_direct_IO+0x470>
         :                      bio_set_polled():
         :                      * must be found by the caller. This is different than IRQ driven IO, where
         :                      * it's safe to wait for IO to complete.
         :                      */
         :                      static inline void bio_set_polled(struct bio *bio, struct kiocb *kiocb)
         :                      {
         :                      bio->bi_opf |= REQ_HIPRI;
    0.00 :   ffff8000102c4ae8:       ldr     w0, [x26, #16]
    0.00 :   ffff8000102c4aec:       orr     w1, w0, #0x2000000
    0.00 :   ffff8000102c4af0:       str     w1, [x26, #16]
         :                      if (!is_sync_kiocb(kiocb))
    0.00 :   ffff8000102c4af4:       ldr     x1, [x20, #16]
    0.00 :   ffff8000102c4af8:       cbz     x1, ffff8000102c4b98 <blkdev_direct_IO+0x480>
         :                      bio->bi_opf |= REQ_NOWAIT;
    0.00 :   ffff8000102c4afc:       mov     w1, #0x2200000                  // #35651584
    0.00 :   ffff8000102c4b00:       orr     w0, w0, w1
    0.00 :   ffff8000102c4b04:       str     w0, [x26, #16]
         :                      __blkdev_direct_IO():
         :                      qc = submit_bio(bio);
    0.00 :   ffff8000102c4b08:       mov     x0, x26
    0.00 :   ffff8000102c4b0c:       bl      ffff800010454dc0 <submit_bio>
    0.00 :   ffff8000102c4b10:       mov     w22, w0
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000102c4b14:       str     w22, [x20, #40]
    0.00 :   ffff8000102c4b18:       b       ffff8000102c4a78 <blkdev_direct_IO+0x360>
         :                      __blkdev_direct_IO():
         :                      blk_start_plug(&plug);
    0.32 :   ffff8000102c4b1c:       add     x0, x29, #0x80
    0.00 :   ffff8000102c4b20:       bl      ffff800010453550 <blk_start_plug>
    0.13 :   ffff8000102c4b24:       b       ffff8000102c487c <blkdev_direct_IO+0x164>
         :                      bio_get():
         :                      bio->bi_flags |= (1 << BIO_REFFED);
    0.00 :   ffff8000102c4b28:       ldrh    w0, [x21, #20]
         :                      get_current():
    0.00 :   ffff8000102c4b2c:       mrs     x1, sp_el0
         :                      bio_get():
    0.00 :   ffff8000102c4b30:       orr     w0, w0, #0x100
         :                      __blkdev_direct_IO():
         :                      dio->waiter = current;
    0.00 :   ffff8000102c4b34:       stur    x1, [x21, #-24]
         :                      bio_get():
    0.00 :   ffff8000102c4b38:       strh    w0, [x21, #20]
         :                      smp_mb__before_atomic();
    0.00 :   ffff8000102c4b3c:       dmb     ish
         :                      atomic_inc(&bio->__bi_cnt);
    0.00 :   ffff8000102c4b40:       add     x1, x21, #0x64
         :                      arch_static_branch_jump():
    0.00 :   ffff8000102c4b44:       b       ffff8000102c4b7c <blkdev_direct_IO+0x464>
    0.00 :   ffff8000102c4b48:       b       ffff8000102c4b7c <blkdev_direct_IO+0x464>
         :                      __lse_atomic_add():
    0.00 :   ffff8000102c4b4c:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000102c4b50:       stadd   w0, [x1]
    0.00 :   ffff8000102c4b54:       b       ffff8000102c4840 <blkdev_direct_IO+0x128>
         :                      __blkdev_direct_IO():
         :                      blk_finish_plug(&plug);
    0.89 :   ffff8000102c4b58:       add     x0, x29, #0x80
    0.00 :   ffff8000102c4b5c:       bl      ffff8000104562e8 <blk_finish_plug>
    0.13 :   ffff8000102c4b60:       b       ffff8000102c4a80 <blkdev_direct_IO+0x368>
         :                      ret = blk_status_to_errno(dio->bio.bi_status);
    0.00 :   ffff8000102c4b64:       ldrb    w0, [x19, #50]
    0.00 :   ffff8000102c4b68:       bl      ffff800010453588 <blk_status_to_errno>
    0.00 :   ffff8000102c4b6c:       mov     w25, w0
         :                      if (likely(!ret))
    0.00 :   ffff8000102c4b70:       cbnz    w0, ffff8000102c49e0 <blkdev_direct_IO+0x2c8>
         :                      ret = dio->size;
    0.00 :   ffff8000102c4b74:       ldur    w25, [x21, #-16]
    0.00 :   ffff8000102c4b78:       b       ffff8000102c49e0 <blkdev_direct_IO+0x2c8>
         :                      __ll_sc_atomic_add():
    0.00 :   ffff8000102c4b7c:       add     x2, x21, #0x64
    0.00 :   ffff8000102c4b80:       b       ffff8000102c561c <iterate_bdevs+0x1b4>
    0.00 :   ffff8000102c4b84:       b       ffff8000102c4840 <blkdev_direct_IO+0x128>
         :                      __blkdev_direct_IO():
         :                      qc = submit_bio(bio);
    4.99 :   ffff8000102c4b88:       mov     x0, x26
    0.00 :   ffff8000102c4b8c:       bl      ffff800010454dc0 <submit_bio>
    0.00 :   ffff8000102c4b90:       mov     w22, w0
    0.00 :   ffff8000102c4b94:       b       ffff8000102c4a78 <blkdev_direct_IO+0x360>
    0.00 :   ffff8000102c4b98:       mov     x0, x26
    0.00 :   ffff8000102c4b9c:       bl      ffff800010454dc0 <submit_bio>
    0.00 :   ffff8000102c4ba0:       mov     w22, w0
         :                      __write_once_size():
    0.00 :   ffff8000102c4ba4:       str     w22, [x20, #40]
    0.00 :   ffff8000102c4ba8:       b       ffff8000102c4a78 <blkdev_direct_IO+0x360>
         :                      __blkdev_direct_IO():
         :                      return -EIOCBQUEUED;
    3.84 :   ffff8000102c4bac:       mov     x0, #0xfffffffffffffdef         // #-529
    0.00 :   ffff8000102c4bb0:       b       ffff8000102c49ec <blkdev_direct_IO+0x2d4>
         :                      return -EINVAL;
    0.00 :   ffff8000102c4bb4:       mov     x0, #0xffffffffffffffea         // #-22
    0.00 :   ffff8000102c4bb8:       ldr     x19, [x29, #16]
    0.00 :   ffff8000102c4bbc:       ldr     x22, [x29, #40]
    0.00 :   ffff8000102c4bc0:       ldr     x24, [x29, #56]
    0.00 :   ffff8000102c4bc4:       ldr     x26, [x29, #72]
    0.00 :   ffff8000102c4bc8:       ldr     x28, [x29, #88]
    0.00 :   ffff8000102c4bcc:       b       ffff8000102c4a00 <blkdev_direct_IO+0x2e8>
    0.00 :   ffff8000102c4bd0:       str     x19, [x29, #16]
    0.00 :   ffff8000102c4bd4:       stp     x21, x22, [x29, #32]
    0.00 :   ffff8000102c4bd8:       str     x24, [x29, #56]
    0.00 :   ffff8000102c4bdc:       stp     x26, x27, [x29, #72]
    0.00 :   ffff8000102c4be0:       str     x28, [x29, #88]
         :                      blkdev_direct_IO():
         :                      }
    0.00 :   ffff8000102c4be4:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (878 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010cb2db8 <_raw_spin_lock_irqsave>:
         :                      _raw_spin_lock_irqsave():
         :                      EXPORT_SYMBOL(_raw_spin_lock);
         :                      #endif
         :
         :                      #ifndef CONFIG_INLINE_SPIN_LOCK_IRQSAVE
         :                      unsigned long __lockfunc _raw_spin_lock_irqsave(raw_spinlock_t *lock)
         :                      {
   38.20 :   ffff800010cb2db8:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff800010cb2dbc:       mov     x3, x0
    0.00 :   ffff800010cb2dc0:       mov     x29, sp
    5.96 :   ffff800010cb2dc4:       str     x19, [sp, #16]
         :                      arch_local_save_flags():
         :                      */
         :                      static inline unsigned long arch_local_save_flags(void)
         :                      {
         :                      unsigned long flags;
         :
         :                      asm volatile(ALTERNATIVE(
    4.52 :   ffff800010cb2dc8:       mrs     x19, daif
         :                      arch_irqs_disabled_flags():
         :
         :                      static inline int arch_irqs_disabled_flags(unsigned long flags)
         :                      {
         :                      int res;
         :
         :                      asm volatile(ALTERNATIVE(
   28.54 :   ffff800010cb2dcc:       and     w0, w19, #0x80
         :                      arch_local_irq_save():
         :
         :                      /*
         :                      * There are too many states with IRQs disabled, just keep the current
         :                      * state if interrupts are already disabled/masked.
         :                      */
         :                      if (!arch_irqs_disabled_flags(flags))
    0.00 :   ffff800010cb2dd0:       cbnz    w0, ffff800010cb2ddc <_raw_spin_lock_irqsave+0x24>
         :                      arch_local_irq_disable():
         :                      asm volatile(ALTERNATIVE(
   20.29 :   ffff800010cb2dd4:       mov     x0, #0x60                       // #96
    2.49 :   ffff800010cb2dd8:       msr     daifset, #0x2
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800010cb2ddc:       mrs     x1, sp_el0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010cb2de0:       ldr     w0, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff800010cb2de4:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010cb2de8:       str     w0, [x1, #16]
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010cb2dec:       b       ffff800010cb2e20 <_raw_spin_lock_irqsave+0x68>
    0.00 :   ffff800010cb2df0:       b       ffff800010cb2e20 <_raw_spin_lock_irqsave+0x68>
         :                      __lse__cmpxchg_case_acq_32():
         :                      __CMPXCHG_CASE(w, h,     , 16,   )
         :                      __CMPXCHG_CASE(w,  ,     , 32,   )
         :                      __CMPXCHG_CASE(x,  ,     , 64,   )
         :                      __CMPXCHG_CASE(w, b, acq_,  8,  a, "memory")
         :                      __CMPXCHG_CASE(w, h, acq_, 16,  a, "memory")
         :                      __CMPXCHG_CASE(w,  , acq_, 32,  a, "memory")
    0.00 :   ffff800010cb2df4:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010cb2df8:       mov     x0, x3
    0.00 :   ffff800010cb2dfc:       mov     w2, #0x1                        // #1
    0.00 :   ffff800010cb2e00:       mov     w4, w1
    0.00 :   ffff800010cb2e04:       casa    w4, w2, [x3]
    0.00 :   ffff800010cb2e08:       mov     w0, w4
         :                      atomic_try_cmpxchg_acquire():
         :                      static inline bool
         :                      atomic_try_cmpxchg_acquire(atomic_t *v, int *old, int new)
         :                      {
         :                      int r, o = *old;
         :                      r = atomic_cmpxchg_acquire(v, o, new);
         :                      if (unlikely(r != o))
    0.00 :   ffff800010cb2e0c:       cbnz    w0, ffff800010cb2e30 <_raw_spin_lock_irqsave+0x78>
         :                      _raw_spin_lock_irqsave():
         :                      return __raw_spin_lock_irqsave(lock);
         :                      }
    0.00 :   ffff800010cb2e10:       mov     x0, x19
    0.00 :   ffff800010cb2e14:       ldr     x19, [sp, #16]
    0.00 :   ffff800010cb2e18:       ldp     x29, x30, [sp], #32
    0.00 :   ffff800010cb2e1c:       ret
         :                      __ll_sc__cmpxchg_case_acq_32():
         :                      __CMPXCHG_CASE(w, h,     , 16,        ,  ,  ,         , K)
         :                      __CMPXCHG_CASE(w,  ,     , 32,        ,  ,  ,         , K)
         :                      __CMPXCHG_CASE( ,  ,     , 64,        ,  ,  ,         , L)
         :                      __CMPXCHG_CASE(w, b, acq_,  8,        , a,  , "memory", K)
         :                      __CMPXCHG_CASE(w, h, acq_, 16,        , a,  , "memory", K)
         :                      __CMPXCHG_CASE(w,  , acq_, 32,        , a,  , "memory", K)
    0.00 :   ffff800010cb2e20:       mov     x1, #0x0                        // #0
    0.00 :   ffff800010cb2e24:       mov     w2, #0x1                        // #1
    0.00 :   ffff800010cb2e28:       b       ffff800010cb30cc <_raw_write_lock_irqsave+0x27c>
         :                      atomic_try_cmpxchg_acquire():
    0.00 :   ffff800010cb2e2c:       cbz     w0, ffff800010cb2e10 <_raw_spin_lock_irqsave+0x58>
         :                      queued_spin_lock():
         :                      u32 val = 0;
         :
         :                      if (likely(atomic_try_cmpxchg_acquire(&lock->val, &val, _Q_LOCKED_VAL)))
         :                      return;
         :
         :                      queued_spin_lock_slowpath(lock, val);
    0.00 :   ffff800010cb2e30:       mov     w1, w0
    0.00 :   ffff800010cb2e34:       mov     x0, x3
    0.00 :   ffff800010cb2e38:       bl      ffff8000101387b0 <queued_spin_lock_slowpath>
         :                      _raw_spin_lock_irqsave():
    0.00 :   ffff800010cb2e3c:       mov     x0, x19
    0.00 :   ffff800010cb2e40:       ldr     x19, [sp, #16]
    0.00 :   ffff800010cb2e44:       ldp     x29, x30, [sp], #32
    0.00 :   ffff800010cb2e48:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (1453 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001045f898 <blk_mq_get_request>:
         :                      blk_mq_get_request():
         :                      }
         :
         :                      static struct request *blk_mq_get_request(struct request_queue *q,
         :                      struct bio *bio,
         :                      struct blk_mq_alloc_data *data)
         :                      {
    0.75 :   ffff80001045f898:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff80001045f89c:       mov     x29, sp
    0.20 :   ffff80001045f8a0:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001045f8a4:       mov     x19, x0
    0.00 :   ffff80001045f8a8:       stp     x21, x22, [sp, #32]
    0.00 :   ffff80001045f8ac:       mov     x20, x2
    0.00 :   ffff80001045f8b0:       str     x23, [sp, #48]
    0.00 :   ffff80001045f8b4:       mov     x22, x1
         :                      struct elevator_queue *e = q->elevator;
    0.61 :   ffff80001045f8b8:       ldr     x21, [x0, #8]
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff80001045f8bc:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.97 :   ffff80001045f8c0:       ldr     x0, [x19, #1480]
         :                      __ref_is_percpu():
         :                      * Theoretically, the following could test just ATOMIC; however,
         :                      * then we'd have to mask off DEAD separately as DEAD may be
         :                      * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
         :                      * implies ATOMIC anyway.  Test them together.
         :                      */
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff80001045f8c4:       tst     x0, #0x3
    0.00 :   ffff80001045f8c8:       b.ne    ffff80001045fc00 <blk_mq_get_request+0x368>  // b.any
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.14 :   ffff80001045f8cc:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.00 :   ffff80001045f8d0:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff80001045f8d4:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.21 :   ffff80001045f8d8:       str     w2, [x1, #16]
         :                      __percpu_add_case_64():
         :
         :                      PERCPU_RW_OPS(8)
         :                      PERCPU_RW_OPS(16)
         :                      PERCPU_RW_OPS(32)
         :                      PERCPU_RW_OPS(64)
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff80001045f8dc:       mov     x3, #0x1                        // #1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    1.24 :   ffff80001045f8e0:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff80001045f8e4:       add     x0, x0, x2
    0.00 :   ffff80001045f8e8:       ldxr    x5, [x0]
    4.12 :   ffff80001045f8ec:       add     x5, x5, x3
    0.41 :   ffff80001045f8f0:       stxr    w4, x5, [x0]
    0.00 :   ffff80001045f8f4:       cbnz    w4, ffff80001045f8e8 <blk_mq_get_request+0x50>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.07 :   ffff80001045f8f8:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff80001045f8fc:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.89 :   ffff80001045f900:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff80001045f904:       cbnz    x0, ffff80001045fb74 <blk_mq_get_request+0x2dc>
         :                      percpu_ref_get_many():
         :                      unsigned long __percpu *percpu_count;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count))
         :                      this_cpu_add(*percpu_count, nr);
    0.00 :   ffff80001045f908:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.00 :   ffff80001045f90c:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_mq_get_request():
         :
         :                      /* alloc_time includes depth and tag waits */
         :                      if (blk_queue_rq_alloc_time(q))
         :                      alloc_time_ns = ktime_get_ns();
         :
         :                      data->q = q;
    0.00 :   ffff80001045f910:       str     x19, [x20]
         :                      if (likely(!data->ctx)) {
    0.00 :   ffff80001045f914:       ldr     x0, [x20, #24]
    0.00 :   ffff80001045f918:       cbnz    x0, ffff80001045fb8c <blk_mq_get_request+0x2f4>
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.75 :   ffff80001045f91c:       mrs     x2, tpidr_el1
         :                      blk_mq_get_ctx():
         :                      * care about preemption, since we know the ctx's are persistent. This does
         :                      * mean that we can't rely on ctx always matching the currently running CPU.
         :                      */
         :                      static inline struct blk_mq_ctx *blk_mq_get_ctx(struct request_queue *q)
         :                      {
         :                      return __blk_mq_get_ctx(q, raw_smp_processor_id());
    0.07 :   ffff80001045f920:       adrp    x0, ffff8000114ca000 <bp_hardening_data>
    0.00 :   ffff80001045f924:       add     x0, x0, #0x18
         :                      __blk_mq_get_ctx():
         :                      return per_cpu_ptr(q->queue_ctx, cpu);
    0.00 :   ffff80001045f928:       ldr     w2, [x0, x2]
    0.00 :   ffff80001045f92c:       adrp    x1, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff80001045f930:       add     x1, x1, #0x8e8
    0.62 :   ffff80001045f934:       ldr     x0, [x19, #56]
         :                      blk_mq_get_request():
         :                      data->ctx = blk_mq_get_ctx(q);
         :                      clear_ctx_on_error = true;
    0.00 :   ffff80001045f938:       mov     w23, #0x1                       // #1
         :                      __blk_mq_get_ctx():
    0.69 :   ffff80001045f93c:       ldr     x1, [x1, x2, lsl #3]
    0.20 :   ffff80001045f940:       add     x0, x1, x0
         :                      blk_mq_get_request():
         :                      data->ctx = blk_mq_get_ctx(q);
    1.23 :   ffff80001045f944:       str     x0, [x20, #24]
         :                      }
         :                      if (likely(!data->hctx))
    0.07 :   ffff80001045f948:       ldr     x1, [x20, #32]
    0.07 :   ffff80001045f94c:       ldr     w0, [x20, #16]
    0.00 :   ffff80001045f950:       cbnz    x1, ffff80001045f970 <blk_mq_get_request+0xd8>
         :                      blk_mq_map_queue():
         :                      type = HCTX_TYPE_POLL;
    0.20 :   ffff80001045f954:       mov     w1, #0x2                        // #2
         :                      blk_mq_get_request():
         :                      data->hctx = blk_mq_map_queue(q, data->cmd_flags,
    2.48 :   ffff80001045f958:       ldr     x2, [x20, #24]
         :                      blk_mq_map_queue():
         :                      if (flags & REQ_HIPRI)
    0.00 :   ffff80001045f95c:       tbz     w0, #25, ffff80001045fb54 <blk_mq_get_request+0x2bc>
         :                      return ctx->hctxs[type];
    0.00 :   ffff80001045f960:       ubfiz   x1, x1, #3, #2
    0.00 :   ffff80001045f964:       add     x1, x2, x1
    0.00 :   ffff80001045f968:       ldr     x1, [x1, #80]
         :                      blk_mq_get_request():
    0.00 :   ffff80001045f96c:       str     x1, [x20, #32]
         :                      data->ctx);
         :                      if (data->cmd_flags & REQ_NOWAIT)
    0.00 :   ffff80001045f970:       tbz     w0, #21, ffff80001045f980 <blk_mq_get_request+0xe8>
         :                      data->flags |= BLK_MQ_REQ_NOWAIT;
    0.00 :   ffff80001045f974:       ldr     w1, [x20, #8]
    0.00 :   ffff80001045f978:       orr     w1, w1, #0x1
    0.00 :   ffff80001045f97c:       str     w1, [x20, #8]
         :
         :                      if (e) {
    0.00 :   ffff80001045f980:       cbz     x21, ffff80001045fbc0 <blk_mq_get_request+0x328>
         :                      data->flags |= BLK_MQ_REQ_INTERNAL;
    0.00 :   ffff80001045f984:       ldr     w1, [x20, #8]
         :                      /*
         :                      * Flush requests are special and go directly to the
         :                      * dispatch list. Don't include reserved tags in the
         :                      * limiting, as it isn't useful.
         :                      */
         :                      if (!op_is_flush(data->cmd_flags) &&
    0.00 :   ffff80001045f988:       tst     w0, #0x60000
         :                      data->flags |= BLK_MQ_REQ_INTERNAL;
    0.00 :   ffff80001045f98c:       orr     w2, w1, #0x4
    0.00 :   ffff80001045f990:       str     w2, [x20, #8]
         :                      if (!op_is_flush(data->cmd_flags) &&
    0.00 :   ffff80001045f994:       b.ne    ffff80001045f9b0 <blk_mq_get_request+0x118>  // b.any
         :                      e->type->ops.limit_depth &&
    0.00 :   ffff80001045f998:       ldr     x2, [x21]
    0.00 :   ffff80001045f99c:       ldr     x2, [x2, #88]
         :                      if (!op_is_flush(data->cmd_flags) &&
    0.00 :   ffff80001045f9a0:       cbz     x2, ffff80001045f9b0 <blk_mq_get_request+0x118>
         :                      e->type->ops.limit_depth &&
    0.00 :   ffff80001045f9a4:       tbnz    w1, #1, ffff80001045f9b0 <blk_mq_get_request+0x118>
         :                      !(data->flags & BLK_MQ_REQ_RESERVED))
         :                      e->type->ops.limit_depth(data->cmd_flags, data);
    0.00 :   ffff80001045f9a8:       mov     x1, x20
    0.00 :   ffff80001045f9ac:       blr     x2
         :                      } else {
         :                      blk_mq_tag_busy(data->hctx);
         :                      }
         :
         :                      tag = blk_mq_get_tag(data);
    4.74 :   ffff80001045f9b0:       mov     x0, x20
    0.00 :   ffff80001045f9b4:       bl      ffff800010463b98 <blk_mq_get_tag>
         :                      if (tag == BLK_MQ_TAG_FAIL) {
    0.69 :   ffff80001045f9b8:       cmn     w0, #0x1
    0.00 :   ffff80001045f9bc:       b.eq    ffff80001045fc24 <blk_mq_get_request+0x38c>  // b.none
         :                      blk_mq_tags_from_data():
         :                      struct blk_mq_hw_ctx *hctx;
         :                      };
         :
         :                      static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data *data)
         :                      {
         :                      if (data->flags & BLK_MQ_REQ_INTERNAL)
    0.48 :   ffff80001045f9c0:       ldr     w1, [x20, #8]
    0.00 :   ffff80001045f9c4:       mov     w3, w0
         :                      blk_mq_get_request():
         :                      data->ctx = NULL;
         :                      blk_queue_exit(q);
         :                      return NULL;
         :                      }
         :
         :                      rq = blk_mq_rq_ctx_init(data, tag, data->cmd_flags, alloc_time_ns);
    0.34 :   ffff80001045f9c8:       ldr     w23, [x20, #16]
         :                      blk_mq_rq_ctx_init():
         :                      struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
    0.07 :   ffff80001045f9cc:       ldr     x2, [x20, #32]
         :                      blk_mq_tags_from_data():
    0.00 :   ffff80001045f9d0:       tbnz    w1, #2, ffff80001045fb38 <blk_mq_get_request+0x2a0>
         :                      blk_mq_rq_ctx_init():
         :                      struct request *rq = tags->static_rqs[tag];
    0.55 :   ffff80001045f9d4:       ldr     x5, [x2, #336]
         :                      req_flags_t rq_flags = 0;
    0.00 :   ffff80001045f9d8:       mov     w1, #0x0                        // #0
         :                      if (data->hctx->flags & BLK_MQ_F_TAG_SHARED) {
    1.72 :   ffff80001045f9dc:       ldr     x4, [x2, #192]
         :                      struct request *rq = tags->static_rqs[tag];
    0.14 :   ffff80001045f9e0:       ldr     x5, [x5, #152]
    4.28 :   ffff80001045f9e4:       ldr     x19, [x5, x3, lsl #3]
         :                      if (data->hctx->flags & BLK_MQ_F_TAG_SHARED) {
    0.00 :   ffff80001045f9e8:       tbnz    w4, #1, ffff80001045fbd4 <blk_mq_get_request+0x33c>
         :                      rq->internal_tag = -1;
   12.52 :   ffff80001045f9ec:       mov     w2, #0xffffffff                 // #-1
    0.00 :   ffff80001045f9f0:       stp     w0, w2, [x19, #32]
         :                      data->hctx->tags->rqs[rq->tag] = rq;
   12.46 :   ffff80001045f9f4:       ldr     x2, [x20, #32]
    0.00 :   ffff80001045f9f8:       ldr     x2, [x2, #336]
    0.00 :   ffff80001045f9fc:       ldr     x2, [x2, #144]
    0.07 :   ffff80001045fa00:       str     x19, [x2, w0, sxtw #3]
         :                      rq->q = data->q;
    4.70 :   ffff80001045fa04:       ldr     x0, [x20]
    0.00 :   ffff80001045fa08:       str     x0, [x19]
         :                      rq->mq_ctx = data->ctx;
    1.31 :   ffff80001045fa0c:       ldr     x2, [x20, #24]
    0.00 :   ffff80001045fa10:       str     x2, [x19, #8]
         :                      rq->mq_hctx = data->hctx;
    1.45 :   ffff80001045fa14:       ldr     x2, [x20, #32]
    0.00 :   ffff80001045fa18:       str     x2, [x19, #16]
         :                      rq->rq_flags = rq_flags;
    0.75 :   ffff80001045fa1c:       stp     w23, w1, [x19, #24]
         :                      if (data->flags & BLK_MQ_REQ_PREEMPT)
    0.35 :   ffff80001045fa20:       ldr     w2, [x20, #8]
    0.00 :   ffff80001045fa24:       tbz     w2, #3, ffff80001045fa30 <blk_mq_get_request+0x198>
         :                      rq->rq_flags |= RQF_PREEMPT;
    0.00 :   ffff80001045fa28:       orr     w1, w1, #0x100
    0.00 :   ffff80001045fa2c:       str     w1, [x19, #28]
         :                      if (blk_queue_io_stat(data->q))
    0.55 :   ffff80001045fa30:       ldr     x2, [x20]
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff80001045fa34:       ldr     x2, [x2, #104]
         :                      blk_mq_rq_ctx_init():
    0.00 :   ffff80001045fa38:       tst     w2, #0x80
    0.00 :   ffff80001045fa3c:       b.eq    ffff80001045fb94 <blk_mq_get_request+0x2fc>  // b.none
         :                      INIT_LIST_HEAD(&rq->queuelist);
    0.00 :   ffff80001045fa40:       add     x0, x19, #0x48
         :                      rq->rq_flags |= RQF_IO_STAT;
    0.00 :   ffff80001045fa44:       orr     w1, w1, #0x2000
         :                      RB_CLEAR_NODE(&rq->rb_node);
    0.00 :   ffff80001045fa48:       add     x2, x19, #0x68
         :                      rq->rq_flags |= RQF_IO_STAT;
    0.63 :   ffff80001045fa4c:       str     w1, [x19, #28]
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.48 :   ffff80001045fa50:       str     x0, [x19, #72]
         :                      INIT_HLIST_NODE():
         :                      #define HLIST_HEAD_INIT { .first = NULL }
         :                      #define HLIST_HEAD(name) struct hlist_head name = {  .first = NULL }
         :                      #define INIT_HLIST_HEAD(ptr) ((ptr)->first = NULL)
         :                      static inline void INIT_HLIST_NODE(struct hlist_node *h)
         :                      {
         :                      h->next = NULL;
    0.00 :   ffff80001045fa54:       stp     x0, xzr, [x19, #80]
         :                      blk_mq_rq_ctx_init():
         :                      RB_CLEAR_NODE(&rq->rb_node);
    0.14 :   ffff80001045fa58:       stp     xzr, x2, [x19, #96]
         :                      rq->part = NULL;
    0.55 :   ffff80001045fa5c:       stp     xzr, xzr, [x19, #160]
         :                      ktime_get_ns():
         :                      return ktime_mono_to_any(mono, TK_OFFS_REAL);
         :                      }
         :
         :                      static inline u64 ktime_get_ns(void)
         :                      {
         :                      return ktime_to_ns(ktime_get());
    0.14 :   ffff80001045fa60:       bl      ffff80001016ad10 <ktime_get>
         :                      blk_mq_rq_ctx_init():
         :                      rq->start_time_ns = ktime_get_ns();
    0.00 :   ffff80001045fa64:       str     x0, [x19, #176]
         :                      rq->io_start_time_ns = 0;
    0.00 :   ffff80001045fa68:       str     xzr, [x19, #184]
         :                      op_is_sync():
         :                      * PREFLUSH flag.  Other operations may be marked as synchronous using the
         :                      * REQ_SYNC flag.
         :                      */
         :                      static inline bool op_is_sync(unsigned int op)
         :                      {
         :                      return (op & REQ_OP_MASK) == REQ_OP_READ ||
    0.00 :   ffff80001045fa6c:       and     w1, w23, #0xff
         :                      blk_mq_rq_ctx_init():
    0.00 :   ffff80001045fa70:       str     wzr, [x19, #192]
         :                      op_is_sync():
    0.00 :   ffff80001045fa74:       mov     w0, #0x1                        // #1
         :                      blk_mq_rq_ctx_init():
         :                      rq->nr_phys_segments = 0;
    0.90 :   ffff80001045fa78:       strh    wzr, [x19, #196]
         :                      rq->extra_len = 0;
    0.00 :   ffff80001045fa7c:       str     wzr, [x19, #204]
         :                      rq->timeout = 0;
    0.00 :   ffff80001045fa80:       str     wzr, [x19, #216]
         :                      __write_once_size():
    0.00 :   ffff80001045fa84:       str     xzr, [x19, #224]
         :                      blk_mq_rq_ctx_init():
         :                      rq->end_io_data = NULL;
    0.83 :   ffff80001045fa88:       stp     xzr, xzr, [x19, #264]
         :                      data->ctx->rq_dispatched[op_is_sync(op)]++;
    0.00 :   ffff80001045fa8c:       ldr     x3, [x20, #24]
         :                      op_is_sync():
    0.00 :   ffff80001045fa90:       cbz     w1, ffff80001045faa4 <blk_mq_get_request+0x20c>
    0.00 :   ffff80001045fa94:       and     w23, w23, #0x7f800
    0.00 :   ffff80001045fa98:       and     w23, w23, #0xfffe0fff
    0.00 :   ffff80001045fa9c:       cmp     w23, #0x0
    0.00 :   ffff80001045faa0:       cset    w0, ne  // ne = any
    0.00 :   ffff80001045faa4:       sxtw    x0, w0
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001045faa8:       mov     w2, #0x1                        // #1
    0.00 :   ffff80001045faac:       add     x0, x3, x0, lsl #3
         :                      blk_mq_rq_ctx_init():
    0.00 :   ffff80001045fab0:       ldr     x1, [x0, #104]
    0.00 :   ffff80001045fab4:       add     x1, x1, #0x1
    0.48 :   ffff80001045fab8:       str     x1, [x0, #104]
         :                      __write_once_size():
    0.00 :   ffff80001045fabc:       str     w2, [x19, #212]
         :                      blk_mq_get_request():
         :                      if (!op_is_flush(data->cmd_flags)) {
    0.00 :   ffff80001045fac0:       ldr     w0, [x20, #16]
    0.00 :   ffff80001045fac4:       tst     w0, #0x60000
    0.00 :   ffff80001045fac8:       b.ne    ffff80001045fb10 <blk_mq_get_request+0x278>  // b.any
         :                      rq->elv.icq = NULL;
    0.00 :   ffff80001045facc:       str     xzr, [x19, #128]
         :                      if (e && e->type->ops.prepare_request) {
    0.00 :   ffff80001045fad0:       cbz     x21, ffff80001045fb10 <blk_mq_get_request+0x278>
    0.00 :   ffff80001045fad4:       ldr     x0, [x21]
    0.00 :   ffff80001045fad8:       ldr     x2, [x0, #96]
    0.00 :   ffff80001045fadc:       cbz     x2, ffff80001045fb10 <blk_mq_get_request+0x278>
         :                      if (e->type->icq_cache)
    0.00 :   ffff80001045fae0:       ldr     x0, [x0]
    0.00 :   ffff80001045fae4:       cbz     x0, ffff80001045faf8 <blk_mq_get_request+0x260>
         :                      blk_mq_sched_assign_ioc(rq);
    0.00 :   ffff80001045fae8:       mov     x0, x19
    0.00 :   ffff80001045faec:       bl      ffff800010465ea0 <blk_mq_sched_assign_ioc>
    0.00 :   ffff80001045faf0:       ldr     x0, [x21]
    0.00 :   ffff80001045faf4:       ldr     x2, [x0, #96]
         :
         :                      e->type->ops.prepare_request(rq, bio);
    0.00 :   ffff80001045faf8:       mov     x0, x19
    0.00 :   ffff80001045fafc:       mov     x1, x22
    0.00 :   ffff80001045fb00:       blr     x2
         :                      rq->rq_flags |= RQF_ELVPRIV;
    0.00 :   ffff80001045fb04:       ldr     w0, [x19, #28]
    0.00 :   ffff80001045fb08:       orr     w0, w0, #0x1000
    0.00 :   ffff80001045fb0c:       str     w0, [x19, #28]
         :                      }
         :                      }
         :                      data->hctx->queued++;
    1.10 :   ffff80001045fb10:       ldr     x1, [x20, #32]
    0.07 :   ffff80001045fb14:       ldr     x0, [x1, #352]
    0.00 :   ffff80001045fb18:       add     x0, x0, #0x1
    0.00 :   ffff80001045fb1c:       str     x0, [x1, #352]
         :                      return rq;
         :                      }
    0.00 :   ffff80001045fb20:       mov     x0, x19
    0.00 :   ffff80001045fb24:       ldr     x23, [sp, #48]
    0.96 :   ffff80001045fb28:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001045fb2c:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff80001045fb30:       ldp     x29, x30, [sp], #64
    0.00 :   ffff80001045fb34:       ret
         :                      blk_mq_rq_ctx_init():
         :                      struct request *rq = tags->static_rqs[tag];
    0.00 :   ffff80001045fb38:       ldr     x4, [x2, #344]
         :                      req_flags_t rq_flags = 0;
    0.00 :   ffff80001045fb3c:       mov     w1, #0x0                        // #0
         :                      rq->tag = -1;
    0.00 :   ffff80001045fb40:       mov     w2, #0xffffffff                 // #-1
         :                      struct request *rq = tags->static_rqs[tag];
    0.00 :   ffff80001045fb44:       ldr     x4, [x4, #152]
    0.00 :   ffff80001045fb48:       ldr     x19, [x4, x3, lsl #3]
         :                      rq->internal_tag = tag;
    0.00 :   ffff80001045fb4c:       stp     w2, w0, [x19, #32]
    0.00 :   ffff80001045fb50:       b       ffff80001045fa04 <blk_mq_get_request+0x16c>
         :                      blk_mq_map_queue():
         :                      else if ((flags & REQ_OP_MASK) == REQ_OP_READ)
    6.55 :   ffff80001045fb54:       and     w1, w0, #0xff
    0.00 :   ffff80001045fb58:       cmp     w1, #0x0
    0.00 :   ffff80001045fb5c:       cset    w1, eq  // eq = none
         :                      return ctx->hctxs[type];
    0.00 :   ffff80001045fb60:       ubfiz   x1, x1, #3, #2
    0.00 :   ffff80001045fb64:       add     x1, x2, x1
    0.00 :   ffff80001045fb68:       ldr     x1, [x1, #80]
         :                      blk_mq_get_request():
         :                      data->hctx = blk_mq_map_queue(q, data->cmd_flags,
    4.90 :   ffff80001045fb6c:       str     x1, [x20, #32]
    0.00 :   ffff80001045fb70:       b       ffff80001045f970 <blk_mq_get_request+0xd8>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.34 :   ffff80001045fb74:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff80001045fb78:       cbz     x0, ffff80001045f908 <blk_mq_get_request+0x70>
         :                      rcu_read_unlock():
    9.50 :   ffff80001045fb7c:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_mq_get_request():
         :                      data->q = q;
    0.00 :   ffff80001045fb80:       str     x19, [x20]
         :                      if (likely(!data->ctx)) {
    0.14 :   ffff80001045fb84:       ldr     x0, [x20, #24]
    0.00 :   ffff80001045fb88:       cbz     x0, ffff80001045f91c <blk_mq_get_request+0x84>
         :                      bool clear_ctx_on_error = false;
    0.00 :   ffff80001045fb8c:       mov     w23, #0x0                       // #0
    0.00 :   ffff80001045fb90:       b       ffff80001045f948 <blk_mq_get_request+0xb0>
         :                      blk_mq_rq_ctx_init():
         :                      INIT_LIST_HEAD(&rq->queuelist);
    0.00 :   ffff80001045fb94:       add     x1, x19, #0x48
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff80001045fb98:       str     x1, [x19, #72]
         :                      INIT_HLIST_NODE():
    0.00 :   ffff80001045fb9c:       stp     x1, xzr, [x19, #80]
         :                      blk_mq_rq_ctx_init():
         :                      RB_CLEAR_NODE(&rq->rb_node);
    0.00 :   ffff80001045fba0:       add     x1, x19, #0x68
         :                      INIT_HLIST_NODE():
         :                      h->pprev = NULL;
    0.00 :   ffff80001045fba4:       str     xzr, [x19, #96]
         :                      blk_mq_rq_ctx_init():
    0.00 :   ffff80001045fba8:       str     x1, [x19, #104]
         :                      rq->part = NULL;
    0.00 :   ffff80001045fbac:       stp     xzr, xzr, [x19, #160]
         :                      blk_mq_need_time_stamp():
         :                      return (rq->rq_flags & (RQF_IO_STAT | RQF_STATS)) || rq->q->elevator;
    0.00 :   ffff80001045fbb0:       ldr     x0, [x0, #8]
    0.00 :   ffff80001045fbb4:       cbnz    x0, ffff80001045fa60 <blk_mq_get_request+0x1c8>
         :                      blk_mq_rq_ctx_init():
         :                      rq->start_time_ns = 0;
    0.00 :   ffff80001045fbb8:       str     xzr, [x19, #176]
    0.00 :   ffff80001045fbbc:       b       ffff80001045fa68 <blk_mq_get_request+0x1d0>
         :                      blk_mq_get_request():
         :                      blk_mq_tag_busy(data->hctx);
    0.00 :   ffff80001045fbc0:       ldr     x0, [x20, #32]
         :                      blk_mq_tag_busy():
         :                      extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
         :                      extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
         :
         :                      static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
         :                      {
         :                      if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
    9.16 :   ffff80001045fbc4:       ldr     x1, [x0, #192]
    0.00 :   ffff80001045fbc8:       tbz     w1, #1, ffff80001045f9b0 <blk_mq_get_request+0x118>
         :                      return false;
         :
         :                      return __blk_mq_tag_busy(hctx);
    0.00 :   ffff80001045fbcc:       bl      ffff800010463a08 <__blk_mq_tag_busy>
    0.00 :   ffff80001045fbd0:       b       ffff80001045f9b0 <blk_mq_get_request+0x118>
         :                      blk_mq_rq_ctx_init():
         :                      atomic_inc(&data->hctx->nr_active);
    0.00 :   ffff80001045fbd4:       add     x3, x2, #0x1b0
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff80001045fbd8:       b       ffff80001045fbf0 <blk_mq_get_request+0x358>
    0.00 :   ffff80001045fbdc:       b       ffff80001045fbf0 <blk_mq_get_request+0x358>
         :                      __lse_atomic_add():
         :                      }
         :
         :                      ATOMIC_OP(andnot, stclr)
         :                      ATOMIC_OP(or, stset)
         :                      ATOMIC_OP(xor, steor)
         :                      ATOMIC_OP(add, stadd)
    0.00 :   ffff80001045fbe0:       mov     w1, #0x1                        // #1
    0.00 :   ffff80001045fbe4:       stadd   w1, [x3]
         :                      blk_mq_rq_ctx_init():
         :                      rq_flags = RQF_MQ_INFLIGHT;
    0.00 :   ffff80001045fbe8:       mov     w1, #0x40                       // #64
    0.00 :   ffff80001045fbec:       b       ffff80001045f9ec <blk_mq_get_request+0x154>
         :                      __ll_sc_atomic_add():
         :                      ATOMIC_FETCH_OP (        , dmb ish,  , l, "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
    0.00 :   ffff80001045fbf0:       add     x2, x2, #0x1b0
    0.00 :   ffff80001045fbf4:       b       ffff8000104634b4 <blk_mq_update_nr_requests+0x2dc>
         :                      blk_mq_rq_ctx_init():
    0.00 :   ffff80001045fbf8:       mov     w1, #0x40                       // #64
    0.00 :   ffff80001045fbfc:       b       ffff80001045f9ec <blk_mq_get_request+0x154>
         :                      percpu_ref_get_many():
         :                      else
         :                      atomic_long_add(nr, &ref->count);
    0.00 :   ffff80001045fc00:       add     x1, x19, #0x5c0
         :                      arch_static_branch_jump():
    0.00 :   ffff80001045fc04:       b       ffff80001045fc18 <blk_mq_get_request+0x380>
    0.00 :   ffff80001045fc08:       b       ffff80001045fc18 <blk_mq_get_request+0x380>
         :                      __lse_atomic64_add():
         :                      }
         :
         :                      ATOMIC64_OP(andnot, stclr)
         :                      ATOMIC64_OP(or, stset)
         :                      ATOMIC64_OP(xor, steor)
         :                      ATOMIC64_OP(add, stadd)
    0.00 :   ffff80001045fc0c:       mov     x0, #0x1                        // #1
    0.00 :   ffff80001045fc10:       stadd   x0, [x1]
    0.00 :   ffff80001045fc14:       b       ffff80001045f90c <blk_mq_get_request+0x74>
         :                      __ll_sc_atomic64_add():
         :                      ATOMIC64_FETCH_OP (, dmb ish,  , l, "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(add, add, I)
    0.00 :   ffff80001045fc18:       add     x2, x19, #0x5c0
    0.00 :   ffff80001045fc1c:       b       ffff8000104634cc <blk_mq_update_nr_requests+0x2f4>
    0.00 :   ffff80001045fc20:       b       ffff80001045f90c <blk_mq_get_request+0x74>
         :                      blk_mq_get_request():
         :                      if (clear_ctx_on_error)
    0.00 :   ffff80001045fc24:       cbz     w23, ffff80001045fc2c <blk_mq_get_request+0x394>
         :                      data->ctx = NULL;
    0.00 :   ffff80001045fc28:       str     xzr, [x20, #24]
         :                      blk_queue_exit(q);
    0.00 :   ffff80001045fc2c:       mov     x0, x19
         :                      return NULL;
    0.00 :   ffff80001045fc30:       mov     x19, #0x0                       // #0
         :                      blk_queue_exit(q);
    0.00 :   ffff80001045fc34:       bl      ffff800010455110 <blk_queue_exit>
         :                      return NULL;
    0.00 :   ffff80001045fc38:       b       ffff80001045fb20 <blk_mq_get_request+0x288>
 Percent |	Source code & Disassembly of vmlinux for cycles (726 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010702008 <free_iova_fast>:
         :                      free_iova_fast():
         :                      * This functions frees an iova range by trying to put it into the rcache,
         :                      * falling back to regular iova deallocation via free_iova() if this fails.
         :                      */
         :                      void
         :                      free_iova_fast(struct iova_domain *iovad, unsigned long pfn, unsigned long size)
         :                      {
   20.90 :   ffff800010702008:       stp     x29, x30, [sp, #-80]!
         :                      __order_base_2():
         :                      )
         :
         :                      static inline __attribute_const__
         :                      int __order_base_2(unsigned long n)
         :                      {
         :                      return n > 1 ? ilog2(n - 1) + 1 : 0;
    0.00 :   ffff80001070200c:       cmp     x2, #0x1
         :                      free_iova_fast():
    0.00 :   ffff800010702010:       mov     x29, sp
    0.28 :   ffff800010702014:       stp     x22, x23, [sp, #40]
    0.00 :   ffff800010702018:       mov     x22, x0
    0.00 :   ffff80001070201c:       mov     x23, x1
         :                      __order_base_2():
    0.00 :   ffff800010702020:       b.ls    ffff800010702058 <free_iova_fast+0x50>  // b.plast
    0.00 :   ffff800010702024:       sub     x2, x2, #0x1
         :                      __fls():
         :                      *
         :                      * Undefined if no set bit exists, so code should check against 0 first.
         :                      */
         :                      static __always_inline unsigned long __fls(unsigned long word)
         :                      {
         :                      return (sizeof(word) * 8) - 1 - __builtin_clzl(word);
    0.00 :   ffff800010702028:       mov     x1, #0x3f                       // #63
    0.00 :   ffff80001070202c:       clz     x2, x2
    0.00 :   ffff800010702030:       sub     x1, x1, x2
         :                      fls64():
         :                      #elif BITS_PER_LONG == 64
         :                      static __always_inline int fls64(__u64 x)
         :                      {
         :                      if (x == 0)
         :                      return 0;
         :                      return __fls(x) + 1;
    0.00 :   ffff800010702034:       add     w1, w1, #0x1
         :                      iova_rcache_insert():
         :                      static bool iova_rcache_insert(struct iova_domain *iovad, unsigned long pfn,
         :                      unsigned long size)
         :                      {
         :                      unsigned int log_size = order_base_2(size);
         :
         :                      if (log_size >= IOVA_RANGE_CACHE_MAX_SIZE)
    0.00 :   ffff800010702038:       cmp     w1, #0x5
    0.00 :   ffff80001070203c:       b.ls    ffff80001070226c <free_iova_fast+0x264>  // b.plast
         :                      free_iova_fast():
         :                      free_iova(iovad, pfn);
    0.00 :   ffff800010702040:       mov     x1, x23
    0.00 :   ffff800010702044:       mov     x0, x22
    0.00 :   ffff800010702048:       bl      ffff800010701d70 <free_iova>
         :                      }
    0.00 :   ffff80001070204c:       ldp     x22, x23, [sp, #40]
    0.00 :   ffff800010702050:       ldp     x29, x30, [sp], #80
    0.00 :   ffff800010702054:       ret
    0.69 :   ffff800010702058:       stp     x19, x20, [x29, #16]
         :                      iova_rcache_insert():
         :                      unsigned int log_size = order_base_2(size);
    0.00 :   ffff80001070205c:       mov     x1, #0x0                        // #0
    2.63 :   ffff800010702060:       str     x21, [x29, #32]
         :                      __order_base_2():
    0.00 :   ffff800010702064:       mov     x21, #0x80                      // #128
   18.94 :   ffff800010702068:       stp     x24, x25, [x29, #56]
         :                      __iova_rcache_insert():
         :                      cpu_rcache = raw_cpu_ptr(rcache->cpu_rcaches);
    0.00 :   ffff80001070206c:       add     x20, x1, x1, lsl #3
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.28 :   ffff800010702070:       mrs     x0, tpidr_el1
         :                      __iova_rcache_insert():
    0.00 :   ffff800010702074:       lsl     x20, x20, #2
    0.00 :   ffff800010702078:       sub     x20, x20, x1
    0.00 :   ffff80001070207c:       add     x24, x22, x20, lsl #3
    0.56 :   ffff800010702080:       ldr     x19, [x24, #400]
    0.00 :   ffff800010702084:       add     x19, x19, x0
         :                      spin_lock_irqsave(&cpu_rcache->lock, flags);
    0.00 :   ffff800010702088:       mov     x0, x19
    0.00 :   ffff80001070208c:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
    0.00 :   ffff800010702090:       mov     x25, x0
         :                      if (!iova_magazine_full(cpu_rcache->loaded)) {
    0.00 :   ffff800010702094:       ldr     x2, [x19, #8]
         :                      iova_magazine_full():
         :                      return (mag && mag->size == IOVA_MAG_SIZE);
    0.00 :   ffff800010702098:       cbz     x2, ffff8000107020dc <free_iova_fast+0xd4>
    0.00 :   ffff80001070209c:       ldr     x0, [x2]
    0.00 :   ffff8000107020a0:       cmp     x0, #0x80
    0.00 :   ffff8000107020a4:       b.eq    ffff80001070210c <free_iova_fast+0x104>  // b.none
         :                      iova_magazine_push():
         :                      mag->pfns[mag->size++] = pfn;
    0.00 :   ffff8000107020a8:       add     x3, x2, x0, lsl #3
    0.00 :   ffff8000107020ac:       add     x0, x0, #0x1
    0.00 :   ffff8000107020b0:       str     x0, [x2]
         :                      spin_unlock_irqrestore():
         :                      raw_spin_unlock_irq(&lock->rlock);
         :                      }
         :
         :                      static __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
         :                      {
         :                      raw_spin_unlock_irqrestore(&lock->rlock, flags);
    0.00 :   ffff8000107020b4:       mov     x1, x25
    0.00 :   ffff8000107020b8:       mov     x0, x19
         :                      iova_magazine_push():
    0.00 :   ffff8000107020bc:       str     x23, [x3, #8]
         :                      spin_unlock_irqrestore():
    0.00 :   ffff8000107020c0:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
   16.94 :   ffff8000107020c4:       ldr     x21, [x29, #32]
    1.39 :   ffff8000107020c8:       ldp     x19, x20, [x29, #16]
    0.41 :   ffff8000107020cc:       ldp     x24, x25, [x29, #56]
         :                      free_iova_fast():
         :                      }
   16.23 :   ffff8000107020d0:       ldp     x22, x23, [sp, #40]
   20.21 :   ffff8000107020d4:       ldp     x29, x30, [sp], #80
    0.00 :   ffff8000107020d8:       ret
    0.00 :   ffff8000107020dc:       ldr     x3, [x2]
         :                      spin_unlock_irqrestore():
    0.00 :   ffff8000107020e0:       mov     x1, x0
    0.00 :   ffff8000107020e4:       mov     x0, x19
         :                      iova_magazine_push():
         :                      mag->pfns[mag->size++] = pfn;
    0.00 :   ffff8000107020e8:       add     x4, x3, #0x1
    0.00 :   ffff8000107020ec:       str     x4, [x2]
    0.00 :   ffff8000107020f0:       lsl     x3, x3, #3
    0.00 :   ffff8000107020f4:       str     x23, [x3, #8]
         :                      spin_unlock_irqrestore():
    0.00 :   ffff8000107020f8:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
    0.00 :   ffff8000107020fc:       ldr     x21, [x29, #32]
    0.00 :   ffff800010702100:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff800010702104:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff800010702108:       b       ffff8000107020d0 <free_iova_fast+0xc8>
    0.00 :   ffff80001070210c:       str     x26, [x29, #72]
         :                      __iova_rcache_insert():
         :                      } else if (!iova_magazine_full(cpu_rcache->prev)) {
    0.00 :   ffff800010702110:       ldr     x3, [x19, #16]
         :                      iova_magazine_full():
         :                      return (mag && mag->size == IOVA_MAG_SIZE);
    0.00 :   ffff800010702114:       cbz     x3, ffff800010702170 <free_iova_fast+0x168>
    0.00 :   ffff800010702118:       ldr     x0, [x3]
    0.00 :   ffff80001070211c:       cmp     x0, #0x80
    0.00 :   ffff800010702120:       b.eq    ffff800010702190 <free_iova_fast+0x188>  // b.none
         :                      __iova_rcache_insert():
         :                      swap(cpu_rcache->prev, cpu_rcache->loaded);
    0.00 :   ffff800010702124:       stp     x3, x2, [x19, #8]
    0.00 :   ffff800010702128:       ldr     x0, [x3]
         :                      iova_magazine_full():
         :                      return (mag && mag->size == IOVA_MAG_SIZE);
    0.00 :   ffff80001070212c:       cmp     x0, #0x80
    0.00 :   ffff800010702130:       b.eq    ffff80001070216c <free_iova_fast+0x164>  // b.none
         :                      iova_magazine_push():
         :                      mag->pfns[mag->size++] = pfn;
    0.00 :   ffff800010702134:       add     x2, x3, x0, lsl #3
    0.00 :   ffff800010702138:       add     x0, x0, #0x1
         :                      spin_unlock_irqrestore():
    0.00 :   ffff80001070213c:       mov     x1, x25
         :                      iova_magazine_push():
    0.00 :   ffff800010702140:       str     x0, [x3]
         :                      spin_unlock_irqrestore():
    0.00 :   ffff800010702144:       mov     x0, x19
         :                      iova_magazine_push():
    0.00 :   ffff800010702148:       str     x23, [x2, #8]
         :                      spin_unlock_irqrestore():
    0.00 :   ffff80001070214c:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
    0.00 :   ffff800010702150:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff800010702154:       ldr     x21, [x29, #32]
    0.00 :   ffff800010702158:       ldp     x24, x25, [x29, #56]
    0.27 :   ffff80001070215c:       ldr     x26, [x29, #72]
         :                      free_iova_fast():
         :                      }
    0.00 :   ffff800010702160:       ldp     x22, x23, [sp, #40]
    0.28 :   ffff800010702164:       ldp     x29, x30, [sp], #80
    0.00 :   ffff800010702168:       ret
         :                      iova_magazine_push():
         :                      BUG_ON(iova_magazine_full(mag));
    0.00 :   ffff80001070216c:       brk     #0x800
         :                      __iova_rcache_insert():
         :                      swap(cpu_rcache->prev, cpu_rcache->loaded);
    0.00 :   ffff800010702170:       stp     xzr, x2, [x19, #8]
         :                      spin_unlock_irqrestore():
    0.00 :   ffff800010702174:       mov     x1, x25
    0.00 :   ffff800010702178:       mov     x0, x19
         :                      iova_magazine_push():
         :                      mag->pfns[mag->size++] = pfn;
    0.00 :   ffff80001070217c:       ldr     x2, [x3]
    0.00 :   ffff800010702180:       add     x4, x2, #0x1
    0.00 :   ffff800010702184:       str     x4, [x3]
    0.00 :   ffff800010702188:       lsl     x2, x2, #3
    0.00 :   ffff80001070218c:       b       ffff800010702148 <free_iova_fast+0x140>
         :                      kmalloc():
         :                      index = kmalloc_index(size);
         :
         :                      if (!index)
         :                      return ZERO_SIZE_PTR;
         :
         :                      return kmem_cache_alloc_trace(
    0.00 :   ffff800010702190:       adrp    x0, ffff8000112ae000 <cpu_ops+0x248>
         :                      kmem_cache_alloc_trace():
         :                      void *ret = kmem_cache_alloc(s, flags);
    0.00 :   ffff800010702194:       mov     w1, #0xb20                      // #2848
    0.00 :   ffff800010702198:       ldr     x0, [x0, #2456]
    0.00 :   ffff80001070219c:       bl      ffff800010252178 <kmem_cache_alloc>
    0.00 :   ffff8000107021a0:       mov     x26, x0
         :                      __iova_rcache_insert():
         :                      if (new_mag) {
    0.00 :   ffff8000107021a4:       cbz     x0, ffff80001070224c <free_iova_fast+0x244>
         :                      iova_rcache_insert():
         :                      return false;
         :
         :                      return __iova_rcache_insert(iovad, &iovad->rcaches[log_size], pfn);
    0.00 :   ffff8000107021a8:       add     x21, x22, x21
         :                      spin_lock():
         :                      raw_spin_lock(&lock->rlock);
    0.00 :   ffff8000107021ac:       mov     x0, x21
    0.00 :   ffff8000107021b0:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      __iova_rcache_insert():
         :                      if (rcache->depot_size < MAX_GLOBAL_MAGS) {
    0.00 :   ffff8000107021b4:       ldr     x0, [x24, #136]
    0.00 :   ffff8000107021b8:       cmp     x0, #0x1f
    0.00 :   ffff8000107021bc:       b.ls    ffff80001070222c <free_iova_fast+0x224>  // b.plast
         :                      mag_to_free = cpu_rcache->loaded;
    0.00 :   ffff8000107021c0:       ldr     x20, [x19, #8]
         :                      spin_unlock():
         :                      raw_spin_unlock(&lock->rlock);
    0.00 :   ffff8000107021c4:       mov     x0, x21
    0.00 :   ffff8000107021c8:       bl      ffff800010cb2408 <_raw_spin_unlock>
         :                      __iova_rcache_insert():
         :                      cpu_rcache->loaded = new_mag;
    0.00 :   ffff8000107021cc:       str     x26, [x19, #8]
    0.00 :   ffff8000107021d0:       ldr     x0, [x26]
         :                      iova_magazine_full():
         :                      return (mag && mag->size == IOVA_MAG_SIZE);
    0.00 :   ffff8000107021d4:       cmp     x0, #0x80
    0.00 :   ffff8000107021d8:       b.eq    ffff80001070216c <free_iova_fast+0x164>  // b.none
         :                      iova_magazine_push():
         :                      mag->pfns[mag->size++] = pfn;
    0.00 :   ffff8000107021dc:       add     x2, x26, x0, lsl #3
    0.00 :   ffff8000107021e0:       add     x0, x0, #0x1
    0.00 :   ffff8000107021e4:       str     x0, [x26]
         :                      spin_unlock_irqrestore():
         :                      raw_spin_unlock_irqrestore(&lock->rlock, flags);
    0.00 :   ffff8000107021e8:       mov     x1, x25
    0.00 :   ffff8000107021ec:       mov     x0, x19
         :                      iova_magazine_push():
    0.00 :   ffff8000107021f0:       str     x23, [x2, #8]
         :                      spin_unlock_irqrestore():
    0.00 :   ffff8000107021f4:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      __iova_rcache_insert():
         :                      if (mag_to_free) {
    0.00 :   ffff8000107021f8:       cbz     x20, ffff800010702150 <free_iova_fast+0x148>
         :                      iova_magazine_free_pfns(mag_to_free, iovad);
    0.00 :   ffff8000107021fc:       mov     x1, x22
    0.00 :   ffff800010702200:       mov     x0, x20
    0.00 :   ffff800010702204:       bl      ffff800010701da0 <iova_magazine_free_pfns>
         :                      iova_magazine_free():
         :                      kfree(mag);
    0.00 :   ffff800010702208:       mov     x0, x20
    0.00 :   ffff80001070220c:       bl      ffff80001024fe88 <kfree>
    0.00 :   ffff800010702210:       ldr     x21, [x29, #32]
    0.00 :   ffff800010702214:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff800010702218:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff80001070221c:       ldr     x26, [x29, #72]
         :                      free_iova_fast():
         :                      }
    0.00 :   ffff800010702220:       ldp     x22, x23, [sp, #40]
    0.00 :   ffff800010702224:       ldp     x29, x30, [sp], #80
    0.00 :   ffff800010702228:       ret
         :                      __iova_rcache_insert():
         :                      rcache->depot[rcache->depot_size++] =
    0.00 :   ffff80001070222c:       add     x1, x20, x0
    0.00 :   ffff800010702230:       add     x0, x0, #0x1
    0.00 :   ffff800010702234:       add     x1, x1, #0x12
    0.00 :   ffff800010702238:       str     x0, [x24, #136]
         :                      struct iova_magazine *mag_to_free = NULL;
    0.00 :   ffff80001070223c:       mov     x20, #0x0                       // #0
         :                      cpu_rcache->loaded;
    0.00 :   ffff800010702240:       ldr     x0, [x19, #8]
         :                      rcache->depot[rcache->depot_size++] =
    0.00 :   ffff800010702244:       str     x0, [x22, x1, lsl #3]
    0.00 :   ffff800010702248:       b       ffff8000107021c4 <free_iova_fast+0x1bc>
         :                      spin_unlock_irqrestore():
    0.00 :   ffff80001070224c:       mov     x1, x25
    0.00 :   ffff800010702250:       mov     x0, x19
    0.00 :   ffff800010702254:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
    0.00 :   ffff800010702258:       ldr     x21, [x29, #32]
    0.00 :   ffff80001070225c:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff800010702260:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff800010702264:       ldr     x26, [x29, #72]
    0.00 :   ffff800010702268:       b       ffff800010702040 <free_iova_fast+0x38>
    0.00 :   ffff80001070226c:       mov     x0, #0x80                       // #128
    0.00 :   ffff800010702270:       str     x21, [x29, #32]
    0.00 :   ffff800010702274:       mov     w21, #0x118                     // #280
    0.00 :   ffff800010702278:       stp     x19, x20, [x29, #16]
    0.00 :   ffff80001070227c:       stp     x24, x25, [x29, #56]
    0.00 :   ffff800010702280:       umaddl  x21, w1, w21, x0
    0.00 :   ffff800010702284:       b       ffff80001070206c <free_iova_fast+0x64>
 Percent |	Source code & Disassembly of vmlinux for cycles (711 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000107b9868 <nvme_complete_rq>:
         :                      nvme_complete_rq():
         :                      blk_mq_requeue_request(req, false);
         :                      blk_mq_delay_kick_requeue_list(req->q, delay);
         :                      }
         :
         :                      void nvme_complete_rq(struct request *req)
         :                      {
   16.43 :   ffff8000107b9868:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff8000107b986c:       mov     x29, sp
    3.78 :   ffff8000107b9870:       stp     x19, x20, [sp, #16]
         :                      blk_status_t status = nvme_error_status(nvme_req(req)->status);
    0.00 :   ffff8000107b9874:       add     x20, x0, #0x118
         :                      {
    0.70 :   ffff8000107b9878:       stp     x21, x22, [sp, #32]
    0.00 :   ffff8000107b987c:       mov     x19, x0
         :                      blk_status_t status = nvme_error_status(nvme_req(req)->status);
    0.84 :   ffff8000107b9880:       ldrh    w0, [x20, #18]
    0.00 :   ffff8000107b9884:       bl      ffff8000107b5270 <nvme_error_status>
    7.14 :   ffff8000107b9888:       and     w22, w0, #0xff
         :                      nvme_cleanup_cmd():
         :                      return 0;
         :                      }
         :
         :                      void nvme_cleanup_cmd(struct request *req)
         :                      {
         :                      if (req->rq_flags & RQF_SPECIAL_PAYLOAD) {
    7.60 :   ffff8000107b988c:       ldr     w0, [x19, #28]
         :                      nvme_complete_rq():
         :                      blk_status_t status = nvme_error_status(nvme_req(req)->status);
    0.00 :   ffff8000107b9890:       mov     w21, w22
         :                      nvme_cleanup_cmd():
         :                      if (req->rq_flags & RQF_SPECIAL_PAYLOAD) {
    0.00 :   ffff8000107b9894:       tbnz    w0, #18, ffff8000107b98cc <nvme_complete_rq+0x64>
         :                      nvme_complete_rq():
         :                      if (nvme_req(req)->ctrl->kas)
    8.60 :   ffff8000107b9898:       ldr     x0, [x20, #24]
    3.25 :   ffff8000107b989c:       ldrh    w1, [x0, #1256]
    0.00 :   ffff8000107b98a0:       cbz     w1, ffff8000107b98ac <nvme_complete_rq+0x44>
         :                      nvme_req(req)->ctrl->comp_seen = true;
    0.00 :   ffff8000107b98a4:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000107b98a8:       strb    w1, [x0]
         :                      if (unlikely(status != BLK_STS_OK && nvme_req_needs_retry(req))) {
   15.91 :   ffff8000107b98ac:       cbnz    w22, ffff8000107b98d8 <nvme_complete_rq+0x70>
         :                      blk_mq_end_request(req, status);
    7.45 :   ffff8000107b98b0:       mov     w1, w21
    0.00 :   ffff8000107b98b4:       mov     x0, x19
    0.00 :   ffff8000107b98b8:       bl      ffff80001045e968 <blk_mq_end_request>
         :                      }
    0.27 :   ffff8000107b98bc:       ldp     x19, x20, [sp, #16]
    1.53 :   ffff8000107b98c0:       ldp     x21, x22, [sp, #32]
   26.49 :   ffff8000107b98c4:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000107b98c8:       ret
         :                      nvme_cleanup_cmd():
    0.00 :   ffff8000107b98cc:       mov     x0, x19
    0.00 :   ffff8000107b98d0:       bl      ffff8000107b97c8 <nvme_cleanup_cmd.part.58>
    0.00 :   ffff8000107b98d4:       b       ffff8000107b9898 <nvme_complete_rq+0x30>
         :                      nvme_req_needs_retry():
         :                      if (blk_noretry_request(req))
    0.00 :   ffff8000107b98d8:       ldr     w0, [x19, #24]
    0.00 :   ffff8000107b98dc:       tst     w0, #0x700
    0.00 :   ffff8000107b98e0:       b.ne    ffff8000107b98b0 <nvme_complete_rq+0x48>  // b.any
         :                      if (nvme_req(req)->status & NVME_SC_DNR)
    0.00 :   ffff8000107b98e4:       ldrh    w1, [x20, #18]
    0.00 :   ffff8000107b98e8:       tbnz    w1, #14, ffff8000107b98b0 <nvme_complete_rq+0x48>
         :                      if (nvme_req(req)->retries >= nvme_max_retries)
    0.00 :   ffff8000107b98ec:       adrp    x3, ffff800011a1a000 <hisi_sas_v1_driver+0x28>
    0.00 :   ffff8000107b98f0:       ldrb    w2, [x20, #16]
    0.00 :   ffff8000107b98f4:       ldrb    w3, [x3, #2120]
    0.00 :   ffff8000107b98f8:       cmp     w3, w2
    0.00 :   ffff8000107b98fc:       b.ls    ffff8000107b98b0 <nvme_complete_rq+0x48>  // b.plast
         :                      nvme_complete_rq():
         :                      if ((req->cmd_flags & REQ_NVME_MPATH) &&
    0.00 :   ffff8000107b9900:       tbz     w0, #26, ffff8000107b9918 <nvme_complete_rq+0xb0>
         :                      blk_path_error():
         :                      *     %false - retrying failover path will not help
         :                      *     %true  - may succeed if retried
         :                      */
         :                      static inline bool blk_path_error(blk_status_t error)
         :                      {
         :                      switch (error) {
    0.00 :   ffff8000107b9904:       cmp     w22, #0x3
    0.00 :   ffff8000107b9908:       b.eq    ffff8000107b9918 <nvme_complete_rq+0xb0>  // b.none
    0.00 :   ffff8000107b990c:       b.hi    ffff8000107b9980 <nvme_complete_rq+0x118>  // b.pmore
    0.00 :   ffff8000107b9910:       cmp     w22, #0x1
    0.00 :   ffff8000107b9914:       b.ne    ffff8000107b98bc <nvme_complete_rq+0x54>  // b.any
         :                      nvme_complete_rq():
         :                      if (!blk_queue_dying(req->q)) {
    0.00 :   ffff8000107b9918:       ldr     x0, [x19]
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff8000107b991c:       ldr     x3, [x0, #104]
         :                      nvme_complete_rq():
    0.00 :   ffff8000107b9920:       tst     w3, #0x2
    0.00 :   ffff8000107b9924:       b.ne    ffff8000107b98b0 <nvme_complete_rq+0x48>  // b.any
         :                      nvme_retry_req():
         :                      struct nvme_ns *ns = req->q->queuedata;
    0.00 :   ffff8000107b9928:       ldr     x0, [x0, #96]
         :                      crd = (nvme_req(req)->status & NVME_SC_CRD) >> 11;
    0.00 :   ffff8000107b992c:       ubfx    x1, x1, #11, #2
         :                      unsigned long delay = 0;
    0.00 :   ffff8000107b9930:       mov     x21, #0x0                       // #0
         :                      if (ns && crd)
    0.00 :   ffff8000107b9934:       cmp     x0, #0x0
    0.00 :   ffff8000107b9938:       ccmp    w1, #0x0, #0x4, ne  // ne = any
    0.00 :   ffff8000107b993c:       b.eq    ffff8000107b995c <nvme_complete_rq+0xf4>  // b.none
         :                      delay = ns->ctrl->crdt[crd - 1] * 100;
    0.00 :   ffff8000107b9940:       ldr     x0, [x0, #16]
    0.00 :   ffff8000107b9944:       sub     w1, w1, #0x1
    0.00 :   ffff8000107b9948:       mov     w3, #0x64                       // #100
    0.00 :   ffff8000107b994c:       add     x0, x0, w1, sxtw #1
    0.00 :   ffff8000107b9950:       ldrh    w21, [x0, #1220]
    0.00 :   ffff8000107b9954:       mul     w21, w21, w3
    0.00 :   ffff8000107b9958:       sxtw    x21, w21
         :                      nvme_req(req)->retries++;
    0.00 :   ffff8000107b995c:       add     w2, w2, #0x1
    0.00 :   ffff8000107b9960:       strb    w2, [x20, #16]
         :                      blk_mq_requeue_request(req, false);
    0.00 :   ffff8000107b9964:       mov     x0, x19
    0.00 :   ffff8000107b9968:       mov     w1, #0x0                        // #0
    0.00 :   ffff8000107b996c:       bl      ffff800010460038 <blk_mq_requeue_request>
         :                      blk_mq_delay_kick_requeue_list(req->q, delay);
    0.00 :   ffff8000107b9970:       ldr     x0, [x19]
    0.00 :   ffff8000107b9974:       mov     x1, x21
    0.00 :   ffff8000107b9978:       bl      ffff80001045daf8 <blk_mq_delay_kick_requeue_list>
    0.00 :   ffff8000107b997c:       b       ffff8000107b98bc <nvme_complete_rq+0x54>
         :                      blk_path_error():
    0.00 :   ffff8000107b9980:       sub     w22, w22, #0x5
    0.00 :   ffff8000107b9984:       cmp     w22, #0x3
    0.00 :   ffff8000107b9988:       b.hi    ffff8000107b98bc <nvme_complete_rq+0x54>  // b.pmore
    0.00 :   ffff8000107b998c:       b       ffff8000107b9918 <nvme_complete_rq+0xb0>
 Percent |	Source code & Disassembly of vmlinux for cycles (683 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001024d338 <__cmpxchg_double>:
         :                      __cmpxchg_double():
         :                      {                                                                       \
         :                      return __lse_ll_sc_body(_cmpxchg_double##name,                  \
         :                      old1, old2, new1, new2, ptr);           \
         :                      }
         :
         :                      __CMPXCHG_DBL(   )
    0.44 :   ffff80001024d338:       mov     x5, x0
    0.00 :   ffff80001024d33c:       mov     x6, x1
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff80001024d340:       b       ffff80001024d35c <__cmpxchg_double+0x24>
    3.07 :   ffff80001024d344:       b       ffff80001024d35c <__cmpxchg_double+0x24>
         :                      __lse__cmpxchg_double():
         :                      : cl);                                                          \
         :                      \
         :                      return x0;                                                      \
         :                      }
         :
         :                      __CMPXCHG_DBL(   ,   )
    0.59 :   ffff80001024d348:       casp    x0, x1, x2, x3, [x4]
   95.90 :   ffff80001024d34c:       eor     x0, x0, x5
    0.00 :   ffff80001024d350:       eor     x1, x1, x6
    0.00 :   ffff80001024d354:       orr     x0, x0, x1
         :                      __cmpxchg_double():
    0.00 :   ffff80001024d358:       ret
         :                      __ll_sc__cmpxchg_double():
         :                      : cl);                                                          \
         :                      \
         :                      return ret;                                                     \
         :                      }
         :
         :                      __CMPXCHG_DBL(   ,        ,  ,         )
    0.00 :   ffff80001024d35c:       b       ffff800010254110 <slabinfo_write+0x20>
         :                      __cmpxchg_double():
    0.00 :   ffff80001024d360:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (672 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000107b5270 <nvme_error_status>:
         :                      nvme_error_status():
         :                      return ns->pi_type && ns->ms == sizeof(struct t10_pi_tuple);
         :                      }
         :
         :                      static blk_status_t nvme_error_status(u16 status)
         :                      {
         :                      switch (status & 0x7ff) {
   29.03 :   ffff8000107b5270:       and     w1, w0, #0x7ff
    0.00 :   ffff8000107b5274:       cmp     w1, #0x180
    0.00 :   ffff8000107b5278:       b.eq    ffff8000107b52e8 <nvme_error_status+0x78>  // b.none
    1.63 :   ffff8000107b527c:       b.hi    ffff8000107b52b4 <nvme_error_status+0x44>  // b.pmore
   20.64 :   ffff8000107b5280:       cmp     w1, #0x21
    0.00 :   ffff8000107b5284:       b.eq    ffff8000107b5308 <nvme_error_status+0x98>  // b.none
   12.33 :   ffff8000107b5288:       b.ls    ffff8000107b52d8 <nvme_error_status+0x68>  // b.plast
         :                      case NVME_SC_SUCCESS:
         :                      return BLK_STS_OK;
         :                      case NVME_SC_CAP_EXCEEDED:
         :                      return BLK_STS_NOSPC;
    0.00 :   ffff8000107b528c:       mov     w0, #0x3                        // #3
         :                      switch (status & 0x7ff) {
    0.00 :   ffff8000107b5290:       cmp     w1, #0x81
    0.00 :   ffff8000107b5294:       b.eq    ffff8000107b52b0 <nvme_error_status+0x40>  // b.none
    0.00 :   ffff8000107b5298:       b.ls    ffff8000107b5300 <nvme_error_status+0x90>  // b.plast
    0.00 :   ffff8000107b529c:       cmp     w1, #0x82
    0.00 :   ffff8000107b52a0:       b.eq    ffff8000107b5308 <nvme_error_status+0x98>  // b.none
         :                      case NVME_SC_APPTAG_CHECK:
         :                      case NVME_SC_REFTAG_CHECK:
         :                      case NVME_SC_INVALID_PI:
         :                      return BLK_STS_PROTECTION;
         :                      case NVME_SC_RESERVATION_CONFLICT:
         :                      return BLK_STS_NEXUS;
    0.00 :   ffff8000107b52a4:       mov     w0, #0x6                        // #6
         :                      switch (status & 0x7ff) {
    0.00 :   ffff8000107b52a8:       cmp     w1, #0x83
    0.00 :   ffff8000107b52ac:       b.ne    ffff8000107b52d0 <nvme_error_status+0x60>  // b.any
         :                      case NVME_SC_HOST_PATH_ERROR:
         :                      return BLK_STS_TRANSPORT;
         :                      default:
         :                      return BLK_STS_IOERR;
         :                      }
         :                      }
    9.36 :   ffff8000107b52b0:       ret
         :                      switch (status & 0x7ff) {
    0.00 :   ffff8000107b52b4:       cmp     w1, #0x281
    0.00 :   ffff8000107b52b8:       b.ls    ffff8000107b5318 <nvme_error_status+0xa8>  // b.plast
    0.00 :   ffff8000107b52bc:       cmp     w1, #0x287
    0.00 :   ffff8000107b52c0:       b.ls    ffff8000107b52f0 <nvme_error_status+0x80>  // b.plast
         :                      return BLK_STS_TRANSPORT;
    0.00 :   ffff8000107b52c4:       mov     w0, #0x4                        // #4
         :                      switch (status & 0x7ff) {
    0.00 :   ffff8000107b52c8:       cmp     w1, #0x370
    0.00 :   ffff8000107b52cc:       b.eq    ffff8000107b52b0 <nvme_error_status+0x40>  // b.none
         :                      return BLK_STS_IOERR;
    0.00 :   ffff8000107b52d0:       mov     w0, #0xa                        // #10
         :                      }
    0.00 :   ffff8000107b52d4:       ret
         :                      switch (status & 0x7ff) {
   22.08 :   ffff8000107b52d8:       cmp     w1, #0x2
    0.00 :   ffff8000107b52dc:       b.hi    ffff8000107b533c <nvme_error_status+0xcc>  // b.pmore
         :                      return BLK_STS_OK;
    4.92 :   ffff8000107b52e0:       mov     w0, #0x0                        // #0
         :                      switch (status & 0x7ff) {
    0.00 :   ffff8000107b52e4:       cbz     w1, ffff8000107b52b0 <nvme_error_status+0x40>
         :                      return BLK_STS_NOTSUPP;
    0.00 :   ffff8000107b52e8:       mov     w0, #0x1                        // #1
         :                      }
    0.00 :   ffff8000107b52ec:       ret
         :                      switch (status & 0x7ff) {
    0.00 :   ffff8000107b52f0:       cmp     w1, #0x285
    0.00 :   ffff8000107b52f4:       b.cs    ffff8000107b5310 <nvme_error_status+0xa0>  // b.hs, b.nlast
         :                      return BLK_STS_PROTECTION;
    0.00 :   ffff8000107b52f8:       mov     w0, #0x8                        // #8
         :                      }
    0.00 :   ffff8000107b52fc:       ret
         :                      switch (status & 0x7ff) {
    0.00 :   ffff8000107b5300:       cmp     w1, #0x80
    0.00 :   ffff8000107b5304:       b.ne    ffff8000107b52d0 <nvme_error_status+0x60>  // b.any
         :                      return BLK_STS_TARGET;
    0.00 :   ffff8000107b5308:       mov     w0, #0x5                        // #5
         :                      }
    0.00 :   ffff8000107b530c:       ret
         :                      return BLK_STS_MEDIUM;
    0.00 :   ffff8000107b5310:       mov     w0, #0x7                        // #7
         :                      }
    0.00 :   ffff8000107b5314:       ret
         :                      switch (status & 0x7ff) {
    0.00 :   ffff8000107b5318:       cmp     w1, #0x280
    0.00 :   ffff8000107b531c:       b.cs    ffff8000107b5310 <nvme_error_status+0xa0>  // b.hs, b.nlast
    0.00 :   ffff8000107b5320:       cmp     w1, #0x182
    0.00 :   ffff8000107b5324:       b.eq    ffff8000107b5310 <nvme_error_status+0xa0>  // b.none
    0.00 :   ffff8000107b5328:       b.cc    ffff8000107b52f8 <nvme_error_status+0x88>  // b.lo, b.ul, b.last
    0.00 :   ffff8000107b532c:       cmp     w1, #0x183
    0.00 :   ffff8000107b5330:       b.ne    ffff8000107b52d0 <nvme_error_status+0x60>  // b.any
         :                      return BLK_STS_NOTSUPP;
    0.00 :   ffff8000107b5334:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000107b5338:       b       ffff8000107b52ec <nvme_error_status+0x7c>
         :                      switch (status & 0x7ff) {
    0.00 :   ffff8000107b533c:       cmp     w1, #0xb
    0.00 :   ffff8000107b5340:       b.ne    ffff8000107b52d0 <nvme_error_status+0x60>  // b.any
         :                      return BLK_STS_NOTSUPP;
    0.00 :   ffff8000107b5344:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000107b5348:       b       ffff8000107b52ec <nvme_error_status+0x7c>
 Percent |	Source code & Disassembly of vmlinux for cycles (660 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010700f10 <arm_lpae_unmap>:
         :                      arm_lpae_unmap():
         :                      return __arm_lpae_unmap(data, gather, iova, size, lvl + 1, ptep);
         :                      }
         :
         :                      static size_t arm_lpae_unmap(struct io_pgtable_ops *ops, unsigned long iova,
         :                      size_t size, struct iommu_iotlb_gather *gather)
         :                      {
    3.95 :   ffff800010700f10:       stp     x29, x30, [sp, #-16]!
    0.15 :   ffff800010700f14:       mov     x29, sp
         :                      struct arm_lpae_io_pgtable *data = io_pgtable_ops_to_data(ops);
         :                      struct io_pgtable_cfg *cfg = &data->iop.cfg;
         :                      arm_lpae_iopte *ptep = data->pgd;
         :
         :                      if (WARN_ON(!size || (size & cfg->pgsize_bitmap) != size))
    1.50 :   ffff800010700f18:       cbnz    x2, ffff800010700f2c <arm_lpae_unmap+0x1c>
    0.00 :   ffff800010700f1c:       brk     #0x800
         :                      return 0;
    0.00 :   ffff800010700f20:       mov     x0, #0x0                        // #0
         :
         :                      if (WARN_ON(iova >> data->iop.cfg.ias))
         :                      return 0;
         :
         :                      return __arm_lpae_unmap(data, gather, iova, size, data->start_level, ptep);
         :                      }
    0.00 :   ffff800010700f24:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010700f28:       ret
         :                      if (WARN_ON(!size || (size & cfg->pgsize_bitmap) != size))
   13.83 :   ffff800010700f2c:       ldur    x7, [x0, #-72]
    2.58 :   ffff800010700f30:       and     x7, x2, x7
   17.13 :   ffff800010700f34:       cmp     x7, x2
    3.02 :   ffff800010700f38:       b.ne    ffff800010700f1c <arm_lpae_unmap+0xc>  // b.any
         :                      if (WARN_ON(iova >> data->iop.cfg.ias))
    8.35 :   ffff800010700f3c:       ldur    w2, [x0, #-64]
   13.72 :   ffff800010700f40:       lsr     x2, x1, x2
    3.95 :   ffff800010700f44:       cbnz    x2, ffff800010700f74 <arm_lpae_unmap+0x64>
    8.47 :   ffff800010700f48:       sub     x6, x0, #0x60
         :                      return __arm_lpae_unmap(data, gather, iova, size, data->start_level, ptep);
    5.45 :   ffff800010700f4c:       ldr     w4, [x6, #124]
    1.95 :   ffff800010700f50:       ldr     x5, [x6, #136]
    1.53 :   ffff800010700f54:       mov     x0, x3
    0.15 :   ffff800010700f58:       mov     x2, x1
    0.61 :   ffff800010700f5c:       mov     x3, x7
    8.33 :   ffff800010700f60:       mov     x1, x0
    0.30 :   ffff800010700f64:       mov     x0, x6
    0.45 :   ffff800010700f68:       bl      ffff800010700aa0 <__arm_lpae_unmap>
         :                      }
    2.74 :   ffff800010700f6c:       ldp     x29, x30, [sp], #16
    1.82 :   ffff800010700f70:       ret
         :                      if (WARN_ON(iova >> data->iop.cfg.ias))
    0.00 :   ffff800010700f74:       brk     #0x800
         :                      return 0;
    0.00 :   ffff800010700f78:       mov     x0, #0x0                        // #0
    0.00 :   ffff800010700f7c:       b       ffff800010700f24 <arm_lpae_unmap+0x14>
 Percent |	Source code & Disassembly of vmlinux for cycles (681 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000106fb958 <iommu_pgsize.isra.21>:
         :                      __fls():
         :                      *
         :                      * Undefined if no set bit exists, so code should check against 0 first.
         :                      */
         :                      static __always_inline unsigned long __fls(unsigned long word)
         :                      {
         :                      return (sizeof(word) * 8) - 1 - __builtin_clzl(word);
    4.69 :   ffff8000106fb958:       clz     x3, x2
    0.00 :   ffff8000106fb95c:       mov     x2, #0x3f                       // #63
    0.42 :   ffff8000106fb960:       sub     x2, x2, x3
         :                      iommu_pgsize():
         :                      {
         :                      unsigned int pgsize_idx;
         :                      size_t pgsize;
         :
         :                      /* Max page size that still fits into 'size' */
         :                      pgsize_idx = __fls(size);
    0.00 :   ffff8000106fb964:       mov     w3, w2
         :
         :                      /* need to consider alignment requirements ? */
         :                      if (likely(addr_merge)) {
    4.87 :   ffff8000106fb968:       cbz     x1, ffff8000106fb97c <iommu_pgsize.isra.21+0x24>
         :                      __ffs():
         :                      *
         :                      * Undefined if no bit exists, so code should check against 0 first.
         :                      */
         :                      static __always_inline unsigned long __ffs(unsigned long word)
         :                      {
         :                      return __builtin_ctzl(word);
    5.87 :   ffff8000106fb96c:       rbit    x1, x1
    1.61 :   ffff8000106fb970:       clz     x1, x1
         :                      iommu_pgsize():
         :                      /* Max page size allowed by address */
         :                      unsigned int align_pgsize_idx = __ffs(addr_merge);
         :                      pgsize_idx = min(pgsize_idx, align_pgsize_idx);
    0.58 :   ffff8000106fb974:       cmp     w2, w1
    0.14 :   ffff8000106fb978:       csel    w3, w2, w1, ls  // ls = plast
         :                      }
         :
         :                      /* build a mask of acceptable page sizes */
         :                      pgsize = (1UL << (pgsize_idx + 1)) - 1;
   12.49 :   ffff8000106fb97c:       add     w3, w3, #0x1
         :
         :                      /* throw away page sizes not supported by the hardware */
         :                      pgsize &= domain->pgsize_bitmap;
    0.00 :   ffff8000106fb980:       mov     x1, #0xffffffffffffffff         // #-1
    0.15 :   ffff8000106fb984:       lsl     x1, x1, x3
         :
         :                      /* make sure we're still sane */
         :                      BUG_ON(!pgsize);
    0.00 :   ffff8000106fb988:       bics    x1, x0, x1
    3.78 :   ffff8000106fb98c:       b.eq    ffff8000106fb9a8 <iommu_pgsize.isra.21+0x50>  // b.none
         :                      __fls():
    2.21 :   ffff8000106fb990:       clz     x1, x1
    4.51 :   ffff8000106fb994:       mov     x0, #0x3f                       // #63
    5.45 :   ffff8000106fb998:       sub     x1, x0, x1
         :                      iommu_pgsize():
         :
         :                      /* pick the biggest page */
         :                      pgsize_idx = __fls(pgsize);
         :                      pgsize = 1UL << pgsize_idx;
    2.51 :   ffff8000106fb99c:       mov     x0, #0x1                        // #1
         :
         :                      return pgsize;
         :                      }
   48.78 :   ffff8000106fb9a0:       lsl     x0, x0, x1
    1.92 :   ffff8000106fb9a4:       ret
         :                      BUG_ON(!pgsize);
    0.00 :   ffff8000106fb9a8:       brk     #0x800
 Percent |	Source code & Disassembly of vmlinux for cycles (1163 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000107be160 <nvme_queue_rq>:
         :                      nvme_queue_rq():
         :                      /*
         :                      * NOTE: ns is NULL when called on the admin queue.
         :                      */
         :                      static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
         :                      const struct blk_mq_queue_data *bd)
         :                      {
    4.87 :   ffff8000107be160:       stp     x29, x30, [sp, #-256]!
    0.00 :   ffff8000107be164:       mov     x29, sp
    4.04 :   ffff8000107be168:       stp     x21, x22, [sp, #32]
    0.00 :   ffff8000107be16c:       mov     x21, x1
    0.51 :   ffff8000107be170:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000107be174:       adrp    x19, ffff800011899000 <page_wait_table+0x1500>
    0.60 :   ffff8000107be178:       str     x23, [sp, #48]
    0.00 :   ffff8000107be17c:       add     x1, x19, #0x8c8
    0.35 :   ffff8000107be180:       str     x28, [sp, #88]
         :                      struct nvme_ns *ns = hctx->queue->queuedata;
         :                      struct nvme_queue *nvmeq = hctx->driver_data;
         :                      struct nvme_dev *dev = nvmeq->dev;
         :                      struct request *req = bd->rq;
    5.85 :   ffff8000107be184:       ldr     x28, [x21]
         :                      {
    0.00 :   ffff8000107be188:       ldr     x2, [x1]
    1.38 :   ffff8000107be18c:       str     x2, [x29, #248]
    0.00 :   ffff8000107be190:       mov     x2, #0x0                        // #0
         :                      struct nvme_ns *ns = hctx->queue->queuedata;
    0.00 :   ffff8000107be194:       ldr     x3, [x0, #208]
         :                      struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
         :                      struct nvme_command cmnd;
         :                      blk_status_t ret;
         :
         :                      iod->aborted = 0;
    0.00 :   ffff8000107be198:       mov     x2, #0xffffffff00000000         // #-4294967296
         :                      struct nvme_queue *nvmeq = hctx->driver_data;
    1.12 :   ffff8000107be19c:       ldr     x20, [x0, #224]
         :                      iod->aborted = 0;
    0.00 :   ffff8000107be1a0:       add     x1, x28, #0x200
         :                      struct nvme_ns *ns = hctx->queue->queuedata;
    0.00 :   ffff8000107be1a4:       ldr     x0, [x3, #96]
         :                      struct nvme_dev *dev = nvmeq->dev;
    5.07 :   ffff8000107be1a8:       ldr     x22, [x20]
         :                      iod->aborted = 0;
   10.32 :   ffff8000107be1ac:       stur    x2, [x1, #-188]
         :                      iod->npages = -1;
    0.08 :   ffff8000107be1b0:       str     wzr, [x28, #332]
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.08 :   ffff8000107be1b4:       ldr     x1, [x20, #128]
         :                      nvme_queue_rq():
         :
         :                      /*
         :                      * We should not need to do this, but we're still using this to
         :                      * ensure we can drain requests on a dying queue.
         :                      */
         :                      if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
    0.00 :   ffff8000107be1b8:       tbz     w1, #0, ffff8000107be64c <nvme_queue_rq+0x4ec>
    6.81 :   ffff8000107be1bc:       str     x25, [x29, #64]
         :                      return BLK_STS_IOERR;
         :
         :                      ret = nvme_setup_cmd(ns, req, &cmnd);
    1.04 :   ffff8000107be1c0:       add     x2, x29, #0xb8
    0.00 :   ffff8000107be1c4:       and     w25, w1, #0x1
    0.00 :   ffff8000107be1c8:       mov     x1, x28
    0.17 :   ffff8000107be1cc:       bl      ffff8000107ba148 <nvme_setup_cmd>
         :                      if (ret)
    0.26 :   ffff8000107be1d0:       ands    w23, w0, #0xff
    0.00 :   ffff8000107be1d4:       b.ne    ffff8000107be42c <nvme_queue_rq+0x2cc>  // b.any
    0.95 :   ffff8000107be1d8:       str     x24, [x29, #56]
    1.46 :   ffff8000107be1dc:       stp     x26, x27, [x29, #72]
         :                      blk_rq_nr_phys_segments():
         :                      * own special payload.  In that case we still return 1 here so that this
         :                      * special payload will be mapped.
         :                      */
         :                      static inline unsigned short blk_rq_nr_phys_segments(struct request *rq)
         :                      {
         :                      if (rq->rq_flags & RQF_SPECIAL_PAYLOAD)
    0.26 :   ffff8000107be1e0:       ldr     w0, [x28, #28]
    0.00 :   ffff8000107be1e4:       tbnz    w0, #18, ffff8000107be654 <nvme_queue_rq+0x4f4>
         :                      return 1;
         :                      return rq->nr_phys_segments;
    0.00 :   ffff8000107be1e8:       ldrh    w0, [x28, #194]
         :                      nvme_queue_rq():
         :                      return ret;
         :
         :                      if (blk_rq_nr_phys_segments(req)) {
    0.00 :   ffff8000107be1ec:       cbz     w0, ffff8000107be4d0 <nvme_queue_rq+0x370>
    1.46 :   ffff8000107be1f0:       add     x24, x28, #0x118
         :                      nvme_map_data():
         :                      if (blk_rq_nr_phys_segments(req) == 1) {
    0.00 :   ffff8000107be1f4:       cmp     w0, #0x1
    0.00 :   ffff8000107be1f8:       b.eq    ffff8000107be564 <nvme_queue_rq+0x404>  // b.none
         :                      iod->dma_len = 0;
    0.00 :   ffff8000107be1fc:       str     wzr, [x24, #64]
         :                      iod->sg = mempool_alloc(dev->iod_mempool, GFP_ATOMIC);
    0.00 :   ffff8000107be200:       mov     w1, #0xa20                      // #2592
    0.00 :   ffff8000107be204:       ldr     x0, [x22, #3176]
    0.00 :   ffff8000107be208:       bl      ffff8000101d5c38 <mempool_alloc>
    0.00 :   ffff8000107be20c:       str     x0, [x24, #80]
         :                      if (!iod->sg)
    0.00 :   ffff8000107be210:       cbz     x0, ffff8000107be6e4 <nvme_queue_rq+0x584>
         :                      blk_rq_nr_phys_segments():
         :                      if (rq->rq_flags & RQF_SPECIAL_PAYLOAD)
    0.00 :   ffff8000107be214:       ldr     w2, [x28, #28]
    0.00 :   ffff8000107be218:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000107be21c:       tbnz    w2, #18, ffff8000107be224 <nvme_queue_rq+0xc4>
    0.00 :   ffff8000107be220:       ldrh    w1, [x28, #194]
         :                      nvme_map_data():
         :                      sg_init_table(iod->sg, blk_rq_nr_phys_segments(req));
    0.00 :   ffff8000107be224:       bl      ffff800010480d00 <sg_init_table>
         :                      iod->nents = blk_rq_map_sg(req->q, req, iod->sg);
    0.00 :   ffff8000107be228:       ldr     x2, [x24, #80]
    0.00 :   ffff8000107be22c:       mov     x1, x28
    0.00 :   ffff8000107be230:       ldr     x0, [x28]
    0.00 :   ffff8000107be234:       bl      ffff80001045a3b0 <blk_rq_map_sg>
    0.00 :   ffff8000107be238:       str     w0, [x24, #52]
    0.00 :   ffff8000107be23c:       mov     w2, w0
         :                      if (!iod->nents)
    0.00 :   ffff8000107be240:       cbz     w0, ffff8000107be618 <nvme_queue_rq+0x4b8>
         :                      nr_mapped = dma_map_sg_attrs(dev->dev, iod->sg, iod->nents,
    0.00 :   ffff8000107be244:       ldr     x0, [x22, #336]
         :                      op_is_write():
         :                      bio->bi_opf = op | op_flags;
         :                      }
         :
         :                      static inline bool op_is_write(unsigned int op)
         :                      {
         :                      return (op & 1);
    0.00 :   ffff8000107be248:       ldr     w3, [x28, #24]
         :                      nvme_map_data():
         :                      if (is_pci_p2pdma_page(sg_page(iod->sg)))
    0.00 :   ffff8000107be24c:       ldr     x1, [x24, #80]
         :                      dma_map_sg_attrs():
         :                      */
         :                      static inline int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,
         :                      int nents, enum dma_data_direction dir,
         :                      unsigned long attrs)
         :                      {
         :                      const struct dma_map_ops *ops = get_dma_ops(dev);
    0.00 :   ffff8000107be250:       ldr     x4, [x0, #576]
         :                      nvme_map_data():
         :                      nr_mapped = dma_map_sg_attrs(dev->dev, iod->sg, iod->nents,
    0.00 :   ffff8000107be254:       tst     x3, #0x1
    0.00 :   ffff8000107be258:       cset    w3, eq  // eq = none
    0.00 :   ffff8000107be25c:       add     w3, w3, #0x1
         :                      get_dma_ops():
         :                      if (dev->dma_ops)
    0.00 :   ffff8000107be260:       cbz     x4, ffff8000107be63c <nvme_queue_rq+0x4dc>
         :                      dma_map_sg_attrs():
         :
         :                      BUG_ON(!valid_dma_direction(dir));
         :                      if (dma_is_direct(ops))
         :                      ents = dma_direct_map_sg(dev, sg, nents, dir, attrs);
         :                      else
         :                      ents = ops->map_sg(dev, sg, nents, dir, attrs);
    0.00 :   ffff8000107be264:       ldr     x5, [x4, #48]
    0.00 :   ffff8000107be268:       mov     x4, #0x100                      // #256
    0.00 :   ffff8000107be26c:       blr     x5
    0.00 :   ffff8000107be270:       mov     w27, w0
         :                      BUG_ON(ents < 0);
    0.00 :   ffff8000107be274:       cmp     w27, #0x0
    0.00 :   ffff8000107be278:       b.lt    ffff8000107be788 <nvme_queue_rq+0x628>  // b.tstop
         :                      nvme_map_data():
         :                      if (!nr_mapped)
    0.00 :   ffff8000107be27c:       b.eq    ffff8000107be618 <nvme_queue_rq+0x4b8>  // b.none
         :                      blk_rq_nr_phys_segments():
    0.00 :   ffff8000107be280:       ldr     w0, [x28, #28]
    0.00 :   ffff8000107be284:       and     w7, w0, #0x40000
    0.00 :   ffff8000107be288:       tbz     w0, #18, ffff8000107be460 <nvme_queue_rq+0x300>
         :                      blk_rq_payload_bytes():
         :                      return rq->special_vec.bv_len;
    0.00 :   ffff8000107be28c:       ldr     w0, [x28, #112]
    0.00 :   ffff8000107be290:       mov     w2, #0x1                        // #1
         :                      blk_rq_nr_phys_segments():
         :                      return 1;
    0.00 :   ffff8000107be294:       mov     w1, w2
         :                      nvme_pci_use_sgls():
         :                      if (!(dev->ctrl.sgls & ((1 << 0) | (1 << 1))))
    0.00 :   ffff8000107be298:       ldr     w5, [x22, #1764]
    0.00 :   ffff8000107be29c:       ldr     x26, [x24, #80]
    0.00 :   ffff8000107be2a0:       tst     x5, #0x3
    0.00 :   ffff8000107be2a4:       b.eq    ffff8000107be6ec <nvme_queue_rq+0x58c>  // b.none
         :                      if (!iod->nvmeq->qid)
    0.00 :   ffff8000107be2a8:       ldr     x5, [x24, #32]
    0.00 :   ffff8000107be2ac:       ldrh    w5, [x5, #122]
    0.00 :   ffff8000107be2b0:       cbz     w5, ffff8000107be6ec <nvme_queue_rq+0x58c>
         :                      if (!sgl_threshold || avg_seg_size < sgl_threshold)
    0.00 :   ffff8000107be2b4:       adrp    x5, ffff800011a1a000 <hisi_sas_v1_driver+0x28>
    0.00 :   ffff8000107be2b8:       ldr     w5, [x5, #3376]
    0.00 :   ffff8000107be2bc:       cbz     w5, ffff8000107be6ec <nvme_queue_rq+0x58c>
         :                      avg_seg_size = DIV_ROUND_UP(blk_rq_payload_bytes(req), nseg);
    0.00 :   ffff8000107be2c0:       add     w0, w2, w0
    0.00 :   ffff8000107be2c4:       sub     w0, w0, #0x1
    0.00 :   ffff8000107be2c8:       udiv    w0, w0, w2
         :                      if (!sgl_threshold || avg_seg_size < sgl_threshold)
    0.00 :   ffff8000107be2cc:       cmp     w0, w5
    0.00 :   ffff8000107be2d0:       b.cc    ffff8000107be6ec <nvme_queue_rq+0x58c>  // b.lo, b.ul, b.last
         :                      nvme_map_data():
         :                      iod->use_sgl = nvme_pci_use_sgls(dev, req);
    0.00 :   ffff8000107be2d4:       mov     w1, #0x1                        // #1
         :                      nvme_pci_setup_sgls():
         :                      cmd->flags = NVME_CMD_SGL_METABUF;
    0.00 :   ffff8000107be2d8:       mov     w0, #0x40                       // #64
         :                      nvme_map_data():
         :                      iod->use_sgl = nvme_pci_use_sgls(dev, req);
    0.00 :   ffff8000107be2dc:       strb    w1, [x24, #40]
         :                      nvme_pci_setup_sgls():
         :                      if (entries == 1) {
    0.00 :   ffff8000107be2e0:       cmp     w27, #0x1
         :                      cmd->flags = NVME_CMD_SGL_METABUF;
    0.00 :   ffff8000107be2e4:       strb    w0, [x29, #185]
         :                      if (entries == 1) {
    0.00 :   ffff8000107be2e8:       b.eq    ffff8000107be8f0 <nvme_queue_rq+0x790>  // b.none
         :                      if (entries <= (256 / sizeof(struct nvme_sgl_desc))) {
    0.00 :   ffff8000107be2ec:       cmp     w27, #0x10
    0.00 :   ffff8000107be2f0:       b.gt    ffff8000107be798 <nvme_queue_rq+0x638>
         :                      pool = dev->prp_small_pool;
    0.00 :   ffff8000107be2f4:       ldr     x0, [x22, #352]
    0.00 :   ffff8000107be2f8:       str     x0, [x29, #136]
         :                      iod->npages = 0;
    0.00 :   ffff8000107be2fc:       str     wzr, [x24, #48]
         :                      sg_list = dma_pool_alloc(pool, GFP_ATOMIC, &sgl_dma);
    0.00 :   ffff8000107be300:       ldr     x0, [x29, #136]
    0.00 :   ffff8000107be304:       mov     w1, #0xa20                      // #2592
    0.00 :   ffff8000107be308:       add     x2, x29, #0xb0
    0.00 :   ffff8000107be30c:       bl      ffff8000102384a8 <dma_pool_alloc>
    0.00 :   ffff8000107be310:       mov     x5, x0
         :                      if (!sg_list) {
    0.00 :   ffff8000107be314:       cbz     x0, ffff8000107be988 <nvme_queue_rq+0x828>
         :                      blk_rq_nr_phys_segments():
         :                      if (rq->rq_flags & RQF_SPECIAL_PAYLOAD)
    0.00 :   ffff8000107be318:       ldr     w2, [x28, #28]
    0.00 :   ffff8000107be31c:       mov     x0, #0x20                       // #32
         :                      nvme_pci_iod_list():
         :                      return (void **)(iod->sg + blk_rq_nr_phys_segments(req));
    0.00 :   ffff8000107be320:       ldr     x1, [x24, #80]
         :                      blk_rq_nr_phys_segments():
    0.00 :   ffff8000107be324:       tbnz    w2, #18, ffff8000107be330 <nvme_queue_rq+0x1d0>
    0.00 :   ffff8000107be328:       ldrh    w0, [x28, #194]
    0.00 :   ffff8000107be32c:       lsl     x0, x0, #5
         :                      nvme_pci_setup_sgls():
         :                      nvme_pci_iod_list(req)[0] = sg_list;
    0.00 :   ffff8000107be330:       str     x5, [x1, x0]
         :                      nvme_pci_sgl_set_seg():
         :                      if (entries < SGES_PER_PAGE) {
    0.00 :   ffff8000107be334:       cmp     w27, #0xff
         :                      nvme_pci_setup_sgls():
         :                      iod->first_dma = sgl_dma;
    0.00 :   ffff8000107be338:       ldr     x0, [x29, #176]
    0.00 :   ffff8000107be33c:       str     x0, [x24, #56]
         :                      nvme_pci_sgl_set_seg():
         :                      sge->addr = cpu_to_le64(dma_addr);
    0.00 :   ffff8000107be340:       str     x0, [x29, #208]
         :                      if (entries < SGES_PER_PAGE) {
    0.00 :   ffff8000107be344:       b.gt    ffff8000107be908 <nvme_queue_rq+0x7a8>
         :                      sge->length = cpu_to_le32(entries * sizeof(*sge));
    0.00 :   ffff8000107be348:       lsl     w1, w27, #4
         :                      sge->type = NVME_SGL_FMT_LAST_SEG_DESC << 4;
    0.00 :   ffff8000107be34c:       mov     w0, #0x30                       // #48
         :                      sge->length = cpu_to_le32(entries * sizeof(*sge));
    0.00 :   ffff8000107be350:       str     w1, [x29, #216]
         :                      sge->type = NVME_SGL_FMT_LAST_SEG_DESC << 4;
    0.00 :   ffff8000107be354:       strb    w0, [x29, #223]
         :                      sge->type = NVME_SGL_FMT_SEG_DESC << 4;
    0.00 :   ffff8000107be358:       mov     w1, #0x0                        // #0
    0.00 :   ffff8000107be35c:       nop
         :                      nvme_pci_setup_sgls():
         :                      nvme_pci_sgl_set_data(&sg_list[i++], sg);
    0.00 :   ffff8000107be360:       sbfiz   x0, x1, #4, #32
    0.00 :   ffff8000107be364:       ldr     x7, [x26, #16]
    0.00 :   ffff8000107be368:       add     x2, x5, x0
    0.00 :   ffff8000107be36c:       add     w1, w1, #0x1
    0.00 :   ffff8000107be370:       str     w1, [x29, #148]
         :                      nvme_pci_sgl_set_data():
         :                      sge->addr = cpu_to_le64(sg_dma_address(sg));
    0.00 :   ffff8000107be374:       str     x7, [x5, x0]
         :                      nvme_pci_setup_sgls():
         :                      sg = sg_next(sg);
    0.00 :   ffff8000107be378:       mov     x0, x26
         :                      nvme_pci_sgl_set_data():
         :                      sge->addr = cpu_to_le64(sg_dma_address(sg));
    0.00 :   ffff8000107be37c:       str     x5, [x29, #152]
         :                      sge->length = cpu_to_le32(sg_dma_len(sg));
    0.00 :   ffff8000107be380:       ldr     w4, [x26, #24]
    0.00 :   ffff8000107be384:       str     w4, [x2, #8]
         :                      sge->type = NVME_SGL_FMT_DATA_DESC << 4;
    0.00 :   ffff8000107be388:       strb    wzr, [x2, #15]
         :                      nvme_pci_setup_sgls():
         :                      sg = sg_next(sg);
    0.00 :   ffff8000107be38c:       bl      ffff800010480bb8 <sg_next>
    0.00 :   ffff8000107be390:       mov     x26, x0
         :                      } while (--entries > 0);
    0.00 :   ffff8000107be394:       subs    w27, w27, #0x1
    0.00 :   ffff8000107be398:       ldr     w1, [x29, #148]
    0.00 :   ffff8000107be39c:       ldr     x5, [x29, #152]
    0.00 :   ffff8000107be3a0:       b.eq    ffff8000107be4d0 <nvme_queue_rq+0x370>  // b.none
         :                      if (i == SGES_PER_PAGE) {
    0.00 :   ffff8000107be3a4:       cmp     w1, #0x100
    0.00 :   ffff8000107be3a8:       b.ne    ffff8000107be360 <nvme_queue_rq+0x200>  // b.any
         :                      sg_list = dma_pool_alloc(pool, GFP_ATOMIC, &sgl_dma);
    0.00 :   ffff8000107be3ac:       ldr     x0, [x29, #136]
    0.00 :   ffff8000107be3b0:       add     x2, x29, #0xb0
    0.00 :   ffff8000107be3b4:       mov     w1, #0xa20                      // #2592
    0.00 :   ffff8000107be3b8:       str     x5, [x29, #152]
    0.00 :   ffff8000107be3bc:       bl      ffff8000102384a8 <dma_pool_alloc>
         :                      if (!sg_list)
    0.00 :   ffff8000107be3c0:       cbz     x0, ffff8000107be94c <nvme_queue_rq+0x7ec>
         :                      blk_rq_nr_phys_segments():
    0.00 :   ffff8000107be3c4:       ldr     w2, [x28, #28]
    0.00 :   ffff8000107be3c8:       mov     x1, #0x20                       // #32
         :                      nvme_pci_iod_list():
         :                      return (void **)(iod->sg + blk_rq_nr_phys_segments(req));
    0.00 :   ffff8000107be3cc:       ldr     x7, [x24, #80]
         :                      blk_rq_nr_phys_segments():
    0.00 :   ffff8000107be3d0:       ldr     x5, [x29, #152]
    0.00 :   ffff8000107be3d4:       tbnz    w2, #18, ffff8000107be3e0 <nvme_queue_rq+0x280>
    0.00 :   ffff8000107be3d8:       ldrh    w1, [x28, #194]
    0.00 :   ffff8000107be3dc:       lsl     x1, x1, #5
         :                      nvme_pci_setup_sgls():
         :                      nvme_pci_iod_list(req)[iod->npages++] = sg_list;
    0.00 :   ffff8000107be3e0:       ldr     w2, [x24, #48]
         :                      nvme_pci_sgl_set_seg():
         :                      if (entries < SGES_PER_PAGE) {
    0.00 :   ffff8000107be3e4:       cmp     w27, #0xff
         :                      nvme_pci_setup_sgls():
         :                      nvme_pci_iod_list(req)[iod->npages++] = sg_list;
    0.00 :   ffff8000107be3e8:       add     x1, x1, w2, sxtw #3
    0.00 :   ffff8000107be3ec:       add     w2, w2, #0x1
    0.00 :   ffff8000107be3f0:       str     w2, [x24, #48]
         :                      sg_list[i++] = *link;
    0.00 :   ffff8000107be3f4:       add     x2, x5, #0xff0
         :                      nvme_pci_iod_list(req)[iod->npages++] = sg_list;
    0.00 :   ffff8000107be3f8:       str     x0, [x7, x1]
         :                      sg_list[i++] = *link;
    0.00 :   ffff8000107be3fc:       ldp     x8, x9, [x2]
    0.00 :   ffff8000107be400:       stp     x8, x9, [x0]
         :                      nvme_pci_sgl_set_seg():
         :                      sge->addr = cpu_to_le64(dma_addr);
    0.00 :   ffff8000107be404:       ldr     x1, [x29, #176]
    0.00 :   ffff8000107be408:       str     x1, [x5, #4080]
         :                      if (entries < SGES_PER_PAGE) {
    0.00 :   ffff8000107be40c:       b.gt    ffff8000107be714 <nvme_queue_rq+0x5b4>
         :                      sge->length = cpu_to_le32(entries * sizeof(*sge));
    0.00 :   ffff8000107be410:       lsl     w5, w27, #4
         :                      sge->type = NVME_SGL_FMT_LAST_SEG_DESC << 4;
    0.00 :   ffff8000107be414:       mov     w1, #0x30                       // #48
         :                      sge->length = cpu_to_le32(entries * sizeof(*sge));
    0.00 :   ffff8000107be418:       str     w5, [x2, #8]
         :                      sge->type = NVME_SGL_FMT_LAST_SEG_DESC << 4;
    0.00 :   ffff8000107be41c:       mov     x5, x0
    0.00 :   ffff8000107be420:       strb    w1, [x2, #15]
         :                      nvme_pci_setup_sgls():
         :                      sg_list[i++] = *link;
    0.00 :   ffff8000107be424:       mov     w1, w25
    0.00 :   ffff8000107be428:       b       ffff8000107be360 <nvme_queue_rq+0x200>
    0.00 :   ffff8000107be42c:       ldr     x25, [x29, #64]
         :                      nvme_queue_rq():
         :                      out_unmap_data:
         :                      nvme_unmap_data(dev, req);
         :                      out_free_cmd:
         :                      nvme_cleanup_cmd(req);
         :                      return ret;
         :                      }
    0.00 :   ffff8000107be430:       add     x19, x19, #0x8c8
    0.00 :   ffff8000107be434:       mov     w0, w23
    0.00 :   ffff8000107be438:       ldr     x2, [x29, #248]
    1.28 :   ffff8000107be43c:       ldr     x1, [x19]
    0.09 :   ffff8000107be440:       eor     x1, x2, x1
    0.00 :   ffff8000107be444:       cbnz    x1, ffff8000107bea18 <nvme_queue_rq+0x8b8>
    0.00 :   ffff8000107be448:       ldp     x19, x20, [sp, #16]
    0.43 :   ffff8000107be44c:       ldp     x21, x22, [sp, #32]
    0.34 :   ffff8000107be450:       ldr     x23, [sp, #48]
    0.17 :   ffff8000107be454:       ldr     x28, [sp, #88]
    0.00 :   ffff8000107be458:       ldp     x29, x30, [sp], #256
    0.00 :   ffff8000107be45c:       ret
         :                      blk_rq_nr_phys_segments():
         :                      return rq->nr_phys_segments;
    0.00 :   ffff8000107be460:       ldrh    w1, [x28, #194]
    0.00 :   ffff8000107be464:       ldr     w0, [x28, #40]
         :                      nvme_pci_use_sgls():
         :                      if (nseg == 0)
    0.00 :   ffff8000107be468:       cbnz    w1, ffff8000107be780 <nvme_queue_rq+0x620>
         :                      nvme_map_data():
         :                      iod->use_sgl = nvme_pci_use_sgls(dev, req);
    0.00 :   ffff8000107be46c:       strb    wzr, [x24, #40]
    0.00 :   ffff8000107be470:       mov     x4, #0x0                        // #0
         :                      nvme_pci_setup_prps():
         :                      struct scatterlist *sg = iod->sg;
    0.00 :   ffff8000107be474:       ldr     x26, [x24, #80]
         :                      u32 page_size = dev->ctrl.page_size;
    0.00 :   ffff8000107be478:       ldr     w7, [x22, #1720]
         :                      u64 dma_addr = sg_dma_address(sg);
    0.00 :   ffff8000107be47c:       ldr     x5, [x26, #16]
         :                      int offset = dma_addr & (page_size - 1);
    0.00 :   ffff8000107be480:       sub     w27, w7, #0x1
         :                      int dma_len = sg_dma_len(sg);
    0.00 :   ffff8000107be484:       ldr     w2, [x26, #24]
         :                      int offset = dma_addr & (page_size - 1);
    0.00 :   ffff8000107be488:       and     w1, w27, w5
         :                      length -= (page_size - offset);
    0.00 :   ffff8000107be48c:       sub     w3, w1, w7
    0.00 :   ffff8000107be490:       add     w25, w3, w0
    0.00 :   ffff8000107be494:       mov     w8, w25
         :                      if (length <= 0) {
    0.00 :   ffff8000107be498:       cmp     w25, #0x0
    0.00 :   ffff8000107be49c:       b.le    ffff8000107be78c <nvme_queue_rq+0x62c>
         :                      if (dma_len) {
    0.00 :   ffff8000107be4a0:       adds    w3, w3, w2
    0.00 :   ffff8000107be4a4:       b.eq    ffff8000107be91c <nvme_queue_rq+0x7bc>  // b.none
         :                      dma_addr += (page_size - offset);
    0.00 :   ffff8000107be4a8:       sub     w1, w7, w1
    0.00 :   ffff8000107be4ac:       mov     x9, x26
    0.00 :   ffff8000107be4b0:       add     x5, x1, x5
         :                      if (length <= page_size) {
    0.00 :   ffff8000107be4b4:       cmp     w25, w7
    0.00 :   ffff8000107be4b8:       b.hi    ffff8000107be7ac <nvme_queue_rq+0x64c>  // b.pmore
    0.00 :   ffff8000107be4bc:       ldr     x26, [x24, #80]
         :                      iod->first_dma = dma_addr;
    0.00 :   ffff8000107be4c0:       str     x5, [x24, #56]
         :                      cmnd->dptr.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
    0.00 :   ffff8000107be4c4:       ldr     x0, [x26, #16]
         :                      cmnd->dptr.prp2 = cpu_to_le64(iod->first_dma);
    0.00 :   ffff8000107be4c8:       stp     x0, x5, [x29, #208]
    0.00 :   ffff8000107be4cc:       nop
         :                      nvme_queue_rq():
         :                      if (blk_integrity_rq(req)) {
    3.02 :   ffff8000107be4d0:       ldr     w3, [x28, #24]
    0.00 :   ffff8000107be4d4:       tbz     w3, #16, ffff8000107be534 <nvme_queue_rq+0x3d4>
         :                      rq_integrity_vec():
         :                      * Return the first bvec that contains integrity data.  Only drivers that are
         :                      * limited to a single integrity segment should use this helper.
         :                      */
         :                      static inline struct bio_vec *rq_integrity_vec(struct request *rq)
         :                      {
         :                      if (WARN_ON_ONCE(queue_max_integrity_segments(rq->q) > 1))
    0.00 :   ffff8000107be4d8:       ldr     x1, [x28]
         :                      nvme_queue_rq():
         :                      ret = nvme_map_metadata(dev, req, &cmnd);
    0.00 :   ffff8000107be4dc:       ldr     x0, [x22, #336]
         :                      rq_integrity_vec():
    0.00 :   ffff8000107be4e0:       ldrh    w1, [x1, #1130]
    0.00 :   ffff8000107be4e4:       cmp     w1, #0x1
    0.00 :   ffff8000107be4e8:       b.hi    ffff8000107be73c <nvme_queue_rq+0x5dc>  // b.pmore
         :                      return NULL;
         :                      return rq->bio->bi_integrity->bip_vec;
    0.00 :   ffff8000107be4ec:       ldr     x1, [x28, #56]
    0.00 :   ffff8000107be4f0:       ldr     x1, [x1, #88]
    0.00 :   ffff8000107be4f4:       ldr     x5, [x1, #96]
         :                      nvme_map_metadata():
         :                      iod->meta_dma = dma_map_bvec(dev->dev, rq_integrity_vec(req),
    0.00 :   ffff8000107be4f8:       ldr     x1, [x5]
    0.00 :   ffff8000107be4fc:       ldr     w2, [x5, #12]
         :                      dma_map_page_attrs():
         :                      const struct dma_map_ops *ops = get_dma_ops(dev);
    0.00 :   ffff8000107be500:       ldr     x7, [x0, #576]
         :                      nvme_map_metadata():
    0.00 :   ffff8000107be504:       tst     x3, #0x1
    0.00 :   ffff8000107be508:       cset    w4, eq  // eq = none
    0.00 :   ffff8000107be50c:       ldr     w3, [x5, #8]
    0.00 :   ffff8000107be510:       add     w4, w4, #0x1
         :                      get_dma_ops():
         :                      if (dev->dma_ops)
    0.00 :   ffff8000107be514:       cbz     x7, ffff8000107be558 <nvme_queue_rq+0x3f8>
         :                      dma_map_page_attrs():
         :                      addr = ops->map_page(dev, page, offset, size, dir, attrs);
    0.00 :   ffff8000107be518:       ldr     x7, [x7, #32]
    0.00 :   ffff8000107be51c:       mov     x5, #0x0                        // #0
    0.00 :   ffff8000107be520:       blr     x7
         :                      nvme_map_metadata():
    0.00 :   ffff8000107be524:       str     x0, [x28, #352]
         :                      dma_mapping_error():
         :
         :                      static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
         :                      {
         :                      debug_dma_mapping_error(dev, dma_addr);
         :
         :                      if (dma_addr == DMA_MAPPING_ERROR)
    0.00 :   ffff8000107be528:       cmn     x0, #0x1
    0.00 :   ffff8000107be52c:       b.eq    ffff8000107be6d0 <nvme_queue_rq+0x570>  // b.none
         :                      nvme_map_metadata():
         :                      cmnd->rw.metadata = cpu_to_le64(iod->meta_dma);
    0.00 :   ffff8000107be530:       str     x0, [x29, #200]
         :                      nvme_queue_rq():
         :                      blk_mq_start_request(req);
    2.74 :   ffff8000107be534:       mov     x0, x28
    0.00 :   ffff8000107be538:       bl      ffff80001045da10 <blk_mq_start_request>
         :                      nvme_submit_cmd(nvmeq, &cmnd, bd->last);
    1.46 :   ffff8000107be53c:       ldrb    w2, [x21, #8]
    0.00 :   ffff8000107be540:       add     x1, x29, #0xb8
    0.00 :   ffff8000107be544:       mov     x0, x20
    0.00 :   ffff8000107be548:       bl      ffff8000107bcb50 <nvme_submit_cmd>
         :                      return BLK_STS_OK;
    1.03 :   ffff8000107be54c:       ldp     x24, x25, [x29, #56]
    1.97 :   ffff8000107be550:       ldp     x26, x27, [x29, #72]
    0.00 :   ffff8000107be554:       b       ffff8000107be430 <nvme_queue_rq+0x2d0>
         :                      dma_map_page_attrs():
         :                      addr = dma_direct_map_page(dev, page, offset, size, dir, attrs);
    0.00 :   ffff8000107be558:       mov     x5, #0x0                        // #0
    0.00 :   ffff8000107be55c:       bl      ffff800010161f00 <dma_direct_map_page>
    0.00 :   ffff8000107be560:       b       ffff8000107be524 <nvme_queue_rq+0x3c4>
         :                      req_bvec():
         :                      return mp_bvec_iter_bvec(rq->bio->bi_io_vec, rq->bio->bi_iter);
    0.34 :   ffff8000107be564:       ldr     x1, [x28, #56]
    0.60 :   ffff8000107be568:       ldp     w2, w0, [x1, #40]
    3.18 :   ffff8000107be56c:       ldr     x4, [x1, #104]
    2.58 :   ffff8000107be570:       ldr     w27, [x1, #48]
    0.00 :   ffff8000107be574:       lsl     x0, x0, #4
    0.00 :   ffff8000107be578:       add     x3, x4, x0
    0.43 :   ffff8000107be57c:       ldr     x1, [x4, x0]
    7.48 :   ffff8000107be580:       ldp     w26, w0, [x3, #8]
    0.00 :   ffff8000107be584:       sub     w26, w26, w27
    0.00 :   ffff8000107be588:       add     w27, w27, w0
    0.00 :   ffff8000107be58c:       cmp     w26, w2
    1.89 :   ffff8000107be590:       csel    w26, w26, w2, ls  // ls = plast
         :                      nvme_map_data():
         :                      if (bv.bv_offset + bv.bv_len <= dev->ctrl.page_size * 2)
    0.00 :   ffff8000107be594:       ldr     w24, [x22, #1720]
    0.00 :   ffff8000107be598:       add     w0, w27, w26
    0.00 :   ffff8000107be59c:       cmp     w0, w24, lsl #1
    0.17 :   ffff8000107be5a0:       b.ls    ffff8000107be660 <nvme_queue_rq+0x500>  // b.plast
         :                      if (iod->nvmeq->qid &&
    0.00 :   ffff8000107be5a4:       add     x24, x28, #0x118
    0.00 :   ffff8000107be5a8:       ldr     x0, [x24, #32]
    0.00 :   ffff8000107be5ac:       ldrh    w0, [x0, #122]
    0.00 :   ffff8000107be5b0:       cbz     w0, ffff8000107be1fc <nvme_queue_rq+0x9c>
         :                      dev->ctrl.sgls & ((1 << 0) | (1 << 1)))
    0.00 :   ffff8000107be5b4:       ldr     w0, [x22, #1764]
         :                      if (iod->nvmeq->qid &&
    0.00 :   ffff8000107be5b8:       tst     x0, #0x3
    0.00 :   ffff8000107be5bc:       b.eq    ffff8000107be1fc <nvme_queue_rq+0x9c>  // b.none
         :                      return nvme_setup_sgl_simple(dev, req,
    0.00 :   ffff8000107be5c0:       ldr     x0, [x22, #336]
         :                      nvme_setup_sgl_simple():
         :                      iod->first_dma = dma_map_bvec(dev->dev, bv, rq_dma_dir(req), 0);
    0.00 :   ffff8000107be5c4:       mov     w2, w27
         :                      op_is_write():
    0.00 :   ffff8000107be5c8:       ldr     w4, [x28, #24]
         :                      nvme_setup_sgl_simple():
    0.00 :   ffff8000107be5cc:       mov     w3, w26
         :                      dma_map_page_attrs():
         :                      const struct dma_map_ops *ops = get_dma_ops(dev);
    0.00 :   ffff8000107be5d0:       ldr     x5, [x0, #576]
         :                      nvme_setup_sgl_simple():
    0.00 :   ffff8000107be5d4:       tst     x4, #0x1
    0.00 :   ffff8000107be5d8:       cset    w4, eq  // eq = none
    0.00 :   ffff8000107be5dc:       add     w4, w4, #0x1
         :                      get_dma_ops():
         :                      if (dev->dma_ops)
    0.00 :   ffff8000107be5e0:       cbz     x5, ffff8000107be730 <nvme_queue_rq+0x5d0>
         :                      dma_map_page_attrs():
         :                      addr = ops->map_page(dev, page, offset, size, dir, attrs);
    0.00 :   ffff8000107be5e4:       ldr     x7, [x5, #32]
    0.00 :   ffff8000107be5e8:       mov     x5, #0x0                        // #0
    0.00 :   ffff8000107be5ec:       blr     x7
         :                      nvme_setup_sgl_simple():
    0.00 :   ffff8000107be5f0:       str     x0, [x24, #56]
         :                      dma_mapping_error():
         :                      if (dma_addr == DMA_MAPPING_ERROR)
    0.00 :   ffff8000107be5f4:       cmn     x0, #0x1
    0.00 :   ffff8000107be5f8:       b.eq    ffff8000107be6e4 <nvme_queue_rq+0x584>  // b.none
         :                      nvme_setup_sgl_simple():
         :                      cmnd->flags = NVME_CMD_SGL_METABUF;
    0.00 :   ffff8000107be5fc:       mov     w1, #0x40                       // #64
         :                      cmnd->dptr.sgl.type = NVME_SGL_FMT_DATA_DESC << 4;
    0.00 :   ffff8000107be600:       strb    wzr, [x29, #223]
         :                      cmnd->flags = NVME_CMD_SGL_METABUF;
    0.00 :   ffff8000107be604:       strb    w1, [x29, #185]
         :                      iod->dma_len = bv->bv_len;
    0.00 :   ffff8000107be608:       str     w26, [x24, #64]
         :                      cmnd->dptr.sgl.addr = cpu_to_le64(iod->first_dma);
    0.00 :   ffff8000107be60c:       str     x0, [x29, #208]
         :                      cmnd->dptr.sgl.length = cpu_to_le32(iod->dma_len);
    0.00 :   ffff8000107be610:       str     w26, [x29, #216]
    0.00 :   ffff8000107be614:       b       ffff8000107be4d0 <nvme_queue_rq+0x370>
         :                      nvme_map_data():
         :                      nvme_unmap_data(dev, req);
    0.00 :   ffff8000107be618:       mov     x1, x28
    0.00 :   ffff8000107be61c:       mov     x0, x22
         :                      blk_status_t ret = BLK_STS_RESOURCE;
    0.00 :   ffff8000107be620:       mov     w23, #0x9                       // #9
         :                      nvme_unmap_data(dev, req);
    0.00 :   ffff8000107be624:       bl      ffff8000107bce88 <nvme_unmap_data>
         :                      nvme_queue_rq():
         :                      nvme_cleanup_cmd(req);
    0.00 :   ffff8000107be628:       mov     x0, x28
    0.00 :   ffff8000107be62c:       bl      ffff8000107b9848 <nvme_cleanup_cmd>
         :                      return ret;
    0.00 :   ffff8000107be630:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff8000107be634:       ldp     x26, x27, [x29, #72]
    0.00 :   ffff8000107be638:       b       ffff8000107be430 <nvme_queue_rq+0x2d0>
         :                      dma_map_sg_attrs():
         :                      ents = dma_direct_map_sg(dev, sg, nents, dir, attrs);
    0.00 :   ffff8000107be63c:       mov     x4, #0x100                      // #256
    0.00 :   ffff8000107be640:       bl      ffff800010162040 <dma_direct_map_sg>
    0.00 :   ffff8000107be644:       mov     w27, w0
    0.00 :   ffff8000107be648:       b       ffff8000107be274 <nvme_queue_rq+0x114>
         :                      nvme_queue_rq():
         :                      return BLK_STS_IOERR;
    0.00 :   ffff8000107be64c:       mov     w23, #0xa                       // #10
    0.00 :   ffff8000107be650:       b       ffff8000107be430 <nvme_queue_rq+0x2d0>
         :                      req_bvec():
         :                      return rq->special_vec;
    0.00 :   ffff8000107be654:       ldp     w26, w27, [x28, #112]
    0.00 :   ffff8000107be658:       ldr     x1, [x28, #104]
    0.00 :   ffff8000107be65c:       b       ffff8000107be594 <nvme_queue_rq+0x434>
         :                      nvme_map_data():
         :                      return nvme_setup_prp_simple(dev, req,
    2.93 :   ffff8000107be660:       ldr     x0, [x22, #336]
         :                      nvme_setup_prp_simple():
         :                      iod->first_dma = dma_map_bvec(dev->dev, bv, rq_dma_dir(req), 0);
    0.00 :   ffff8000107be664:       mov     w2, w27
         :                      op_is_write():
    7.66 :   ffff8000107be668:       ldr     w4, [x28, #24]
         :                      nvme_setup_prp_simple():
    0.00 :   ffff8000107be66c:       mov     w3, w26
         :                      dma_map_page_attrs():
         :                      const struct dma_map_ops *ops = get_dma_ops(dev);
    0.42 :   ffff8000107be670:       ldr     x5, [x0, #576]
         :                      nvme_setup_prp_simple():
    0.00 :   ffff8000107be674:       tst     x4, #0x1
    0.00 :   ffff8000107be678:       cset    w4, eq  // eq = none
    0.00 :   ffff8000107be67c:       add     w4, w4, #0x1
         :                      get_dma_ops():
         :                      if (dev->dma_ops)
    7.49 :   ffff8000107be680:       cbz     x5, ffff8000107be6c4 <nvme_queue_rq+0x564>
         :                      dma_map_page_attrs():
         :                      addr = ops->map_page(dev, page, offset, size, dir, attrs);
    0.00 :   ffff8000107be684:       ldr     x7, [x5, #32]
    0.00 :   ffff8000107be688:       mov     x5, #0x0                        // #0
    0.00 :   ffff8000107be68c:       blr     x7
         :                      nvme_setup_prp_simple():
    2.75 :   ffff8000107be690:       str     x0, [x28, #336]
         :                      dma_mapping_error():
         :                      if (dma_addr == DMA_MAPPING_ERROR)
    0.00 :   ffff8000107be694:       cmn     x0, #0x1
    0.18 :   ffff8000107be698:       b.eq    ffff8000107be6e4 <nvme_queue_rq+0x584>  // b.none
         :                      nvme_setup_prp_simple():
         :                      unsigned int offset = bv->bv_offset & (dev->ctrl.page_size - 1);
    1.62 :   ffff8000107be69c:       sub     w1, w24, #0x1
         :                      iod->dma_len = bv->bv_len;
    0.09 :   ffff8000107be6a0:       str     w26, [x28, #344]
         :                      unsigned int offset = bv->bv_offset & (dev->ctrl.page_size - 1);
    0.00 :   ffff8000107be6a4:       and     w27, w1, w27
         :                      cmnd->dptr.prp1 = cpu_to_le64(iod->first_dma);
    0.94 :   ffff8000107be6a8:       str     x0, [x29, #208]
         :                      unsigned int first_prp_len = dev->ctrl.page_size - offset;
    0.00 :   ffff8000107be6ac:       sub     w24, w24, w27
         :                      if (bv->bv_len > first_prp_len)
    0.00 :   ffff8000107be6b0:       cmp     w24, w26
    0.00 :   ffff8000107be6b4:       b.cs    ffff8000107be4d0 <nvme_queue_rq+0x370>  // b.hs, b.nlast
         :                      cmnd->dptr.prp2 = cpu_to_le64(iod->first_dma + first_prp_len);
    0.00 :   ffff8000107be6b8:       add     x24, x0, w24, uxtw
    0.00 :   ffff8000107be6bc:       str     x24, [x29, #216]
    0.00 :   ffff8000107be6c0:       b       ffff8000107be4d0 <nvme_queue_rq+0x370>
         :                      dma_map_page_attrs():
         :                      addr = dma_direct_map_page(dev, page, offset, size, dir, attrs);
    0.00 :   ffff8000107be6c4:       mov     x5, #0x0                        // #0
    0.00 :   ffff8000107be6c8:       bl      ffff800010161f00 <dma_direct_map_page>
    0.00 :   ffff8000107be6cc:       b       ffff8000107be690 <nvme_queue_rq+0x530>
         :                      nvme_queue_rq():
         :                      nvme_unmap_data(dev, req);
    0.00 :   ffff8000107be6d0:       mov     x1, x28
    0.00 :   ffff8000107be6d4:       mov     x0, x22
         :                      nvme_map_metadata():
         :                      return BLK_STS_IOERR;
    0.00 :   ffff8000107be6d8:       mov     w23, #0xa                       // #10
         :                      nvme_queue_rq():
         :                      nvme_unmap_data(dev, req);
    0.00 :   ffff8000107be6dc:       bl      ffff8000107bce88 <nvme_unmap_data>
    0.00 :   ffff8000107be6e0:       b       ffff8000107be628 <nvme_queue_rq+0x4c8>
         :                      nvme_setup_prp_simple():
         :                      return BLK_STS_RESOURCE;
    0.00 :   ffff8000107be6e4:       mov     w23, #0x9                       // #9
    0.00 :   ffff8000107be6e8:       b       ffff8000107be628 <nvme_queue_rq+0x4c8>
         :                      nvme_map_data():
         :                      iod->use_sgl = nvme_pci_use_sgls(dev, req);
    0.00 :   ffff8000107be6ec:       strb    wzr, [x24, #40]
    0.00 :   ffff8000107be6f0:       ubfiz   x4, x1, #5, #16
         :                      blk_rq_payload_bytes():
         :                      if (rq->rq_flags & RQF_SPECIAL_PAYLOAD)
    0.00 :   ffff8000107be6f4:       cbz     w7, ffff8000107bea2c <nvme_queue_rq+0x8cc>
         :                      nvme_pci_setup_prps():
         :                      u32 page_size = dev->ctrl.page_size;
    0.00 :   ffff8000107be6f8:       ldr     w7, [x22, #1720]
         :                      u64 dma_addr = sg_dma_address(sg);
    0.00 :   ffff8000107be6fc:       ldr     x5, [x26, #16]
         :                      int offset = dma_addr & (page_size - 1);
    0.00 :   ffff8000107be700:       sub     w27, w7, #0x1
         :                      int dma_len = sg_dma_len(sg);
    0.00 :   ffff8000107be704:       ldr     w2, [x26, #24]
         :                      blk_rq_payload_bytes():
         :                      return rq->special_vec.bv_len;
    0.00 :   ffff8000107be708:       ldr     w0, [x28, #112]
         :                      nvme_pci_setup_prps():
         :                      int offset = dma_addr & (page_size - 1);
    0.00 :   ffff8000107be70c:       and     w1, w27, w5
    0.00 :   ffff8000107be710:       b       ffff8000107be48c <nvme_queue_rq+0x32c>
         :                      nvme_pci_sgl_set_seg():
         :                      sge->type = NVME_SGL_FMT_SEG_DESC << 4;
    0.00 :   ffff8000107be714:       mov     x5, x0
         :                      sge->length = cpu_to_le32(PAGE_SIZE);
    0.00 :   ffff8000107be718:       mov     w0, #0x1000                     // #4096
         :                      nvme_pci_setup_sgls():
         :                      sg_list[i++] = *link;
    0.00 :   ffff8000107be71c:       mov     w1, w25
         :                      nvme_pci_sgl_set_seg():
         :                      sge->length = cpu_to_le32(PAGE_SIZE);
    0.00 :   ffff8000107be720:       str     w0, [x2, #8]
         :                      sge->type = NVME_SGL_FMT_SEG_DESC << 4;
    0.00 :   ffff8000107be724:       mov     w0, #0x20                       // #32
    0.00 :   ffff8000107be728:       strb    w0, [x2, #15]
    0.00 :   ffff8000107be72c:       b       ffff8000107be360 <nvme_queue_rq+0x200>
         :                      dma_map_page_attrs():
    0.00 :   ffff8000107be730:       mov     x5, #0x0                        // #0
    0.00 :   ffff8000107be734:       bl      ffff800010161f00 <dma_direct_map_page>
    0.00 :   ffff8000107be738:       b       ffff8000107be5f0 <nvme_queue_rq+0x490>
         :                      rq_integrity_vec():
         :                      if (WARN_ON_ONCE(queue_max_integrity_segments(rq->q) > 1))
    0.00 :   ffff8000107be73c:       brk     #0x800
    0.00 :   ffff8000107be740:       ldr     x3, [x28]
         :                      nvme_map_metadata():
         :                      iod->meta_dma = dma_map_bvec(dev->dev, rq_integrity_vec(req),
    0.00 :   ffff8000107be744:       mov     x2, #0x0                        // #0
    0.00 :   ffff8000107be748:       ldr     x1, [x2]
         :                      rq_integrity_vec():
    0.00 :   ffff8000107be74c:       ldrh    w3, [x3, #1130]
    0.00 :   ffff8000107be750:       cmp     w3, #0x1
    0.00 :   ffff8000107be754:       b.ls    ffff8000107bea48 <nvme_queue_rq+0x8e8>  // b.plast
    0.00 :   ffff8000107be758:       brk     #0x800
    0.00 :   ffff8000107be75c:       ldr     x3, [x28]
         :                      nvme_map_metadata():
    0.00 :   ffff8000107be760:       ldr     w2, [x2, #12]
         :                      rq_integrity_vec():
    0.00 :   ffff8000107be764:       ldrh    w3, [x3, #1130]
    0.00 :   ffff8000107be768:       cmp     w3, #0x1
    0.00 :   ffff8000107be76c:       b.ls    ffff8000107bea34 <nvme_queue_rq+0x8d4>  // b.plast
    0.00 :   ffff8000107be770:       brk     #0x800
         :                      return NULL;
    0.00 :   ffff8000107be774:       mov     x5, #0x0                        // #0
    0.00 :   ffff8000107be778:       ldr     w3, [x28, #24]
    0.00 :   ffff8000107be77c:       b       ffff8000107be500 <nvme_queue_rq+0x3a0>
    0.00 :   ffff8000107be780:       mov     w2, w1
    0.00 :   ffff8000107be784:       b       ffff8000107be298 <nvme_queue_rq+0x138>
         :                      dma_map_sg_attrs():
         :                      BUG_ON(ents < 0);
    0.00 :   ffff8000107be788:       brk     #0x800
         :                      nvme_pci_setup_prps():
         :                      iod->first_dma = 0;
    0.00 :   ffff8000107be78c:       mov     x5, #0x0                        // #0
    0.00 :   ffff8000107be790:       str     xzr, [x24, #56]
    0.00 :   ffff8000107be794:       b       ffff8000107be4c4 <nvme_queue_rq+0x364>
         :                      nvme_pci_setup_sgls():
         :                      pool = dev->prp_page_pool;
    0.00 :   ffff8000107be798:       ldr     x0, [x22, #344]
    0.00 :   ffff8000107be79c:       str     x0, [x29, #136]
         :                      iod->npages = 1;
    0.00 :   ffff8000107be7a0:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000107be7a4:       str     w0, [x24, #48]
    0.00 :   ffff8000107be7a8:       b       ffff8000107be300 <nvme_queue_rq+0x1a0>
         :                      nvme_pci_setup_prps():
         :                      nprps = DIV_ROUND_UP(length, page_size);
    0.00 :   ffff8000107be7ac:       add     w0, w27, w25
         :                      pool = dev->prp_small_pool;
    0.00 :   ffff8000107be7b0:       ldp     x1, x2, [x22, #344]
    0.00 :   ffff8000107be7b4:       str     x2, [x29, #136]
         :                      nprps = DIV_ROUND_UP(length, page_size);
    0.00 :   ffff8000107be7b8:       udiv    w0, w0, w7
         :                      if (nprps <= (256 / 8)) {
    0.00 :   ffff8000107be7bc:       cmp     w0, #0x20
    0.00 :   ffff8000107be7c0:       b.gt    ffff8000107be960 <nvme_queue_rq+0x800>
         :                      iod->npages = 0;
    0.00 :   ffff8000107be7c4:       str     wzr, [x24, #48]
         :                      prp_list = dma_pool_alloc(pool, GFP_ATOMIC, &prp_dma);
    0.00 :   ffff8000107be7c8:       ldr     x0, [x29, #136]
    0.00 :   ffff8000107be7cc:       mov     w1, #0xa20                      // #2592
    0.00 :   ffff8000107be7d0:       str     w7, [x29, #104]
    0.00 :   ffff8000107be7d4:       add     x2, x29, #0xa8
    0.00 :   ffff8000107be7d8:       stp     x9, x5, [x29, #112]
    0.00 :   ffff8000107be7dc:       str     w8, [x29, #128]
    0.00 :   ffff8000107be7e0:       str     w3, [x29, #148]
    0.00 :   ffff8000107be7e4:       str     x4, [x29, #152]
    0.00 :   ffff8000107be7e8:       bl      ffff8000102384a8 <dma_pool_alloc>
         :                      if (!prp_list) {
    0.00 :   ffff8000107be7ec:       ldr     w7, [x29, #104]
         :                      prp_list = dma_pool_alloc(pool, GFP_ATOMIC, &prp_dma);
    0.00 :   ffff8000107be7f0:       mov     x27, x0
         :                      if (!prp_list) {
    0.00 :   ffff8000107be7f4:       ldr     w8, [x29, #128]
    0.00 :   ffff8000107be7f8:       ldr     w3, [x29, #148]
    0.00 :   ffff8000107be7fc:       ldp     x9, x5, [x29, #112]
    0.00 :   ffff8000107be800:       ldr     x4, [x29, #152]
    0.00 :   ffff8000107be804:       cbz     x0, ffff8000107bea04 <nvme_queue_rq+0x8a4>
         :                      list[0] = prp_list;
    0.00 :   ffff8000107be808:       str     x0, [x26, x4]
         :                      nvme_pci_iod_list():
         :                      return (void **)(iod->sg + blk_rq_nr_phys_segments(req));
    0.00 :   ffff8000107be80c:       add     x0, x26, x4
         :                      nvme_pci_setup_prps():
         :                      if (i == page_size >> 3) {
    0.00 :   ffff8000107be810:       lsr     w26, w7, #3
         :                      prp_list[0] = old_prp_list[i - 1];
    0.00 :   ffff8000107be814:       mov     x1, #0xfffffffffffffff8         // #-8
         :                      nvme_pci_iod_list():
         :                      return (void **)(iod->sg + blk_rq_nr_phys_segments(req));
    0.00 :   ffff8000107be818:       str     x0, [x29, #104]
         :                      nvme_pci_setup_prps():
         :                      i = 0;
    0.00 :   ffff8000107be81c:       mov     w25, #0x0                       // #0
         :                      iod->first_dma = prp_dma;
    0.00 :   ffff8000107be820:       ldr     x0, [x29, #168]
    0.00 :   ffff8000107be824:       str     x0, [x24, #56]
         :                      prp_list[0] = old_prp_list[i - 1];
    0.00 :   ffff8000107be828:       add     x0, x1, w26, sxtw #3
    0.00 :   ffff8000107be82c:       str     x0, [x29, #96]
         :                      if (i == page_size >> 3) {
    0.00 :   ffff8000107be830:       cmp     w26, w25
    0.00 :   ffff8000107be834:       b.eq    ffff8000107be884 <nvme_queue_rq+0x724>  // b.none
    0.00 :   ffff8000107be838:       sbfiz   x2, x25, #3, #32
    0.00 :   ffff8000107be83c:       add     w25, w25, #0x1
         :                      prp_list[i++] = cpu_to_le64(dma_addr);
    0.00 :   ffff8000107be840:       str     x5, [x27, x2]
    0.00 :   ffff8000107be844:       sub     w8, w8, w7
         :                      dma_len -= page_size;
    0.00 :   ffff8000107be848:       sub     w3, w3, w7
         :                      if (length <= 0)
    0.00 :   ffff8000107be84c:       cmp     w8, #0x0
    0.00 :   ffff8000107be850:       b.le    ffff8000107be97c <nvme_queue_rq+0x81c>
         :                      if (dma_len > 0)
    0.00 :   ffff8000107be854:       cmp     w3, #0x0
    0.00 :   ffff8000107be858:       b.gt    ffff8000107be8e8 <nvme_queue_rq+0x788>
    0.00 :   ffff8000107be85c:       stp     w7, w8, [x29, #148]
         :                      if (unlikely(dma_len < 0))
    0.00 :   ffff8000107be860:       b.ne    ffff8000107be970 <nvme_queue_rq+0x810>  // b.any
         :                      sg = sg_next(sg);
    0.00 :   ffff8000107be864:       mov     x0, x9
    0.00 :   ffff8000107be868:       bl      ffff800010480bb8 <sg_next>
    0.00 :   ffff8000107be86c:       ldp     w7, w8, [x29, #148]
    0.00 :   ffff8000107be870:       mov     x9, x0
         :                      dma_len = sg_dma_len(sg);
    0.00 :   ffff8000107be874:       ldr     w3, [x0, #24]
         :                      if (i == page_size >> 3) {
    0.00 :   ffff8000107be878:       cmp     w26, w25
         :                      dma_addr = sg_dma_address(sg);
    0.00 :   ffff8000107be87c:       ldr     x5, [x0, #16]
         :                      if (i == page_size >> 3) {
    0.00 :   ffff8000107be880:       b.ne    ffff8000107be838 <nvme_queue_rq+0x6d8>  // b.any
         :                      prp_list = dma_pool_alloc(pool, GFP_ATOMIC, &prp_dma);
    0.00 :   ffff8000107be884:       ldr     x0, [x29, #136]
    0.00 :   ffff8000107be888:       add     x2, x29, #0xa8
    0.00 :   ffff8000107be88c:       mov     w1, #0xa20                      // #2592
    0.00 :   ffff8000107be890:       str     w7, [x29, #112]
    0.00 :   ffff8000107be894:       stp     x9, x5, [x29, #120]
    0.00 :   ffff8000107be898:       stp     w8, w3, [x29, #148]
    0.00 :   ffff8000107be89c:       bl      ffff8000102384a8 <dma_pool_alloc>
         :                      if (!prp_list)
    0.00 :   ffff8000107be8a0:       cbz     x0, ffff8000107be94c <nvme_queue_rq+0x7ec>
         :                      list[iod->npages++] = prp_list;
    0.00 :   ffff8000107be8a4:       ldr     w4, [x24, #48]
         :                      old_prp_list[i - 1] = cpu_to_le64(prp_dma);
    0.00 :   ffff8000107be8a8:       mov     x2, #0x8                        // #8
         :                      list[iod->npages++] = prp_list;
    0.00 :   ffff8000107be8ac:       ldr     x1, [x29, #104]
         :                      old_prp_list[i - 1] = cpu_to_le64(prp_dma);
    0.00 :   ffff8000107be8b0:       mov     w25, #0x2                       // #2
         :                      list[iod->npages++] = prp_list;
    0.00 :   ffff8000107be8b4:       add     w11, w4, #0x1
    0.00 :   ffff8000107be8b8:       str     w11, [x24, #48]
         :                      old_prp_list[i - 1] = cpu_to_le64(prp_dma);
    0.00 :   ffff8000107be8bc:       ldr     w7, [x29, #112]
         :                      list[iod->npages++] = prp_list;
    0.00 :   ffff8000107be8c0:       str     x0, [x1, w4, sxtw #3]
         :                      prp_list[0] = old_prp_list[i - 1];
    0.00 :   ffff8000107be8c4:       ldr     x1, [x29, #96]
         :                      old_prp_list[i - 1] = cpu_to_le64(prp_dma);
    0.00 :   ffff8000107be8c8:       ldp     w8, w3, [x29, #148]
         :                      prp_list[0] = old_prp_list[i - 1];
    0.00 :   ffff8000107be8cc:       ldr     x4, [x27, x1]
    0.00 :   ffff8000107be8d0:       str     x4, [x0]
         :                      old_prp_list[i - 1] = cpu_to_le64(prp_dma);
    0.00 :   ffff8000107be8d4:       ldp     x9, x5, [x29, #120]
    0.00 :   ffff8000107be8d8:       ldr     x4, [x29, #168]
    0.00 :   ffff8000107be8dc:       str     x4, [x27, x1]
    0.00 :   ffff8000107be8e0:       mov     x27, x0
    0.00 :   ffff8000107be8e4:       b       ffff8000107be840 <nvme_queue_rq+0x6e0>
         :                      dma_addr += page_size;
    0.00 :   ffff8000107be8e8:       add     x5, x5, w7, uxtw
    0.00 :   ffff8000107be8ec:       b       ffff8000107be830 <nvme_queue_rq+0x6d0>
         :                      nvme_pci_setup_sgls():
         :                      nvme_pci_sgl_set_data(&cmd->dptr.sgl, sg);
    0.00 :   ffff8000107be8f0:       ldr     x0, [x26, #16]
         :                      nvme_pci_sgl_set_data():
         :                      sge->addr = cpu_to_le64(sg_dma_address(sg));
    0.00 :   ffff8000107be8f4:       str     x0, [x29, #208]
         :                      sge->length = cpu_to_le32(sg_dma_len(sg));
    0.00 :   ffff8000107be8f8:       ldr     w0, [x26, #24]
    0.00 :   ffff8000107be8fc:       str     w0, [x29, #216]
         :                      sge->type = NVME_SGL_FMT_DATA_DESC << 4;
    0.00 :   ffff8000107be900:       strb    wzr, [x29, #223]
    0.00 :   ffff8000107be904:       b       ffff8000107be4d0 <nvme_queue_rq+0x370>
         :                      nvme_pci_sgl_set_seg():
         :                      sge->length = cpu_to_le32(PAGE_SIZE);
    0.00 :   ffff8000107be908:       mov     w1, #0x1000                     // #4096
         :                      sge->type = NVME_SGL_FMT_SEG_DESC << 4;
    0.00 :   ffff8000107be90c:       mov     w0, #0x20                       // #32
         :                      sge->length = cpu_to_le32(PAGE_SIZE);
    0.00 :   ffff8000107be910:       str     w1, [x29, #216]
         :                      sge->type = NVME_SGL_FMT_SEG_DESC << 4;
    0.00 :   ffff8000107be914:       strb    w0, [x29, #223]
    0.00 :   ffff8000107be918:       b       ffff8000107be358 <nvme_queue_rq+0x1f8>
    0.00 :   ffff8000107be91c:       str     w7, [x29, #136]
         :                      nvme_pci_setup_prps():
         :                      sg = sg_next(sg);
    0.00 :   ffff8000107be920:       mov     x0, x26
    0.00 :   ffff8000107be924:       str     w25, [x29, #148]
    0.00 :   ffff8000107be928:       str     x4, [x29, #152]
    0.00 :   ffff8000107be92c:       bl      ffff800010480bb8 <sg_next>
         :                      dma_len = sg_dma_len(sg);
    0.00 :   ffff8000107be930:       ldr     w7, [x29, #136]
         :                      sg = sg_next(sg);
    0.00 :   ffff8000107be934:       mov     x9, x0
         :                      dma_len = sg_dma_len(sg);
    0.00 :   ffff8000107be938:       ldr     w3, [x0, #24]
    0.00 :   ffff8000107be93c:       ldr     w8, [x29, #148]
         :                      dma_addr = sg_dma_address(sg);
    0.00 :   ffff8000107be940:       ldr     x5, [x0, #16]
         :                      dma_len = sg_dma_len(sg);
    0.00 :   ffff8000107be944:       ldr     x4, [x29, #152]
    0.00 :   ffff8000107be948:       b       ffff8000107be4b4 <nvme_queue_rq+0x354>
         :                      return BLK_STS_RESOURCE;
    0.00 :   ffff8000107be94c:       mov     w23, #0x9                       // #9
         :                      nvme_map_data():
         :                      nvme_unmap_data(dev, req);
    0.00 :   ffff8000107be950:       mov     x1, x28
    0.00 :   ffff8000107be954:       mov     x0, x22
    0.00 :   ffff8000107be958:       bl      ffff8000107bce88 <nvme_unmap_data>
    0.00 :   ffff8000107be95c:       b       ffff8000107be628 <nvme_queue_rq+0x4c8>
         :                      nvme_pci_setup_prps():
         :                      iod->npages = 1;
    0.00 :   ffff8000107be960:       mov     w0, #0x1                        // #1
         :                      pool = dev->prp_page_pool;
    0.00 :   ffff8000107be964:       str     x1, [x29, #136]
         :                      iod->npages = 1;
    0.00 :   ffff8000107be968:       str     w0, [x24, #48]
    0.00 :   ffff8000107be96c:       b       ffff8000107be7c8 <nvme_queue_rq+0x668>
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000107be970:       b       ffff8000107be998 <nvme_queue_rq+0x838>
         :                      nvme_pci_setup_prps():
    0.00 :   ffff8000107be974:       mov     w23, #0xa                       // #10
    0.00 :   ffff8000107be978:       b       ffff8000107be950 <nvme_queue_rq+0x7f0>
    0.00 :   ffff8000107be97c:       ldr     x5, [x24, #56]
    0.00 :   ffff8000107be980:       ldr     x26, [x24, #80]
    0.00 :   ffff8000107be984:       b       ffff8000107be4c4 <nvme_queue_rq+0x364>
         :                      nvme_pci_setup_sgls():
         :                      iod->npages = -1;
    0.00 :   ffff8000107be988:       mov     w0, #0xffffffff                 // #-1
    0.00 :   ffff8000107be98c:       mov     w23, #0x9                       // #9
    0.00 :   ffff8000107be990:       str     w0, [x24, #48]
    0.00 :   ffff8000107be994:       b       ffff8000107be950 <nvme_queue_rq+0x7f0>
         :                      nvme_pci_setup_prps():
         :                      WARN(DO_ONCE(nvme_print_sgl, iod->sg, iod->nents),
    0.00 :   ffff8000107be998:       adrp    x20, ffff800011ad9000 <megasas_mgmt_info+0x15b0>
    0.00 :   ffff8000107be99c:       add     x20, x20, #0xae8
    0.00 :   ffff8000107be9a0:       add     x20, x20, #0x10
    0.00 :   ffff8000107be9a4:       add     x1, x29, #0xb0
    0.00 :   ffff8000107be9a8:       mov     x0, x20
    0.00 :   ffff8000107be9ac:       bl      ffff80001048b900 <__do_once_start>
    0.00 :   ffff8000107be9b0:       tst     w0, #0xff
    0.00 :   ffff8000107be9b4:       b.eq    ffff8000107be974 <nvme_queue_rq+0x814>  // b.none
    0.00 :   ffff8000107be9b8:       ldr     w1, [x24, #52]
    0.00 :   ffff8000107be9bc:       ldr     x0, [x24, #80]
    0.00 :   ffff8000107be9c0:       bl      ffff8000107bca68 <nvme_print_sgl>
    0.00 :   ffff8000107be9c4:       adrp    x1, ffff800011a1a000 <hisi_sas_v1_driver+0x28>
    0.00 :   ffff8000107be9c8:       add     x1, x1, #0xc28
    0.00 :   ffff8000107be9cc:       mov     x0, x20
    0.00 :   ffff8000107be9d0:       add     x2, x29, #0xb0
    0.00 :   ffff8000107be9d4:       add     x1, x1, #0x110
    0.00 :   ffff8000107be9d8:       bl      ffff80001048b968 <__do_once_done>
         :                      blk_rq_payload_bytes():
         :                      if (rq->rq_flags & RQF_SPECIAL_PAYLOAD)
    0.00 :   ffff8000107be9dc:       ldr     w0, [x28, #28]
    0.00 :   ffff8000107be9e0:       tbz     w0, #18, ffff8000107bea24 <nvme_queue_rq+0x8c4>
         :                      return rq->special_vec.bv_len;
    0.00 :   ffff8000107be9e4:       ldr     w1, [x28, #112]
         :                      nvme_pci_setup_prps():
    0.00 :   ffff8000107be9e8:       ldr     w2, [x24, #52]
    0.00 :   ffff8000107be9ec:       adrp    x0, ffff80001123e000 <kallsyms_token_index+0xc4330>
    0.00 :   ffff8000107be9f0:       add     x0, x0, #0xd60
    0.00 :   ffff8000107be9f4:       bl      ffff8000100e5288 <__warn_printk>
    0.00 :   ffff8000107be9f8:       brk     #0x800
    0.00 :   ffff8000107be9fc:       mov     w23, #0xa                       // #10
    0.00 :   ffff8000107bea00:       b       ffff8000107be950 <nvme_queue_rq+0x7f0>
         :                      iod->npages = -1;
    0.00 :   ffff8000107bea04:       mov     w0, #0xffffffff                 // #-1
    0.00 :   ffff8000107bea08:       mov     w23, #0x9                       // #9
    0.00 :   ffff8000107bea0c:       str     w0, [x24, #48]
         :                      iod->first_dma = dma_addr;
    0.00 :   ffff8000107bea10:       str     x5, [x24, #56]
    0.00 :   ffff8000107bea14:       b       ffff8000107be950 <nvme_queue_rq+0x7f0>
    0.00 :   ffff8000107bea18:       stp     x24, x25, [x29, #56]
    0.00 :   ffff8000107bea1c:       stp     x26, x27, [x29, #72]
         :                      nvme_queue_rq():
         :                      }
    0.00 :   ffff8000107bea20:       bl      ffff8000100e5630 <__stack_chk_fail>
         :                      blk_rq_payload_bytes():
         :                      return blk_rq_bytes(rq);
    0.00 :   ffff8000107bea24:       ldr     w1, [x28, #40]
    0.00 :   ffff8000107bea28:       b       ffff8000107be9e8 <nvme_queue_rq+0x888>
    0.00 :   ffff8000107bea2c:       ldr     w0, [x28, #40]
    0.00 :   ffff8000107bea30:       b       ffff8000107be474 <nvme_queue_rq+0x314>
    0.00 :   ffff8000107bea34:       ldr     x4, [x28, #56]
    0.00 :   ffff8000107bea38:       ldr     w3, [x28, #24]
    0.00 :   ffff8000107bea3c:       ldr     x4, [x4, #88]
    0.00 :   ffff8000107bea40:       ldr     x5, [x4, #96]
    0.00 :   ffff8000107bea44:       b       ffff8000107be500 <nvme_queue_rq+0x3a0>
    0.00 :   ffff8000107bea48:       ldr     x2, [x28, #56]
    0.00 :   ffff8000107bea4c:       ldr     w3, [x28, #24]
    0.00 :   ffff8000107bea50:       ldr     x2, [x2, #88]
    0.00 :   ffff8000107bea54:       ldr     x5, [x2, #96]
    0.00 :   ffff8000107bea58:       b       ffff8000107be4fc <nvme_queue_rq+0x39c>
 Percent |	Source code & Disassembly of vmlinux for cycles (618 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001044ffc0 <bio_free>:
         :                      bio_free():
         :                      bio_integrity_free(bio);
         :                      }
         :                      EXPORT_SYMBOL(bio_uninit);
         :
         :                      static void bio_free(struct bio *bio)
         :                      {
    2.94 :   ffff80001044ffc0:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff80001044ffc4:       mov     x29, sp
    6.79 :   ffff80001044ffc8:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001044ffcc:       mov     x19, x0
         :                      struct bio_set *bs = bio->bi_pool;
   25.42 :   ffff80001044ffd0:       ldr     x20, [x0, #112]
         :                      void *p;
         :
         :                      bio_uninit(bio);
    0.00 :   ffff80001044ffd4:       bl      ffff80001044fa80 <bio_uninit>
         :
         :                      if (bs) {
    1.45 :   ffff80001044ffd8:       cbz     x20, ffff80001045000c <bio_free+0x4c>
         :                      bvec_free(&bs->bvec_pool, bio->bi_io_vec, BVEC_POOL_IDX(bio));
   24.42 :   ffff80001044ffdc:       ldr     x1, [x19, #104]
    0.49 :   ffff80001044ffe0:       add     x0, x20, #0x58
    5.76 :   ffff80001044ffe4:       ldrh    w2, [x19, #20]
    0.00 :   ffff80001044ffe8:       lsr     w2, w2, #13
    0.00 :   ffff80001044ffec:       bl      ffff80001044ff60 <bvec_free>
         :
         :                      /*
         :                      * If we have front padding, adjust the bio pointer before freeing
         :                      */
         :                      p = bio;
         :                      p -= bs->front_pad;
   26.89 :   ffff80001044fff0:       ldr     w0, [x20, #8]
         :
         :                      mempool_free(p, &bs->bio_pool);
    0.00 :   ffff80001044fff4:       add     x1, x20, #0x10
    0.00 :   ffff80001044fff8:       sub     x0, x19, x0
    0.00 :   ffff80001044fffc:       bl      ffff8000101d5b40 <mempool_free>
         :                      } else {
         :                      /* Bio was allocated by bio_kmalloc() */
         :                      kfree(bio);
         :                      }
         :                      }
    0.33 :   ffff800010450000:       ldp     x19, x20, [sp, #16]
    5.36 :   ffff800010450004:       ldp     x29, x30, [sp], #32
    0.16 :   ffff800010450008:       ret
         :                      kfree(bio);
    0.00 :   ffff80001045000c:       mov     x0, x19
    0.00 :   ffff800010450010:       bl      ffff80001024fe88 <kfree>
         :                      }
    0.00 :   ffff800010450014:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010450018:       ldp     x29, x30, [sp], #32
    0.00 :   ffff80001045001c:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (781 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010081000 <__do_softirq>:
         :                      __do_softirq():
         :                      static inline bool lockdep_softirq_start(void) { return false; }
         :                      static inline void lockdep_softirq_end(bool in_hardirq) { }
         :                      #endif
         :
         :                      asmlinkage __visible void __softirq_entry __do_softirq(void)
         :                      {
    0.00 :   ffff800010081000:       stp     x29, x30, [sp, #-144]!
         :                      unsigned long end = jiffies + MAX_SOFTIRQ_TIME;
    0.00 :   ffff800010081004:       adrp    x1, ffff800011897000 <bit_wait_table+0xe80>
         :                      {
    0.00 :   ffff800010081008:       mov     x29, sp
    0.00 :   ffff80001008100c:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010081010:       stp     x21, x22, [sp, #32]
         :                      * softirq. A softirq handled, such as network RX, might set PF_MEMALLOC
         :                      * again if the socket is related to swapping.
         :                      */
         :                      current->flags &= ~PF_MEMALLOC;
         :
         :                      pending = local_softirq_pending();
    0.00 :   ffff800010081014:       adrp    x21, ffff8000114d5000 <tegra_to+0x180>
         :                      {
    0.00 :   ffff800010081018:       stp     x23, x24, [sp, #48]
         :                      pending = local_softirq_pending();
    0.00 :   ffff80001008101c:       add     x21, x21, #0x6c0
         :                      {
    0.00 :   ffff800010081020:       stp     x25, x26, [sp, #64]
    0.00 :   ffff800010081024:       stp     x27, x28, [sp, #80]
         :                      unsigned long end = jiffies + MAX_SOFTIRQ_TIME;
    0.00 :   ffff800010081028:       ldr     x24, [x1, #2432]
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80001008102c:       mrs     x19, sp_el0
         :                      __do_softirq():
         :                      unsigned long old_flags = current->flags;
    0.00 :   ffff800010081030:       ldr     w0, [x19, #44]
         :                      unsigned long end = jiffies + MAX_SOFTIRQ_TIME;
    0.00 :   ffff800010081034:       add     x2, x24, #0x1
         :                      unsigned long old_flags = current->flags;
    0.00 :   ffff800010081038:       str     w0, [x29, #128]
         :                      current->flags &= ~PF_MEMALLOC;
    0.00 :   ffff80001008103c:       and     w0, w0, #0xfffff7ff
    0.00 :   ffff800010081040:       str     w0, [x19, #44]
         :                      unsigned long end = jiffies + MAX_SOFTIRQ_TIME;
    0.00 :   ffff800010081044:       str     x2, [x29, #112]
         :                      pending = local_softirq_pending();
    0.00 :   ffff800010081048:       mov     x0, x21
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff80001008104c:       mrs     x1, tpidr_el1
         :                      __do_softirq():
    0.00 :   ffff800010081050:       ldr     w24, [x0, x1]
         :                      account_irq_enter_time():
         :                      #endif
         :
         :                      static inline void account_irq_enter_time(struct task_struct *tsk)
         :                      {
         :                      vtime_account_irq_enter(tsk);
         :                      irqtime_account_irq(tsk);
    0.00 :   ffff800010081054:       mov     x0, x19
    0.00 :   ffff800010081058:       bl      ffff800010119da8 <irqtime_account_irq>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001008105c:       ldr     w0, [x19, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff800010081060:       add     w0, w0, #0x100
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010081064:       str     w0, [x19, #16]
         :                      __do_softirq():
         :
         :                      trace_softirq_entry(vec_nr);
         :                      h->action(h);
         :                      trace_softirq_exit(vec_nr);
         :                      if (unlikely(prev_count != preempt_count())) {
         :                      pr_err("huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\n",
    0.00 :   ffff800010081068:       adrp    x26, ffff800010ce5000 <dummy_vm_ops.29358+0x70>
    0.00 :   ffff80001008106c:       add     x0, x26, #0xb50
    0.00 :   ffff800010081070:       str     x0, [x29, #104]
         :                      __local_bh_disable_ip():
         :                      extern void __local_bh_disable_ip(unsigned long ip, unsigned int cnt);
         :                      #else
         :                      static __always_inline void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
         :                      {
         :                      preempt_count_add(cnt);
         :                      barrier();
    0.00 :   ffff800010081074:       mov     w0, #0xa                        // #10
    0.00 :   ffff800010081078:       str     w0, [x29, #132]
         :                      __do_softirq():
         :                      set_softirq_pending(0);
    0.00 :   ffff80001008107c:       str     x21, [x29, #136]
         :                      __my_cpu_offset():
    0.00 :   ffff800010081080:       mrs     x1, tpidr_el1
         :                      __do_softirq():
    0.00 :   ffff800010081084:       ldr     x0, [x29, #136]
    0.00 :   ffff800010081088:       str     wzr, [x0, x1]
         :                      arch_local_irq_enable():
         :                      u32 pmr = read_sysreg_s(SYS_ICC_PMR_EL1);
         :
         :                      WARN_ON_ONCE(pmr != GIC_PRIO_IRQON && pmr != GIC_PRIO_IRQOFF);
         :                      }
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001008108c:       mov     x0, #0xe0                       // #224
    0.00 :   ffff800010081090:       msr     daifclr, #0x2
         :                      ffs():
         :                      * the libc and compiler builtin ffs routines, therefore
         :                      * differs in spirit from the above ffz (man ffs).
         :                      */
         :                      static __always_inline int ffs(int x)
         :                      {
         :                      return __builtin_ffs(x);
   98.16 :   ffff800010081094:       rbit    w27, w24
    0.00 :   ffff800010081098:       cmp     w24, #0x0
    0.00 :   ffff80001008109c:       clz     w27, w27
    0.00 :   ffff8000100810a0:       csinc   w27, wzr, w27, eq  // eq = none
         :                      __do_softirq():
         :                      while ((softirq_bit = ffs(pending))) {
    0.00 :   ffff8000100810a4:       cbz     w27, ffff800010081130 <__do_softirq+0x130>
    0.00 :   ffff8000100810a8:       adrp    x19, ffff800011896000 <boot_args>
    0.00 :   ffff8000100810ac:       add     x19, x19, #0xc0
    0.00 :   ffff8000100810b0:       adrp    x20, ffff8000114cc000 <kernel_cpustat+0x48>
         :                      h = softirq_vec;
    0.00 :   ffff8000100810b4:       mov     x26, x19
         :                      kstat_incr_softirqs_this_cpu():
         :                      extern unsigned int kstat_irqs_cpu(unsigned int irq, int cpu);
         :                      extern void kstat_incr_irq_this_cpu(unsigned int irq);
         :
         :                      static inline void kstat_incr_softirqs_this_cpu(unsigned int irq)
         :                      {
         :                      __this_cpu_inc(kstat.softirqs[irq]);
    0.13 :   ffff8000100810b8:       add     x20, x20, #0x8
         :                      __do_softirq():
         :                      pr_err("huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\n",
    0.00 :   ffff8000100810bc:       adrp    x22, ffff800011184000 <kallsyms_token_index+0xa330>
    0.00 :   ffff8000100810c0:       add     x0, x22, #0x130
    0.00 :   ffff8000100810c4:       str     x0, [x29, #120]
         :                      h += softirq_bit - 1;
    0.26 :   ffff8000100810c8:       sbfiz   x28, x27, #3, #32
         :                      vec_nr, softirq_to_name[vec_nr], h->action,
         :                      prev_count, preempt_count());
         :                      preempt_count_set(prev_count);
         :                      }
         :                      h++;
         :                      pending >>= softirq_bit;
    0.00 :   ffff8000100810cc:       lsr     w24, w24, w27
         :                      h += softirq_bit - 1;
    0.00 :   ffff8000100810d0:       sub     x21, x28, #0x8
         :                      ffs():
    0.13 :   ffff8000100810d4:       rbit    w27, w24
         :                      __do_softirq():
    0.00 :   ffff8000100810d8:       add     x0, x26, x21
         :                      ffs():
    0.00 :   ffff8000100810dc:       clz     w27, w27
         :                      __do_softirq():
         :                      vec_nr = h - softirq_vec;
    0.00 :   ffff8000100810e0:       sub     x1, x0, x19
         :                      __my_cpu_offset():
    0.00 :   ffff8000100810e4:       mrs     x5, tpidr_el1
         :                      __do_softirq():
    0.00 :   ffff8000100810e8:       asr     x23, x1, #3
         :                      get_current():
    0.00 :   ffff8000100810ec:       mrs     x22, sp_el0
         :                      kstat_incr_softirqs_this_cpu():
    0.14 :   ffff8000100810f0:       ubfiz   x2, x23, #2, #32
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000100810f4:       ldr     w25, [x22, #16]
         :                      kstat_incr_softirqs_this_cpu():
    0.00 :   ffff8000100810f8:       add     x2, x2, #0x8
    0.00 :   ffff8000100810fc:       add     x2, x2, x20
    0.52 :   ffff800010081100:       ldr     w3, [x2, x5]
    0.00 :   ffff800010081104:       add     w3, w3, #0x1
    0.00 :   ffff800010081108:       str     w3, [x2, x5]
         :                      __do_softirq():
         :                      h->action(h);
    0.00 :   ffff80001008110c:       ldr     x2, [x26, x21]
    0.00 :   ffff800010081110:       blr     x2
         :                      __read_once_size():
    0.00 :   ffff800010081114:       ldr     w0, [x22, #16]
         :                      __do_softirq():
         :                      if (unlikely(prev_count != preempt_count())) {
    0.00 :   ffff800010081118:       cmp     w0, w25
    0.00 :   ffff80001008111c:       b.ne    ffff8000100811fc <__do_softirq+0x1fc>  // b.any
         :                      ffs():
    0.40 :   ffff800010081120:       cmp     w24, #0x0
         :                      __do_softirq():
         :                      h++;
    0.00 :   ffff800010081124:       add     x26, x26, x28
         :                      ffs():
    0.00 :   ffff800010081128:       csinc   w27, wzr, w27, eq  // eq = none
         :                      __do_softirq():
         :                      while ((softirq_bit = ffs(pending))) {
    0.00 :   ffff80001008112c:       cbnz    w27, ffff8000100810c8 <__do_softirq+0xc8>
         :                      }
         :
         :                      if (__this_cpu_read(ksoftirqd) == current)
    0.00 :   ffff800010081130:       adrp    x0, ffff8000114cb000 <overflow_stack+0xd60>
    0.00 :   ffff800010081134:       add     x0, x0, #0xf78
         :                      __my_cpu_offset():
    0.26 :   ffff800010081138:       mrs     x1, tpidr_el1
         :                      __do_softirq():
    0.00 :   ffff80001008113c:       ldr     x1, [x0, x1]
         :                      get_current():
    0.00 :   ffff800010081140:       mrs     x0, sp_el0
         :                      __do_softirq():
    0.00 :   ffff800010081144:       cmp     x1, x0
    0.00 :   ffff800010081148:       b.eq    ffff800010081224 <__do_softirq+0x224>  // b.none
         :                      arch_local_irq_disable():
         :                      u32 pmr = read_sysreg_s(SYS_ICC_PMR_EL1);
         :
         :                      WARN_ON_ONCE(pmr != GIC_PRIO_IRQON && pmr != GIC_PRIO_IRQOFF);
         :                      }
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001008114c:       mov     x0, #0x60                       // #96
    0.00 :   ffff800010081150:       msr     daifset, #0x2
         :                      __my_cpu_offset():
    0.00 :   ffff800010081154:       mrs     x1, tpidr_el1
         :                      __do_softirq():
         :                      rcu_softirq_qs();
         :                      local_irq_disable();
         :
         :                      pending = local_softirq_pending();
    0.00 :   ffff800010081158:       ldr     x0, [x29, #136]
    0.00 :   ffff80001008115c:       ldr     w24, [x0, x1]
         :                      if (pending) {
    0.00 :   ffff800010081160:       cbz     w24, ffff800010081198 <__do_softirq+0x198>
         :                      if (time_before(jiffies, end) && !need_resched() &&
    0.00 :   ffff800010081164:       adrp    x0, ffff800011897000 <bit_wait_table+0xe80>
    0.00 :   ffff800010081168:       ldr     x1, [x29, #112]
    0.00 :   ffff80001008116c:       ldr     x0, [x0, #2432]
    0.00 :   ffff800010081170:       sub     x0, x0, x1
    0.00 :   ffff800010081174:       tbz     x0, #63, ffff800010081194 <__do_softirq+0x194>
         :                      get_current():
    0.00 :   ffff800010081178:       mrs     x0, sp_el0
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff80001008117c:       ldr     x0, [x0]
         :                      __do_softirq():
    0.00 :   ffff800010081180:       tbnz    w0, #1, ffff800010081194 <__do_softirq+0x194>
    0.00 :   ffff800010081184:       ldr     w0, [x29, #132]
    0.00 :   ffff800010081188:       subs    w0, w0, #0x1
    0.00 :   ffff80001008118c:       str     w0, [x29, #132]
    0.00 :   ffff800010081190:       b.ne    ffff800010081080 <__do_softirq+0x80>  // b.any
         :                      --max_restart)
         :                      goto restart;
         :
         :                      wakeup_softirqd();
    0.00 :   ffff800010081194:       bl      ffff8000100eb6d8 <wakeup_softirqd>
         :                      get_current():
    0.00 :   ffff800010081198:       mrs     x19, sp_el0
         :                      account_irq_exit_time():
         :                      }
         :
         :                      static inline void account_irq_exit_time(struct task_struct *tsk)
         :                      {
         :                      vtime_account_irq_exit(tsk);
         :                      irqtime_account_irq(tsk);
    0.00 :   ffff80001008119c:       mov     x0, x19
    0.00 :   ffff8000100811a0:       bl      ffff800010119da8 <irqtime_account_irq>
         :                      __read_once_size():
    0.00 :   ffff8000100811a4:       ldr     w0, [x19, #16]
    0.00 :   ffff8000100811a8:       ldr     w0, [x19, #16]
    0.00 :   ffff8000100811ac:       ldr     w0, [x19, #16]
         :                      __preempt_count_sub():
         :                      }
         :
         :                      static inline void __preempt_count_sub(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc -= val;
    0.00 :   ffff8000100811b0:       sub     w0, w0, #0x100
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000100811b4:       str     w0, [x19, #16]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000100811b8:       ldr     w0, [x19, #16]
         :                      __do_softirq():
         :                      }
         :
         :                      lockdep_softirq_end(in_hardirq);
         :                      account_irq_exit_time(current);
         :                      __local_bh_enable(SOFTIRQ_OFFSET);
         :                      WARN_ON_ONCE(in_interrupt());
    0.00 :   ffff8000100811bc:       tst     w0, #0x1fff00
    0.00 :   ffff8000100811c0:       b.ne    ffff80001008122c <__do_softirq+0x22c>  // b.any
         :                      current_restore_flags():
         :
         :                      static inline void
         :                      current_restore_flags(unsigned long orig_flags, unsigned long flags)
         :                      {
         :                      current->flags &= ~flags;
         :                      current->flags |= orig_flags & flags;
    0.00 :   ffff8000100811c4:       ldr     w1, [x29, #128]
         :                      get_current():
    0.00 :   ffff8000100811c8:       mrs     x2, sp_el0
         :                      current_restore_flags():
         :                      current->flags &= ~flags;
    0.00 :   ffff8000100811cc:       ldr     w0, [x2, #44]
         :                      current->flags |= orig_flags & flags;
    0.00 :   ffff8000100811d0:       and     w1, w1, #0x800
         :                      current->flags &= ~flags;
    0.00 :   ffff8000100811d4:       and     w0, w0, #0xfffff7ff
         :                      current->flags |= orig_flags & flags;
    0.00 :   ffff8000100811d8:       orr     w0, w0, w1
    0.00 :   ffff8000100811dc:       str     w0, [x2, #44]
         :                      __do_softirq():
         :                      current_restore_flags(old_flags, PF_MEMALLOC);
         :                      }
    0.00 :   ffff8000100811e0:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000100811e4:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000100811e8:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff8000100811ec:       ldp     x25, x26, [sp, #64]
    0.00 :   ffff8000100811f0:       ldp     x27, x28, [sp, #80]
    0.00 :   ffff8000100811f4:       ldp     x29, x30, [sp], #144
    0.00 :   ffff8000100811f8:       ret
         :                      pr_err("huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\n",
    0.00 :   ffff8000100811fc:       ldr     x0, [x29, #104]
    0.00 :   ffff800010081200:       mov     w4, w25
    0.00 :   ffff800010081204:       ldr     x3, [x26, x21]
    0.00 :   ffff800010081208:       mov     w1, w23
         :                      __read_once_size():
    0.00 :   ffff80001008120c:       ldr     w5, [x22, #16]
         :                      __do_softirq():
    0.00 :   ffff800010081210:       ldr     x2, [x0, w23, uxtw #3]
    0.00 :   ffff800010081214:       ldr     x0, [x29, #120]
    0.00 :   ffff800010081218:       bl      ffff800010148e94 <printk>
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001008121c:       str     w25, [x22, #16]
    0.00 :   ffff800010081220:       b       ffff800010081120 <__do_softirq+0x120>
         :                      __do_softirq():
         :                      rcu_softirq_qs();
    0.00 :   ffff800010081224:       bl      ffff80001015f070 <rcu_softirq_qs>
    0.00 :   ffff800010081228:       b       ffff80001008114c <__do_softirq+0x14c>
         :                      WARN_ON_ONCE(in_interrupt());
    0.00 :   ffff80001008122c:       brk     #0x800
    0.00 :   ffff800010081230:       b       ffff8000100811c4 <__do_softirq+0x1c4>
 Percent |	Source code & Disassembly of vmlinux for cycles (566 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010463e98 <blk_mq_put_tag>:
         :                      blk_mq_put_tag():
         :                      return tag + tag_offset;
         :                      }
         :
         :                      void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_tags *tags,
         :                      struct blk_mq_ctx *ctx, unsigned int tag)
         :                      {
   48.75 :   ffff800010463e98:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff800010463e9c:       mov     x0, x1
    4.11 :   ffff800010463ea0:       mov     x29, sp
         :                      if (!blk_mq_tag_is_reserved(tags, tag)) {
    0.36 :   ffff800010463ea4:       ldr     w1, [x1, #4]
    0.00 :   ffff800010463ea8:       cmp     w3, w1
    0.00 :   ffff800010463eac:       b.cc    ffff800010463ed4 <blk_mq_put_tag+0x3c>  // b.lo, b.ul, b.last
         :                      const int real_tag = tag - tags->nr_reserved_tags;
         :
         :                      BUG_ON(real_tag >= tags->nr_tags);
   28.66 :   ffff800010463eb0:       ldr     w4, [x0]
         :                      const int real_tag = tag - tags->nr_reserved_tags;
    0.17 :   ffff800010463eb4:       sub     w1, w3, w1
         :                      BUG_ON(real_tag >= tags->nr_tags);
    0.00 :   ffff800010463eb8:       cmp     w1, w4
    0.00 :   ffff800010463ebc:       b.cs    ffff800010463eec <blk_mq_put_tag+0x54>  // b.hs, b.nlast
         :                      sbitmap_queue_clear(&tags->bitmap_tags, real_tag, ctx->cpu);
   11.02 :   ffff800010463ec0:       ldr     w2, [x2, #64]
    0.00 :   ffff800010463ec4:       add     x0, x0, #0x10
    0.00 :   ffff800010463ec8:       bl      ffff8000104ab970 <sbitmap_queue_clear>
         :                      } else {
         :                      BUG_ON(tag >= tags->nr_reserved_tags);
         :                      sbitmap_queue_clear(&tags->breserved_tags, tag, ctx->cpu);
         :                      }
         :                      }
    6.93 :   ffff800010463ecc:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010463ed0:       ret
         :                      sbitmap_queue_clear(&tags->breserved_tags, tag, ctx->cpu);
    0.00 :   ffff800010463ed4:       ldr     w2, [x2, #64]
    0.00 :   ffff800010463ed8:       mov     w1, w3
    0.00 :   ffff800010463edc:       add     x0, x0, #0x50
    0.00 :   ffff800010463ee0:       bl      ffff8000104ab970 <sbitmap_queue_clear>
         :                      }
    0.00 :   ffff800010463ee4:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010463ee8:       ret
         :                      BUG_ON(real_tag >= tags->nr_tags);
    0.00 :   ffff800010463eec:       brk     #0x800
 Percent |	Source code & Disassembly of vmlinux for cycles (1087 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104559b8 <blk_account_io_start>:
         :                      blk_do_io_stat():
         :                      *       a) it's attached to a gendisk, and
         :                      *       b) the queue had IO stats enabled when this request was started
         :                      */
         :                      static inline bool blk_do_io_stat(struct request *rq)
         :                      {
         :                      return rq->rq_disk && (rq->rq_flags & RQF_IO_STAT);
   12.60 :   ffff8000104559b8:       ldr     x3, [x0, #160]
         :                      blk_account_io_start():
         :                      }
         :
         :                      void blk_account_io_start(struct request *rq, bool new_io)
         :                      {
         :                      struct hd_struct *part;
         :                      int rw = rq_data_dir(rq);
   13.62 :   ffff8000104559bc:       ldr     w2, [x0, #24]
         :                      blk_do_io_stat():
    0.00 :   ffff8000104559c0:       cbz     x3, ffff800010455ac0 <blk_account_io_start+0x108>
    4.61 :   ffff8000104559c4:       ldr     w3, [x0, #28]
    0.00 :   ffff8000104559c8:       tbz     w3, #13, ffff800010455ac4 <blk_account_io_start+0x10c>
         :                      blk_account_io_start():
         :                      {
    9.92 :   ffff8000104559cc:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff8000104559d0:       mov     x29, sp
    1.29 :   ffff8000104559d4:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000104559d8:       mov     x19, x0
    0.74 :   ffff8000104559dc:       stp     x21, x22, [sp, #32]
    0.09 :   ffff8000104559e0:       and     w20, w1, #0xff
    0.00 :   ffff8000104559e4:       and     w22, w2, #0x1
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff8000104559e8:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff8000104559ec:       mrs     x21, sp_el0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000104559f0:       ldr     w0, [x21, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.36 :   ffff8000104559f4:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    2.29 :   ffff8000104559f8:       str     w0, [x21, #16]
         :                      __my_cpu_offset():
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
         :                      "mrs %0, tpidr_el2",
         :                      ARM64_HAS_VIRT_HOST_EXTN)
         :                      : "=r" (off) :
         :                      "Q" (*(const unsigned long *)current_stack_pointer));
    0.00 :   ffff8000104559fc:       mov     x5, sp
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.09 :   ffff800010455a00:       mrs     x2, tpidr_el1
         :                      blk_account_io_start():
         :                      if (!blk_do_io_stat(rq))
         :                      return;
         :
         :                      part_stat_lock();
         :
         :                      if (!new_io) {
    0.09 :   ffff800010455a04:       cbnz    w20, ffff800010455ad4 <blk_account_io_start+0x11c>
         :                      part = rq->part;
         :                      part_stat_inc(part, merges[rw]);
    0.00 :   ffff800010455a08:       adrp    x1, ffff8000114ca000 <bp_hardening_data>
    0.00 :   ffff800010455a0c:       add     x0, x1, #0x18
    0.00 :   ffff800010455a10:       ldrsw   x6, [x0, x2]
    0.00 :   ffff800010455a14:       adrp    x4, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010455a18:       add     x3, x4, #0x8e8
         :                      part = rq->part;
    0.00 :   ffff800010455a1c:       ldr     x20, [x19, #168]
    0.00 :   ffff800010455a20:       mov     w2, w22
         :                      part_stat_inc(part, merges[rw]);
    0.00 :   ffff800010455a24:       ldr     x3, [x3, x6, lsl #3]
    0.00 :   ffff800010455a28:       lsl     x2, x2, #3
    0.00 :   ffff800010455a2c:       ldr     x0, [x20, #840]
    0.00 :   ffff800010455a30:       add     x0, x0, x3
    0.00 :   ffff800010455a34:       add     x0, x0, x2
    0.00 :   ffff800010455a38:       ldr     x3, [x0, #96]
    0.00 :   ffff800010455a3c:       add     x3, x3, #0x1
    0.00 :   ffff800010455a40:       str     x3, [x0, #96]
    0.00 :   ffff800010455a44:       ldr     w0, [x20, #820]
    0.00 :   ffff800010455a48:       cbz     w0, ffff800010455a84 <blk_account_io_start+0xcc>
         :                      part_to_disk():
         :                      struct lockdep_map lockdep_map;
         :                      };
         :
         :                      static inline struct gendisk *part_to_disk(struct hd_struct *part)
         :                      {
         :                      if (likely(part)) {
    0.00 :   ffff800010455a4c:       cbz     x20, ffff800010455b5c <blk_account_io_start+0x1a4>
         :                      if (part->partno)
         :                      return dev_to_disk(part_to_dev(part)->parent);
    0.00 :   ffff800010455a50:       ldr     x0, [x20, #104]
    0.00 :   ffff800010455a54:       sub     x0, x0, #0x70
         :                      __my_cpu_offset():
    0.00 :   ffff800010455a58:       mrs     x3, tpidr_el1
         :                      blk_account_io_start():
    0.00 :   ffff800010455a5c:       add     x1, x1, #0x18
    0.00 :   ffff800010455a60:       add     x4, x4, #0x8e8
    0.00 :   ffff800010455a64:       ldrsw   x1, [x1, x3]
    0.00 :   ffff800010455a68:       ldr     x0, [x0, #912]
    0.00 :   ffff800010455a6c:       ldr     x1, [x4, x1, lsl #3]
    0.00 :   ffff800010455a70:       add     x0, x0, x1
    0.00 :   ffff800010455a74:       add     x0, x0, x2
    0.00 :   ffff800010455a78:       ldr     x1, [x0, #96]
    0.00 :   ffff800010455a7c:       add     x1, x1, #0x1
    0.00 :   ffff800010455a80:       str     x1, [x0, #96]
         :                      }
         :                      part_inc_in_flight(rq->q, part, rw);
         :                      rq->part = part;
         :                      }
         :
         :                      update_io_ticks(part, jiffies);
    0.64 :   ffff800010455a84:       adrp    x1, ffff800011897000 <bit_wait_table+0xe80>
    0.00 :   ffff800010455a88:       mov     x0, x20
    0.00 :   ffff800010455a8c:       ldr     x1, [x1, #2432]
    0.00 :   ffff800010455a90:       bl      ffff800010451bf0 <update_io_ticks>
         :                      get_current():
    0.28 :   ffff800010455a94:       mrs     x1, sp_el0
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010455a98:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800010455a9c:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.09 :   ffff800010455aa0:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800010455aa4:       cbnz    x0, ffff800010455ac8 <blk_account_io_start+0x110>
         :                      blk_account_io_start():
         :
         :                      part_stat_unlock();
    0.00 :   ffff800010455aa8:       bl      ffff800010cad640 <preempt_schedule>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.92 :   ffff800010455aac:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_account_io_start():
         :                      }
    0.00 :   ffff800010455ab0:       ldp     x19, x20, [sp, #16]
    0.55 :   ffff800010455ab4:       ldp     x21, x22, [sp, #32]
    0.46 :   ffff800010455ab8:       ldp     x29, x30, [sp], #48
    0.00 :   ffff800010455abc:       ret
    0.00 :   ffff800010455ac0:       ret
    0.00 :   ffff800010455ac4:       ret
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    1.10 :   ffff800010455ac8:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff800010455acc:       cbz     x0, ffff800010455aa8 <blk_account_io_start+0xf0>
    7.08 :   ffff800010455ad0:       b       ffff800010455aac <blk_account_io_start+0xf4>
         :                      blk_account_io_start():
         :                      part = disk_map_sector_rcu(rq->rq_disk, blk_rq_pos(rq));
    0.09 :   ffff800010455ad4:       ldr     x1, [x19, #48]
    4.13 :   ffff800010455ad8:       ldr     x0, [x19, #160]
    0.00 :   ffff800010455adc:       bl      ffff800010467fb8 <disk_map_sector_rcu>
    4.60 :   ffff800010455ae0:       mov     x20, x0
         :                      rcu_read_lock():
         :                      __rcu_read_lock();
    0.00 :   ffff800010455ae4:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
    0.83 :   ffff800010455ae8:       ldr     x0, [x20, #856]
         :                      __ref_is_percpu():
         :                      * Theoretically, the following could test just ATOMIC; however,
         :                      * then we'd have to mask off DEAD separately as DEAD may be
         :                      * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
         :                      * implies ATOMIC anyway.  Test them together.
         :                      */
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff800010455aec:       tst     x0, #0x3
    0.00 :   ffff800010455af0:       b.ne    ffff800010455b64 <blk_account_io_start+0x1ac>  // b.any
         :                      __read_once_size():
    1.66 :   ffff800010455af4:       ldr     w1, [x21, #16]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff800010455af8:       add     w1, w1, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.55 :   ffff800010455afc:       str     w1, [x21, #16]
         :                      __percpu_add_case_64():
         :
         :                      PERCPU_RW_OPS(8)
         :                      PERCPU_RW_OPS(16)
         :                      PERCPU_RW_OPS(32)
         :                      PERCPU_RW_OPS(64)
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff800010455b00:       mov     x2, #0x1                        // #1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.46 :   ffff800010455b04:       mrs     x1, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.55 :   ffff800010455b08:       add     x0, x0, x1
    0.37 :   ffff800010455b0c:       ldxr    x4, [x0]
   11.53 :   ffff800010455b10:       add     x4, x4, x2
    0.00 :   ffff800010455b14:       stxr    w3, x4, [x0]
    0.00 :   ffff800010455b18:       cbnz    w3, ffff800010455b0c <blk_account_io_start+0x154>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    4.15 :   ffff800010455b1c:       ldr     x0, [x21, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800010455b20:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    3.30 :   ffff800010455b24:       str     w0, [x21, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800010455b28:       cbz     x0, ffff800010455b50 <blk_account_io_start+0x198>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    1.20 :   ffff800010455b2c:       ldr     x0, [x21, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff800010455b30:       cbz     x0, ffff800010455b50 <blk_account_io_start+0x198>
         :                      rcu_read_unlock():
         :                      __rcu_read_unlock();
    7.36 :   ffff800010455b34:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_account_io_start():
         :                      part_inc_in_flight(rq->q, part, rw);
    1.65 :   ffff800010455b38:       ldr     x0, [x19]
    0.00 :   ffff800010455b3c:       mov     w2, w22
    0.19 :   ffff800010455b40:       mov     x1, x20
    0.00 :   ffff800010455b44:       bl      ffff8000104694e8 <part_inc_in_flight>
         :                      rq->part = part;
    0.55 :   ffff800010455b48:       str     x20, [x19, #168]
    0.00 :   ffff800010455b4c:       b       ffff800010455a84 <blk_account_io_start+0xcc>
         :                      percpu_ref_tryget_live():
         :                      bool ret = false;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count)) {
         :                      this_cpu_inc(*percpu_count);
    0.00 :   ffff800010455b50:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      rcu_read_unlock():
    0.00 :   ffff800010455b54:       bl      ffff80001015eee0 <__rcu_read_unlock>
    0.00 :   ffff800010455b58:       b       ffff800010455b38 <blk_account_io_start+0x180>
         :                      part_to_disk():
         :                      else
         :                      return dev_to_disk(part_to_dev(part));
         :                      }
         :                      return NULL;
    0.00 :   ffff800010455b5c:       mov     x0, #0x0                        // #0
    0.00 :   ffff800010455b60:       b       ffff800010455a58 <blk_account_io_start+0xa0>
         :                      percpu_ref_tryget_live():
         :                      ret = true;
         :                      } else if (!(ref->percpu_count_ptr & __PERCPU_REF_DEAD)) {
    0.00 :   ffff800010455b64:       add     x3, x20, #0x350
    0.00 :   ffff800010455b68:       ldr     x0, [x3, #8]
    0.00 :   ffff800010455b6c:       tbz     w0, #1, ffff800010455bd8 <blk_account_io_start+0x220>
         :                      rcu_read_unlock():
    0.00 :   ffff800010455b70:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_account_io_start():
         :                      part = &rq->rq_disk->part0;
    0.00 :   ffff800010455b74:       ldr     x21, [x19, #160]
         :                      rcu_read_lock():
         :                      __rcu_read_lock();
    0.00 :   ffff800010455b78:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      blk_account_io_start():
    0.00 :   ffff800010455b7c:       add     x20, x21, #0x48
         :                      __read_once_size():
    0.00 :   ffff800010455b80:       ldr     x0, [x21, #928]
         :                      __ref_is_percpu():
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff800010455b84:       tst     x0, #0x3
    0.00 :   ffff800010455b88:       b.ne    ffff800010455c1c <blk_account_io_start+0x264>  // b.any
         :                      get_current():
    0.00 :   ffff800010455b8c:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.00 :   ffff800010455b90:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff800010455b94:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010455b98:       str     w2, [x1, #16]
         :                      __percpu_add_case_64():
    0.00 :   ffff800010455b9c:       mov     x3, #0x1                        // #1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800010455ba0:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff800010455ba4:       add     x0, x0, x2
    0.00 :   ffff800010455ba8:       ldxr    x5, [x0]
    0.00 :   ffff800010455bac:       add     x5, x5, x3
    0.00 :   ffff800010455bb0:       stxr    w4, x5, [x0]
    0.00 :   ffff800010455bb4:       cbnz    w4, ffff800010455ba8 <blk_account_io_start+0x1f0>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010455bb8:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800010455bbc:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010455bc0:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800010455bc4:       cbz     x0, ffff800010455bd0 <blk_account_io_start+0x218>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010455bc8:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff800010455bcc:       cbnz    x0, ffff800010455b34 <blk_account_io_start+0x17c>
         :                      percpu_ref_get_many():
         :                      this_cpu_add(*percpu_count, nr);
    0.00 :   ffff800010455bd0:       bl      ffff800010cad668 <preempt_schedule_notrace>
    0.00 :   ffff800010455bd4:       b       ffff800010455b34 <blk_account_io_start+0x17c>
         :                      __read_once_size():
    0.00 :   ffff800010455bd8:       ldr     x4, [x20, #848]
         :                      atomic64_fetch_add_unless():
         :                      atomic64_fetch_add_unless(atomic64_t *v, s64 a, s64 u)
         :                      {
         :                      s64 c = atomic64_read(v);
         :
         :                      do {
         :                      if (unlikely(c == u))
    0.00 :   ffff800010455bdc:       cbz     x4, ffff800010455b70 <blk_account_io_start+0x1b8>
         :                      break;
         :                      } while (!atomic64_try_cmpxchg(v, &c, c + a));
    0.00 :   ffff800010455be0:       add     x2, x4, #0x1
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010455be4:       b       ffff800010455c14 <blk_account_io_start+0x25c>
    0.00 :   ffff800010455be8:       b       ffff800010455c14 <blk_account_io_start+0x25c>
         :                      __lse__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,  l, "memory")
         :                      __CMPXCHG_CASE(x,  , rel_, 64,  l, "memory")
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, al, "memory")
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, al, "memory")
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, al, "memory")
         :                      __CMPXCHG_CASE(x,  ,  mb_, 64, al, "memory")
    0.00 :   ffff800010455bec:       mov     x0, x3
    0.00 :   ffff800010455bf0:       mov     x1, x4
    0.00 :   ffff800010455bf4:       mov     x5, x1
    0.00 :   ffff800010455bf8:       casal   x5, x2, [x3]
    0.00 :   ffff800010455bfc:       mov     x0, x5
         :                      atomic64_try_cmpxchg():
         :                      if (unlikely(r != o))
    0.00 :   ffff800010455c00:       cmp     x0, x4
    0.00 :   ffff800010455c04:       b.eq    ffff800010455b34 <blk_account_io_start+0x17c>  // b.none
    0.00 :   ffff800010455c08:       mov     x4, x0
         :                      atomic64_fetch_add_unless():
         :                      if (unlikely(c == u))
    0.00 :   ffff800010455c0c:       cbnz    x0, ffff800010455be0 <blk_account_io_start+0x228>
    0.00 :   ffff800010455c10:       b       ffff800010455b70 <blk_account_io_start+0x1b8>
         :                      __ll_sc__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  , rel_, 64,        ,  , l, "memory", L)
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  ,  mb_, 64, dmb ish,  , l, "memory", L)
    0.00 :   ffff800010455c14:       b       ffff8000104564cc <blk_finish_plug+0x1e4>
    0.00 :   ffff800010455c18:       b       ffff800010455c00 <blk_account_io_start+0x248>
         :                      percpu_ref_get_many():
         :                      atomic_long_add(nr, &ref->count);
    0.00 :   ffff800010455c1c:       add     x1, x21, #0x398
         :                      arch_static_branch_jump():
    0.00 :   ffff800010455c20:       b       ffff800010455c34 <blk_account_io_start+0x27c>
    0.00 :   ffff800010455c24:       b       ffff800010455c34 <blk_account_io_start+0x27c>
         :                      __lse_atomic64_add():
         :                      ATOMIC64_OP(add, stadd)
    0.00 :   ffff800010455c28:       mov     x0, #0x1                        // #1
    0.00 :   ffff800010455c2c:       stadd   x0, [x1]
    0.00 :   ffff800010455c30:       b       ffff800010455b34 <blk_account_io_start+0x17c>
         :                      __ll_sc_atomic64_add():
         :                      ATOMIC64_OPS(add, add, I)
    0.00 :   ffff800010455c34:       add     x2, x21, #0x398
    0.00 :   ffff800010455c38:       b       ffff8000104564ec <blk_finish_plug+0x204>
    0.00 :   ffff800010455c3c:       b       ffff800010455b34 <blk_account_io_start+0x17c>
 Percent |	Source code & Disassembly of vmlinux for cycles (518 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010450020 <bio_put>:
         :                      bio_put():
         :                      * Description:
         :                      *   Put a reference to a &struct bio, either one you have gotten with
         :                      *   bio_alloc, bio_get or bio_clone_*. The last put of a bio will free it.
         :                      **/
         :                      void bio_put(struct bio *bio)
         :                      {
   34.12 :   ffff800010450020:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff800010450024:       mov     x29, sp
         :                      bio_flagged():
         :                      atomic_set(&bio->__bi_cnt, count);
         :                      }
         :
         :                      static inline bool bio_flagged(struct bio *bio, unsigned int bit)
         :                      {
         :                      return (bio->bi_flags & (1U << bit)) != 0;
   33.57 :   ffff800010450028:       ldrb    w1, [x0, #21]
         :                      bio_put():
         :                      if (!bio_flagged(bio, BIO_REFFED))
    0.00 :   ffff80001045002c:       tbz     w1, #0, ffff800010450070 <bio_put+0x50>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010450030:       ldr     w1, [x0, #100]
         :                      bio_put():
         :                      bio_free(bio);
         :                      else {
         :                      BIO_BUG_ON(!atomic_read(&bio->__bi_cnt));
    0.00 :   ffff800010450034:       cbz     w1, ffff80001045007c <bio_put+0x5c>
         :
         :                      /*
         :                      * last put frees it
         :                      */
         :                      if (atomic_dec_and_test(&bio->__bi_cnt))
    0.00 :   ffff800010450038:       add     x2, x0, #0x64
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff80001045003c:       b       ffff800010450060 <bio_put+0x40>
    0.00 :   ffff800010450040:       b       ffff800010450060 <bio_put+0x40>
         :                      __lse_atomic_sub_return():
         :                      }
         :
         :                      ATOMIC_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff800010450044:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010450048:       neg     w1, w1
    0.00 :   ffff80001045004c:       ldaddal w1, w3, [x2]
    0.00 :   ffff800010450050:       add     w1, w1, w3
         :                      bio_put():
    0.00 :   ffff800010450054:       cbz     w1, ffff800010450070 <bio_put+0x50>
         :                      bio_free(bio);
         :                      }
         :                      }
    0.00 :   ffff800010450058:       ldp     x29, x30, [sp], #16
    0.00 :   ffff80001045005c:       ret
         :                      __ll_sc_atomic_sub_return():
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff800010450060:       mov     w2, #0x1                        // #1
    0.00 :   ffff800010450064:       add     x4, x0, #0x64
    0.00 :   ffff800010450068:       b       ffff800010452134 <bio_associate_blkg_from_page+0xfc>
         :                      bio_put():
         :                      if (atomic_dec_and_test(&bio->__bi_cnt))
    0.00 :   ffff80001045006c:       cbnz    w1, ffff800010450058 <bio_put+0x38>
         :                      bio_free(bio);
   10.40 :   ffff800010450070:       bl      ffff80001044ffc0 <bio_free>
         :                      }
   21.91 :   ffff800010450074:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010450078:       ret
         :                      BIO_BUG_ON(!atomic_read(&bio->__bi_cnt));
    0.00 :   ffff80001045007c:       brk     #0x800
 Percent |	Source code & Disassembly of vmlinux for cycles (1024 samples, percent: local period)
-----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010460d40 <blk_mq_make_request>:
         :                      blk_mq_make_request():
         :                      plug->multiple_queues = true;
         :                      }
         :                      }
         :
         :                      static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
         :                      {
    6.93 :   ffff800010460d40:       stp     x29, x30, [sp, #-144]!
    0.00 :   ffff800010460d44:       mov     x29, sp
    3.34 :   ffff800010460d48:       str     x19, [sp, #16]
    0.00 :   ffff800010460d4c:       mov     x19, x0
    0.00 :   ffff800010460d50:       stp     x21, x22, [sp, #32]
    0.00 :   ffff800010460d54:       adrp    x21, ffff800011899000 <page_wait_table+0x1500>
    0.39 :   ffff800010460d58:       str     x23, [sp, #48]
    0.00 :   ffff800010460d5c:       add     x2, x21, #0x8c8
    0.48 :   ffff800010460d60:       str     x1, [x29, #72]
    0.00 :   ffff800010460d64:       mov     w23, #0x1                       // #1
    1.08 :   ffff800010460d68:       ldr     x3, [x2]
    0.29 :   ffff800010460d6c:       str     x3, [x29, #136]
    0.00 :   ffff800010460d70:       mov     x3, #0x0                        // #0
         :                      const int is_sync = op_is_sync(bio->bi_opf);
    0.48 :   ffff800010460d74:       ldr     w22, [x1, #16]
         :                      op_is_sync():
         :                      * PREFLUSH flag.  Other operations may be marked as synchronous using the
         :                      * REQ_SYNC flag.
         :                      */
         :                      static inline bool op_is_sync(unsigned int op)
         :                      {
         :                      return (op & REQ_OP_MASK) == REQ_OP_READ ||
    0.00 :   ffff800010460d78:       and     w0, w22, #0xff
    0.00 :   ffff800010460d7c:       cbz     w0, ffff800010460d90 <blk_mq_make_request+0x50>
    0.00 :   ffff800010460d80:       and     w0, w22, #0x7f800
    0.00 :   ffff800010460d84:       and     w0, w0, #0xfffe0fff
    0.00 :   ffff800010460d88:       cmp     w0, #0x0
    0.00 :   ffff800010460d8c:       cset    w23, ne  // ne = any
         :                      blk_mq_make_request():
         :                      struct blk_plug *plug;
         :                      struct request *same_queue_rq = NULL;
         :                      unsigned int nr_segs;
         :                      blk_qc_t cookie;
         :
         :                      blk_queue_bounce(q, &bio);
    1.18 :   ffff800010460d90:       mov     x0, x19
    0.00 :   ffff800010460d94:       add     x1, x29, #0x48
         :                      struct request *same_queue_rq = NULL;
    2.14 :   ffff800010460d98:       str     xzr, [x29, #88]
         :                      struct blk_mq_alloc_data data = { .flags = 0};
    2.63 :   ffff800010460d9c:       stp     xzr, xzr, [x29, #96]
    0.39 :   ffff800010460da0:       stp     xzr, xzr, [x29, #112]
    0.98 :   ffff800010460da4:       str     xzr, [x29, #128]
         :                      blk_queue_bounce(q, &bio);
    0.00 :   ffff800010460da8:       bl      ffff80001046fbe8 <blk_queue_bounce>
         :                      __blk_queue_split(q, &bio, &nr_segs);
    0.20 :   ffff800010460dac:       add     x2, x29, #0x50
    0.00 :   ffff800010460db0:       add     x1, x29, #0x48
    0.00 :   ffff800010460db4:       mov     x0, x19
    0.00 :   ffff800010460db8:       bl      ffff80001045a9f8 <__blk_queue_split>
         :
         :                      if (!bio_integrity_prep(bio))
    0.00 :   ffff800010460dbc:       ldr     x0, [x29, #72]
    0.78 :   ffff800010460dc0:       bl      ffff80001047a738 <bio_integrity_prep>
    0.59 :   ffff800010460dc4:       tst     w0, #0xff
    0.00 :   ffff800010460dc8:       b.eq    ffff800010460f98 <blk_mq_make_request+0x258>  // b.none
         :                      return BLK_QC_T_NONE;
         :
         :                      if (!is_flush_fua && !blk_queue_nomerges(q) &&
    0.49 :   ffff800010460dcc:       ands    w22, w22, #0x60000
    1.27 :   ffff800010460dd0:       ldr     w2, [x29, #80]
    0.49 :   ffff800010460dd4:       ldr     x1, [x29, #72]
    0.00 :   ffff800010460dd8:       b.ne    ffff800010460de8 <blk_mq_make_request+0xa8>  // b.any
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff800010460ddc:       ldr     x0, [x19, #104]
         :                      blk_mq_make_request():
    0.00 :   ffff800010460de0:       tst     w0, #0x8
    0.00 :   ffff800010460de4:       b.eq    ffff800010460fb0 <blk_mq_make_request+0x270>  // b.none
         :                      test_bit():
    0.00 :   ffff800010460de8:       ldr     x0, [x19, #104]
         :                      blk_mq_sched_bio_merge():
         :
         :                      static inline bool
         :                      blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio,
         :                      unsigned int nr_segs)
         :                      {
         :                      if (blk_queue_nomerges(q) || !bio_mergeable(bio))
    0.00 :   ffff800010460dec:       tst     w0, #0x8
    0.00 :   ffff800010460df0:       b.ne    ffff800010460e04 <blk_mq_make_request+0xc4>  // b.any
         :                      bio_mergeable():
         :                      bio_op(bio) == REQ_OP_WRITE_ZEROES;
         :                      }
         :
         :                      static inline bool bio_mergeable(struct bio *bio)
         :                      {
         :                      if (bio->bi_opf & REQ_NOMERGE_FLAGS)
    0.29 :   ffff800010460df4:       ldr     w0, [x1, #16]
    0.00 :   ffff800010460df8:       and     w0, w0, #0x7c000
    0.00 :   ffff800010460dfc:       and     w0, w0, #0xfffe7fff
    1.66 :   ffff800010460e00:       cbz     w0, ffff800010460fdc <blk_mq_make_request+0x29c>
    0.00 :   ffff800010460e04:       str     x20, [x29, #24]
         :                      bio_set_flag():
         :                      return (bio->bi_flags & (1U << bit)) != 0;
         :                      }
         :
         :                      static inline void bio_set_flag(struct bio *bio, unsigned int bit)
         :                      {
         :                      bio->bi_flags |= (1U << bit);
    0.00 :   ffff800010460e08:       ldrh    w0, [x1, #20]
    0.00 :   ffff800010460e0c:       orr     w0, w0, #0x1000
    1.56 :   ffff800010460e10:       strh    w0, [x1, #20]
         :                      rq_qos_throttle():
         :                      /*
         :                      * BIO_TRACKED lets controllers know that a bio went through the
         :                      * normal rq_qos path.
         :                      */
         :                      bio_set_flag(bio, BIO_TRACKED);
         :                      if (q->rq_qos)
    0.00 :   ffff800010460e14:       ldr     x0, [x19, #24]
    0.00 :   ffff800010460e18:       cbz     x0, ffff800010460e24 <blk_mq_make_request+0xe4>
         :                      __rq_qos_throttle(q->rq_qos, bio);
    0.00 :   ffff800010460e1c:       bl      ffff80001046f118 <__rq_qos_throttle>
    0.00 :   ffff800010460e20:       ldr     x1, [x29, #72]
         :                      blk_mq_make_request():
         :                      if (blk_mq_sched_bio_merge(q, bio, nr_segs))
         :                      return BLK_QC_T_NONE;
         :
         :                      rq_qos_throttle(q, bio);
         :
         :                      data.cmd_flags = bio->bi_opf;
    0.00 :   ffff800010460e24:       ldr     w0, [x1, #16]
         :                      rq = blk_mq_get_request(q, bio, &data);
    0.00 :   ffff800010460e28:       add     x2, x29, #0x60
         :                      data.cmd_flags = bio->bi_opf;
    1.36 :   ffff800010460e2c:       str     w0, [x29, #112]
         :                      rq = blk_mq_get_request(q, bio, &data);
    0.00 :   ffff800010460e30:       mov     x0, x19
    0.00 :   ffff800010460e34:       bl      ffff80001045f898 <blk_mq_get_request>
    0.00 :   ffff800010460e38:       mov     x20, x0
         :                      if (unlikely(!rq)) {
         :                      rq_qos_cleanup(q, bio);
    1.95 :   ffff800010460e3c:       ldr     x0, [x19, #24]
         :                      if (unlikely(!rq)) {
    0.00 :   ffff800010460e40:       cbz     x20, ffff800010460f78 <blk_mq_make_request+0x238>
         :                      if (bio->bi_opf & REQ_NOWAIT)
         :                      bio_wouldblock_error(bio);
         :                      return BLK_QC_T_NONE;
         :                      }
         :
         :                      trace_block_getrq(q, bio, bio->bi_opf);
    0.10 :   ffff800010460e44:       ldr     x2, [x29, #72]
         :                      rq_qos_track():
         :                      }
         :
         :                      static inline void rq_qos_track(struct request_queue *q, struct request *rq,
         :                      struct bio *bio)
         :                      {
         :                      if (q->rq_qos)
    0.00 :   ffff800010460e48:       cbz     x0, ffff800010460e58 <blk_mq_make_request+0x118>
         :                      __rq_qos_track(q->rq_qos, rq, bio);
    0.00 :   ffff800010460e4c:       mov     x1, x20
    0.00 :   ffff800010460e50:       bl      ffff80001046f160 <__rq_qos_track>
    0.00 :   ffff800010460e54:       ldr     x2, [x29, #72]
         :                      blk_mq_make_request():
         :
         :                      rq_qos_track(q, rq, bio);
         :
         :                      cookie = request_to_qc_t(data.hctx, rq);
    1.08 :   ffff800010460e58:       ldr     x1, [x29, #128]
    0.10 :   ffff800010460e5c:       ldr     w0, [x20, #32]
    0.58 :   ffff800010460e60:       ldr     w1, [x1, #428]
         :                      request_to_qc_t():
         :                      ({ ctx = (hctx)->ctxs[(i)]; 1; }); (i)++)
         :
         :                      static inline blk_qc_t request_to_qc_t(struct blk_mq_hw_ctx *hctx,
         :                      struct request *rq)
         :                      {
         :                      if (rq->tag != -1)
    0.00 :   ffff800010460e64:       cmn     w0, #0x1
    0.00 :   ffff800010460e68:       lsl     w3, w1, #16
    0.00 :   ffff800010460e6c:       b.eq    ffff80001046102c <blk_mq_make_request+0x2ec>  // b.none
         :                      return rq->tag | (hctx->queue_num << BLK_QC_T_SHIFT);
    1.27 :   ffff800010460e70:       orr     w0, w0, w3
         :                      blk_mq_bio_to_request():
         :                      if (bio->bi_opf & REQ_RAHEAD)
    0.10 :   ffff800010460e74:       ldr     w3, [x2, #16]
         :                      blk_mq_make_request():
         :                      cookie = request_to_qc_t(data.hctx, rq);
    3.22 :   ffff800010460e78:       str     w0, [x29, #84]
         :
         :                      blk_mq_bio_to_request(rq, bio, nr_segs);
    0.00 :   ffff800010460e7c:       ldr     w1, [x29, #80]
         :                      blk_mq_bio_to_request():
         :                      if (bio->bi_opf & REQ_RAHEAD)
    0.00 :   ffff800010460e80:       tbnz    w3, #19, ffff800010460fa0 <blk_mq_make_request+0x260>
         :                      rq->__sector = bio->bi_iter.bi_sector;
    0.68 :   ffff800010460e84:       ldr     x0, [x2, #32]
    1.08 :   ffff800010460e88:       str     x0, [x20, #48]
         :                      rq->write_hint = bio->bi_write_hint;
    0.00 :   ffff800010460e8c:       ldrh    w0, [x2, #24]
         :                      blk_rq_bio_prep():
         :                      }
         :
         :                      static inline void blk_rq_bio_prep(struct request *rq, struct bio *bio,
         :                      unsigned int nr_segs)
         :                      {
         :                      rq->nr_phys_segments = nr_segs;
    0.10 :   ffff800010460e90:       strh    w1, [x20, #194]
         :                      blk_mq_bio_to_request():
    0.30 :   ffff800010460e94:       strh    w0, [x20, #198]
         :                      blk_rq_bio_prep():
         :                      rq->__data_len = bio->bi_iter.bi_size;
    2.74 :   ffff800010460e98:       ldr     w0, [x2, #40]
    0.19 :   ffff800010460e9c:       str     w0, [x20, #40]
         :                      rq->bio = rq->biotail = bio;
    0.78 :   ffff800010460ea0:       stp     x2, x2, [x20, #56]
         :                      rq->ioprio = bio_prio(bio);
    1.46 :   ffff800010460ea4:       ldrh    w0, [x2, #22]
    0.20 :   ffff800010460ea8:       strh    w0, [x20, #200]
         :
         :                      if (bio->bi_disk)
    1.95 :   ffff800010460eac:       ldr     x0, [x2, #8]
    0.00 :   ffff800010460eb0:       cbz     x0, ffff800010460eb8 <blk_mq_make_request+0x178>
         :                      rq->rq_disk = bio->bi_disk;
    0.00 :   ffff800010460eb4:       str     x0, [x20, #160]
         :                      blk_mq_bio_to_request():
         :                      blk_account_io_start(rq, true);
    0.00 :   ffff800010460eb8:       mov     x0, x20
    0.00 :   ffff800010460ebc:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010460ec0:       bl      ffff8000104559b8 <blk_account_io_start>
         :                      blk_queue_is_zoned():
         :                      return q->limits.zoned;
         :                      }
         :
         :                      static inline bool blk_queue_is_zoned(struct request_queue *q)
         :                      {
         :                      switch (blk_queue_zoned_model(q)) {
    0.39 :   ffff800010460ec4:       ldr     w0, [x19, #1140]
    0.00 :   ffff800010460ec8:       sub     w0, w0, #0x1
    0.00 :   ffff800010460ecc:       cmp     w0, #0x1
    0.00 :   ffff800010460ed0:       b.hi    ffff800010460f04 <blk_mq_make_request+0x1c4>  // b.pmore
         :                      blk_mq_plug():
         :                      {
         :                      /*
         :                      * For regular block devices or read operations, use the context plug
         :                      * which may be NULL if blk_start_plug() was not executed.
         :                      */
         :                      if (!blk_queue_is_zoned(q) || !op_is_write(bio_op(bio)))
    0.00 :   ffff800010460ed4:       ldr     x0, [x29, #72]
         :                      op_is_write():
         :                      return (op & 1);
    0.00 :   ffff800010460ed8:       ldr     w0, [x0, #16]
         :                      blk_mq_plug():
    0.00 :   ffff800010460edc:       tbz     w0, #0, ffff800010460f04 <blk_mq_make_request+0x1c4>
         :                      blk_mq_make_request():
         :
         :                      plug = blk_mq_plug(q, bio);
         :                      if (unlikely(is_flush_fua)) {
    0.00 :   ffff800010460ee0:       cbnz    w22, ffff8000104610c4 <blk_mq_make_request+0x384>
         :                      blk_flush_plug_list(plug, false);
         :                      trace_block_plug(q);
         :                      }
         :
         :                      blk_add_rq_to_plug(plug, rq);
         :                      } else if (q->elevator) {
    0.00 :   ffff800010460ee4:       ldr     x0, [x19, #8]
    0.00 :   ffff800010460ee8:       cbz     x0, ffff800010461114 <blk_mq_make_request+0x3d4>
         :                      }
         :                      } else if ((q->nr_hw_queues > 1 && is_sync) ||
         :                      !data.hctx->dispatch_busy) {
         :                      blk_mq_try_issue_directly(data.hctx, rq, &cookie);
         :                      } else {
         :                      blk_mq_sched_insert_request(rq, false, true, true);
    0.00 :   ffff800010460eec:       mov     w3, #0x1                        // #1
    0.00 :   ffff800010460ef0:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010460ef4:       mov     w2, w3
    0.00 :   ffff800010460ef8:       mov     x0, x20
    0.00 :   ffff800010460efc:       bl      ffff8000104662c0 <blk_mq_sched_insert_request>
    0.00 :   ffff800010460f00:       b       ffff800010460f48 <blk_mq_make_request+0x208>
    1.95 :   ffff800010460f04:       str     x24, [x29, #56]
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    2.72 :   ffff800010460f08:       mrs     x0, sp_el0
         :                      blk_mq_plug():
         :                      return current->plug;
    0.00 :   ffff800010460f0c:       ldr     x24, [x0, #1840]
         :                      blk_mq_make_request():
         :                      if (unlikely(is_flush_fua)) {
    0.00 :   ffff800010460f10:       cbnz    w22, ffff8000104610c0 <blk_mq_make_request+0x380>
         :                      } else if (plug && (q->nr_hw_queues == 1 || q->mq_ops->commit_rqs ||
    0.78 :   ffff800010460f14:       cbz     x24, ffff8000104610dc <blk_mq_make_request+0x39c>
    0.00 :   ffff800010460f18:       ldr     w0, [x19, #80]
    0.00 :   ffff800010460f1c:       cmp     w0, #0x1
    0.00 :   ffff800010460f20:       b.eq    ffff800010460f30 <blk_mq_make_request+0x1f0>  // b.none
    1.67 :   ffff800010460f24:       ldr     x1, [x19, #48]
    0.69 :   ffff800010460f28:       ldr     x1, [x1, #8]
    0.00 :   ffff800010460f2c:       cbz     x1, ffff80001046103c <blk_mq_make_request+0x2fc>
         :                      unsigned int request_count = plug->rq_count;
   20.25 :   ffff800010460f30:       ldrh    w0, [x24, #32]
         :                      if (!request_count)
    0.00 :   ffff800010460f34:       cbnz    w0, ffff800010460ff8 <blk_mq_make_request+0x2b8>
         :                      blk_add_rq_to_plug(plug, rq);
    1.07 :   ffff800010460f38:       mov     x0, x24
    0.00 :   ffff800010460f3c:       mov     x1, x20
    0.00 :   ffff800010460f40:       bl      ffff80001045dd88 <blk_add_rq_to_plug>
         :                      !blk_queue_nonrot(q))) {
    0.78 :   ffff800010460f44:       ldr     x24, [x29, #56]
         :                      }
         :
         :                      return cookie;
    6.33 :   ffff800010460f48:       ldr     w0, [x29, #84]
    0.00 :   ffff800010460f4c:       ldr     x20, [x29, #24]
         :                      }
    0.00 :   ffff800010460f50:       add     x21, x21, #0x8c8
    1.86 :   ffff800010460f54:       ldr     x2, [x29, #136]
    0.10 :   ffff800010460f58:       ldr     x1, [x21]
    0.00 :   ffff800010460f5c:       eor     x1, x2, x1
    0.10 :   ffff800010460f60:       cbnz    x1, ffff800010461148 <blk_mq_make_request+0x408>
    1.66 :   ffff800010460f64:       ldr     x19, [sp, #16]
    1.95 :   ffff800010460f68:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010460f6c:       ldr     x23, [sp, #48]
    0.00 :   ffff800010460f70:       ldp     x29, x30, [sp], #144
    0.00 :   ffff800010460f74:       ret
         :                      rq_qos_cleanup(q, bio);
    0.00 :   ffff800010460f78:       ldr     x1, [x29, #72]
         :                      rq_qos_cleanup():
         :                      if (q->rq_qos)
    0.00 :   ffff800010460f7c:       cbz     x0, ffff800010460f88 <blk_mq_make_request+0x248>
         :                      __rq_qos_cleanup(q->rq_qos, bio);
    0.00 :   ffff800010460f80:       bl      ffff80001046eff8 <__rq_qos_cleanup>
    0.00 :   ffff800010460f84:       ldr     x1, [x29, #72]
         :                      blk_mq_make_request():
         :                      if (bio->bi_opf & REQ_NOWAIT)
    0.00 :   ffff800010460f88:       ldr     w0, [x1, #16]
    0.00 :   ffff800010460f8c:       tbnz    w0, #21, ffff80001046111c <blk_mq_make_request+0x3dc>
    0.00 :   ffff800010460f90:       ldr     x20, [x29, #24]
    0.00 :   ffff800010460f94:       nop
         :                      return BLK_QC_T_NONE;
    0.00 :   ffff800010460f98:       mov     w0, #0xffffffff                 // #-1
    0.00 :   ffff800010460f9c:       b       ffff800010460f50 <blk_mq_make_request+0x210>
         :                      blk_mq_bio_to_request():
         :                      rq->cmd_flags |= REQ_FAILFAST_MASK;
    0.00 :   ffff800010460fa0:       ldr     w0, [x20, #24]
    0.00 :   ffff800010460fa4:       orr     w0, w0, #0x700
    0.00 :   ffff800010460fa8:       str     w0, [x20, #24]
    0.00 :   ffff800010460fac:       b       ffff800010460e84 <blk_mq_make_request+0x144>
         :                      blk_mq_make_request():
         :                      blk_attempt_plug_merge(q, bio, nr_segs, &same_queue_rq))
    0.29 :   ffff800010460fb0:       add     x3, x29, #0x58
    0.00 :   ffff800010460fb4:       mov     x0, x19
    0.00 :   ffff800010460fb8:       bl      ffff800010455f58 <blk_attempt_plug_merge>
         :                      if (!is_flush_fua && !blk_queue_nomerges(q) &&
    0.00 :   ffff800010460fbc:       tst     w0, #0xff
    0.68 :   ffff800010460fc0:       b.ne    ffff800010460f98 <blk_mq_make_request+0x258>  // b.any
         :                      test_bit():
    0.30 :   ffff800010460fc4:       ldr     x0, [x19, #104]
    3.04 :   ffff800010460fc8:       ldr     w2, [x29, #80]
    0.00 :   ffff800010460fcc:       ldr     x1, [x29, #72]
         :                      blk_mq_sched_bio_merge():
    0.00 :   ffff800010460fd0:       tst     w0, #0x8
    0.00 :   ffff800010460fd4:       b.eq    ffff800010460df4 <blk_mq_make_request+0xb4>  // b.none
    0.00 :   ffff800010460fd8:       b       ffff800010460e04 <blk_mq_make_request+0xc4>
         :                      return false;
         :
         :                      return __blk_mq_sched_bio_merge(q, bio, nr_segs);
    4.81 :   ffff800010460fdc:       mov     x0, x19
    0.00 :   ffff800010460fe0:       bl      ffff800010466180 <__blk_mq_sched_bio_merge>
         :                      blk_mq_make_request():
         :                      if (blk_mq_sched_bio_merge(q, bio, nr_segs))
    1.75 :   ffff800010460fe4:       tst     w0, #0xff
    0.00 :   ffff800010460fe8:       b.ne    ffff800010460f98 <blk_mq_make_request+0x258>  // b.any
    0.00 :   ffff800010460fec:       ldr     x1, [x29, #72]
    0.00 :   ffff800010460ff0:       str     x20, [x29, #24]
    0.00 :   ffff800010460ff4:       b       ffff800010460e08 <blk_mq_make_request+0xc8>
         :                      if (request_count >= BLK_MAX_REQUEST_COUNT || (last &&
    0.00 :   ffff800010460ff8:       cmp     w0, #0xf
    0.00 :   ffff800010460ffc:       b.hi    ffff80001046101c <blk_mq_make_request+0x2dc>  // b.pmore
         :                      last = list_entry_rq(plug->mq_list.prev);
    0.00 :   ffff800010461000:       ldr     x0, [x24, #8]
         :                      if (request_count >= BLK_MAX_REQUEST_COUNT || (last &&
    0.00 :   ffff800010461004:       cmp     x0, #0x48
    0.00 :   ffff800010461008:       b.eq    ffff800010460f38 <blk_mq_make_request+0x1f8>  // b.none
    0.00 :   ffff80001046100c:       ldur    w1, [x0, #-32]
    0.00 :   ffff800010461010:       mov     w0, #0x1ffff                    // #131071
    0.00 :   ffff800010461014:       cmp     w1, w0
    0.00 :   ffff800010461018:       b.ls    ffff800010460f38 <blk_mq_make_request+0x1f8>  // b.plast
         :                      blk_flush_plug_list(plug, false);
    0.00 :   ffff80001046101c:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010461020:       mov     x0, x24
    0.00 :   ffff800010461024:       bl      ffff8000104561e0 <blk_flush_plug_list>
    0.00 :   ffff800010461028:       b       ffff800010460f38 <blk_mq_make_request+0x1f8>
         :                      request_to_qc_t():
         :
         :                      return rq->internal_tag | (hctx->queue_num << BLK_QC_T_SHIFT) |
    0.00 :   ffff80001046102c:       ldr     w1, [x20, #36]
    0.00 :   ffff800010461030:       orr     w0, w3, #0x80000000
    0.00 :   ffff800010461034:       orr     w0, w0, w1
    0.00 :   ffff800010461038:       b       ffff800010460e74 <blk_mq_make_request+0x134>
         :                      test_bit():
    0.00 :   ffff80001046103c:       ldr     x1, [x19, #104]
         :                      blk_mq_make_request():
         :                      } else if (plug && (q->nr_hw_queues == 1 || q->mq_ops->commit_rqs ||
    0.00 :   ffff800010461040:       tst     w1, #0x40
    0.00 :   ffff800010461044:       b.eq    ffff800010460f30 <blk_mq_make_request+0x1f0>  // b.none
         :                      } else if (q->elevator) {
    0.00 :   ffff800010461048:       ldr     x1, [x19, #8]
    0.00 :   ffff80001046104c:       cbnz    x1, ffff800010461154 <blk_mq_make_request+0x414>
         :                      test_bit():
    0.00 :   ffff800010461050:       ldr     x1, [x19, #104]
         :                      blk_mq_make_request():
         :                      } else if (plug && !blk_queue_nomerges(q)) {
    0.00 :   ffff800010461054:       tst     w1, #0x8
    0.00 :   ffff800010461058:       b.ne    ffff8000104610e4 <blk_mq_make_request+0x3a4>  // b.any
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001046105c:       ldr     x0, [x24]
         :                      blk_mq_make_request():
         :                      if (list_empty(&plug->mq_list))
    0.00 :   ffff800010461060:       cmp     x24, x0
    0.00 :   ffff800010461064:       b.eq    ffff800010461140 <blk_mq_make_request+0x400>  // b.none
         :                      if (same_queue_rq) {
    0.00 :   ffff800010461068:       ldr     x0, [x29, #88]
    0.00 :   ffff80001046106c:       cbz     x0, ffff800010461094 <blk_mq_make_request+0x354>
         :                      __list_del_entry():
         :                      static inline void __list_del_entry(struct list_head *entry)
         :                      {
         :                      if (!__list_del_entry_valid(entry))
         :                      return;
         :
         :                      __list_del(entry->prev, entry->next);
    0.00 :   ffff800010461070:       ldp     x3, x2, [x0, #72]
         :                      __list_del():
         :                      next->prev = prev;
    0.00 :   ffff800010461074:       str     x2, [x3, #8]
         :                      blk_mq_make_request():
         :                      list_del_init(&same_queue_rq->queuelist);
    0.00 :   ffff800010461078:       add     x1, x0, #0x48
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff80001046107c:       str     x3, [x2]
    0.00 :   ffff800010461080:       str     x1, [x0, #72]
         :                      INIT_LIST_HEAD():
         :                      list->prev = list;
    0.00 :   ffff800010461084:       str     x1, [x0, #80]
         :                      blk_mq_make_request():
         :                      plug->rq_count--;
    0.00 :   ffff800010461088:       ldrh    w0, [x24, #32]
    0.00 :   ffff80001046108c:       sub     w0, w0, #0x1
    0.00 :   ffff800010461090:       strh    w0, [x24, #32]
         :                      blk_add_rq_to_plug(plug, rq);
    0.00 :   ffff800010461094:       mov     x1, x20
    0.00 :   ffff800010461098:       mov     x0, x24
    0.00 :   ffff80001046109c:       bl      ffff80001045dd88 <blk_add_rq_to_plug>
         :                      if (same_queue_rq) {
    0.00 :   ffff8000104610a0:       ldr     x1, [x29, #88]
    0.00 :   ffff8000104610a4:       cbz     x1, ffff800010461138 <blk_mq_make_request+0x3f8>
         :                      data.hctx = same_queue_rq->mq_hctx;
    0.00 :   ffff8000104610a8:       ldr     x0, [x1, #16]
         :                      blk_mq_try_issue_directly(data.hctx, same_queue_rq,
    0.00 :   ffff8000104610ac:       add     x2, x29, #0x54
         :                      data.hctx = same_queue_rq->mq_hctx;
    0.00 :   ffff8000104610b0:       str     x0, [x29, #128]
         :                      blk_mq_try_issue_directly(data.hctx, same_queue_rq,
    0.00 :   ffff8000104610b4:       bl      ffff800010460c88 <blk_mq_try_issue_directly>
    0.00 :   ffff8000104610b8:       ldr     x24, [x29, #56]
    0.00 :   ffff8000104610bc:       b       ffff800010460f48 <blk_mq_make_request+0x208>
    0.00 :   ffff8000104610c0:       ldr     x24, [x29, #56]
         :                      blk_insert_flush(rq);
    0.00 :   ffff8000104610c4:       mov     x0, x20
    0.00 :   ffff8000104610c8:       bl      ffff8000104583d0 <blk_insert_flush>
         :                      blk_mq_run_hw_queue(data.hctx, true);
    0.00 :   ffff8000104610cc:       ldr     x0, [x29, #128]
    0.00 :   ffff8000104610d0:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000104610d4:       bl      ffff80001045ef88 <blk_mq_run_hw_queue>
    0.00 :   ffff8000104610d8:       b       ffff800010460f48 <blk_mq_make_request+0x208>
    0.00 :   ffff8000104610dc:       ldr     x24, [x29, #56]
    0.00 :   ffff8000104610e0:       b       ffff800010460ee4 <blk_mq_make_request+0x1a4>
    0.00 :   ffff8000104610e4:       ldr     x24, [x29, #56]
         :                      } else if ((q->nr_hw_queues > 1 && is_sync) ||
    0.00 :   ffff8000104610e8:       cmp     w0, #0x1
    0.00 :   ffff8000104610ec:       ldr     x0, [x29, #128]
    0.00 :   ffff8000104610f0:       cset    w1, hi  // hi = pmore
    0.00 :   ffff8000104610f4:       tst     w1, w23
    0.00 :   ffff8000104610f8:       b.ne    ffff800010461104 <blk_mq_make_request+0x3c4>  // b.any
    0.00 :   ffff8000104610fc:       ldr     w1, [x0, #264]
    0.00 :   ffff800010461100:       cbnz    w1, ffff800010460eec <blk_mq_make_request+0x1ac>
         :                      blk_mq_try_issue_directly(data.hctx, rq, &cookie);
    0.00 :   ffff800010461104:       add     x2, x29, #0x54
    0.00 :   ffff800010461108:       mov     x1, x20
    0.00 :   ffff80001046110c:       bl      ffff800010460c88 <blk_mq_try_issue_directly>
    0.00 :   ffff800010461110:       b       ffff800010460f48 <blk_mq_make_request+0x208>
    0.00 :   ffff800010461114:       ldr     w0, [x19, #80]
    0.00 :   ffff800010461118:       b       ffff8000104610e8 <blk_mq_make_request+0x3a8>
         :                      bio_wouldblock_error():
         :                      bio_endio(bio);
         :                      }
         :
         :                      static inline void bio_wouldblock_error(struct bio *bio)
         :                      {
         :                      bio->bi_status = BLK_STS_AGAIN;
    0.00 :   ffff80001046111c:       mov     w0, #0xc                        // #12
    0.00 :   ffff800010461120:       strb    w0, [x1, #26]
         :                      bio_endio(bio);
    0.00 :   ffff800010461124:       mov     x0, x1
    0.00 :   ffff800010461128:       bl      ffff800010450080 <bio_endio>
    0.00 :   ffff80001046112c:       ldr     x20, [x29, #24]
         :                      blk_mq_make_request():
         :                      return BLK_QC_T_NONE;
    0.00 :   ffff800010461130:       mov     w0, #0xffffffff                 // #-1
    0.00 :   ffff800010461134:       b       ffff800010460f50 <blk_mq_make_request+0x210>
    0.00 :   ffff800010461138:       ldr     x24, [x29, #56]
    0.00 :   ffff80001046113c:       b       ffff800010460f48 <blk_mq_make_request+0x208>
         :                      same_queue_rq = NULL;
    0.00 :   ffff800010461140:       str     xzr, [x29, #88]
    0.00 :   ffff800010461144:       b       ffff800010461094 <blk_mq_make_request+0x354>
    0.00 :   ffff800010461148:       str     x20, [x29, #24]
    0.00 :   ffff80001046114c:       str     x24, [x29, #56]
         :                      }
    0.00 :   ffff800010461150:       bl      ffff8000100e5630 <__stack_chk_fail>
    0.00 :   ffff800010461154:       ldr     x24, [x29, #56]
    0.00 :   ffff800010461158:       b       ffff800010460eec <blk_mq_make_request+0x1ac>
 Percent |	Source code & Disassembly of vmlinux for cycles (986 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102d96d8 <__arm64_sys_io_submit>:
         :                      __arm64_sys_io_submit():
         :                      *       fail with -EBADF if the file descriptor specified in the first
         :                      *       iocb is invalid.  May fail with -EAGAIN if insufficient resources
         :                      *       are available to queue any iocbs.  Will return 0 if nr is 0.  Will
         :                      *       fail with -ENOSYS if not implemented.
         :                      */
         :                      SYSCALL_DEFINE3(io_submit, aio_context_t, ctx_id, long, nr,
   10.01 :   ffff8000102d96d8:       stp     x29, x30, [sp, #-128]!
    0.00 :   ffff8000102d96dc:       mov     x29, sp
    1.62 :   ffff8000102d96e0:       str     x19, [sp, #16]
    0.40 :   ffff8000102d96e4:       stp     x22, x23, [sp, #40]
    0.00 :   ffff8000102d96e8:       adrp    x23, ffff800011899000 <page_wait_table+0x1500>
    1.12 :   ffff8000102d96ec:       str     x25, [sp, #64]
    0.00 :   ffff8000102d96f0:       add     x1, x23, #0x8c8
    0.51 :   ffff8000102d96f4:       ldr     x2, [x1]
    1.92 :   ffff8000102d96f8:       str     x2, [x29, #120]
    0.00 :   ffff8000102d96fc:       mov     x2, #0x0                        // #0
         :                      __se_sys_io_submit():
    0.20 :   ffff8000102d9700:       ldr     x22, [x0, #16]
         :                      __arm64_sys_io_submit():
    0.70 :   ffff8000102d9704:       ldp     x1, x19, [x0]
         :                      __do_sys_io_submit():
         :                      struct kioctx *ctx;
         :                      long ret = 0;
         :                      int i = 0;
         :                      struct blk_plug plug;
         :
         :                      if (unlikely(nr < 0))
    0.00 :   ffff8000102d9708:       tbnz    x19, #63, ffff8000102d98d0 <__arm64_sys_io_submit+0x1f8>
    5.48 :   ffff8000102d970c:       str     x21, [x29, #32]
         :                      return -EINVAL;
         :
         :                      ctx = lookup_ioctx(ctx_id);
    0.00 :   ffff8000102d9710:       mov     x0, x1
    0.00 :   ffff8000102d9714:       bl      ffff8000102d66c8 <lookup_ioctx>
    1.32 :   ffff8000102d9718:       mov     x21, x0
         :                      if (unlikely(!ctx)) {
    0.00 :   ffff8000102d971c:       cbz     x0, ffff8000102d98cc <__arm64_sys_io_submit+0x1f4>
    0.00 :   ffff8000102d9720:       str     x20, [x29, #24]
    0.51 :   ffff8000102d9724:       str     x26, [x29, #72]
         :                      pr_debug("EINVAL: invalid context id\n");
         :                      return -EINVAL;
         :                      }
         :
         :                      if (nr > ctx->nr_events)
    0.41 :   ffff8000102d9728:       ldr     w20, [x0, #144]
    0.00 :   ffff8000102d972c:       cmp     x20, x19
    0.10 :   ffff8000102d9730:       csel    x20, x20, x19, le
         :                      nr = ctx->nr_events;
         :
         :                      if (nr > AIO_PLUG_THRESHOLD)
    0.00 :   ffff8000102d9734:       cmp     x20, #0x2
    1.21 :   ffff8000102d9738:       b.gt    ffff8000102d98bc <__arm64_sys_io_submit+0x1e4>
         :                      blk_start_plug(&plug);
         :                      for (i = 0; i < nr; i++) {
    0.00 :   ffff8000102d973c:       mov     x25, #0x0                       // #0
    0.00 :   ffff8000102d9740:       mov     x26, #0x0                       // #0
    0.00 :   ffff8000102d9744:       mov     w19, #0x0                       // #0
    0.00 :   ffff8000102d9748:       cbz     x20, ffff8000102d97b0 <__arm64_sys_io_submit+0xd8>
    0.10 :   ffff8000102d974c:       str     x24, [x29, #56]
         :                      __arm64_sys_io_submit():
         :                      SYSCALL_DEFINE3(io_submit, aio_context_t, ctx_id, long, nr,
    0.00 :   ffff8000102d9750:       mov     x25, #0x0                       // #0
    0.00 :   ffff8000102d9754:       mov     w19, #0x0                       // #0
    0.00 :   ffff8000102d9758:       mov     x1, #0x0                        // #0
         :                      __do_sys_io_submit():
         :                      struct iocb __user *user_iocb;
         :
         :                      if (unlikely(get_user(user_iocb, iocbpp + i))) {
    0.40 :   ffff8000102d975c:       mov     w24, #0x0                       // #0
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    1.10 :   ffff8000102d9760:       mrs     x0, sp_el0
         :                      __range_ok():
         :                      * Asynchronous I/O running in a kernel thread does not have the
         :                      * TIF_TAGGED_ADDR flag of the process owning the mm, so always untag
         :                      * the user address before checking.
         :                      */
         :                      if (IS_ENABLED(CONFIG_ARM64_TAGGED_ADDR_ABI) &&
         :                      (current->flags & PF_KTHREAD || test_thread_flag(TIF_TAGGED_ADDR)))
    0.00 :   ffff8000102d9764:       ldr     w3, [x0, #44]
         :                      __do_sys_io_submit():
    0.00 :   ffff8000102d9768:       add     x1, x22, x1, lsl #3
         :                      __range_ok():
         :                      unsigned long ret, limit = current_thread_info()->addr_limit;
    0.91 :   ffff8000102d976c:       ldr     x2, [x0, #8]
         :                      if (IS_ENABLED(CONFIG_ARM64_TAGGED_ADDR_ABI) &&
    0.00 :   ffff8000102d9770:       tbnz    w3, #21, ffff8000102d9780 <__arm64_sys_io_submit+0xa8>
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.31 :   ffff8000102d9774:       ldr     x3, [x0]
         :                      __range_ok():
    0.00 :   ffff8000102d9778:       mov     x0, x1
    0.00 :   ffff8000102d977c:       tbz     w3, #26, ffff8000102d9788 <__arm64_sys_io_submit+0xb0>
         :                      sign_extend64():
         :                      * @index: 0 based bit index (0<=index<64) to sign bit
         :                      */
         :                      static inline __s64 sign_extend64(__u64 value, int index)
         :                      {
         :                      __u8 shift = 63 - index;
         :                      return (__s64)(value << shift) >> shift;
    0.00 :   ffff8000102d9780:       sbfx    x0, x1, #0, #56
         :                      __range_ok():
         :                      addr = untagged_addr(addr);
    0.00 :   ffff8000102d9784:       and     x0, x1, x0
         :
         :                      __chk_user_ptr(addr);
         :                      asm volatile(
    1.02 :   ffff8000102d9788:       adds    x0, x0, #0x8
    0.00 :   ffff8000102d978c:       csel    x2, xzr, x2, hi  // hi = pmore
    0.00 :   ffff8000102d9790:       csinv   x0, x0, xzr, cc  // cc = lo, ul, last
    0.00 :   ffff8000102d9794:       sbcs    xzr, x0, x2
    0.20 :   ffff8000102d9798:       cset    x0, ls  // ls = plast
         :                      __do_sys_io_submit():
    0.00 :   ffff8000102d979c:       cbnz    x0, ffff8000102d9840 <__arm64_sys_io_submit+0x168>
         :                      ret = -EFAULT;
    0.00 :   ffff8000102d97a0:       mov     x26, #0xfffffffffffffff2        // #-14
         :
         :                      ret = io_submit_one(ctx, user_iocb, false);
         :                      if (ret)
         :                      break;
         :                      }
         :                      if (nr > AIO_PLUG_THRESHOLD)
    0.00 :   ffff8000102d97a4:       cmp     x20, #0x2
    0.00 :   ffff8000102d97a8:       b.gt    ffff8000102d98a0 <__arm64_sys_io_submit+0x1c8>
    1.73 :   ffff8000102d97ac:       ldr     x24, [x29, #56]
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff8000102d97b0:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.92 :   ffff8000102d97b4:       ldr     x0, [x21, #8]
         :                      __ref_is_percpu():
         :                      * Theoretically, the following could test just ATOMIC; however,
         :                      * then we'd have to mask off DEAD separately as DEAD may be
         :                      * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
         :                      * implies ATOMIC anyway.  Test them together.
         :                      */
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff8000102d97b8:       tst     x0, #0x3
    0.00 :   ffff8000102d97bc:       b.ne    ffff8000102d98d8 <__arm64_sys_io_submit+0x200>  // b.any
         :                      get_current():
   20.75 :   ffff8000102d97c0:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.82 :   ffff8000102d97c4:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff8000102d97c8:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.31 :   ffff8000102d97cc:       str     w2, [x1, #16]
         :                      __percpu_add_case_64():
         :
         :                      PERCPU_RW_OPS(8)
         :                      PERCPU_RW_OPS(16)
         :                      PERCPU_RW_OPS(32)
         :                      PERCPU_RW_OPS(64)
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff8000102d97d0:       mov     x3, #0xffffffffffffffff         // #-1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.92 :   ffff8000102d97d4:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff8000102d97d8:       add     x0, x0, x2
    0.81 :   ffff8000102d97dc:       ldxr    x5, [x0]
   23.88 :   ffff8000102d97e0:       add     x5, x5, x3
    0.00 :   ffff8000102d97e4:       stxr    w4, x5, [x0]
    0.00 :   ffff8000102d97e8:       cbnz    w4, ffff8000102d97dc <__arm64_sys_io_submit+0x104>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d97ec:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000102d97f0:       add     x0, x0, x3
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    2.95 :   ffff8000102d97f4:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102d97f8:       cbnz    x0, ffff8000102d98b0 <__arm64_sys_io_submit+0x1d8>
         :                      percpu_ref_put_many():
         :                      unsigned long __percpu *percpu_count;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count))
         :                      this_cpu_sub(*percpu_count, nr);
    0.00 :   ffff8000102d97fc:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.71 :   ffff8000102d9800:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      __do_sys_io_submit():
         :                      blk_finish_plug(&plug);
         :
         :                      percpu_ref_put(&ctx->users);
         :                      return i ? i : ret;
    0.10 :   ffff8000102d9804:       cmp     w19, #0x0
    0.00 :   ffff8000102d9808:       csel    x25, x25, x26, ne  // ne = any
    0.91 :   ffff8000102d980c:       ldr     x26, [x29, #72]
    1.01 :   ffff8000102d9810:       ldp     x20, x21, [x29, #24]
         :                      __arm64_sys_io_submit():
         :                      SYSCALL_DEFINE3(io_submit, aio_context_t, ctx_id, long, nr,
    0.00 :   ffff8000102d9814:       add     x23, x23, #0x8c8
    0.00 :   ffff8000102d9818:       mov     x0, x25
    0.30 :   ffff8000102d981c:       ldr     x2, [x29, #120]
    0.10 :   ffff8000102d9820:       ldr     x1, [x23]
    0.00 :   ffff8000102d9824:       eor     x1, x2, x1
    0.00 :   ffff8000102d9828:       cbnz    x1, ffff8000102d9914 <__arm64_sys_io_submit+0x23c>
    0.81 :   ffff8000102d982c:       ldr     x19, [sp, #16]
    1.02 :   ffff8000102d9830:       ldp     x22, x23, [sp, #40]
    0.20 :   ffff8000102d9834:       ldr     x25, [sp, #64]
    0.00 :   ffff8000102d9838:       ldp     x29, x30, [sp], #128
    0.00 :   ffff8000102d983c:       ret
         :                      sign_extend64():
    0.20 :   ffff8000102d9840:       sbfx    x0, x1, #0, #56
         :                      get_current():
    0.61 :   ffff8000102d9844:       mrs     x2, sp_el0
         :                      __uaccess_mask_ptr():
         :                      asm volatile(
         :                      "       bics    xzr, %3, %2\n"
         :                      "       csel    %0, %1, xzr, eq\n"
         :                      : "=&r" (safe_ptr)
         :                      : "r" (ptr), "r" (current_thread_info()->addr_limit),
         :                      "r" (untagged_addr(ptr))
    1.11 :   ffff8000102d9848:       and     x0, x1, x0
         :                      asm volatile(
    0.00 :   ffff8000102d984c:       ldr     x3, [x2, #8]
    0.00 :   ffff8000102d9850:       bics    xzr, x0, x3
    0.00 :   ffff8000102d9854:       csel    x2, x1, xzr, eq  // eq = none
         :                      : "cc");
         :
         :                      csdb();
    0.00 :   ffff8000102d9858:       csdb
         :                      uaccess_enable_not_uao():
         :                      __uaccess_enable(ARM64_ALT_PAN_NOT_UAO);
    0.81 :   ffff8000102d985c:       nop
         :                      __do_sys_io_submit():
         :                      if (unlikely(get_user(user_iocb, iocbpp + i))) {
    0.20 :   ffff8000102d9860:       mov     w0, w24
    0.51 :   ffff8000102d9864:       ldr     x1, [x2]
         :                      uaccess_disable_not_uao():
         :                      __uaccess_disable(ARM64_ALT_PAN_NOT_UAO);
    0.00 :   ffff8000102d9868:       nop
         :                      __do_sys_io_submit():
    2.04 :   ffff8000102d986c:       cbnz    w0, ffff8000102d97a0 <__arm64_sys_io_submit+0xc8>
         :                      ret = io_submit_one(ctx, user_iocb, false);
    0.20 :   ffff8000102d9870:       mov     w2, #0x0                        // #0
    0.00 :   ffff8000102d9874:       mov     x0, x21
    0.00 :   ffff8000102d9878:       bl      ffff8000102d7f50 <io_submit_one>
    1.82 :   ffff8000102d987c:       sxtw    x26, w0
         :                      if (ret)
    0.30 :   ffff8000102d9880:       cbnz    x26, ffff8000102d97a4 <__arm64_sys_io_submit+0xcc>
         :                      for (i = 0; i < nr; i++) {
    0.50 :   ffff8000102d9884:       add     w19, w19, #0x1
    0.00 :   ffff8000102d9888:       sxtw    x25, w19
    0.00 :   ffff8000102d988c:       mov     x1, x25
    0.00 :   ffff8000102d9890:       cmp     x20, x25
    1.12 :   ffff8000102d9894:       b.gt    ffff8000102d9760 <__arm64_sys_io_submit+0x88>
         :                      if (nr > AIO_PLUG_THRESHOLD)
    0.51 :   ffff8000102d9898:       cmp     x20, #0x2
    0.00 :   ffff8000102d989c:       b.le    ffff8000102d97ac <__arm64_sys_io_submit+0xd4>
         :                      blk_finish_plug(&plug);
    0.00 :   ffff8000102d98a0:       add     x0, x29, #0x50
    0.00 :   ffff8000102d98a4:       bl      ffff8000104562e8 <blk_finish_plug>
    0.00 :   ffff8000102d98a8:       ldr     x24, [x29, #56]
    0.00 :   ffff8000102d98ac:       b       ffff8000102d97b0 <__arm64_sys_io_submit+0xd8>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d98b0:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102d98b4:       cbz     x0, ffff8000102d97fc <__arm64_sys_io_submit+0x124>
    2.23 :   ffff8000102d98b8:       b       ffff8000102d9800 <__arm64_sys_io_submit+0x128>
         :                      __do_sys_io_submit():
         :                      blk_start_plug(&plug);
    0.00 :   ffff8000102d98bc:       add     x0, x29, #0x50
    0.00 :   ffff8000102d98c0:       str     x24, [x29, #56]
    0.00 :   ffff8000102d98c4:       bl      ffff800010453550 <blk_start_plug>
    0.00 :   ffff8000102d98c8:       b       ffff8000102d9750 <__arm64_sys_io_submit+0x78>
    0.00 :   ffff8000102d98cc:       ldr     x21, [x29, #32]
         :                      return -EINVAL;
    0.00 :   ffff8000102d98d0:       mov     x25, #0xffffffffffffffea        // #-22
    0.00 :   ffff8000102d98d4:       b       ffff8000102d9814 <__arm64_sys_io_submit+0x13c>
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000102d98d8:       b       ffff8000102d9904 <__arm64_sys_io_submit+0x22c>
    0.00 :   ffff8000102d98dc:       b       ffff8000102d9904 <__arm64_sys_io_submit+0x22c>
         :                      __lse_atomic64_sub_return():
         :                      }
         :
         :                      ATOMIC64_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC64_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff8000102d98e0:       mov     x0, #0x1                        // #1
    0.00 :   ffff8000102d98e4:       neg     x0, x0
    0.00 :   ffff8000102d98e8:       ldaddal x0, x1, [x21]
    0.00 :   ffff8000102d98ec:       add     x0, x0, x1
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff8000102d98f0:       cbnz    x0, ffff8000102d9800 <__arm64_sys_io_submit+0x128>
         :                      ref->release(ref);
    0.00 :   ffff8000102d98f4:       ldr     x1, [x21, #16]
    0.00 :   ffff8000102d98f8:       mov     x0, x21
    0.00 :   ffff8000102d98fc:       blr     x1
    0.00 :   ffff8000102d9900:       b       ffff8000102d9800 <__arm64_sys_io_submit+0x128>
         :                      __ll_sc_atomic64_sub_return():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(add, add, I)
         :                      ATOMIC64_OPS(sub, sub, J)
    0.00 :   ffff8000102d9904:       mov     x1, #0x1                        // #1
    0.00 :   ffff8000102d9908:       b       ffff8000102da7a8 <__arm64_compat_sys_io_pgetevents_time64+0x4e8>
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff8000102d990c:       cbnz    x0, ffff8000102d9800 <__arm64_sys_io_submit+0x128>
    0.00 :   ffff8000102d9910:       b       ffff8000102d98f4 <__arm64_sys_io_submit+0x21c>
    0.00 :   ffff8000102d9914:       stp     x20, x21, [x29, #24]
    0.00 :   ffff8000102d9918:       str     x24, [x29, #56]
    0.00 :   ffff8000102d991c:       str     x26, [x29, #72]
         :                      __arm64_sys_io_submit():
         :                      SYSCALL_DEFINE3(io_submit, aio_context_t, ctx_id, long, nr,
    0.00 :   ffff8000102d9920:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (963 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010466180 <__blk_mq_sched_bio_merge>:
         :                      __blk_mq_sched_bio_merge():
         :                      return false;
         :                      }
         :
         :                      bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio,
         :                      unsigned int nr_segs)
         :                      {
    9.65 :   ffff800010466180:       stp     x29, x30, [sp, #-64]!
         :                      blk_mq_get_ctx():
         :                      * care about preemption, since we know the ctx's are persistent. This does
         :                      * mean that we can't rely on ctx always matching the currently running CPU.
         :                      */
         :                      static inline struct blk_mq_ctx *blk_mq_get_ctx(struct request_queue *q)
         :                      {
         :                      return __blk_mq_get_ctx(q, raw_smp_processor_id());
    0.00 :   ffff800010466184:       adrp    x3, ffff8000114ca000 <bp_hardening_data>
    0.00 :   ffff800010466188:       add     x3, x3, #0x18
         :                      __blk_mq_get_ctx():
         :                      return per_cpu_ptr(q->queue_ctx, cpu);
    0.00 :   ffff80001046618c:       adrp    x5, ffff800011899000 <page_wait_table+0x1500>
         :                      __blk_mq_sched_bio_merge():
   15.92 :   ffff800010466190:       mov     x29, sp
    0.00 :   ffff800010466194:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010466198:       mov     x20, x0
    0.84 :   ffff80001046619c:       stp     x21, x22, [sp, #32]
         :                      blk_mq_get_ctx():
         :                      return __blk_mq_get_ctx(q, raw_smp_processor_id());
    0.00 :   ffff8000104661a0:       mov     x0, x3
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    2.08 :   ffff8000104661a4:       mrs     x3, tpidr_el1
         :                      __blk_mq_get_ctx():
         :                      return per_cpu_ptr(q->queue_ctx, cpu);
    0.00 :   ffff8000104661a8:       ldr     w8, [x0, x3]
    0.00 :   ffff8000104661ac:       add     x5, x5, #0x8e8
         :                      __blk_mq_sched_bio_merge():
         :                      struct elevator_queue *e = q->elevator;
         :                      struct blk_mq_ctx *ctx = blk_mq_get_ctx(q);
         :                      struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, bio->bi_opf, ctx);
    7.45 :   ffff8000104661b0:       ldr     w6, [x1, #16]
    0.00 :   ffff8000104661b4:       mov     w4, #0x2                        // #2
         :                      struct elevator_queue *e = q->elevator;
    0.00 :   ffff8000104661b8:       ldr     x7, [x20, #8]
         :                      __blk_mq_get_ctx():
    0.00 :   ffff8000104661bc:       ldr     x0, [x20, #56]
    0.00 :   ffff8000104661c0:       ldr     x19, [x5, x8, lsl #3]
    0.00 :   ffff8000104661c4:       add     x19, x0, x19
         :                      blk_mq_map_queue():
         :                      if (flags & REQ_HIPRI)
    0.00 :   ffff8000104661c8:       tbnz    w6, #25, ffff8000104661d8 <__blk_mq_sched_bio_merge+0x58>
         :                      else if ((flags & REQ_OP_MASK) == REQ_OP_READ)
    8.71 :   ffff8000104661cc:       and     w6, w6, #0xff
    0.00 :   ffff8000104661d0:       cmp     w6, #0x0
    0.00 :   ffff8000104661d4:       cset    w4, eq  // eq = none
         :                      return ctx->hctxs[type];
    0.00 :   ffff8000104661d8:       ubfiz   x4, x4, #3, #2
    0.00 :   ffff8000104661dc:       add     x4, x19, x4
    0.00 :   ffff8000104661e0:       ldr     x21, [x4, #80]
         :                      __blk_mq_sched_bio_merge():
         :                      bool ret = false;
         :                      enum hctx_type type;
         :
         :                      if (e && e->type->ops.bio_merge)
    0.00 :   ffff8000104661e4:       cbz     x7, ffff800010466214 <__blk_mq_sched_bio_merge+0x94>
    0.00 :   ffff8000104661e8:       ldr     x0, [x7]
    0.00 :   ffff8000104661ec:       ldr     x4, [x0, #56]
    0.00 :   ffff8000104661f0:       cbz     x4, ffff800010466214 <__blk_mq_sched_bio_merge+0x94>
         :                      return e->type->ops.bio_merge(hctx, bio, nr_segs);
    0.00 :   ffff8000104661f4:       mov     x0, x21
    0.00 :   ffff8000104661f8:       blr     x4
    0.00 :   ffff8000104661fc:       and     w22, w0, #0xff
         :                      ret = blk_mq_attempt_merge(q, hctx, ctx, bio, nr_segs);
         :                      spin_unlock(&ctx->lock);
         :                      }
         :
         :                      return ret;
         :                      }
    1.35 :   ffff800010466200:       mov     w0, w22
    0.00 :   ffff800010466204:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010466208:       ldp     x21, x22, [sp, #32]
    0.21 :   ffff80001046620c:       ldp     x29, x30, [sp], #64
    0.00 :   ffff800010466210:       ret
         :                      if ((hctx->flags & BLK_MQ_F_SHOULD_MERGE) &&
   22.98 :   ffff800010466214:       ldr     x0, [x21, #192]
         :                      bool ret = false;
    0.00 :   ffff800010466218:       mov     w22, #0x0                       // #0
         :                      if ((hctx->flags & BLK_MQ_F_SHOULD_MERGE) &&
    0.00 :   ffff80001046621c:       tbz     w0, #0, ffff800010466200 <__blk_mq_sched_bio_merge+0x80>
   15.38 :   ffff800010466220:       ldrh    w0, [x21, #268]
    0.00 :   ffff800010466224:       lsl     x0, x0, #4
    0.00 :   ffff800010466228:       add     x3, x19, x0
         :                      !list_empty_careful(&ctx->rq_lists[type])) {
    0.00 :   ffff80001046622c:       add     x0, x0, #0x8
    3.93 :   ffff800010466230:       add     x0, x19, x0
         :                      list_empty_careful():
         :                      * if another CPU could re-list_add() it.
         :                      */
         :                      static inline int list_empty_careful(const struct list_head *head)
         :                      {
         :                      struct list_head *next = head->next;
         :                      return (next == head) && (next == head->prev);
    0.00 :   ffff800010466234:       ldr     x4, [x3, #8]
    0.00 :   ffff800010466238:       cmp     x0, x4
    0.00 :   ffff80001046623c:       b.eq    ffff8000104662b0 <__blk_mq_sched_bio_merge+0x130>  // b.none
         :                      spin_lock():
         :                      raw_spin_lock_init(&(_lock)->rlock);            \
         :                      } while (0)
         :
         :                      static __always_inline void spin_lock(spinlock_t *lock)
         :                      {
         :                      raw_spin_lock(&lock->rlock);
    0.00 :   ffff800010466240:       mov     x0, x19
    0.00 :   ffff800010466244:       str     w2, [x29, #52]
    0.00 :   ffff800010466248:       str     x1, [x29, #56]
    0.00 :   ffff80001046624c:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      blk_mq_attempt_merge():
         :                      if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio, nr_segs)) {
    0.00 :   ffff800010466250:       ldrh    w4, [x21, #268]
    0.00 :   ffff800010466254:       mov     x5, #0x8                        // #8
    0.00 :   ffff800010466258:       ldr     w2, [x29, #52]
    0.00 :   ffff80001046625c:       mov     x0, x20
    0.00 :   ffff800010466260:       ldr     x1, [x29, #56]
         :                      return false;
    0.00 :   ffff800010466264:       mov     w22, #0x0                       // #0
         :                      if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio, nr_segs)) {
    0.00 :   ffff800010466268:       mov     w3, w2
    0.00 :   ffff80001046626c:       mov     x2, x1
    0.00 :   ffff800010466270:       add     x1, x5, w4, uxtw #4
    0.00 :   ffff800010466274:       add     x1, x19, x1
    0.00 :   ffff800010466278:       bl      ffff800010465c00 <blk_mq_bio_list_merge>
    0.00 :   ffff80001046627c:       tst     w0, #0xff
    0.00 :   ffff800010466280:       b.eq    ffff800010466294 <__blk_mq_sched_bio_merge+0x114>  // b.none
         :                      ctx->rq_merged++;
    0.00 :   ffff800010466284:       ldr     x0, [x19, #120]
         :                      return true;
    0.00 :   ffff800010466288:       mov     w22, #0x1                       // #1
         :                      ctx->rq_merged++;
    0.00 :   ffff80001046628c:       add     x0, x0, #0x1
    0.00 :   ffff800010466290:       str     x0, [x19, #120]
         :                      spin_unlock():
         :                      raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
         :                      } while (0)
         :
         :                      static __always_inline void spin_unlock(spinlock_t *lock)
         :                      {
         :                      raw_spin_unlock(&lock->rlock);
    0.00 :   ffff800010466294:       mov     x0, x19
    0.00 :   ffff800010466298:       bl      ffff800010cb2408 <_raw_spin_unlock>
         :                      __blk_mq_sched_bio_merge():
         :                      }
    0.00 :   ffff80001046629c:       mov     w0, w22
    0.00 :   ffff8000104662a0:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000104662a4:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000104662a8:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000104662ac:       ret
         :                      list_empty_careful():
    9.75 :   ffff8000104662b0:       ldr     x3, [x3, #16]
    0.00 :   ffff8000104662b4:       cmp     x0, x3
    0.00 :   ffff8000104662b8:       b.ne    ffff800010466240 <__blk_mq_sched_bio_merge+0xc0>  // b.any
    1.76 :   ffff8000104662bc:       b       ffff800010466200 <__blk_mq_sched_bio_merge+0x80>
 Percent |	Source code & Disassembly of vmlinux for cycles (866 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102d5708 <aio_prep_rw>:
         :                      aio_prep_rw():
         :                      iocb->ki_res.res2 = res2;
         :                      iocb_put(iocb);
         :                      }
         :
         :                      static int aio_prep_rw(struct kiocb *req, const struct iocb *iocb)
         :                      {
    1.63 :   ffff8000102d5708:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff8000102d570c:       mov     x29, sp
    6.01 :   ffff8000102d5710:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000102d5714:       mov     x19, x0
         :                      int ret;
         :
         :                      req->ki_complete = aio_complete_rw;
    0.00 :   ffff8000102d5718:       adrp    x0, ffff8000102d7000 <aio_fsync_work+0x290>
    0.00 :   ffff8000102d571c:       add     x0, x0, #0xb8
         :                      {
    1.84 :   ffff8000102d5720:       mov     x20, x1
         :                      req->private = NULL;
    0.00 :   ffff8000102d5724:       stp     x0, xzr, [x19, #16]
         :                      req->ki_pos = iocb->aio_offset;
         :                      req->ki_flags = iocb_flags(req->ki_filp);
    1.96 :   ffff8000102d5728:       ldr     x2, [x19]
         :                      req->ki_pos = iocb->aio_offset;
    0.11 :   ffff8000102d572c:       ldr     x0, [x1, #40]
    0.11 :   ffff8000102d5730:       str     x0, [x19, #8]
         :                      req->ki_flags = iocb_flags(req->ki_filp);
    2.42 :   ffff8000102d5734:       ldr     w1, [x2, #64]
         :                      iocb_flags():
         :                      return true;
         :                      }
         :
         :                      static inline int iocb_flags(struct file *file)
         :                      {
         :                      int res = 0;
    0.00 :   ffff8000102d5738:       ubfx    x0, x1, #10, #1
         :                      if (file->f_flags & O_APPEND)
         :                      res |= IOCB_APPEND;
         :                      if (io_is_direct(file))
         :                      res |= IOCB_DIRECT;
    0.00 :   ffff8000102d573c:       tst     x1, #0x10000
         :                      int res = 0;
    1.15 :   ffff8000102d5740:       lsl     w0, w0, #1
         :                      res |= IOCB_DIRECT;
    0.81 :   ffff8000102d5744:       orr     w3, w0, #0x4
    0.00 :   ffff8000102d5748:       csel    w0, w3, w0, ne  // ne = any
         :                      if ((file->f_flags & O_DSYNC) || IS_SYNC(file->f_mapping->host))
    0.00 :   ffff8000102d574c:       tbnz    w1, #12, ffff8000102d5764 <aio_prep_rw+0x5c>
    0.12 :   ffff8000102d5750:       ldr     x3, [x2, #240]
    4.83 :   ffff8000102d5754:       ldr     x3, [x3]
    5.76 :   ffff8000102d5758:       ldr     x4, [x3, #40]
    3.93 :   ffff8000102d575c:       ldr     x4, [x4, #80]
    0.00 :   ffff8000102d5760:       tbz     w4, #4, ffff8000102d5868 <aio_prep_rw+0x160>
         :                      res |= IOCB_DSYNC;
    0.00 :   ffff8000102d5764:       orr     w0, w0, #0x10
         :                      if (file->f_flags & __O_SYNC)
         :                      res |= IOCB_SYNC;
    1.05 :   ffff8000102d5768:       tst     x1, #0x100000
    0.00 :   ffff8000102d576c:       orr     w3, w0, #0x20
    0.00 :   ffff8000102d5770:       csel    w0, w3, w0, ne  // ne = any
         :                      aio_prep_rw():
    0.47 :   ffff8000102d5774:       str     w0, [x19, #32]
         :                      if (iocb->aio_flags & IOCB_FLAG_RESFD)
    1.62 :   ffff8000102d5778:       ldr     w1, [x20, #56]
    0.00 :   ffff8000102d577c:       tbz     w1, #0, ffff8000102d5788 <aio_prep_rw+0x80>
         :                      req->ki_flags |= IOCB_EVENTFD;
    0.00 :   ffff8000102d5780:       orr     w0, w0, #0x1
    0.00 :   ffff8000102d5784:       str     w0, [x19, #32]
         :                      file_write_hint():
         :                      if (file->f_write_hint != WRITE_LIFE_NOT_SET)
    4.05 :   ffff8000102d5788:       ldr     w0, [x2, #52]
    0.00 :   ffff8000102d578c:       cbnz    w0, ffff8000102d582c <aio_prep_rw+0x124>
         :                      return file_inode(file)->i_write_hint;
    6.37 :   ffff8000102d5790:       ldr     x0, [x2, #32]
    4.40 :   ffff8000102d5794:       ldrb    w0, [x0, #143]
         :                      ki_hint_validate():
         :                      return hint;
    0.00 :   ffff8000102d5798:       and     w1, w0, #0xffff
         :                      aio_prep_rw():
         :                      req->ki_hint = ki_hint_validate(file_write_hint(req->ki_filp));
    6.94 :   ffff8000102d579c:       strh    w1, [x19, #36]
         :                      if (iocb->aio_flags & IOCB_FLAG_IOPRIO) {
    0.00 :   ffff8000102d57a0:       ldr     w0, [x20, #56]
    0.00 :   ffff8000102d57a4:       tbnz    w0, #1, ffff8000102d5848 <aio_prep_rw+0x140>
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.23 :   ffff8000102d57a8:       mrs     x0, sp_el0
         :                      get_current_ioprio():
         :                      * If the calling process has set an I/O priority, use that. Otherwise, return
         :                      * the default I/O priority.
         :                      */
         :                      static inline int get_current_ioprio(void)
         :                      {
         :                      struct io_context *ioc = current->io_context;
    0.00 :   ffff8000102d57ac:       ldr     x1, [x0, #1864]
         :
         :                      if (ioc)
    0.00 :   ffff8000102d57b0:       mov     w0, #0x0                        // #0
    0.00 :   ffff8000102d57b4:       cbz     x1, ffff8000102d57bc <aio_prep_rw+0xb4>
         :                      return ioc->ioprio;
    1.39 :   ffff8000102d57b8:       ldrh    w0, [x1, #20]
         :                      aio_prep_rw():
         :                      return ret;
         :                      }
         :
         :                      req->ki_ioprio = iocb->aio_reqprio;
         :                      } else
         :                      req->ki_ioprio = get_current_ioprio();
    8.20 :   ffff8000102d57bc:       strh    w0, [x19, #38]
         :
         :                      ret = kiocb_set_rw_flags(req, iocb->aio_rw_flags);
    0.57 :   ffff8000102d57c0:       ldr     w2, [x20, #12]
         :                      kiocb_set_rw_flags():
         :                      return res;
         :                      }
         :
         :                      static inline int kiocb_set_rw_flags(struct kiocb *ki, rwf_t flags)
         :                      {
         :                      if (unlikely(flags & ~RWF_SUPPORTED))
    0.00 :   ffff8000102d57c4:       ands    w0, w2, #0xffffffe0
    0.00 :   ffff8000102d57c8:       b.ne    ffff8000102d5874 <aio_prep_rw+0x16c>  // b.any
         :                      return -EOPNOTSUPP;
         :
         :                      if (flags & RWF_NOWAIT) {
    0.00 :   ffff8000102d57cc:       tbz     w2, #3, ffff8000102d5860 <aio_prep_rw+0x158>
         :                      if (!(ki->ki_filp->f_mode & FMODE_NOWAIT))
    0.00 :   ffff8000102d57d0:       ldr     x1, [x19]
    0.00 :   ffff8000102d57d4:       ldr     w1, [x1, #68]
    0.00 :   ffff8000102d57d8:       tbz     w1, #27, ffff8000102d5874 <aio_prep_rw+0x16c>
         :                      return -EOPNOTSUPP;
         :                      ki->ki_flags |= IOCB_NOWAIT;
    0.00 :   ffff8000102d57dc:       ldr     w1, [x19, #32]
    0.00 :   ffff8000102d57e0:       orr     w1, w1, #0x80
    0.00 :   ffff8000102d57e4:       str     w1, [x19, #32]
         :                      }
         :                      if (flags & RWF_HIPRI)
    4.98 :   ffff8000102d57e8:       tbz     w2, #0, ffff8000102d57f4 <aio_prep_rw+0xec>
         :                      ki->ki_flags |= IOCB_HIPRI;
    0.00 :   ffff8000102d57ec:       orr     w1, w1, #0x8
    0.00 :   ffff8000102d57f0:       str     w1, [x19, #32]
         :                      if (flags & RWF_DSYNC)
    0.12 :   ffff8000102d57f4:       tbz     w2, #1, ffff8000102d5800 <aio_prep_rw+0xf8>
         :                      ki->ki_flags |= IOCB_DSYNC;
    0.00 :   ffff8000102d57f8:       orr     w1, w1, #0x10
    0.00 :   ffff8000102d57fc:       str     w1, [x19, #32]
         :                      if (flags & RWF_SYNC)
    0.23 :   ffff8000102d5800:       tbz     w2, #2, ffff8000102d580c <aio_prep_rw+0x104>
         :                      ki->ki_flags |= (IOCB_DSYNC | IOCB_SYNC);
    0.00 :   ffff8000102d5804:       orr     w1, w1, #0x30
    0.00 :   ffff8000102d5808:       str     w1, [x19, #32]
         :                      if (flags & RWF_APPEND)
         :                      ki->ki_flags |= IOCB_APPEND;
    0.46 :   ffff8000102d580c:       orr     w3, w1, #0x2
    0.00 :   ffff8000102d5810:       tst     x2, #0x10
    0.00 :   ffff8000102d5814:       csel    w1, w3, w1, ne  // ne = any
         :                      aio_prep_rw():
         :                      if (unlikely(ret))
         :                      return ret;
         :
         :                      req->ki_flags &= ~IOCB_HIPRI; /* no one is going to poll for this I/O */
    0.00 :   ffff8000102d5818:       and     w1, w1, #0xfffffff7
    1.39 :   ffff8000102d581c:       str     w1, [x19, #32]
         :                      return 0;
         :                      }
    5.21 :   ffff8000102d5820:       ldp     x19, x20, [sp, #16]
    1.96 :   ffff8000102d5824:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000102d5828:       ret
         :                      ki_hint_validate():
         :                      if (hint <= max_hint)
    0.00 :   ffff8000102d582c:       mov     w2, #0xffff                     // #65535
         :                      return 0;
    0.00 :   ffff8000102d5830:       mov     w1, #0x0                        // #0
         :                      if (hint <= max_hint)
    0.00 :   ffff8000102d5834:       cmp     w0, w2
    0.00 :   ffff8000102d5838:       b.ls    ffff8000102d5798 <aio_prep_rw+0x90>  // b.plast
         :                      aio_prep_rw():
         :                      req->ki_hint = ki_hint_validate(file_write_hint(req->ki_filp));
    0.00 :   ffff8000102d583c:       strh    w1, [x19, #36]
         :                      if (iocb->aio_flags & IOCB_FLAG_IOPRIO) {
    0.00 :   ffff8000102d5840:       ldr     w0, [x20, #56]
    0.00 :   ffff8000102d5844:       tbz     w0, #1, ffff8000102d57a8 <aio_prep_rw+0xa0>
         :                      ret = ioprio_check_cap(iocb->aio_reqprio);
    0.00 :   ffff8000102d5848:       ldrsh   w0, [x20, #18]
    0.00 :   ffff8000102d584c:       bl      ffff80001046c338 <ioprio_check_cap>
         :                      if (ret) {
    0.00 :   ffff8000102d5850:       cbnz    w0, ffff8000102d5820 <aio_prep_rw+0x118>
         :                      req->ki_ioprio = iocb->aio_reqprio;
    0.00 :   ffff8000102d5854:       ldrh    w0, [x20, #18]
    0.00 :   ffff8000102d5858:       strh    w0, [x19, #38]
    0.00 :   ffff8000102d585c:       b       ffff8000102d57c0 <aio_prep_rw+0xb8>
    6.24 :   ffff8000102d5860:       ldr     w1, [x19, #32]
    0.00 :   ffff8000102d5864:       b       ffff8000102d57e8 <aio_prep_rw+0xe0>
         :                      iocb_flags():
         :                      if ((file->f_flags & O_DSYNC) || IS_SYNC(file->f_mapping->host))
   13.33 :   ffff8000102d5868:       ldr     w3, [x3, #12]
    0.12 :   ffff8000102d586c:       tbz     w3, #0, ffff8000102d5768 <aio_prep_rw+0x60>
    0.00 :   ffff8000102d5870:       b       ffff8000102d5764 <aio_prep_rw+0x5c>
         :                      kiocb_set_rw_flags():
         :                      return -EOPNOTSUPP;
    0.00 :   ffff8000102d5874:       mov     w0, #0xffffffa1                 // #-95
    0.00 :   ffff8000102d5878:       b       ffff8000102d5820 <aio_prep_rw+0x118>
 Percent |	Source code & Disassembly of vmlinux for cycles (884 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010466420 <blk_mq_sched_insert_requests>:
         :                      blk_mq_sched_insert_requests():
         :                      }
         :
         :                      void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
         :                      struct blk_mq_ctx *ctx,
         :                      struct list_head *list, bool run_queue_async)
         :                      {
    0.11 :   ffff800010466420:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff800010466424:       mov     x29, sp
   10.39 :   ffff800010466428:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001046642c:       mov     x19, x0
    1.58 :   ffff800010466430:       stp     x21, x22, [sp, #32]
    0.00 :   ffff800010466434:       mov     x21, x2
    1.58 :   ffff800010466438:       stp     x23, x24, [sp, #48]
    0.00 :   ffff80001046643c:       and     w22, w3, #0xff
    1.02 :   ffff800010466440:       mov     x24, x1
         :                      struct elevator_queue *e;
         :                      struct request_queue *q = hctx->queue;
    0.69 :   ffff800010466444:       ldr     x20, [x0, #208]
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff800010466448:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      blk_mq_sched_insert_requests():
         :                      /*
         :                      * blk_mq_sched_insert_requests() is called from flush plug
         :                      * context only, and hold one usage counter to prevent queue
         :                      * from being released.
         :                      */
         :                      percpu_ref_get(&q->q_usage_counter);
    0.34 :   ffff80001046644c:       add     x23, x20, #0x5c0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010466450:       ldr     x0, [x20, #1480]
         :                      __ref_is_percpu():
         :                      * Theoretically, the following could test just ATOMIC; however,
         :                      * then we'd have to mask off DEAD separately as DEAD may be
         :                      * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
         :                      * implies ATOMIC anyway.  Test them together.
         :                      */
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff800010466454:       tst     x0, #0x3
    0.00 :   ffff800010466458:       b.ne    ffff8000104665b0 <blk_mq_sched_insert_requests+0x190>  // b.any
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.79 :   ffff80001046645c:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.68 :   ffff800010466460:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff800010466464:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    2.49 :   ffff800010466468:       str     w2, [x1, #16]
         :                      __percpu_add_case_64():
         :
         :                      PERCPU_RW_OPS(8)
         :                      PERCPU_RW_OPS(16)
         :                      PERCPU_RW_OPS(32)
         :                      PERCPU_RW_OPS(64)
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff80001046646c:       mov     x3, #0x1                        // #1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.45 :   ffff800010466470:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff800010466474:       add     x0, x0, x2
    0.45 :   ffff800010466478:       ldxr    x5, [x0]
   16.74 :   ffff80001046647c:       add     x5, x5, x3
    0.00 :   ffff800010466480:       stxr    w4, x5, [x0]
    0.00 :   ffff800010466484:       cbnz    w4, ffff800010466478 <blk_mq_sched_insert_requests+0x58>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010466488:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff80001046648c:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    2.15 :   ffff800010466490:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800010466494:       cbnz    x0, ffff80001046655c <blk_mq_sched_insert_requests+0x13c>
         :                      percpu_ref_get_many():
         :                      unsigned long __percpu *percpu_count;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count))
         :                      this_cpu_add(*percpu_count, nr);
    0.00 :   ffff800010466498:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.00 :   ffff80001046649c:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_mq_sched_insert_requests():
         :
         :                      e = hctx->queue->elevator;
    0.00 :   ffff8000104664a0:       ldr     x0, [x19, #208]
    0.00 :   ffff8000104664a4:       ldr     x0, [x0, #8]
         :                      if (e && e->type->ops.insert_requests)
    0.00 :   ffff8000104664a8:       cbz     x0, ffff800010466574 <blk_mq_sched_insert_requests+0x154>
    0.00 :   ffff8000104664ac:       ldr     x0, [x0]
    0.00 :   ffff8000104664b0:       ldr     x3, [x0, #112]
    0.00 :   ffff8000104664b4:       cbz     x3, ffff800010466580 <blk_mq_sched_insert_requests+0x160>
         :                      e->type->ops.insert_requests(hctx, list, false);
    0.00 :   ffff8000104664b8:       mov     w2, #0x0                        // #0
    0.00 :   ffff8000104664bc:       mov     x1, x21
    0.00 :   ffff8000104664c0:       mov     x0, x19
    0.00 :   ffff8000104664c4:       blr     x3
         :                      goto out;
         :                      }
         :                      blk_mq_insert_requests(hctx, ctx, list);
         :                      }
         :
         :                      blk_mq_run_hw_queue(hctx, run_queue_async);
    0.00 :   ffff8000104664c8:       mov     w1, w22
    0.00 :   ffff8000104664cc:       mov     x0, x19
    0.00 :   ffff8000104664d0:       bl      ffff80001045ef88 <blk_mq_run_hw_queue>
         :                      rcu_read_lock():
         :                      __rcu_read_lock();
    2.84 :   ffff8000104664d4:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    1.01 :   ffff8000104664d8:       ldr     x0, [x20, #1480]
         :                      __ref_is_percpu():
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff8000104664dc:       tst     x0, #0x3
    0.00 :   ffff8000104664e0:       b.ne    ffff8000104665d0 <blk_mq_sched_insert_requests+0x1b0>  // b.any
         :                      get_current():
    0.23 :   ffff8000104664e4:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.45 :   ffff8000104664e8:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff8000104664ec:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    1.02 :   ffff8000104664f0:       str     w2, [x1, #16]
         :                      __percpu_add_case_64():
    0.00 :   ffff8000104664f4:       mov     x3, #0xffffffffffffffff         // #-1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.79 :   ffff8000104664f8:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.23 :   ffff8000104664fc:       add     x0, x0, x2
    0.11 :   ffff800010466500:       ldxr    x5, [x0]
   28.66 :   ffff800010466504:       add     x5, x5, x3
    0.00 :   ffff800010466508:       stxr    w4, x5, [x0]
    0.00 :   ffff80001046650c:       cbnz    w4, ffff800010466500 <blk_mq_sched_insert_requests+0xe0>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010466510:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800010466514:       add     x0, x0, x3
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    1.92 :   ffff800010466518:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff80001046651c:       cbnz    x0, ffff80001046653c <blk_mq_sched_insert_requests+0x11c>
         :                      percpu_ref_put_many():
         :                      unsigned long __percpu *percpu_count;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count))
         :                      this_cpu_sub(*percpu_count, nr);
    0.00 :   ffff800010466520:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      rcu_read_unlock():
         :                      __rcu_read_unlock();
    0.00 :   ffff800010466524:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_mq_sched_insert_requests():
         :                      out:
         :                      percpu_ref_put(&q->q_usage_counter);
         :                      }
    0.00 :   ffff800010466528:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001046652c:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010466530:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff800010466534:       ldp     x29, x30, [sp], #64
    0.00 :   ffff800010466538:       ret
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.23 :   ffff80001046653c:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.22 :   ffff800010466540:       cbz     x0, ffff800010466520 <blk_mq_sched_insert_requests+0x100>
         :                      rcu_read_unlock():
    2.26 :   ffff800010466544:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_mq_sched_insert_requests():
    0.00 :   ffff800010466548:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001046654c:       ldp     x21, x22, [sp, #32]
    0.79 :   ffff800010466550:       ldp     x23, x24, [sp, #48]
    0.23 :   ffff800010466554:       ldp     x29, x30, [sp], #64
    0.00 :   ffff800010466558:       ret
         :                      __read_once_size():
    0.45 :   ffff80001046655c:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    1.80 :   ffff800010466560:       cbz     x0, ffff800010466498 <blk_mq_sched_insert_requests+0x78>
         :                      rcu_read_unlock():
    4.86 :   ffff800010466564:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_mq_sched_insert_requests():
         :                      e = hctx->queue->elevator;
    0.11 :   ffff800010466568:       ldr     x0, [x19, #208]
    1.35 :   ffff80001046656c:       ldr     x0, [x0, #8]
         :                      if (e && e->type->ops.insert_requests)
    0.00 :   ffff800010466570:       cbnz    x0, ffff8000104664ac <blk_mq_sched_insert_requests+0x8c>
         :                      if (!hctx->dispatch_busy && !e && !run_queue_async) {
    3.96 :   ffff800010466574:       ldr     w0, [x19, #264]
    0.00 :   ffff800010466578:       cbnz    w0, ffff800010466580 <blk_mq_sched_insert_requests+0x160>
    0.79 :   ffff80001046657c:       cbz     w22, ffff800010466594 <blk_mq_sched_insert_requests+0x174>
         :                      blk_mq_insert_requests(hctx, ctx, list);
    0.00 :   ffff800010466580:       mov     x2, x21
    0.00 :   ffff800010466584:       mov     x1, x24
    0.00 :   ffff800010466588:       mov     x0, x19
    0.00 :   ffff80001046658c:       bl      ffff8000104612d0 <blk_mq_insert_requests>
    0.00 :   ffff800010466590:       b       ffff8000104664c8 <blk_mq_sched_insert_requests+0xa8>
         :                      blk_mq_try_issue_list_directly(hctx, list);
    0.11 :   ffff800010466594:       mov     x0, x19
    0.00 :   ffff800010466598:       mov     x1, x21
    0.00 :   ffff80001046659c:       bl      ffff8000104615d0 <blk_mq_try_issue_list_directly>
         :                      __read_once_size():
    1.13 :   ffff8000104665a0:       ldr     x0, [x21]
         :                      blk_mq_sched_insert_requests():
         :                      if (list_empty(list))
    0.00 :   ffff8000104665a4:       cmp     x21, x0
    0.00 :   ffff8000104665a8:       b.ne    ffff800010466580 <blk_mq_sched_insert_requests+0x160>  // b.any
         :                      out:
    4.96 :   ffff8000104665ac:       b       ffff8000104664d4 <blk_mq_sched_insert_requests+0xb4>
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000104665b0:       b       ffff8000104665c4 <blk_mq_sched_insert_requests+0x1a4>
    0.00 :   ffff8000104665b4:       b       ffff8000104665c4 <blk_mq_sched_insert_requests+0x1a4>
         :                      __lse_atomic64_add():
         :                      }
         :
         :                      ATOMIC64_OP(andnot, stclr)
         :                      ATOMIC64_OP(or, stset)
         :                      ATOMIC64_OP(xor, steor)
         :                      ATOMIC64_OP(add, stadd)
    0.00 :   ffff8000104665b8:       mov     x0, #0x1                        // #1
    0.00 :   ffff8000104665bc:       stadd   x0, [x23]
    0.00 :   ffff8000104665c0:       b       ffff80001046649c <blk_mq_sched_insert_requests+0x7c>
         :                      __ll_sc_atomic64_add():
         :                      ATOMIC64_FETCH_OP (, dmb ish,  , l, "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(add, add, I)
    0.00 :   ffff8000104665c4:       add     x2, x20, #0x5c0
    0.00 :   ffff8000104665c8:       b       ffff800010466954 <blk_mq_init_sched+0x234>
    0.00 :   ffff8000104665cc:       b       ffff80001046649c <blk_mq_sched_insert_requests+0x7c>
         :                      arch_static_branch_jump():
    0.00 :   ffff8000104665d0:       b       ffff800010466600 <blk_mq_sched_insert_requests+0x1e0>
    0.00 :   ffff8000104665d4:       b       ffff800010466600 <blk_mq_sched_insert_requests+0x1e0>
         :                      __lse_atomic64_sub_return():
         :                      }
         :
         :                      ATOMIC64_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC64_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff8000104665d8:       mov     x0, #0x1                        // #1
    0.00 :   ffff8000104665dc:       add     x2, x20, #0x5c0
    0.00 :   ffff8000104665e0:       neg     x0, x0
    0.00 :   ffff8000104665e4:       ldaddal x0, x1, [x2]
    0.00 :   ffff8000104665e8:       add     x0, x0, x1
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff8000104665ec:       cbnz    x0, ffff800010466524 <blk_mq_sched_insert_requests+0x104>
         :                      ref->release(ref);
    0.00 :   ffff8000104665f0:       ldr     x1, [x23, #16]
    0.00 :   ffff8000104665f4:       mov     x0, x23
    0.00 :   ffff8000104665f8:       blr     x1
    0.00 :   ffff8000104665fc:       b       ffff800010466524 <blk_mq_sched_insert_requests+0x104>
         :                      __ll_sc_atomic64_sub_return():
         :                      ATOMIC64_OPS(sub, sub, J)
    0.00 :   ffff800010466600:       mov     x1, #0x1                        // #1
    0.00 :   ffff800010466604:       add     x3, x20, #0x5c0
    0.00 :   ffff800010466608:       b       ffff80001046696c <blk_mq_init_sched+0x24c>
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff80001046660c:       cbnz    x0, ffff800010466524 <blk_mq_sched_insert_requests+0x104>
    0.00 :   ffff800010466610:       b       ffff8000104665f0 <blk_mq_sched_insert_requests+0x1d0>
 Percent |	Source code & Disassembly of vmlinux for cycles (853 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010454aa0 <generic_make_request>:
         :                      generic_make_request():
         :                      * bio happens to be merged with someone else, and may resubmit the bio to
         :                      * a lower device by calling into generic_make_request recursively, which
         :                      * means the bio should NOT be touched after the call to ->make_request_fn.
         :                      */
         :                      blk_qc_t generic_make_request(struct bio *bio)
         :                      {
    0.00 :   ffff800010454aa0:       stp     x29, x30, [sp, #-128]!
    0.00 :   ffff800010454aa4:       mov     x29, sp
    0.35 :   ffff800010454aa8:       stp     x21, x22, [sp, #32]
    0.00 :   ffff800010454aac:       adrp    x21, ffff800011899000 <page_wait_table+0x1500>
    0.35 :   ffff800010454ab0:       str     x26, [sp, #72]
    0.00 :   ffff800010454ab4:       add     x1, x21, #0x8c8
    0.00 :   ffff800010454ab8:       mov     x26, x0
    0.94 :   ffff800010454abc:       ldr     x2, [x1]
    0.00 :   ffff800010454ac0:       str     x2, [x29, #120]
    0.00 :   ffff800010454ac4:       mov     x2, #0x0                        // #0
         :                      * yet.
         :                      */
         :                      struct bio_list bio_list_on_stack[2];
         :                      blk_qc_t ret = BLK_QC_T_NONE;
         :
         :                      if (!generic_make_request_checks(bio))
    0.00 :   ffff800010454ac8:       bl      ffff8000104542f0 <generic_make_request_checks>
    1.29 :   ffff800010454acc:       tst     w0, #0xff
    0.00 :   ffff800010454ad0:       b.eq    ffff800010454d44 <generic_make_request+0x2a4>  // b.none
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.12 :   ffff800010454ad4:       mrs     x1, sp_el0
         :                      generic_make_request():
         :                      * flag to say if generic_make_request is currently active in this
         :                      * task or not.  If it is NULL, then no make_request is active.  If
         :                      * it is non-NULL, then a make_request is active, and new requests
         :                      * should be added at the tail
         :                      */
         :                      if (current->bio_list) {
    0.48 :   ffff800010454ad8:       ldr     x0, [x1, #1832]
    0.00 :   ffff800010454adc:       cbz     x0, ffff800010454b20 <generic_make_request+0x80>
         :                      bio_list_add():
         :                      return sz;
         :                      }
         :
         :                      static inline void bio_list_add(struct bio_list *bl, struct bio *bio)
         :                      {
         :                      bio->bi_next = NULL;
    0.00 :   ffff800010454ae0:       str     xzr, [x26]
         :
         :                      if (bl->tail)
    0.00 :   ffff800010454ae4:       ldr     x1, [x0, #8]
    0.00 :   ffff800010454ae8:       cbz     x1, ffff800010454d88 <generic_make_request+0x2e8>
         :                      bl->tail->bi_next = bio;
    0.00 :   ffff800010454aec:       str     x26, [x1]
         :                      generic_make_request():
         :                      blk_qc_t ret = BLK_QC_T_NONE;
    0.00 :   ffff800010454af0:       mov     w22, #0xffffffff                // #-1
         :                      bio_list_add():
         :                      else
         :                      bl->head = bio;
         :
         :                      bl->tail = bio;
    0.00 :   ffff800010454af4:       str     x26, [x0, #8]
         :                      generic_make_request():
         :                      } while (bio);
         :                      current->bio_list = NULL; /* deactivate */
         :
         :                      out:
         :                      return ret;
         :                      }
    0.12 :   ffff800010454af8:       add     x21, x21, #0x8c8
    0.00 :   ffff800010454afc:       mov     w0, w22
    1.17 :   ffff800010454b00:       ldr     x2, [x29, #120]
    0.46 :   ffff800010454b04:       ldr     x1, [x21]
    0.00 :   ffff800010454b08:       eor     x1, x2, x1
    0.00 :   ffff800010454b0c:       cbnz    x1, ffff800010454db0 <generic_make_request+0x310>
    0.70 :   ffff800010454b10:       ldp     x21, x22, [sp, #32]
    0.47 :   ffff800010454b14:       ldr     x26, [sp, #72]
    0.93 :   ffff800010454b18:       ldp     x29, x30, [sp], #128
    0.00 :   ffff800010454b1c:       ret
         :                      BUG_ON(bio->bi_next);
    0.00 :   ffff800010454b20:       ldr     x0, [x26]
    1.17 :   ffff800010454b24:       stp     x19, x20, [x29, #16]
    0.71 :   ffff800010454b28:       stp     x23, x24, [x29, #48]
    1.52 :   ffff800010454b2c:       str     x25, [x29, #64]
    0.00 :   ffff800010454b30:       cbnz    x0, ffff800010454dac <generic_make_request+0x30c>
         :                      bio_list_init():
         :                      bl->head = bl->tail = NULL;
    0.12 :   ffff800010454b34:       add     x0, x29, #0x80
    0.59 :   ffff800010454b38:       str     xzr, [x29, #96]
         :                      generic_make_request():
         :                      blk_qc_t ret = BLK_QC_T_NONE;
    0.00 :   ffff800010454b3c:       mov     w22, #0xffffffff                // #-1
         :                      bio_io_error():
         :                      bio->bi_status = BLK_STS_IOERR;
    0.00 :   ffff800010454b40:       mov     w24, #0xa                       // #10
         :                      bio_wouldblock_error():
         :                      bio->bi_status = BLK_STS_AGAIN;
    0.00 :   ffff800010454b44:       mov     w25, #0xc                       // #12
         :                      __percpu_add_case_64():
         :
         :                      PERCPU_RW_OPS(8)
         :                      PERCPU_RW_OPS(16)
         :                      PERCPU_RW_OPS(32)
         :                      PERCPU_RW_OPS(64)
         :                      PERCPU_OP(add, add, stadd)
    0.47 :   ffff800010454b48:       mov     x20, #0xffffffffffffffff        // #-1
         :                      bio_list_init():
         :                      bl->head = bl->tail = NULL;
    0.59 :   ffff800010454b4c:       str     xzr, [x0, #-40]!
         :                      __ll_sc_atomic64_sub_return():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(add, add, I)
         :                      ATOMIC64_OPS(sub, sub, J)
    0.00 :   ffff800010454b50:       mov     x23, #0x1                       // #1
         :                      generic_make_request():
         :                      current->bio_list = bio_list_on_stack;
    0.23 :   ffff800010454b54:       str     x0, [x1, #1832]
         :                      struct request_queue *q = bio->bi_disk->queue;
    1.76 :   ffff800010454b58:       ldr     x0, [x26, #8]
         :                      BLK_MQ_REQ_NOWAIT : 0;
    0.00 :   ffff800010454b5c:       ldr     w1, [x26, #16]
         :                      struct request_queue *q = bio->bi_disk->queue;
    1.29 :   ffff800010454b60:       ldr     x19, [x0, #1040]
         :                      if (likely(blk_queue_enter(q, flags) == 0)) {
    0.00 :   ffff800010454b64:       ubfx    w1, w1, #21, #1
    0.00 :   ffff800010454b68:       mov     x0, x19
    0.00 :   ffff800010454b6c:       bl      ffff8000104547d8 <blk_queue_enter>
    4.10 :   ffff800010454b70:       cbnz    w0, ffff800010454cb0 <generic_make_request+0x210>
    2.09 :   ffff800010454b74:       nop
         :                      bio_list_on_stack[1] = bio_list_on_stack[0];
    0.12 :   ffff800010454b78:       ldp     x2, x3, [x29, #88]
   23.83 :   ffff800010454b7c:       stp     x2, x3, [x29, #104]
         :                      ret = q->make_request_fn(q, bio);
    0.00 :   ffff800010454b80:       mov     x1, x26
         :                      bio_list_init():
    0.00 :   ffff800010454b84:       stp     xzr, xzr, [x29, #88]
         :                      generic_make_request():
    0.00 :   ffff800010454b88:       ldr     x2, [x19, #32]
    0.00 :   ffff800010454b8c:       mov     x0, x19
    0.00 :   ffff800010454b90:       blr     x2
    0.00 :   ffff800010454b94:       mov     w22, w0
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff800010454b98:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.12 :   ffff800010454b9c:       ldr     x0, [x19, #1480]
         :                      __ref_is_percpu():
         :                      * Theoretically, the following could test just ATOMIC; however,
         :                      * then we'd have to mask off DEAD separately as DEAD may be
         :                      * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
         :                      * implies ATOMIC anyway.  Test them together.
         :                      */
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff800010454ba0:       tst     x0, #0x3
    0.00 :   ffff800010454ba4:       b.ne    ffff800010454d4c <generic_make_request+0x2ac>  // b.any
         :                      get_current():
    0.12 :   ffff800010454ba8:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.00 :   ffff800010454bac:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff800010454bb0:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    3.75 :   ffff800010454bb4:       str     w2, [x1, #16]
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800010454bb8:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.12 :   ffff800010454bbc:       add     x0, x0, x2
    0.12 :   ffff800010454bc0:       ldxr    x4, [x0]
   17.79 :   ffff800010454bc4:       add     x4, x4, x20
    0.00 :   ffff800010454bc8:       stxr    w3, x4, [x0]
    0.00 :   ffff800010454bcc:       cbnz    w3, ffff800010454bc0 <generic_make_request+0x120>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010454bd0:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800010454bd4:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    2.35 :   ffff800010454bd8:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800010454bdc:       cbz     x0, ffff800010454cf0 <generic_make_request+0x250>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010454be0:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff800010454be4:       cbz     x0, ffff800010454cf0 <generic_make_request+0x250>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    4.22 :   ffff800010454be8:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      bio_list_init():
    0.00 :   ffff800010454bec:       mov     x3, #0x0                        // #0
    0.00 :   ffff800010454bf0:       mov     x5, #0x0                        // #0
    0.00 :   ffff800010454bf4:       mov     x2, #0x0                        // #0
    0.00 :   ffff800010454bf8:       mov     x4, #0x0                        // #0
         :                      bio_list_pop():
         :                      return bl->head;
         :                      }
         :
         :                      static inline struct bio *bio_list_pop(struct bio_list *bl)
         :                      {
         :                      struct bio *bio = bl->head;
    0.00 :   ffff800010454bfc:       ldr     x0, [x29, #88]
         :
         :                      if (bio) {
    0.47 :   ffff800010454c00:       cbz     x0, ffff800010454c38 <generic_make_request+0x198>
         :                      bl->head = bl->head->bi_next;
    0.00 :   ffff800010454c04:       ldr     x1, [x0]
    0.00 :   ffff800010454c08:       str     x1, [x29, #88]
         :                      if (!bl->head)
    0.00 :   ffff800010454c0c:       cbz     x1, ffff800010454d08 <generic_make_request+0x268>
         :                      generic_make_request():
         :                      if (q == bio->bi_disk->queue)
    0.00 :   ffff800010454c10:       ldr     x1, [x0, #8]
         :                      bio_list_pop():
         :                      bl->tail = NULL;
         :
         :                      bio->bi_next = NULL;
    0.00 :   ffff800010454c14:       str     xzr, [x0]
         :                      generic_make_request():
    0.00 :   ffff800010454c18:       ldr     x1, [x1, #1040]
    0.00 :   ffff800010454c1c:       cmp     x19, x1
    0.00 :   ffff800010454c20:       b.eq    ffff800010454cf8 <generic_make_request+0x258>  // b.none
         :                      bio_list_add():
         :                      if (bl->tail)
    0.00 :   ffff800010454c24:       cbz     x2, ffff800010454d10 <generic_make_request+0x270>
         :                      bl->tail->bi_next = bio;
    0.00 :   ffff800010454c28:       str     x0, [x2]
    0.00 :   ffff800010454c2c:       mov     x2, x0
         :                      bio_list_pop():
         :                      struct bio *bio = bl->head;
    0.00 :   ffff800010454c30:       ldr     x0, [x29, #88]
         :                      if (bio) {
    0.00 :   ffff800010454c34:       cbnz    x0, ffff800010454c04 <generic_make_request+0x164>
         :                      bio_list_merge():
         :                      if (!bl2->head)
    1.05 :   ffff800010454c38:       cbz     x4, ffff800010454c4c <generic_make_request+0x1ac>
         :                      if (bl->tail)
    0.00 :   ffff800010454c3c:       ldr     x0, [x29, #96]
    0.00 :   ffff800010454c40:       cbz     x0, ffff800010454d38 <generic_make_request+0x298>
         :                      bl->tail->bi_next = bl2->head;
    0.00 :   ffff800010454c44:       str     x4, [x0]
         :                      bl->tail = bl2->tail;
    0.00 :   ffff800010454c48:       str     x2, [x29, #96]
         :                      if (!bl2->head)
    1.29 :   ffff800010454c4c:       cbz     x5, ffff800010454c60 <generic_make_request+0x1c0>
         :                      if (bl->tail)
    0.00 :   ffff800010454c50:       ldr     x0, [x29, #96]
    0.00 :   ffff800010454c54:       cbz     x0, ffff800010454d30 <generic_make_request+0x290>
         :                      bl->tail->bi_next = bl2->head;
    0.00 :   ffff800010454c58:       str     x5, [x0]
         :                      bl->tail = bl2->tail;
    0.00 :   ffff800010454c5c:       str     x3, [x29, #96]
         :                      if (!bl2->head)
    3.79 :   ffff800010454c60:       ldr     x26, [x29, #104]
    0.00 :   ffff800010454c64:       cbz     x26, ffff800010454cc4 <generic_make_request+0x224>
         :                      if (bl->tail)
    0.00 :   ffff800010454c68:       ldr     x0, [x29, #96]
    0.00 :   ffff800010454c6c:       cbz     x0, ffff800010454d28 <generic_make_request+0x288>
         :                      bl->tail->bi_next = bl2->head;
    0.00 :   ffff800010454c70:       str     x26, [x0]
    0.00 :   ffff800010454c74:       ldr     x26, [x29, #88]
         :                      bl->tail = bl2->tail;
    0.00 :   ffff800010454c78:       ldr     x0, [x29, #112]
    0.00 :   ffff800010454c7c:       str     x0, [x29, #96]
         :                      bio_list_pop():
         :                      if (bio) {
    0.00 :   ffff800010454c80:       cbz     x26, ffff800010454ccc <generic_make_request+0x22c>
         :                      bl->head = bl->head->bi_next;
    0.00 :   ffff800010454c84:       ldr     x0, [x26]
    0.00 :   ffff800010454c88:       str     x0, [x29, #88]
         :                      if (!bl->head)
    0.00 :   ffff800010454c8c:       cbz     x0, ffff800010454ce4 <generic_make_request+0x244>
         :                      bio->bi_next = NULL;
    0.00 :   ffff800010454c90:       str     xzr, [x26]
         :                      generic_make_request():
         :                      struct request_queue *q = bio->bi_disk->queue;
    0.00 :   ffff800010454c94:       ldr     x0, [x26, #8]
         :                      BLK_MQ_REQ_NOWAIT : 0;
    0.00 :   ffff800010454c98:       ldr     w1, [x26, #16]
         :                      struct request_queue *q = bio->bi_disk->queue;
    0.00 :   ffff800010454c9c:       ldr     x19, [x0, #1040]
         :                      if (likely(blk_queue_enter(q, flags) == 0)) {
    0.00 :   ffff800010454ca0:       ubfx    w1, w1, #21, #1
    0.00 :   ffff800010454ca4:       mov     x0, x19
    0.00 :   ffff800010454ca8:       bl      ffff8000104547d8 <blk_queue_enter>
    0.00 :   ffff800010454cac:       cbz     w0, ffff800010454b78 <generic_make_request+0xd8>
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff800010454cb0:       ldr     x0, [x19, #104]
         :                      generic_make_request():
         :                      if (unlikely(!blk_queue_dying(q) &&
    0.00 :   ffff800010454cb4:       tbz     w0, #1, ffff800010454d90 <generic_make_request+0x2f0>
         :                      bio_io_error():
         :                      bio->bi_status = BLK_STS_IOERR;
    0.00 :   ffff800010454cb8:       strb    w24, [x26, #26]
         :                      bio_endio(bio);
    0.00 :   ffff800010454cbc:       mov     x0, x26
    0.00 :   ffff800010454cc0:       bl      ffff800010450080 <bio_endio>
   11.48 :   ffff800010454cc4:       ldr     x26, [x29, #88]
         :                      bio_list_pop():
         :                      if (bio) {
    0.00 :   ffff800010454cc8:       cbnz    x26, ffff800010454c84 <generic_make_request+0x1e4>
         :                      get_current():
    2.70 :   ffff800010454ccc:       mrs     x0, sp_el0
         :                      generic_make_request():
         :                      current->bio_list = NULL; /* deactivate */
    1.75 :   ffff800010454cd0:       str     xzr, [x0, #1832]
    2.82 :   ffff800010454cd4:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff800010454cd8:       ldp     x23, x24, [x29, #48]
    0.12 :   ffff800010454cdc:       ldr     x25, [x29, #64]
    0.00 :   ffff800010454ce0:       b       ffff800010454af8 <generic_make_request+0x58>
         :                      bio_list_pop():
         :                      bl->tail = NULL;
    0.00 :   ffff800010454ce4:       str     xzr, [x29, #96]
         :                      bio->bi_next = NULL;
    0.00 :   ffff800010454ce8:       str     xzr, [x26]
    0.00 :   ffff800010454cec:       b       ffff800010454c94 <generic_make_request+0x1f4>
         :                      percpu_ref_put_many():
         :                      unsigned long __percpu *percpu_count;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count))
         :                      this_cpu_sub(*percpu_count, nr);
    0.00 :   ffff800010454cf0:       bl      ffff800010cad668 <preempt_schedule_notrace>
    0.00 :   ffff800010454cf4:       b       ffff800010454be8 <generic_make_request+0x148>
         :                      bio_list_add():
         :                      if (bl->tail)
    0.00 :   ffff800010454cf8:       cbz     x3, ffff800010454d1c <generic_make_request+0x27c>
         :                      bl->tail->bi_next = bio;
    0.00 :   ffff800010454cfc:       str     x0, [x3]
    0.00 :   ffff800010454d00:       mov     x3, x0
    0.00 :   ffff800010454d04:       b       ffff800010454bfc <generic_make_request+0x15c>
         :                      bio_list_pop():
         :                      bl->tail = NULL;
    0.00 :   ffff800010454d08:       str     xzr, [x29, #96]
    0.00 :   ffff800010454d0c:       b       ffff800010454c10 <generic_make_request+0x170>
         :                      bio_list_add():
         :                      if (bl->tail)
    0.00 :   ffff800010454d10:       mov     x2, x0
    0.00 :   ffff800010454d14:       mov     x4, x0
    0.00 :   ffff800010454d18:       b       ffff800010454bfc <generic_make_request+0x15c>
    0.00 :   ffff800010454d1c:       mov     x3, x0
    0.00 :   ffff800010454d20:       mov     x5, x0
    0.00 :   ffff800010454d24:       b       ffff800010454bfc <generic_make_request+0x15c>
         :                      bio_list_merge():
         :                      bl->head = bl2->head;
    0.00 :   ffff800010454d28:       str     x26, [x29, #88]
    0.00 :   ffff800010454d2c:       b       ffff800010454c78 <generic_make_request+0x1d8>
    0.00 :   ffff800010454d30:       str     x5, [x29, #88]
    0.00 :   ffff800010454d34:       b       ffff800010454c5c <generic_make_request+0x1bc>
    0.00 :   ffff800010454d38:       str     x4, [x29, #88]
         :                      bl->tail = bl2->tail;
    0.00 :   ffff800010454d3c:       str     x2, [x29, #96]
    0.00 :   ffff800010454d40:       b       ffff800010454c4c <generic_make_request+0x1ac>
         :                      generic_make_request():
         :                      blk_qc_t ret = BLK_QC_T_NONE;
    0.00 :   ffff800010454d44:       mov     w22, #0xffffffff                // #-1
         :                      return ret;
    0.00 :   ffff800010454d48:       b       ffff800010454af8 <generic_make_request+0x58>
         :                      blk_queue_exit():
         :                      percpu_ref_put(&q->q_usage_counter);
    0.00 :   ffff800010454d4c:       add     x0, x19, #0x5c0
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010454d50:       b       ffff800010454d78 <generic_make_request+0x2d8>
    0.00 :   ffff800010454d54:       b       ffff800010454d78 <generic_make_request+0x2d8>
         :                      __lse_atomic64_sub_return():
         :                      }
         :
         :                      ATOMIC64_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC64_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff800010454d58:       mov     x1, x23
    0.00 :   ffff800010454d5c:       neg     x1, x1
    0.00 :   ffff800010454d60:       ldaddal x1, x2, [x0]
    0.00 :   ffff800010454d64:       add     x1, x1, x2
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff800010454d68:       cbnz    x1, ffff800010454be8 <generic_make_request+0x148>
         :                      ref->release(ref);
    0.00 :   ffff800010454d6c:       ldr     x1, [x0, #16]
    0.00 :   ffff800010454d70:       blr     x1
    0.00 :   ffff800010454d74:       b       ffff800010454be8 <generic_make_request+0x148>
         :                      __ll_sc_atomic64_sub_return():
    0.00 :   ffff800010454d78:       add     x3, x19, #0x5c0
    0.00 :   ffff800010454d7c:       b       ffff80001045645c <blk_finish_plug+0x174>
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff800010454d80:       cbnz    x1, ffff800010454be8 <generic_make_request+0x148>
    0.00 :   ffff800010454d84:       b       ffff800010454d6c <generic_make_request+0x2cc>
         :                      bio_list_add():
         :                      bl->head = bio;
    0.00 :   ffff800010454d88:       str     x26, [x0]
    0.00 :   ffff800010454d8c:       b       ffff800010454af0 <generic_make_request+0x50>
         :                      generic_make_request():
         :                      if (unlikely(!blk_queue_dying(q) &&
    0.00 :   ffff800010454d90:       ldr     w0, [x26, #16]
    0.00 :   ffff800010454d94:       tbz     w0, #21, ffff800010454cb8 <generic_make_request+0x218>
         :                      bio_wouldblock_error():
         :                      bio->bi_status = BLK_STS_AGAIN;
    0.00 :   ffff800010454d98:       strb    w25, [x26, #26]
         :                      bio_endio(bio);
    0.00 :   ffff800010454d9c:       mov     x0, x26
    0.00 :   ffff800010454da0:       bl      ffff800010450080 <bio_endio>
    0.00 :   ffff800010454da4:       ldr     x26, [x29, #88]
    0.00 :   ffff800010454da8:       b       ffff800010454c80 <generic_make_request+0x1e0>
         :                      generic_make_request():
         :                      BUG_ON(bio->bi_next);
    0.00 :   ffff800010454dac:       brk     #0x800
    0.00 :   ffff800010454db0:       stp     x19, x20, [x29, #16]
    0.00 :   ffff800010454db4:       stp     x23, x24, [x29, #48]
    0.00 :   ffff800010454db8:       str     x25, [x29, #64]
         :                      }
    0.00 :   ffff800010454dbc:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (461 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000106fe5e8 <iommu_dma_unmap_page>:
         :                      iommu_dma_unmap_page():
         :                      return dma_handle;
         :                      }
         :
         :                      static void iommu_dma_unmap_page(struct device *dev, dma_addr_t dma_handle,
         :                      size_t size, enum dma_data_direction dir, unsigned long attrs)
         :                      {
   11.68 :   ffff8000106fe5e8:       stp     x29, x30, [sp, #-64]!
    0.63 :   ffff8000106fe5ec:       mov     x29, sp
   21.12 :   ffff8000106fe5f0:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000106fe5f4:       mov     x19, x0
    1.06 :   ffff8000106fe5f8:       str     x21, [sp, #32]
    0.44 :   ffff8000106fe5fc:       mov     x20, x1
    0.43 :   ffff8000106fe600:       mov     x21, x2
         :                      if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC))
    0.00 :   ffff8000106fe604:       tbnz    w4, #5, ffff8000106fe610 <iommu_dma_unmap_page+0x28>
         :                      dev_is_dma_coherent():
         :                      #elif defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
         :                      defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \
         :                      defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL)
         :                      static inline bool dev_is_dma_coherent(struct device *dev)
         :                      {
         :                      return dev->dma_coherent;
    2.19 :   ffff8000106fe608:       ldrb    w1, [x0, #760]
         :                      iommu_dma_sync_single_for_cpu():
         :                      if (dev_is_dma_coherent(dev))
    0.00 :   ffff8000106fe60c:       tbz     w1, #4, ffff8000106fe630 <iommu_dma_unmap_page+0x48>
         :                      iommu_dma_unmap_page():
         :                      iommu_dma_sync_single_for_cpu(dev, dma_handle, size, dir);
         :                      __iommu_dma_unmap(dev, dma_handle, size);
   26.91 :   ffff8000106fe610:       mov     x2, x21
    0.00 :   ffff8000106fe614:       mov     x1, x20
    0.22 :   ffff8000106fe618:       mov     x0, x19
    0.44 :   ffff8000106fe61c:       bl      ffff8000106fde00 <__iommu_dma_unmap>
         :                      }
   15.00 :   ffff8000106fe620:       ldp     x19, x20, [sp, #16]
   19.88 :   ffff8000106fe624:       ldr     x21, [sp, #32]
    0.00 :   ffff8000106fe628:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000106fe62c:       ret
    0.00 :   ffff8000106fe630:       str     w3, [x29, #60]
         :                      iommu_dma_sync_single_for_cpu():
         :                      phys = iommu_iova_to_phys(iommu_get_dma_domain(dev), dma_handle);
    0.00 :   ffff8000106fe634:       bl      ffff8000106fcb18 <iommu_get_dma_domain>
    0.00 :   ffff8000106fe638:       mov     x1, x20
    0.00 :   ffff8000106fe63c:       bl      ffff8000106fa9b0 <iommu_iova_to_phys>
         :                      arch_sync_dma_for_cpu(phys, size, dir);
    0.00 :   ffff8000106fe640:       ldr     w3, [x29, #60]
    0.00 :   ffff8000106fe644:       mov     x1, x21
    0.00 :   ffff8000106fe648:       mov     w2, w3
    0.00 :   ffff8000106fe64c:       bl      ffff8000100a11b0 <arch_sync_dma_for_cpu>
         :                      iommu_dma_unmap_page():
         :                      __iommu_dma_unmap(dev, dma_handle, size);
    0.00 :   ffff8000106fe650:       mov     x2, x21
    0.00 :   ffff8000106fe654:       mov     x1, x20
    0.00 :   ffff8000106fe658:       mov     x0, x19
    0.00 :   ffff8000106fe65c:       bl      ffff8000106fde00 <__iommu_dma_unmap>
         :                      }
    0.00 :   ffff8000106fe660:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000106fe664:       ldr     x21, [sp, #32]
    0.00 :   ffff8000106fe668:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000106fe66c:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (429 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001070a500 <arm_smmu_iotlb_sync>:
         :                      arm_smmu_iotlb_sync():
         :                      arm_smmu_tlb_inv_context(smmu_domain);
         :                      }
         :
         :                      static void arm_smmu_iotlb_sync(struct iommu_domain *domain,
         :                      struct iommu_iotlb_gather *gather)
         :                      {
   36.31 :   ffff80001070a500:       stp     x29, x30, [sp, #-16]!
         :                      struct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);
         :
         :                      arm_smmu_tlb_inv_range(gather->start, gather->end - gather->start,
    0.00 :   ffff80001070a504:       sub     x4, x0, #0x70
    0.00 :   ffff80001070a508:       mov     w3, #0x1                        // #1
         :                      {
    0.00 :   ffff80001070a50c:       mov     x29, sp
         :                      arm_smmu_tlb_inv_range(gather->start, gather->end - gather->start,
   25.92 :   ffff80001070a510:       ldr     x5, [x1]
    0.93 :   ffff80001070a514:       ldr     x2, [x1, #16]
    0.48 :   ffff80001070a518:       ldr     x1, [x1, #8]
    0.00 :   ffff80001070a51c:       mov     x0, x5
    0.00 :   ffff80001070a520:       sub     x1, x1, x5
    0.00 :   ffff80001070a524:       bl      ffff80001070a388 <arm_smmu_tlb_inv_range>
         :                      gather->pgsize, true, smmu_domain);
         :                      }
   36.37 :   ffff80001070a528:       ldp     x29, x30, [sp], #16
    0.00 :   ffff80001070a52c:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (773 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000106fe8c8 <iommu_dma_map_page>:
         :                      iommu_dma_map_page():
         :                      }
         :
         :                      static dma_addr_t iommu_dma_map_page(struct device *dev, struct page *page,
         :                      unsigned long offset, size_t size, enum dma_data_direction dir,
         :                      unsigned long attrs)
         :                      {
   31.89 :   ffff8000106fe8c8:       stp     x29, x30, [sp, #-64]!
         :                      phys_addr_t phys = page_to_phys(page) + offset;
    0.00 :   ffff8000106fe8cc:       adrp    x6, ffff8000112ae000 <cpu_ops+0x248>
         :                      dma_info_to_prot():
         :                      prot |= IOMMU_PRIV;
    0.00 :   ffff8000106fe8d0:       tst     x5, #0x200
         :                      iommu_dma_map_page():
         :                      {
    0.00 :   ffff8000106fe8d4:       mov     x29, sp
    2.71 :   ffff8000106fe8d8:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000106fe8dc:       stp     x21, x22, [sp, #32]
    0.00 :   ffff8000106fe8e0:       mov     w21, w4
    0.26 :   ffff8000106fe8e4:       stp     x23, x24, [sp, #48]
    0.00 :   ffff8000106fe8e8:       mov     x24, x3
         :                      phys_addr_t phys = page_to_phys(page) + offset;
    0.39 :   ffff8000106fe8ec:       ldr     x19, [x6, #1880]
         :                      {
    0.00 :   ffff8000106fe8f0:       mov     x23, x5
         :                      dev_is_dma_coherent():
         :                      #elif defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
         :                      defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \
         :                      defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL)
         :                      static inline bool dev_is_dma_coherent(struct device *dev)
         :                      {
         :                      return dev->dma_coherent;
    5.18 :   ffff8000106fe8f4:       ldrb    w20, [x0, #760]
         :                      iommu_dma_map_page():
         :                      phys_addr_t phys = page_to_phys(page) + offset;
    0.00 :   ffff8000106fe8f8:       sub     x19, x1, x19
         :                      dev_is_dma_coherent():
    0.00 :   ffff8000106fe8fc:       ubfx    x20, x20, #4, #1
         :                      iommu_dma_map_page():
    0.00 :   ffff8000106fe900:       asr     x19, x19, #6
         :                      dma_info_to_prot():
         :                      int prot = coherent ? IOMMU_CACHE : 0;
    0.00 :   ffff8000106fe904:       lsl     w1, w20, #2
         :                      iommu_dma_map_page():
         :                      phys_addr_t phys = page_to_phys(page) + offset;
    0.00 :   ffff8000106fe908:       add     x19, x2, x19, lsl #12
         :                      dma_info_to_prot():
         :                      prot |= IOMMU_PRIV;
    0.00 :   ffff8000106fe90c:       orr     w2, w1, #0x20
    0.00 :   ffff8000106fe910:       csel    w1, w2, w1, ne  // ne = any
         :                      switch (dir) {
    0.00 :   ffff8000106fe914:       cmp     w4, #0x1
         :                      return prot | IOMMU_READ;
    0.00 :   ffff8000106fe918:       orr     w3, w1, #0x1
         :                      switch (dir) {
    0.00 :   ffff8000106fe91c:       b.eq    ffff8000106fe934 <iommu_dma_map_page+0x6c>  // b.none
         :                      return prot | IOMMU_READ | IOMMU_WRITE;
    0.26 :   ffff8000106fe920:       orr     w3, w1, #0x3
         :                      switch (dir) {
    0.00 :   ffff8000106fe924:       cbz     w4, ffff8000106fe934 <iommu_dma_map_page+0x6c>
         :                      return 0;
    1.94 :   ffff8000106fe928:       orr     w1, w1, #0x2
    0.00 :   ffff8000106fe92c:       cmp     w4, #0x2
    0.00 :   ffff8000106fe930:       csel    w3, w1, wzr, eq  // eq = none
         :                      iommu_dma_map_page():
         :                      bool coherent = dev_is_dma_coherent(dev);
         :                      int prot = dma_info_to_prot(dir, coherent, attrs);
         :                      dma_addr_t dma_handle;
         :
         :                      dma_handle = __iommu_dma_map(dev, phys, size, prot, dma_get_mask(dev));
    0.00 :   ffff8000106fe934:       ldr     x1, [x0, #584]
         :                      dma_get_mask():
         :
         :                      static inline u64 dma_get_mask(struct device *dev)
         :                      {
         :                      if (dev->dma_mask && *dev->dma_mask)
         :                      return *dev->dma_mask;
         :                      return DMA_BIT_MASK(32);
    0.00 :   ffff8000106fe938:       mov     x4, #0xffffffff                 // #4294967295
         :                      if (dev->dma_mask && *dev->dma_mask)
    0.00 :   ffff8000106fe93c:       cbz     x1, ffff8000106fe950 <iommu_dma_map_page+0x88>
    0.00 :   ffff8000106fe940:       ldr     x4, [x1]
         :                      return DMA_BIT_MASK(32);
    0.00 :   ffff8000106fe944:       mov     x1, #0xffffffff                 // #4294967295
    0.00 :   ffff8000106fe948:       cmp     x4, #0x0
    0.00 :   ffff8000106fe94c:       csel    x4, x4, x1, ne  // ne = any
         :                      iommu_dma_map_page():
   51.18 :   ffff8000106fe950:       mov     x2, x24
    0.00 :   ffff8000106fe954:       mov     x1, x19
    0.00 :   ffff8000106fe958:       bl      ffff8000106fe780 <__iommu_dma_map>
    1.30 :   ffff8000106fe95c:       mov     x22, x0
         :                      if (!coherent && !(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&
    0.00 :   ffff8000106fe960:       cbnz    w20, ffff8000106fe970 <iommu_dma_map_page+0xa8>
    0.00 :   ffff8000106fe964:       tst     x23, #0x20
    0.00 :   ffff8000106fe968:       ccmn    x0, #0x1, #0x4, eq  // eq = none
    0.00 :   ffff8000106fe96c:       b.ne    ffff8000106fe988 <iommu_dma_map_page+0xc0>  // b.any
         :                      dma_handle != DMA_MAPPING_ERROR)
         :                      arch_sync_dma_for_device(phys, size, dir);
         :                      return dma_handle;
         :                      }
    1.17 :   ffff8000106fe970:       mov     x0, x22
    0.00 :   ffff8000106fe974:       ldp     x19, x20, [sp, #16]
    1.93 :   ffff8000106fe978:       ldp     x21, x22, [sp, #32]
    0.38 :   ffff8000106fe97c:       ldp     x23, x24, [sp, #48]
    1.42 :   ffff8000106fe980:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000106fe984:       ret
         :                      arch_sync_dma_for_device(phys, size, dir);
    0.00 :   ffff8000106fe988:       mov     w2, w21
    0.00 :   ffff8000106fe98c:       mov     x1, x24
    0.00 :   ffff8000106fe990:       mov     x0, x19
    0.00 :   ffff8000106fe994:       bl      ffff8000100a1190 <arch_sync_dma_for_device>
         :                      }
    0.00 :   ffff8000106fe998:       mov     x0, x22
    0.00 :   ffff8000106fe99c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000106fe9a0:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000106fe9a4:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff8000106fe9a8:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000106fe9ac:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (813 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010467fb8 <disk_map_sector_rcu>:
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    3.47 :   ffff800010467fb8:       ldr     x6, [x0, #64]
         :                      disk_map_sector_rcu():
         :                      *
         :                      * RETURNS:
         :                      * Found partition on success, part0 is returned if no partition matches
         :                      */
         :                      struct hd_struct *disk_map_sector_rcu(struct gendisk *disk, sector_t sector)
         :                      {
    0.00 :   ffff800010467fbc:       mov     x7, x0
         :                      __read_once_size():
   14.03 :   ffff800010467fc0:       ldr     x2, [x6, #24]
         :                      disk_map_sector_rcu():
         :                      int i;
         :
         :                      ptbl = rcu_dereference(disk->part_tbl);
         :
         :                      part = rcu_dereference(ptbl->last_lookup);
         :                      if (part && sector_in_part(part, sector))
    0.00 :   ffff800010467fc4:       cbz     x2, ffff800010467fd8 <disk_map_sector_rcu+0x20>
         :                      part = rcu_dereference(ptbl->last_lookup);
   36.79 :   ffff800010467fc8:       mov     x0, x2
         :                      sector_in_part():
         :                      return part->start_sect <= sector &&
    1.35 :   ffff800010467fcc:       ldr     x2, [x2]
    0.24 :   ffff800010467fd0:       cmp     x1, x2
    0.00 :   ffff800010467fd4:       b.cs    ffff800010468034 <disk_map_sector_rcu+0x7c>  // b.hs, b.nlast
         :                      disk_map_sector_rcu():
         :                      return part;
         :
         :                      for (i = 1; i < ptbl->len; i++) {
    0.00 :   ffff800010467fd8:       ldr     w5, [x6, #16]
    0.00 :   ffff800010467fdc:       cmp     w5, #0x1
    0.00 :   ffff800010467fe0:       b.le    ffff80001046802c <disk_map_sector_rcu+0x74>
    0.00 :   ffff800010467fe4:       sub     w3, w5, #0x2
    0.00 :   ffff800010467fe8:       add     x5, x6, #0x30
    0.00 :   ffff800010467fec:       add     x2, x6, #0x28
    0.00 :   ffff800010467ff0:       add     x5, x5, w3, uxtw #3
    0.00 :   ffff800010467ff4:       nop
         :                      __read_once_size():
    0.00 :   ffff800010467ff8:       ldr     x3, [x2]
    0.00 :   ffff800010467ffc:       add     x2, x2, #0x8
         :                      disk_map_sector_rcu():
         :                      part = rcu_dereference(ptbl->part[i]);
    0.00 :   ffff800010468000:       mov     x0, x3
         :
         :                      if (part && sector_in_part(part, sector)) {
    0.00 :   ffff800010468004:       cbz     x3, ffff800010468024 <disk_map_sector_rcu+0x6c>
         :                      sector_in_part():
         :                      return part->start_sect <= sector &&
    0.00 :   ffff800010468008:       ldr     x4, [x3]
    0.00 :   ffff80001046800c:       cmp     x1, x4
    0.00 :   ffff800010468010:       b.cc    ffff800010468024 <disk_map_sector_rcu+0x6c>  // b.lo, b.ul, b.last
         :                      sector < part->start_sect + part_nr_sects_read(part);
    0.00 :   ffff800010468014:       ldr     x3, [x3, #8]
    0.00 :   ffff800010468018:       add     x4, x4, x3
         :                      return part->start_sect <= sector &&
    0.00 :   ffff80001046801c:       cmp     x1, x4
    0.00 :   ffff800010468020:       b.cc    ffff800010468048 <disk_map_sector_rcu+0x90>  // b.lo, b.ul, b.last
         :                      disk_map_sector_rcu():
         :                      for (i = 1; i < ptbl->len; i++) {
    0.00 :   ffff800010468024:       cmp     x2, x5
    0.00 :   ffff800010468028:       b.ne    ffff800010467ff8 <disk_map_sector_rcu+0x40>  // b.any
         :                      rcu_assign_pointer(ptbl->last_lookup, part);
         :                      return part;
         :                      }
         :                      }
         :                      return &disk->part0;
    0.00 :   ffff80001046802c:       add     x0, x7, #0x48
         :                      }
    0.00 :   ffff800010468030:       ret
         :                      sector_in_part():
         :                      sector < part->start_sect + part_nr_sects_read(part);
   28.67 :   ffff800010468034:       ldr     x3, [x0, #8]
    0.00 :   ffff800010468038:       add     x2, x2, x3
         :                      return part->start_sect <= sector &&
    0.00 :   ffff80001046803c:       cmp     x1, x2
   13.23 :   ffff800010468040:       b.cs    ffff800010467fd8 <disk_map_sector_rcu+0x20>  // b.hs, b.nlast
         :                      disk_map_sector_rcu():
         :                      }
    2.21 :   ffff800010468044:       ret
         :                      rcu_assign_pointer(ptbl->last_lookup, part);
    0.00 :   ffff800010468048:       add     x1, x6, #0x18
    0.00 :   ffff80001046804c:       stlr    x0, [x1]
         :                      }
    0.00 :   ffff800010468050:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (414 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000107076f0 <arm_smmu_unmap>:
         :                      arm_smmu_unmap():
         :
         :                      static size_t arm_smmu_unmap(struct iommu_domain *domain, unsigned long iova,
         :                      size_t size, struct iommu_iotlb_gather *gather)
         :                      {
         :                      struct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);
         :                      struct io_pgtable_ops *ops = smmu_domain->pgtbl_ops;
   17.42 :   ffff8000107076f0:       ldur    x0, [x0, #-72]
         :
         :                      if (!ops)
    0.72 :   ffff8000107076f4:       cbz     x0, ffff800010707710 <arm_smmu_unmap+0x20>
         :                      {
   16.02 :   ffff8000107076f8:       stp     x29, x30, [sp, #-16]!
    0.24 :   ffff8000107076fc:       mov     x29, sp
         :                      return 0;
         :
         :                      return ops->unmap(ops, iova, size, gather);
    8.96 :   ffff800010707700:       ldr     x4, [x0, #8]
    3.15 :   ffff800010707704:       blr     x4
         :                      }
   52.53 :   ffff800010707708:       ldp     x29, x30, [sp], #16
    0.96 :   ffff80001070770c:       ret
         :                      return 0;
    0.00 :   ffff800010707710:       mov     x0, #0x0                        // #0
         :                      }
    0.00 :   ffff800010707714:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (809 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010461398 <blk_mq_flush_plug_list>:
         :                      blk_mq_flush_plug_list():
         :
         :                      return blk_rq_pos(rqa) > blk_rq_pos(rqb);
         :                      }
         :
         :                      void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule)
         :                      {
    1.98 :   ffff800010461398:       stp     x29, x30, [sp, #-112]!
    0.00 :   ffff80001046139c:       mov     x29, sp
    2.35 :   ffff8000104613a0:       str     x19, [sp, #16]
    0.00 :   ffff8000104613a4:       mov     x19, x0
    1.60 :   ffff8000104613a8:       stp     x21, x22, [sp, #32]
         :                      struct blk_mq_hw_ctx *this_hctx;
         :                      struct blk_mq_ctx *this_ctx;
         :                      struct request_queue *this_q;
         :                      struct request *rq;
         :                      LIST_HEAD(list);
         :                      LIST_HEAD(rq_list);
    0.00 :   ffff8000104613ac:       add     x21, x29, #0x58
         :                      {
    1.00 :   ffff8000104613b0:       stp     x23, x24, [sp, #48]
    0.00 :   ffff8000104613b4:       adrp    x23, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000104613b8:       add     x2, x23, #0x8c8
         :                      LIST_HEAD(list);
    0.00 :   ffff8000104613bc:       add     x24, x29, #0x48
         :                      {
    2.83 :   ffff8000104613c0:       ldr     x0, [x2]
    7.41 :   ffff8000104613c4:       str     x0, [x29, #104]
    0.00 :   ffff8000104613c8:       mov     x0, #0x0                        // #0
         :                      LIST_HEAD(list);
    0.00 :   ffff8000104613cc:       stp     x24, x24, [x29, #72]
         :                      LIST_HEAD(rq_list);
    0.74 :   ffff8000104613d0:       stp     x21, x21, [x29, #88]
         :                      {
    0.00 :   ffff8000104613d4:       and     w22, w1, #0xff
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    1.85 :   ffff8000104613d8:       ldr     x0, [x19]
         :                      list_splice_init():
         :                      * The list at @list is reinitialised
         :                      */
         :                      static inline void list_splice_init(struct list_head *list,
         :                      struct list_head *head)
         :                      {
         :                      if (!list_empty(list)) {
    0.00 :   ffff8000104613dc:       cmp     x19, x0
    0.00 :   ffff8000104613e0:       b.eq    ffff800010461400 <blk_mq_flush_plug_list+0x68>  // b.none
         :                      __list_splice():
         :                      struct list_head *last = list->prev;
    1.48 :   ffff8000104613e4:       ldp     x1, x0, [x19]
         :                      first->prev = prev;
    2.84 :   ffff8000104613e8:       str     x24, [x1, #8]
         :                      prev->next = first;
    4.70 :   ffff8000104613ec:       str     x1, [x29, #72]
         :                      last->next = next;
    0.00 :   ffff8000104613f0:       str     x24, [x0]
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    2.10 :   ffff8000104613f4:       str     x19, [x19]
         :                      INIT_LIST_HEAD():
         :                      list->prev = list;
    0.00 :   ffff8000104613f8:       str     x19, [x19, #8]
         :                      __list_splice():
         :                      next->prev = last;
    0.00 :   ffff8000104613fc:       str     x0, [x29, #80]
         :                      blk_mq_flush_plug_list():
         :                      unsigned int depth;
         :
         :                      list_splice_init(&plug->mq_list, &list);
         :
         :                      if (plug->rq_count > 2 && plug->multiple_queues)
    0.00 :   ffff800010461400:       ldrh    w0, [x19, #32]
    0.00 :   ffff800010461404:       cmp     w0, #0x2
    0.00 :   ffff800010461408:       b.ls    ffff800010461414 <blk_mq_flush_plug_list+0x7c>  // b.plast
    0.00 :   ffff80001046140c:       ldrb    w0, [x19, #34]
    0.00 :   ffff800010461410:       cbnz    w0, ffff800010461514 <blk_mq_flush_plug_list+0x17c>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    2.34 :   ffff800010461414:       ldr     x0, [x29, #72]
         :                      blk_mq_flush_plug_list():
         :                      list_sort(NULL, &list, plug_rq_cmp);
         :
         :                      plug->rq_count = 0;
    2.22 :   ffff800010461418:       strh    wzr, [x19, #32]
         :                      this_q = NULL;
         :                      this_hctx = NULL;
         :                      this_ctx = NULL;
         :                      depth = 0;
         :
         :                      while (!list_empty(&list)) {
    0.00 :   ffff80001046141c:       cmp     x24, x0
    0.00 :   ffff800010461420:       b.eq    ffff8000104614ec <blk_mq_flush_plug_list+0x154>  // b.none
         :                      rq = list_entry_rq(list.next);
    0.00 :   ffff800010461424:       ldr     x19, [x29, #72]
    1.73 :   ffff800010461428:       str     x20, [x29, #24]
         :                      __list_del_entry():
         :                      __list_del(entry->prev, entry->next);
    0.49 :   ffff80001046142c:       ldp     x1, x0, [x19]
         :                      __list_del():
         :                      next->prev = prev;
   15.33 :   ffff800010461430:       str     x0, [x1, #8]
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    5.57 :   ffff800010461434:       str     x1, [x0]
    0.25 :   ffff800010461438:       str     x19, [x19]
         :                      blk_mq_flush_plug_list():
         :                      list_del_init(&rq->queuelist);
         :                      BUG_ON(!rq->q);
    0.00 :   ffff80001046143c:       ldur    x0, [x19, #-72]
         :                      INIT_LIST_HEAD():
         :                      list->prev = list;
    0.12 :   ffff800010461440:       str     x19, [x19, #8]
         :                      blk_mq_flush_plug_list():
    0.00 :   ffff800010461444:       cbz     x0, ffff8000104614bc <blk_mq_flush_plug_list+0x124>
    0.37 :   ffff800010461448:       sub     x20, x19, #0x48
         :                      this_ctx = NULL;
    0.00 :   ffff80001046144c:       mov     x1, #0x0                        // #0
         :                      this_hctx = NULL;
    0.00 :   ffff800010461450:       mov     x0, #0x0                        // #0
    0.00 :   ffff800010461454:       nop
         :                      if (rq->mq_hctx != this_hctx || rq->mq_ctx != this_ctx) {
    0.61 :   ffff800010461458:       ldr     x2, [x20, #16]
    0.00 :   ffff80001046145c:       cmp     x2, x0
    0.00 :   ffff800010461460:       b.eq    ffff8000104614c0 <blk_mq_flush_plug_list+0x128>  // b.none
         :                      if (this_hctx) {
    0.00 :   ffff800010461464:       cbz     x0, ffff8000104614d0 <blk_mq_flush_plug_list+0x138>
         :                      trace_block_unplug(this_q, depth, !from_schedule);
         :                      blk_mq_sched_insert_requests(this_hctx, this_ctx,
    0.00 :   ffff800010461468:       mov     w3, w22
    0.00 :   ffff80001046146c:       mov     x2, x21
    0.00 :   ffff800010461470:       bl      ffff800010466420 <blk_mq_sched_insert_requests>
    0.00 :   ffff800010461474:       ldr     x0, [x20, #16]
         :                      &rq_list,
         :                      from_schedule);
         :                      }
         :
         :                      this_q = rq->q;
         :                      this_ctx = rq->mq_ctx;
    1.87 :   ffff800010461478:       ldr     x1, [x20, #8]
         :                      list_add_tail():
         :                      __list_add(new, head->prev, head);
    0.74 :   ffff80001046147c:       ldr     x2, [x29, #96]
         :                      __list_add():
         :                      next->prev = new;
    0.00 :   ffff800010461480:       str     x19, [x29, #96]
         :                      new->prev = prev;
    0.12 :   ffff800010461484:       stp     x21, x2, [x19]
         :                      __write_once_size():
    1.61 :   ffff800010461488:       str     x19, [x2]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
   10.55 :   ffff80001046148c:       ldr     x2, [x29, #72]
         :                      blk_mq_flush_plug_list():
         :                      while (!list_empty(&list)) {
    0.00 :   ffff800010461490:       cmp     x24, x2
    0.00 :   ffff800010461494:       b.eq    ffff8000104614d8 <blk_mq_flush_plug_list+0x140>  // b.none
         :                      rq = list_entry_rq(list.next);
    0.00 :   ffff800010461498:       ldr     x19, [x29, #72]
         :                      list_del_init(&rq->queuelist);
    0.00 :   ffff80001046149c:       sub     x20, x19, #0x48
         :                      __list_del_entry():
         :                      __list_del(entry->prev, entry->next);
    0.00 :   ffff8000104614a0:       ldp     x3, x2, [x19]
         :                      __list_del():
         :                      next->prev = prev;
    0.00 :   ffff8000104614a4:       str     x2, [x3, #8]
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff8000104614a8:       str     x3, [x2]
    0.00 :   ffff8000104614ac:       str     x19, [x19]
         :                      blk_mq_flush_plug_list():
         :                      BUG_ON(!rq->q);
    0.00 :   ffff8000104614b0:       ldur    x2, [x19, #-72]
         :                      INIT_LIST_HEAD():
         :                      list->prev = list;
    0.00 :   ffff8000104614b4:       str     x19, [x19, #8]
         :                      blk_mq_flush_plug_list():
    0.00 :   ffff8000104614b8:       cbnz    x2, ffff800010461458 <blk_mq_flush_plug_list+0xc0>
    0.00 :   ffff8000104614bc:       brk     #0x800
         :                      if (rq->mq_hctx != this_hctx || rq->mq_ctx != this_ctx) {
    0.00 :   ffff8000104614c0:       ldr     x3, [x20, #8]
    0.00 :   ffff8000104614c4:       cmp     x3, x1
    0.00 :   ffff8000104614c8:       b.eq    ffff80001046147c <blk_mq_flush_plug_list+0xe4>  // b.none
         :                      if (this_hctx) {
    0.00 :   ffff8000104614cc:       cbnz    x0, ffff800010461468 <blk_mq_flush_plug_list+0xd0>
         :                      if (rq->mq_hctx != this_hctx || rq->mq_ctx != this_ctx) {
    0.00 :   ffff8000104614d0:       mov     x0, x2
    0.00 :   ffff8000104614d4:       b       ffff800010461478 <blk_mq_flush_plug_list+0xe0>
         :
         :                      /*
         :                      * If 'this_hctx' is set, we know we have entries to complete
         :                      * on 'rq_list'. Do those.
         :                      */
         :                      if (this_hctx) {
   11.49 :   ffff8000104614d8:       cbz     x0, ffff80001046152c <blk_mq_flush_plug_list+0x194>
         :                      trace_block_unplug(this_q, depth, !from_schedule);
         :                      blk_mq_sched_insert_requests(this_hctx, this_ctx, &rq_list,
    1.23 :   ffff8000104614dc:       mov     w3, w22
    2.70 :   ffff8000104614e0:       mov     x2, x21
    0.00 :   ffff8000104614e4:       bl      ffff800010466420 <blk_mq_sched_insert_requests>
    0.49 :   ffff8000104614e8:       ldr     x20, [x29, #24]
         :                      from_schedule);
         :                      }
         :                      }
    0.00 :   ffff8000104614ec:       add     x23, x23, #0x8c8
    5.43 :   ffff8000104614f0:       ldr     x1, [x29, #104]
    0.25 :   ffff8000104614f4:       ldr     x0, [x23]
    0.00 :   ffff8000104614f8:       eor     x0, x1, x0
    0.00 :   ffff8000104614fc:       cbnz    x0, ffff800010461534 <blk_mq_flush_plug_list+0x19c>
    1.85 :   ffff800010461500:       ldr     x19, [sp, #16]
    1.62 :   ffff800010461504:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010461508:       ldp     x23, x24, [sp, #48]
    0.12 :   ffff80001046150c:       ldp     x29, x30, [sp], #112
    0.00 :   ffff800010461510:       ret
         :                      list_sort(NULL, &list, plug_rq_cmp);
    0.00 :   ffff800010461514:       adrp    x2, ffff80001045d000 <__blkdev_issue_zero_pages+0x60>
    0.00 :   ffff800010461518:       mov     x1, x24
    0.00 :   ffff80001046151c:       add     x2, x2, #0xd40
    0.00 :   ffff800010461520:       mov     x0, #0x0                        // #0
    0.00 :   ffff800010461524:       bl      ffff800010481cd0 <list_sort>
    0.00 :   ffff800010461528:       b       ffff800010461414 <blk_mq_flush_plug_list+0x7c>
    0.00 :   ffff80001046152c:       ldr     x20, [x29, #24]
    0.00 :   ffff800010461530:       b       ffff8000104614ec <blk_mq_flush_plug_list+0x154>
    0.00 :   ffff800010461534:       str     x20, [x29, #24]
         :                      }
    0.00 :   ffff800010461538:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (778 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102d5980 <aio_read>:
         :                      aio_read():
         :                      }
         :                      }
         :
         :                      static int aio_read(struct kiocb *req, const struct iocb *iocb,
         :                      bool vectored, bool compat)
         :                      {
    1.28 :   ffff8000102d5980:       stp     x29, x30, [sp, #-272]!
    0.00 :   ffff8000102d5984:       mov     x29, sp
    3.19 :   ffff8000102d5988:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000102d598c:       adrp    x19, ffff800011899000 <page_wait_table+0x1500>
    0.13 :   ffff8000102d5990:       str     x21, [sp, #32]
    0.00 :   ffff8000102d5994:       add     x4, x19, #0x8c8
    0.38 :   ffff8000102d5998:       stp     x23, x24, [sp, #48]
    0.00 :   ffff8000102d599c:       and     w24, w2, #0xff
    2.58 :   ffff8000102d59a0:       str     x25, [sp, #64]
    0.00 :   ffff8000102d59a4:       mov     x23, x1
    0.00 :   ffff8000102d59a8:       mov     x21, x0
    0.00 :   ffff8000102d59ac:       and     w25, w3, #0xff
    0.51 :   ffff8000102d59b0:       ldr     x2, [x4]
    0.38 :   ffff8000102d59b4:       str     x2, [x29, #264]
    0.00 :   ffff8000102d59b8:       mov     x2, #0x0                        // #0
         :                      struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
    0.00 :   ffff8000102d59bc:       add     x2, x29, #0x88
    0.51 :   ffff8000102d59c0:       str     x2, [x29, #88]
         :                      struct iov_iter iter;
         :                      struct file *file;
         :                      int ret;
         :
         :                      ret = aio_prep_rw(req, iocb);
    0.00 :   ffff8000102d59c4:       bl      ffff8000102d5708 <aio_prep_rw>
    2.06 :   ffff8000102d59c8:       mov     w20, w0
         :                      if (ret)
    0.00 :   ffff8000102d59cc:       cbnz    w0, ffff8000102d5a60 <aio_read+0xe0>
    1.03 :   ffff8000102d59d0:       str     x22, [x29, #40]
         :                      return ret;
         :                      file = req->ki_filp;
    4.86 :   ffff8000102d59d4:       ldr     x22, [x21]
         :                      if (unlikely(!(file->f_mode & FMODE_READ)))
    0.13 :   ffff8000102d59d8:       ldr     w1, [x22, #68]
    0.00 :   ffff8000102d59dc:       tbz     w1, #0, ffff8000102d5a98 <aio_read+0x118>
         :                      return -EBADF;
         :                      ret = -EINVAL;
         :                      if (unlikely(!file->f_op->read_iter))
    2.96 :   ffff8000102d59e0:       ldr     x1, [x22, #40]
    1.42 :   ffff8000102d59e4:       ldr     x1, [x1, #32]
    0.00 :   ffff8000102d59e8:       cbz     x1, ffff8000102d5ad8 <aio_read+0x158>
         :                      return -EINVAL;
         :
         :                      ret = aio_setup_rw(READ, iocb, &iovec, vectored, compat, &iter);
   14.66 :   ffff8000102d59ec:       ldp     x1, x2, [x23, #24]
    0.00 :   ffff8000102d59f0:       mov     w5, w25
    0.00 :   ffff8000102d59f4:       add     x6, x29, #0x60
    0.00 :   ffff8000102d59f8:       mov     w4, w24
    1.42 :   ffff8000102d59fc:       add     x3, x29, #0x58
    0.00 :   ffff8000102d5a00:       bl      ffff8000102d5908 <aio_setup_rw.isra.26>
    1.03 :   ffff8000102d5a04:       mov     w20, w0
         :                      if (ret < 0)
    0.00 :   ffff8000102d5a08:       tbnz    w0, #31, ffff8000102d5a90 <aio_read+0x110>
         :                      return ret;
         :                      ret = rw_verify_area(READ, file, &req->ki_pos, iov_iter_count(&iter));
    0.00 :   ffff8000102d5a0c:       ldr     x3, [x29, #112]
    0.00 :   ffff8000102d5a10:       add     x2, x21, #0x8
    0.00 :   ffff8000102d5a14:       mov     x1, x22
    0.00 :   ffff8000102d5a18:       mov     w0, #0x0                        // #0
    4.22 :   ffff8000102d5a1c:       bl      ffff8000102782f8 <rw_verify_area>
    0.00 :   ffff8000102d5a20:       mov     w20, w0
         :                      if (!ret)
    0.00 :   ffff8000102d5a24:       cbnz    w0, ffff8000102d5a50 <aio_read+0xd0>
         :                      call_read_iter():
         :                      } ____cacheline_aligned;
         :
         :                      static inline ssize_t call_read_iter(struct file *file, struct kiocb *kio,
         :                      struct iov_iter *iter)
         :                      {
         :                      return file->f_op->read_iter(kio, iter);
    0.00 :   ffff8000102d5a28:       ldr     x2, [x22, #40]
    0.00 :   ffff8000102d5a2c:       add     x1, x29, #0x60
    0.00 :   ffff8000102d5a30:       mov     x0, x21
    5.67 :   ffff8000102d5a34:       ldr     x2, [x2, #32]
    0.00 :   ffff8000102d5a38:       blr     x2
         :                      aio_rw_done():
         :                      switch (ret) {
    4.47 :   ffff8000102d5a3c:       cmn     x0, #0x204
    1.16 :   ffff8000102d5a40:       b.eq    ffff8000102d5ab0 <aio_read+0x130>  // b.none
    0.91 :   ffff8000102d5a44:       b.gt    ffff8000102d5aa4 <aio_read+0x124>
   10.55 :   ffff8000102d5a48:       cmn     x0, #0x211
    0.00 :   ffff8000102d5a4c:       b.ne    ffff8000102d5ab4 <aio_read+0x134>  // b.any
         :                      aio_read():
         :                      aio_rw_done(req, call_read_iter(file, req, &iter));
         :                      kfree(iovec);
    0.77 :   ffff8000102d5a50:       ldr     x0, [x29, #88]
    0.00 :   ffff8000102d5a54:       bl      ffff80001024fe88 <kfree>
         :                      return ret;
    6.83 :   ffff8000102d5a58:       ldr     x22, [x29, #40]
    0.00 :   ffff8000102d5a5c:       nop
         :                      }
   15.29 :   ffff8000102d5a60:       add     x19, x19, #0x8c8
    0.00 :   ffff8000102d5a64:       mov     w0, w20
    0.00 :   ffff8000102d5a68:       ldr     x2, [x29, #264]
    5.14 :   ffff8000102d5a6c:       ldr     x1, [x19]
    0.00 :   ffff8000102d5a70:       eor     x1, x2, x1
    0.00 :   ffff8000102d5a74:       cbnz    x1, ffff8000102d5ae4 <aio_read+0x164>
    0.00 :   ffff8000102d5a78:       ldp     x19, x20, [sp, #16]
    3.60 :   ffff8000102d5a7c:       ldr     x21, [sp, #32]
    0.00 :   ffff8000102d5a80:       ldp     x23, x24, [sp, #48]
    1.80 :   ffff8000102d5a84:       ldr     x25, [sp, #64]
    1.03 :   ffff8000102d5a88:       ldp     x29, x30, [sp], #272
    0.00 :   ffff8000102d5a8c:       ret
    0.00 :   ffff8000102d5a90:       ldr     x22, [x29, #40]
    0.00 :   ffff8000102d5a94:       b       ffff8000102d5a60 <aio_read+0xe0>
         :                      return -EBADF;
    0.00 :   ffff8000102d5a98:       mov     w20, #0xfffffff7                // #-9
    0.00 :   ffff8000102d5a9c:       ldr     x22, [x29, #40]
    0.00 :   ffff8000102d5aa0:       b       ffff8000102d5a60 <aio_read+0xe0>
         :                      aio_rw_done():
         :                      switch (ret) {
    0.00 :   ffff8000102d5aa4:       add     x1, x0, #0x202
    0.00 :   ffff8000102d5aa8:       cmp     x1, #0x2
    0.00 :   ffff8000102d5aac:       b.hi    ffff8000102d5ab4 <aio_read+0x134>  // b.pmore
         :                      ret = -EINTR;
    0.00 :   ffff8000102d5ab0:       mov     x0, #0xfffffffffffffffc         // #-4
         :                      req->ki_complete(req, ret, 0);
    0.00 :   ffff8000102d5ab4:       ldr     x3, [x21, #16]
    0.00 :   ffff8000102d5ab8:       mov     x1, x0
    0.00 :   ffff8000102d5abc:       mov     x2, #0x0                        // #0
    0.00 :   ffff8000102d5ac0:       mov     x0, x21
    0.00 :   ffff8000102d5ac4:       blr     x3
         :                      aio_read():
         :                      kfree(iovec);
    0.00 :   ffff8000102d5ac8:       ldr     x0, [x29, #88]
    0.00 :   ffff8000102d5acc:       bl      ffff80001024fe88 <kfree>
         :                      return ret;
    0.00 :   ffff8000102d5ad0:       ldr     x22, [x29, #40]
    0.00 :   ffff8000102d5ad4:       b       ffff8000102d5a60 <aio_read+0xe0>
         :                      return -EINVAL;
    0.00 :   ffff8000102d5ad8:       mov     w20, #0xffffffea                // #-22
    0.00 :   ffff8000102d5adc:       ldr     x22, [x29, #40]
    0.00 :   ffff8000102d5ae0:       b       ffff8000102d5a60 <aio_read+0xe0>
    0.00 :   ffff8000102d5ae4:       str     x22, [x29, #40]
         :                      }
    0.00 :   ffff8000102d5ae8:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (405 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001027bc88 <fput>:
         :                      fput():
         :                      schedule_delayed_work(&delayed_fput_work, 1);
         :                      }
         :                      }
         :
         :                      void fput(struct file *file)
         :                      {
   11.33 :   ffff80001027bc88:       stp     x29, x30, [sp, #-16]!
         :                      fput_many(file, 1);
    0.00 :   ffff80001027bc8c:       mov     w1, #0x1                        // #1
         :                      {
    0.00 :   ffff80001027bc90:       mov     x29, sp
         :                      fput_many(file, 1);
    0.00 :   ffff80001027bc94:       bl      ffff80001027bbc0 <fput_many>
         :                      }
   88.67 :   ffff80001027bc98:       ldp     x29, x30, [sp], #16
    0.00 :   ffff80001027bc9c:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (733 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104ab038 <__sbitmap_get_word>:
         :                      __sbitmap_get_word():
         :                      }
         :                      EXPORT_SYMBOL_GPL(sbitmap_resize);
         :
         :                      static int __sbitmap_get_word(unsigned long *word, unsigned long depth,
         :                      unsigned int hint, bool wrap)
         :                      {
   20.99 :   ffff8000104ab038:       stp     x29, x30, [sp, #-96]!
    0.00 :   ffff8000104ab03c:       mov     w2, w2
    0.00 :   ffff8000104ab040:       mov     x29, sp
    0.00 :   ffff8000104ab044:       stp     x19, x20, [sp, #16]
         :                      test_and_set_bit_lock():
         :                      */
         :                      static inline int test_and_set_bit_lock(unsigned int nr,
         :                      volatile unsigned long *p)
         :                      {
         :                      long old;
         :                      unsigned long mask = BIT_MASK(nr);
    0.00 :   ffff8000104ab048:       mov     x19, #0x1                       // #1
         :                      __sbitmap_get_word():
    0.00 :   ffff8000104ab04c:       stp     x21, x22, [sp, #32]
    0.00 :   ffff8000104ab050:       adrp    x21, ffff800011899000 <page_wait_table+0x1500>
    0.27 :   ffff8000104ab054:       stp     x23, x24, [sp, #48]
         :                      unsigned int orig_hint = hint;
    0.00 :   ffff8000104ab058:       mov     w22, w2
         :                      {
    1.78 :   ffff8000104ab05c:       stp     x25, x26, [sp, #64]
    0.00 :   ffff8000104ab060:       mov     x26, x1
    0.00 :   ffff8000104ab064:       add     x1, x21, #0x8c8
    0.00 :   ffff8000104ab068:       mov     x24, x0
    0.00 :   ffff8000104ab06c:       mov     x25, x2
    0.00 :   ffff8000104ab070:       and     w23, w3, #0xff
    0.00 :   ffff8000104ab074:       ldr     x0, [x1]
    0.14 :   ffff8000104ab078:       str     x0, [x29, #88]
    0.00 :   ffff8000104ab07c:       mov     x0, #0x0                        // #0
         :
         :                      if (!test_and_set_bit_lock(nr, word))
         :                      break;
         :
         :                      hint = nr + 1;
         :                      if (hint >= depth - 1)
    0.54 :   ffff8000104ab080:       sub     x20, x26, #0x1
    0.00 :   ffff8000104ab084:       nop
         :                      nr = find_next_zero_bit(word, depth, hint);
    0.95 :   ffff8000104ab088:       mov     x1, x26
    0.00 :   ffff8000104ab08c:       mov     x0, x24
    0.00 :   ffff8000104ab090:       bl      ffff800010487e20 <find_next_zero_bit>
    0.69 :   ffff8000104ab094:       mov     w4, w0
         :                      if (unlikely(nr >= depth)) {
    0.00 :   ffff8000104ab098:       cmp     x26, w0, sxtw
    0.00 :   ffff8000104ab09c:       b.ls    ffff8000104ab0e8 <__sbitmap_get_word+0xb0>  // b.plast
         :                      test_and_set_bit_lock():
         :
         :                      p += BIT_WORD(nr);
    4.25 :   ffff8000104ab0a0:       lsr     w1, w0, #6
         :                      unsigned long mask = BIT_MASK(nr);
    0.00 :   ffff8000104ab0a4:       lsl     x2, x19, x0
         :                      p += BIT_WORD(nr);
    0.00 :   ffff8000104ab0a8:       add     x3, x24, x1, lsl #3
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    7.92 :   ffff8000104ab0ac:       ldr     x1, [x24, x1, lsl #3]
         :                      test_and_set_bit_lock():
         :                      if (READ_ONCE(*p) & mask)
    0.00 :   ffff8000104ab0b0:       tst     x2, x1
    0.00 :   ffff8000104ab0b4:       b.eq    ffff8000104ab100 <__sbitmap_get_word+0xc8>  // b.none
         :                      __sbitmap_get_word():
         :                      hint = nr + 1;
    0.00 :   ffff8000104ab0b8:       add     w2, w0, #0x1
    0.00 :   ffff8000104ab0bc:       mov     x25, x2
         :                      if (hint >= depth - 1)
    0.00 :   ffff8000104ab0c0:       cmp     x2, x20
    0.00 :   ffff8000104ab0c4:       b.cc    ffff8000104ab088 <__sbitmap_get_word+0x50>  // b.lo, b.ul, b.last
    0.00 :   ffff8000104ab0c8:       mov     x2, #0x0                        // #0
         :                      nr = find_next_zero_bit(word, depth, hint);
    0.00 :   ffff8000104ab0cc:       mov     x1, x26
    0.00 :   ffff8000104ab0d0:       mov     x0, x24
         :                      hint = orig_hint = 0;
    0.00 :   ffff8000104ab0d4:       mov     w25, #0x0                       // #0
         :                      nr = find_next_zero_bit(word, depth, hint);
    0.00 :   ffff8000104ab0d8:       bl      ffff800010487e20 <find_next_zero_bit>
    0.00 :   ffff8000104ab0dc:       mov     w4, w0
         :                      if (unlikely(nr >= depth)) {
    0.00 :   ffff8000104ab0e0:       cmp     x26, w0, sxtw
    0.00 :   ffff8000104ab0e4:       b.hi    ffff8000104ab0a0 <__sbitmap_get_word+0x68>  // b.pmore
         :                      if (orig_hint && hint && wrap) {
    0.00 :   ffff8000104ab0e8:       cmp     w22, #0x0
    0.00 :   ffff8000104ab0ec:       ccmp    w25, #0x0, #0x4, ne  // ne = any
    0.00 :   ffff8000104ab0f0:       ccmp    w23, #0x0, #0x4, ne  // ne = any
    0.00 :   ffff8000104ab0f4:       b.eq    ffff8000104ab150 <__sbitmap_get_word+0x118>  // b.none
         :                      hint = orig_hint = 0;
    0.00 :   ffff8000104ab0f8:       mov     w22, #0x0                       // #0
    0.00 :   ffff8000104ab0fc:       b       ffff8000104ab0c8 <__sbitmap_get_word+0x90>
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    8.84 :   ffff8000104ab100:       b       ffff8000104ab148 <__sbitmap_get_word+0x110>
    1.49 :   ffff8000104ab104:       b       ffff8000104ab148 <__sbitmap_get_word+0x110>
         :                      __lse_atomic64_fetch_or_acquire():
         :                      ATOMIC64_FETCH_OP(_acquire,  a, op, asm_op, "memory")           \
         :                      ATOMIC64_FETCH_OP(_release,  l, op, asm_op, "memory")           \
         :                      ATOMIC64_FETCH_OP(        , al, op, asm_op, "memory")
         :
         :                      ATOMIC64_FETCH_OPS(andnot, ldclr)
         :                      ATOMIC64_FETCH_OPS(or, ldset)
    0.00 :   ffff8000104ab108:       mov     x1, x2
    0.00 :   ffff8000104ab10c:       ldseta  x1, x1, [x3]
         :                      __sbitmap_get_word():
         :                      if (!test_and_set_bit_lock(nr, word))
   45.98 :   ffff8000104ab110:       tst     x1, x2
    0.00 :   ffff8000104ab114:       b.ne    ffff8000104ab0b8 <__sbitmap_get_word+0x80>  // b.any
         :                      hint = 0;
         :                      }
         :
         :                      return nr;
         :                      }
    0.41 :   ffff8000104ab118:       add     x21, x21, #0x8c8
    0.00 :   ffff8000104ab11c:       mov     w0, w4
    3.15 :   ffff8000104ab120:       ldr     x2, [x29, #88]
    0.27 :   ffff8000104ab124:       ldr     x1, [x21]
    0.00 :   ffff8000104ab128:       eor     x1, x2, x1
    0.00 :   ffff8000104ab12c:       cbnz    x1, ffff8000104ab158 <__sbitmap_get_word+0x120>
    0.00 :   ffff8000104ab130:       ldp     x19, x20, [sp, #16]
    0.27 :   ffff8000104ab134:       ldp     x21, x22, [sp, #32]
    2.06 :   ffff8000104ab138:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff8000104ab13c:       ldp     x25, x26, [sp, #64]
    0.00 :   ffff8000104ab140:       ldp     x29, x30, [sp], #96
    0.00 :   ffff8000104ab144:       ret
         :                      __ll_sc_atomic64_fetch_or_acquire():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(and, and, L)
         :                      ATOMIC64_OPS(or, orr, L)
    0.00 :   ffff8000104ab148:       b       ffff8000104abd20 <__sbitmap_queue_get+0x170>
    0.00 :   ffff8000104ab14c:       b       ffff8000104ab110 <__sbitmap_get_word+0xd8>
         :                      __sbitmap_get_word():
         :                      return -1;
    0.00 :   ffff8000104ab150:       mov     w4, #0xffffffff                 // #-1
    0.00 :   ffff8000104ab154:       b       ffff8000104ab118 <__sbitmap_get_word+0xe0>
         :                      }
    0.00 :   ffff8000104ab158:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (758 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010450aa8 <bio_iov_iter_get_pages>:
         :                      bio_iov_iter_get_pages():
         :                      * fit into the bio, or are requested in *iter, whatever is smaller. If
         :                      * MM encounters an error pinning the requested pages, it stops. Error
         :                      * is returned only if 0 pages could be pinned.
         :                      */
         :                      int bio_iov_iter_get_pages(struct bio *bio, struct iov_iter *iter)
         :                      {
    0.66 :   ffff800010450aa8:       stp     x29, x30, [sp, #-144]!
    0.00 :   ffff800010450aac:       mov     x29, sp
    4.63 :   ffff800010450ab0:       str     x22, [sp, #40]
    0.00 :   ffff800010450ab4:       mov     x22, x1
    2.37 :   ffff800010450ab8:       str     x28, [sp, #88]
    0.00 :   ffff800010450abc:       mov     x28, x0
    1.71 :   ffff800010450ac0:       stp     x25, x26, [sp, #64]
    0.00 :   ffff800010450ac4:       adrp    x0, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010450ac8:       add     x0, x0, #0x8c8
    1.06 :   ffff800010450acc:       ldr     x1, [x0]
   14.04 :   ffff800010450ad0:       str     x1, [x29, #136]
    0.00 :   ffff800010450ad4:       mov     x1, #0x0                        // #0
         :                      iov_iter_type():
         :                      };
         :                      };
         :
         :                      static inline enum iter_type iov_iter_type(const struct iov_iter *i)
         :                      {
         :                      return i->type & ~(READ | WRITE);
    0.67 :   ffff800010450ad8:       ldr     w25, [x22]
         :                      bio_iov_iter_get_pages():
         :                      const bool is_bvec = iov_iter_is_bvec(iter);
         :                      int ret;
         :
         :                      if (WARN_ON_ONCE(bio->bi_vcnt))
    0.00 :   ffff800010450adc:       ldrh    w0, [x28, #96]
         :                      iov_iter_type():
    0.00 :   ffff800010450ae0:       and     w1, w25, #0xfffffffe
    0.00 :   ffff800010450ae4:       str     w1, [x29, #108]
         :                      bio_iov_iter_get_pages():
    0.00 :   ffff800010450ae8:       cbnz    w0, ffff800010450d44 <bio_iov_iter_get_pages+0x29c>
    2.25 :   ffff800010450aec:       stp     x19, x20, [x29, #16]
    0.78 :   ffff800010450af0:       str     x21, [x29, #32]
    0.40 :   ffff800010450af4:       stp     x23, x24, [x29, #48]
         :                      __ll_sc_atomic_sub_return():
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff800010450af8:       mov     w23, #0x1                       // #1
    1.19 :   ffff800010450afc:       str     x27, [x29, #80]
         :                      bio_iov_iter_get_pages():
         :                      return -EINVAL;
         :
         :                      do {
         :                      if (is_bvec)
   13.68 :   ffff800010450b00:       ldr     w0, [x29, #108]
    0.00 :   ffff800010450b04:       cmp     w0, #0x10
    0.00 :   ffff800010450b08:       b.eq    ffff800010450cd0 <bio_iov_iter_get_pages+0x228>  // b.none
         :                      __bio_iov_iter_get_pages():
         :                      unsigned short nr_pages = bio->bi_max_vecs - bio->bi_vcnt;
   10.03 :   ffff800010450b0c:       ldrh    w5, [x28, #96]
         :                      size = iov_iter_get_pages(iter, pages, LONG_MAX, nr_pages, &offset);
    0.00 :   ffff800010450b10:       add     x4, x29, #0x80
         :                      unsigned short nr_pages = bio->bi_max_vecs - bio->bi_vcnt;
    6.60 :   ffff800010450b14:       ldrh    w1, [x28, #98]
         :                      size = iov_iter_get_pages(iter, pages, LONG_MAX, nr_pages, &offset);
    0.00 :   ffff800010450b18:       mov     x2, #0x7fffffffffffffff         // #9223372036854775807
         :                      struct bio_vec *bv = bio->bi_io_vec + bio->bi_vcnt;
    0.00 :   ffff800010450b1c:       ldr     x20, [x28, #104]
         :                      size = iov_iter_get_pages(iter, pages, LONG_MAX, nr_pages, &offset);
    0.00 :   ffff800010450b20:       mov     x0, x22
         :                      unsigned short nr_pages = bio->bi_max_vecs - bio->bi_vcnt;
    0.00 :   ffff800010450b24:       sub     w1, w1, w5
         :                      bool same_page = false;
    1.97 :   ffff800010450b28:       strb    wzr, [x29, #127]
         :                      unsigned short nr_pages = bio->bi_max_vecs - bio->bi_vcnt;
    0.00 :   ffff800010450b2c:       and     w1, w1, #0xffff
         :                      size = iov_iter_get_pages(iter, pages, LONG_MAX, nr_pages, &offset);
    0.00 :   ffff800010450b30:       mov     w3, w1
         :                      pages += entries_left * (PAGE_PTRS_PER_BVEC - 1);
    0.00 :   ffff800010450b34:       ubfiz   x1, x1, #3, #16
    0.39 :   ffff800010450b38:       add     x1, x1, w5, uxtw #4
    0.00 :   ffff800010450b3c:       add     x20, x20, x1
         :                      size = iov_iter_get_pages(iter, pages, LONG_MAX, nr_pages, &offset);
    0.26 :   ffff800010450b40:       mov     x1, x20
    0.00 :   ffff800010450b44:       bl      ffff800010482e28 <iov_iter_get_pages>
    0.00 :   ffff800010450b48:       mov     x24, x0
         :                      if (unlikely(size <= 0))
    0.00 :   ffff800010450b4c:       cmp     x0, #0x0
    0.00 :   ffff800010450b50:       b.le    ffff800010450d20 <bio_iov_iter_get_pages+0x278>
    2.25 :   ffff800010450b54:       mov     x27, x0
         :                      for (left = size, i = 0; left > 0; left -= len, i++) {
    0.00 :   ffff800010450b58:       mov     w26, #0x0                       // #0
         :                      len = min_t(size_t, PAGE_SIZE - offset, left);
    0.00 :   ffff800010450b5c:       mov     x21, #0x1000                    // #4096
    0.00 :   ffff800010450b60:       ldr     x3, [x29, #128]
    0.00 :   ffff800010450b64:       b       ffff800010450b88 <bio_iov_iter_get_pages+0xe0>
         :                      if (same_page)
    0.00 :   ffff800010450b68:       ldrb    w0, [x29, #127]
    0.00 :   ffff800010450b6c:       cbnz    w0, ffff800010450c88 <bio_iov_iter_get_pages+0x1e0>
         :                      offset = 0;
    0.00 :   ffff800010450b70:       str     xzr, [x29, #128]
         :                      for (left = size, i = 0; left > 0; left -= len, i++) {
    0.00 :   ffff800010450b74:       sub     x27, x27, w19, uxtw
    0.00 :   ffff800010450b78:       add     w26, w26, #0x1
    0.00 :   ffff800010450b7c:       mov     x3, #0x0                        // #0
    0.00 :   ffff800010450b80:       cmp     x27, #0x0
    0.00 :   ffff800010450b84:       b.le    ffff800010450c50 <bio_iov_iter_get_pages+0x1a8>
         :                      len = min_t(size_t, PAGE_SIZE - offset, left);
    0.13 :   ffff800010450b88:       sub     x19, x21, x3
         :                      struct page *page = pages[i];
    0.00 :   ffff800010450b8c:       ldr     x25, [x20, w26, uxtw #3]
         :                      len = min_t(size_t, PAGE_SIZE - offset, left);
    0.00 :   ffff800010450b90:       cmp     x19, x27
         :                      if (__bio_try_merge_page(bio, page, len, offset, &same_page)) {
    0.00 :   ffff800010450b94:       add     x4, x29, #0x7f
         :                      len = min_t(size_t, PAGE_SIZE - offset, left);
    0.00 :   ffff800010450b98:       csel    x19, x19, x27, ls  // ls = plast
         :                      if (__bio_try_merge_page(bio, page, len, offset, &same_page)) {
    3.29 :   ffff800010450b9c:       mov     x1, x25
    0.00 :   ffff800010450ba0:       mov     w2, w19
    0.00 :   ffff800010450ba4:       mov     x0, x28
    0.00 :   ffff800010450ba8:       bl      ffff80001044e8e0 <__bio_try_merge_page>
    0.52 :   ffff800010450bac:       tst     w0, #0xff
    0.00 :   ffff800010450bb0:       b.ne    ffff800010450b68 <bio_iov_iter_get_pages+0xc0>  // b.any
         :                      bio_full():
         :                      * Return true if @bio is full and one segment with @len bytes can't be
         :                      * added to the bio, otherwise return false
         :                      */
         :                      static inline bool bio_full(struct bio *bio, unsigned len)
         :                      {
         :                      if (bio->bi_vcnt >= bio->bi_max_vecs)
    0.66 :   ffff800010450bb4:       ldrh    w1, [x28, #96]
    1.19 :   ffff800010450bb8:       ldrh    w0, [x28, #98]
    0.00 :   ffff800010450bbc:       cmp     w1, w0
    0.00 :   ffff800010450bc0:       b.cc    ffff800010450c14 <bio_iov_iter_get_pages+0x16c>  // b.lo, b.ul, b.last
         :                      __bio_iov_iter_get_pages():
         :                      if (WARN_ON_ONCE(bio_full(bio, len)))
    0.00 :   ffff800010450bc4:       brk     #0x800
         :                      return -EINVAL;
    0.00 :   ffff800010450bc8:       mov     w0, #0xffffffea                 // #-22
         :                      bio_iov_iter_get_pages():
         :                      ret = __bio_iov_iter_get_pages(bio, iter);
         :                      } while (!ret && iov_iter_count(iter) && !bio_full(bio, 0));
         :
         :                      if (is_bvec)
         :                      bio_set_flag(bio, BIO_NO_PAGE_REF);
         :                      return bio->bi_vcnt ? 0 : ret;
    1.58 :   ffff800010450bcc:       ldrh    w1, [x28, #96]
    5.41 :   ffff800010450bd0:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff800010450bd4:       cmp     w1, #0x0
    1.05 :   ffff800010450bd8:       ldr     x21, [x29, #32]
    0.00 :   ffff800010450bdc:       csel    w0, w0, wzr, eq  // eq = none
    0.92 :   ffff800010450be0:       ldp     x23, x24, [x29, #48]
    0.79 :   ffff800010450be4:       ldr     x27, [x29, #80]
         :                      }
    0.00 :   ffff800010450be8:       adrp    x1, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010450bec:       add     x26, x1, #0x8c8
    3.81 :   ffff800010450bf0:       ldr     x2, [x29, #136]
    0.26 :   ffff800010450bf4:       ldr     x1, [x26]
    0.00 :   ffff800010450bf8:       eor     x1, x2, x1
    0.00 :   ffff800010450bfc:       cbnz    x1, ffff800010450d58 <bio_iov_iter_get_pages+0x2b0>
    1.58 :   ffff800010450c00:       ldr     x22, [sp, #40]
    3.56 :   ffff800010450c04:       ldp     x25, x26, [sp, #64]
    0.40 :   ffff800010450c08:       ldr     x28, [sp, #88]
    0.40 :   ffff800010450c0c:       ldp     x29, x30, [sp], #144
    0.00 :   ffff800010450c10:       ret
         :                      bio_full():
         :                      return true;
         :
         :                      if (bio->bi_iter.bi_size > UINT_MAX - len)
    0.00 :   ffff800010450c14:       ldr     w1, [x28, #40]
    0.00 :   ffff800010450c18:       mvn     w0, w19
    0.00 :   ffff800010450c1c:       cmp     w1, w0
    0.00 :   ffff800010450c20:       b.hi    ffff800010450bc4 <bio_iov_iter_get_pages+0x11c>  // b.pmore
         :                      __bio_iov_iter_get_pages():
         :                      __bio_add_page(bio, page, len, offset);
    0.13 :   ffff800010450c24:       ldr     w3, [x29, #128]
    0.00 :   ffff800010450c28:       mov     w2, w19
    0.00 :   ffff800010450c2c:       mov     x1, x25
    0.00 :   ffff800010450c30:       mov     x0, x28
         :                      for (left = size, i = 0; left > 0; left -= len, i++) {
    2.63 :   ffff800010450c34:       sub     x27, x27, w19, uxtw
    0.00 :   ffff800010450c38:       add     w26, w26, #0x1
         :                      __bio_add_page(bio, page, len, offset);
    0.00 :   ffff800010450c3c:       bl      ffff80001044e420 <__bio_add_page>
         :                      offset = 0;
    2.38 :   ffff800010450c40:       str     xzr, [x29, #128]
    0.00 :   ffff800010450c44:       mov     x3, #0x0                        // #0
         :                      for (left = size, i = 0; left > 0; left -= len, i++) {
    0.00 :   ffff800010450c48:       cmp     x27, #0x0
    0.00 :   ffff800010450c4c:       b.gt    ffff800010450b88 <bio_iov_iter_get_pages+0xe0>
         :                      iov_iter_advance(iter, size);
    0.00 :   ffff800010450c50:       mov     x1, x24
    0.00 :   ffff800010450c54:       mov     x0, x22
    0.00 :   ffff800010450c58:       bl      ffff800010483fe8 <iov_iter_advance>
         :                      bio_iov_iter_get_pages():
         :                      } while (!ret && iov_iter_count(iter) && !bio_full(bio, 0));
    0.39 :   ffff800010450c5c:       ldr     x0, [x22, #16]
    2.37 :   ffff800010450c60:       ldrh    w1, [x28, #96]
    0.00 :   ffff800010450c64:       cbz     x0, ffff800010450c74 <bio_iov_iter_get_pages+0x1cc>
         :                      bio_full():
         :                      if (bio->bi_vcnt >= bio->bi_max_vecs)
    0.00 :   ffff800010450c68:       ldrh    w0, [x28, #98]
    0.00 :   ffff800010450c6c:       cmp     w0, w1
    0.00 :   ffff800010450c70:       b.hi    ffff800010450b00 <bio_iov_iter_get_pages+0x58>  // b.pmore
         :                      bio_iov_iter_get_pages():
         :                      if (is_bvec)
    0.26 :   ffff800010450c74:       ldr     w1, [x29, #108]
    0.00 :   ffff800010450c78:       mov     w0, #0x0                        // #0
    0.00 :   ffff800010450c7c:       cmp     w1, #0x10
    1.31 :   ffff800010450c80:       b.ne    ffff800010450bcc <bio_iov_iter_get_pages+0x124>  // b.any
    0.00 :   ffff800010450c84:       b       ffff800010450d34 <bio_iov_iter_get_pages+0x28c>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010450c88:       ldr     x0, [x25, #8]
         :                      compound_head():
         :                      static inline struct page *compound_head(struct page *page)
         :                      {
         :                      unsigned long head = READ_ONCE(page->compound_head);
         :
         :                      if (unlikely(head & 1))
         :                      return (struct page *) (head - 1);
    0.00 :   ffff800010450c8c:       sub     x1, x0, #0x1
    0.00 :   ffff800010450c90:       tst     x0, #0x1
    0.00 :   ffff800010450c94:       csel    x25, x1, x25, ne  // ne = any
         :                      page_ref_dec_and_test():
         :                      return ret;
         :                      }
         :
         :                      static inline int page_ref_dec_and_test(struct page *page)
         :                      {
         :                      int ret = atomic_dec_and_test(&page->_refcount);
    0.00 :   ffff800010450c98:       add     x0, x25, #0x34
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010450c9c:       b       ffff800010450cc4 <bio_iov_iter_get_pages+0x21c>
    0.00 :   ffff800010450ca0:       b       ffff800010450cc4 <bio_iov_iter_get_pages+0x21c>
         :                      __lse_atomic_sub_return():
         :                      }
         :
         :                      ATOMIC_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff800010450ca4:       mov     w1, w23
    0.00 :   ffff800010450ca8:       neg     w1, w1
    0.00 :   ffff800010450cac:       ldaddal w1, w2, [x0]
    0.00 :   ffff800010450cb0:       add     w1, w1, w2
         :                      put_page():
         :                      * include/linux/memremap.h and HMM for details.
         :                      */
         :                      if (put_devmap_managed_page(page))
         :                      return;
         :
         :                      if (put_page_testzero(page))
    0.00 :   ffff800010450cb4:       cbnz    w1, ffff800010450b70 <bio_iov_iter_get_pages+0xc8>
         :                      __put_page(page);
    0.00 :   ffff800010450cb8:       mov     x0, x25
    0.00 :   ffff800010450cbc:       bl      ffff8000101de470 <__put_page>
    0.00 :   ffff800010450cc0:       b       ffff800010450b70 <bio_iov_iter_get_pages+0xc8>
         :                      __ll_sc_atomic_sub_return():
    0.00 :   ffff800010450cc4:       b       ffff80001045216c <bio_associate_blkg_from_page+0x134>
         :                      put_page():
         :                      if (put_page_testzero(page))
    0.00 :   ffff800010450cc8:       cbnz    w1, ffff800010450b70 <bio_iov_iter_get_pages+0xc8>
    0.00 :   ffff800010450ccc:       b       ffff800010450cb8 <bio_iov_iter_get_pages+0x210>
         :                      __bio_iov_bvec_add_pages():
         :                      const struct bio_vec *bv = iter->bvec;
    0.00 :   ffff800010450cd0:       ldr     x3, [x22, #24]
         :                      if (WARN_ON_ONCE(iter->iov_offset > bv->bv_len))
    0.00 :   ffff800010450cd4:       ldr     x2, [x22, #8]
    0.00 :   ffff800010450cd8:       ldr     w0, [x3, #8]
    0.00 :   ffff800010450cdc:       cmp     x2, x0
    0.00 :   ffff800010450ce0:       b.hi    ffff800010450d2c <bio_iov_iter_get_pages+0x284>  // b.pmore
         :                      len = min_t(size_t, bv->bv_len - iter->iov_offset, iter->count);
    0.00 :   ffff800010450ce4:       ldr     x4, [x22, #16]
    0.00 :   ffff800010450ce8:       sub     x19, x0, x2
         :                      size = bio_add_page(bio, bv->bv_page, len,
    0.00 :   ffff800010450cec:       ldr     x1, [x3]
    0.00 :   ffff800010450cf0:       mov     x0, x28
         :                      len = min_t(size_t, bv->bv_len - iter->iov_offset, iter->count);
    0.00 :   ffff800010450cf4:       cmp     x19, x4
         :                      size = bio_add_page(bio, bv->bv_page, len,
    0.00 :   ffff800010450cf8:       ldr     w3, [x3, #12]
         :                      len = min_t(size_t, bv->bv_len - iter->iov_offset, iter->count);
    0.00 :   ffff800010450cfc:       csel    x19, x19, x4, ls  // ls = plast
         :                      size = bio_add_page(bio, bv->bv_page, len,
    0.00 :   ffff800010450d00:       add     w3, w3, w2
    0.00 :   ffff800010450d04:       mov     w2, w19
    0.00 :   ffff800010450d08:       bl      ffff80001044ea38 <bio_add_page>
    0.00 :   ffff800010450d0c:       sxtw    x1, w0
         :                      if (unlikely(size != len))
    0.00 :   ffff800010450d10:       cmp     x1, w19, uxtw
    0.00 :   ffff800010450d14:       b.eq    ffff800010450c54 <bio_iov_iter_get_pages+0x1ac>  // b.none
    0.00 :   ffff800010450d18:       mov     w0, #0xffffffea                 // #-22
    0.00 :   ffff800010450d1c:       b       ffff800010450d34 <bio_iov_iter_get_pages+0x28c>
         :                      __bio_iov_iter_get_pages():
         :                      return size ? size : -EFAULT;
    0.00 :   ffff800010450d20:       b.eq    ffff800010450d50 <bio_iov_iter_get_pages+0x2a8>  // b.none
         :                      bio_iov_iter_get_pages():
         :                      } while (!ret && iov_iter_count(iter) && !bio_full(bio, 0));
    0.00 :   ffff800010450d24:       cbz     w0, ffff800010450c5c <bio_iov_iter_get_pages+0x1b4>
    0.00 :   ffff800010450d28:       b       ffff800010450bcc <bio_iov_iter_get_pages+0x124>
         :                      __bio_iov_bvec_add_pages():
         :                      if (WARN_ON_ONCE(iter->iov_offset > bv->bv_len))
    0.00 :   ffff800010450d2c:       brk     #0x800
    0.00 :   ffff800010450d30:       mov     w0, #0xffffffea                 // #-22
         :                      bio_set_flag():
         :                      return (bio->bi_flags & (1U << bit)) != 0;
         :                      }
         :
         :                      static inline void bio_set_flag(struct bio *bio, unsigned int bit)
         :                      {
         :                      bio->bi_flags |= (1U << bit);
    0.00 :   ffff800010450d34:       ldrh    w1, [x28, #20]
    0.00 :   ffff800010450d38:       orr     w1, w1, #0x1
    0.00 :   ffff800010450d3c:       strh    w1, [x28, #20]
    0.00 :   ffff800010450d40:       b       ffff800010450bcc <bio_iov_iter_get_pages+0x124>
         :                      bio_iov_iter_get_pages():
         :                      if (WARN_ON_ONCE(bio->bi_vcnt))
    0.00 :   ffff800010450d44:       brk     #0x800
         :                      return -EINVAL;
    0.00 :   ffff800010450d48:       mov     w0, #0xffffffea                 // #-22
    0.00 :   ffff800010450d4c:       b       ffff800010450be8 <bio_iov_iter_get_pages+0x140>
         :                      __bio_iov_iter_get_pages():
         :                      return size ? size : -EFAULT;
    0.00 :   ffff800010450d50:       mov     w0, #0xfffffff2                 // #-14
    0.00 :   ffff800010450d54:       b       ffff800010450bcc <bio_iov_iter_get_pages+0x124>
    0.00 :   ffff800010450d58:       stp     x19, x20, [x29, #16]
    0.00 :   ffff800010450d5c:       str     x21, [x29, #32]
    0.00 :   ffff800010450d60:       stp     x23, x24, [x29, #48]
    0.00 :   ffff800010450d64:       str     x27, [x29, #80]
         :                      bio_iov_iter_get_pages():
         :                      }
    0.00 :   ffff800010450d68:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (733 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000107ba148 <nvme_setup_cmd>:
         :                      nvme_setup_cmd():
         :                      }
         :                      EXPORT_SYMBOL_GPL(nvme_cleanup_cmd);
         :
         :                      blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
         :                      struct nvme_command *cmd)
         :                      {
    0.00 :   ffff8000107ba148:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff8000107ba14c:       mov     x29, sp
    4.65 :   ffff8000107ba150:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000107ba154:       mov     x20, x1
    0.00 :   ffff8000107ba158:       mov     x19, x2
         :                      nvme_clear_nvme_request():
         :                      if (!(req->rq_flags & RQF_DONTPREP)) {
    0.81 :   ffff8000107ba15c:       ldr     w1, [x1, #28]
    0.00 :   ffff8000107ba160:       tbnz    w1, #7, ffff8000107ba170 <nvme_setup_cmd+0x28>
         :                      req->rq_flags |= RQF_DONTPREP;
    2.85 :   ffff8000107ba164:       orr     w1, w1, #0x80
    0.13 :   ffff8000107ba168:       str     w1, [x20, #28]
         :                      nvme_req(req)->retries = 0;
    0.41 :   ffff8000107ba16c:       strh    wzr, [x20, #296]
         :                      nvme_setup_cmd():
         :                      blk_status_t ret = BLK_STS_OK;
         :
         :                      nvme_clear_nvme_request(req);
         :
         :                      memset(cmd, 0, sizeof(*cmd));
    0.14 :   ffff8000107ba170:       stp     xzr, xzr, [x19]
    0.95 :   ffff8000107ba174:       stp     xzr, xzr, [x19, #16]
    0.41 :   ffff8000107ba178:       stp     xzr, xzr, [x19, #32]
    0.14 :   ffff8000107ba17c:       stp     xzr, xzr, [x19, #48]
         :                      switch (req_op(req)) {
    0.68 :   ffff8000107ba180:       ldr     w2, [x20, #24]
    0.00 :   ffff8000107ba184:       and     w1, w2, #0xff
    0.00 :   ffff8000107ba188:       cmp     w1, #0x3
    0.00 :   ffff8000107ba18c:       b.eq    ffff8000107ba20c <nvme_setup_cmd+0xc4>  // b.none
    8.20 :   ffff8000107ba190:       b.hi    ffff8000107ba1c8 <nvme_setup_cmd+0x80>  // b.pmore
    3.54 :   ffff8000107ba194:       cmp     w1, #0x1
    0.00 :   ffff8000107ba198:       b.ls    ffff8000107ba28c <nvme_setup_cmd+0x144>  // b.plast
         :                      nvme_setup_flush():
         :                      cmnd->common.opcode = nvme_cmd_flush;
    0.00 :   ffff8000107ba19c:       strb    wzr, [x19]
         :                      nvme_setup_cmd():
         :                      blk_status_t ret = BLK_STS_OK;
    0.00 :   ffff8000107ba1a0:       mov     w1, #0x0                        // #0
         :                      nvme_setup_flush():
         :                      cmnd->common.nsid = cpu_to_le32(ns->head->ns_id);
    0.00 :   ffff8000107ba1a4:       ldr     x0, [x0, #72]
    0.00 :   ffff8000107ba1a8:       ldr     w0, [x0, #1968]
    0.00 :   ffff8000107ba1ac:       str     w0, [x19, #4]
         :                      nvme_setup_cmd():
         :                      default:
         :                      WARN_ON_ONCE(1);
         :                      return BLK_STS_IOERR;
         :                      }
         :
         :                      cmd->common.command_id = req->tag;
    0.00 :   ffff8000107ba1b0:       ldr     w0, [x20, #32]
    2.99 :   ffff8000107ba1b4:       strh    w0, [x19, #2]
         :                      trace_nvme_setup_cmd(req, cmd);
         :                      return ret;
         :                      }
    0.00 :   ffff8000107ba1b8:       mov     w0, w1
    0.00 :   ffff8000107ba1bc:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000107ba1c0:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000107ba1c4:       ret
         :                      switch (req_op(req)) {
    0.00 :   ffff8000107ba1c8:       cmp     w1, #0x9
    0.00 :   ffff8000107ba1cc:       b.eq    ffff8000107ba238 <nvme_setup_cmd+0xf0>  // b.none
    0.00 :   ffff8000107ba1d0:       b.cc    ffff8000107ba220 <nvme_setup_cmd+0xd8>  // b.lo, b.ul, b.last
    0.00 :   ffff8000107ba1d4:       sub     w1, w1, #0x22
    0.00 :   ffff8000107ba1d8:       cmp     w1, #0x1
    0.00 :   ffff8000107ba1dc:       b.hi    ffff8000107ba220 <nvme_setup_cmd+0xd8>  // b.pmore
         :                      memcpy(cmd, nvme_req(req)->cmd, sizeof(*cmd));
    0.00 :   ffff8000107ba1e0:       ldr     x0, [x20, #280]
         :                      blk_status_t ret = BLK_STS_OK;
    0.00 :   ffff8000107ba1e4:       mov     w1, #0x0                        // #0
         :                      memcpy(cmd, nvme_req(req)->cmd, sizeof(*cmd));
    0.00 :   ffff8000107ba1e8:       ldp     x2, x3, [x0]
    0.00 :   ffff8000107ba1ec:       stp     x2, x3, [x19]
    0.00 :   ffff8000107ba1f0:       ldp     x2, x3, [x0, #16]
    0.00 :   ffff8000107ba1f4:       stp     x2, x3, [x19, #16]
    0.00 :   ffff8000107ba1f8:       ldp     x2, x3, [x0, #32]
    0.00 :   ffff8000107ba1fc:       stp     x2, x3, [x19, #32]
    0.00 :   ffff8000107ba200:       ldp     x2, x3, [x0, #48]
    0.00 :   ffff8000107ba204:       stp     x2, x3, [x19, #48]
         :                      break;
    0.00 :   ffff8000107ba208:       b       ffff8000107ba1b0 <nvme_setup_cmd+0x68>
         :                      ret = nvme_setup_discard(ns, req, cmd);
    0.00 :   ffff8000107ba20c:       mov     x1, x20
    0.00 :   ffff8000107ba210:       mov     x2, x19
    0.00 :   ffff8000107ba214:       bl      ffff8000107b9f48 <nvme_setup_discard>
    0.00 :   ffff8000107ba218:       and     w1, w0, #0xff
         :                      break;
    0.00 :   ffff8000107ba21c:       b       ffff8000107ba1b0 <nvme_setup_cmd+0x68>
         :                      WARN_ON_ONCE(1);
    0.00 :   ffff8000107ba220:       brk     #0x800
         :                      return BLK_STS_IOERR;
    0.00 :   ffff8000107ba224:       mov     w1, #0xa                        // #10
         :                      }
    0.00 :   ffff8000107ba228:       mov     w0, w1
    0.00 :   ffff8000107ba22c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000107ba230:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000107ba234:       ret
         :                      nvme_setup_write_zeroes():
         :                      if (ns->ctrl->quirks & NVME_QUIRK_DEALLOCATE_ZEROES)
    0.00 :   ffff8000107ba238:       ldr     x1, [x0, #16]
    0.00 :   ffff8000107ba23c:       ldr     x1, [x1, #1288]
    0.00 :   ffff8000107ba240:       tbnz    w1, #2, ffff8000107ba20c <nvme_setup_cmd+0xc4>
         :                      cmnd->write_zeroes.opcode = nvme_cmd_write_zeroes;
    0.00 :   ffff8000107ba244:       mov     w1, #0x8                        // #8
    0.00 :   ffff8000107ba248:       strb    w1, [x19]
         :                      return BLK_STS_OK;
    0.00 :   ffff8000107ba24c:       mov     w1, #0x0                        // #0
         :                      cmnd->write_zeroes.nsid = cpu_to_le32(ns->head->ns_id);
    0.00 :   ffff8000107ba250:       ldr     x2, [x0, #72]
    0.00 :   ffff8000107ba254:       ldr     w2, [x2, #1968]
    0.00 :   ffff8000107ba258:       str     w2, [x19, #4]
         :                      nvme_sect_to_lba():
         :                      /*
         :                      * Convert a 512B sector number to a device logical block number.
         :                      */
         :                      static inline u64 nvme_sect_to_lba(struct nvme_ns *ns, sector_t sector)
         :                      {
         :                      return sector >> (ns->lba_shift - SECTOR_SHIFT);
    0.00 :   ffff8000107ba25c:       ldr     w3, [x0, #80]
    0.00 :   ffff8000107ba260:       ldr     x2, [x20, #48]
    0.00 :   ffff8000107ba264:       sub     w3, w3, #0x9
    0.00 :   ffff8000107ba268:       lsr     x2, x2, x3
         :                      nvme_setup_write_zeroes():
         :                      cmnd->write_zeroes.slba =
    0.00 :   ffff8000107ba26c:       str     x2, [x19, #40]
         :                      cpu_to_le16((blk_rq_bytes(req) >> ns->lba_shift) - 1);
    0.00 :   ffff8000107ba270:       ldr     w2, [x0, #80]
    0.00 :   ffff8000107ba274:       ldr     w0, [x20, #40]
         :                      cmnd->write_zeroes.control = 0;
    0.00 :   ffff8000107ba278:       strh    wzr, [x19, #50]
         :                      cpu_to_le16((blk_rq_bytes(req) >> ns->lba_shift) - 1);
    0.00 :   ffff8000107ba27c:       lsr     w0, w0, w2
    0.00 :   ffff8000107ba280:       sub     w0, w0, #0x1
         :                      cmnd->write_zeroes.length =
    0.00 :   ffff8000107ba284:       strh    w0, [x19, #48]
    0.00 :   ffff8000107ba288:       b       ffff8000107ba1b0 <nvme_setup_cmd+0x68>
         :                      nvme_setup_rw():
         :                      u16 control = 0;
    1.23 :   ffff8000107ba28c:       ubfx    x1, x2, #17, #1
         :                      if (req->cmd_flags & (REQ_FAILFAST_DEV | REQ_RAHEAD))
    0.00 :   ffff8000107ba290:       and     w4, w2, #0xfff00
    0.00 :   ffff8000107ba294:       and     w4, w4, #0xfff801ff
         :                      u32 dsmgmt = 0;
    0.00 :   ffff8000107ba298:       mov     w3, #0x7                        // #7
         :                      u16 control = 0;
    0.14 :   ffff8000107ba29c:       lsl     w1, w1, #14
         :                      control |= NVME_RW_LR;
    0.00 :   ffff8000107ba2a0:       cmp     w4, #0x0
    0.00 :   ffff8000107ba2a4:       orr     w4, w1, #0xffff8000
         :                      struct nvme_ctrl *ctrl = ns->ctrl;
    0.00 :   ffff8000107ba2a8:       ldr     x5, [x0, #16]
         :                      control |= NVME_RW_LR;
    0.00 :   ffff8000107ba2ac:       and     w4, w4, #0xffff
    0.00 :   ffff8000107ba2b0:       csel    w1, w4, w1, ne  // ne = any
         :                      u32 dsmgmt = 0;
    0.00 :   ffff8000107ba2b4:       tst     x2, #0x80000
   35.18 :   ffff8000107ba2b8:       csel    w3, w3, wzr, ne  // ne = any
         :                      cmnd->rw.opcode = (rq_data_dir(req) ? nvme_cmd_write : nvme_cmd_read);
    0.00 :   ffff8000107ba2bc:       tst     x2, #0x1
    0.00 :   ffff8000107ba2c0:       cset    w2, eq  // eq = none
    0.00 :   ffff8000107ba2c4:       add     w2, w2, #0x1
    0.00 :   ffff8000107ba2c8:       strb    w2, [x19]
         :                      cmnd->rw.nsid = cpu_to_le32(ns->head->ns_id);
    0.27 :   ffff8000107ba2cc:       ldr     x2, [x0, #72]
    3.41 :   ffff8000107ba2d0:       ldr     w2, [x2, #1968]
   30.30 :   ffff8000107ba2d4:       str     w2, [x19, #4]
         :                      nvme_sect_to_lba():
    0.00 :   ffff8000107ba2d8:       ldr     w4, [x0, #80]
    0.14 :   ffff8000107ba2dc:       ldr     x2, [x20, #48]
    0.00 :   ffff8000107ba2e0:       sub     w4, w4, #0x9
    0.00 :   ffff8000107ba2e4:       lsr     x2, x2, x4
         :                      nvme_setup_rw():
         :                      cmnd->rw.slba = cpu_to_le64(nvme_sect_to_lba(ns, blk_rq_pos(req)));
    0.00 :   ffff8000107ba2e8:       str     x2, [x19, #40]
         :                      cmnd->rw.length = cpu_to_le16((blk_rq_bytes(req) >> ns->lba_shift) - 1);
    1.50 :   ffff8000107ba2ec:       ldr     w4, [x0, #80]
    0.00 :   ffff8000107ba2f0:       ldr     w2, [x20, #40]
    0.00 :   ffff8000107ba2f4:       lsr     w2, w2, w4
    0.00 :   ffff8000107ba2f8:       sub     w2, w2, #0x1
    0.00 :   ffff8000107ba2fc:       strh    w2, [x19, #48]
         :                      if (req_op(req) == REQ_OP_WRITE && ctrl->nr_streams)
    0.00 :   ffff8000107ba300:       ldrb    w2, [x20, #24]
    0.00 :   ffff8000107ba304:       cmp     w2, #0x1
    0.00 :   ffff8000107ba308:       b.eq    ffff8000107ba36c <nvme_setup_cmd+0x224>  // b.none
         :                      if (ns->ms) {
    1.91 :   ffff8000107ba30c:       ldrh    w2, [x0, #84]
    0.00 :   ffff8000107ba310:       cbz     w2, ffff8000107ba35c <nvme_setup_cmd+0x214>
         :                      if (!blk_integrity_rq(req)) {
    0.00 :   ffff8000107ba314:       ldr     w4, [x20, #24]
    0.00 :   ffff8000107ba318:       ldrb    w0, [x0, #93]
    0.00 :   ffff8000107ba31c:       tbnz    w4, #16, ffff8000107ba340 <nvme_setup_cmd+0x1f8>
         :                      nvme_ns_has_pi():
         :                      return ns->pi_type && ns->ms == sizeof(struct t10_pi_tuple);
    0.00 :   ffff8000107ba320:       cmp     w0, #0x0
    0.00 :   ffff8000107ba324:       ccmp    w2, #0x8, #0x0, ne  // ne = any
    0.00 :   ffff8000107ba328:       b.eq    ffff8000107ba338 <nvme_setup_cmd+0x1f0>  // b.none
         :                      nvme_setup_rw():
         :                      if (WARN_ON_ONCE(!nvme_ns_has_pi(ns)))
    0.00 :   ffff8000107ba32c:       brk     #0x800
         :                      return BLK_STS_NOTSUPP;
    0.00 :   ffff8000107ba330:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000107ba334:       b       ffff8000107ba1b0 <nvme_setup_cmd+0x68>
         :                      control |= NVME_RW_PRINFO_PRACT;
    0.00 :   ffff8000107ba338:       orr     w1, w1, #0x2000
    0.00 :   ffff8000107ba33c:       and     w1, w1, #0xffff
         :                      switch (ns->pi_type) {
    0.00 :   ffff8000107ba340:       cbz     w0, ffff8000107ba35c <nvme_setup_cmd+0x214>
    0.00 :   ffff8000107ba344:       cmp     w0, #0x2
    0.00 :   ffff8000107ba348:       b.ls    ffff8000107ba3c0 <nvme_setup_cmd+0x278>  // b.plast
         :                      control |= NVME_RW_PRINFO_PRCHK_GUARD;
    0.00 :   ffff8000107ba34c:       orr     w2, w1, #0x1000
    0.00 :   ffff8000107ba350:       cmp     w0, #0x3
    0.00 :   ffff8000107ba354:       and     w0, w2, #0xffff
    0.00 :   ffff8000107ba358:       csel    w1, w0, w1, eq  // eq = none
         :                      cmnd->rw.control = cpu_to_le16(control);
    0.00 :   ffff8000107ba35c:       strh    w1, [x19, #50]
         :                      return 0;
    0.00 :   ffff8000107ba360:       mov     w1, #0x0                        // #0
         :                      cmnd->rw.dsmgmt = cpu_to_le32(dsmgmt);
    0.00 :   ffff8000107ba364:       str     w3, [x19, #52]
    0.00 :   ffff8000107ba368:       b       ffff8000107ba1b0 <nvme_setup_cmd+0x68>
         :                      if (req_op(req) == REQ_OP_WRITE && ctrl->nr_streams)
    0.00 :   ffff8000107ba36c:       ldrh    w4, [x5, #1232]
    0.00 :   ffff8000107ba370:       cbz     w4, ffff8000107ba30c <nvme_setup_cmd+0x1c4>
         :                      nvme_assign_write_stream():
         :                      enum rw_hint streamid = req->write_hint;
    0.00 :   ffff8000107ba374:       ldrh    w2, [x20, #198]
         :                      if (streamid == WRITE_LIFE_NOT_SET || streamid == WRITE_LIFE_NONE)
    0.00 :   ffff8000107ba378:       cmp     w2, #0x1
    0.00 :   ffff8000107ba37c:       b.ls    ffff8000107ba410 <nvme_setup_cmd+0x2c8>  // b.plast
         :                      streamid--;
    0.00 :   ffff8000107ba380:       sub     w2, w2, #0x1
         :                      if (WARN_ON_ONCE(streamid > ctrl->nr_streams))
    0.00 :   ffff8000107ba384:       cmp     w2, w4
    0.00 :   ffff8000107ba388:       b.hi    ffff8000107ba418 <nvme_setup_cmd+0x2d0>  // b.pmore
         :                      *control |= NVME_RW_DTYPE_STREAMS;
    0.00 :   ffff8000107ba38c:       orr     w1, w1, #0x10
         :                      *dsmgmt |= streamid << 16;
    0.00 :   ffff8000107ba390:       orr     w3, w3, w2, lsl #16
         :                      *control |= NVME_RW_DTYPE_STREAMS;
    0.00 :   ffff8000107ba394:       and     w1, w1, #0xffff
         :                      if (streamid < ARRAY_SIZE(req->q->write_hints))
    0.00 :   ffff8000107ba398:       cmp     w2, #0x4
    0.00 :   ffff8000107ba39c:       b.hi    ffff8000107ba30c <nvme_setup_cmd+0x1c4>  // b.pmore
    0.00 :   ffff8000107ba3a0:       ldr     x5, [x20]
         :                      req->q->write_hints[streamid] += blk_rq_bytes(req) >> 9;
    0.00 :   ffff8000107ba3a4:       ldr     w4, [x20, #40]
    0.00 :   ffff8000107ba3a8:       add     x2, x5, w2, uxtw #3
    0.00 :   ffff8000107ba3ac:       lsr     w5, w4, #9
    0.00 :   ffff8000107ba3b0:       ldr     x4, [x2, #1992]
    0.00 :   ffff8000107ba3b4:       add     x4, x4, x5
    0.00 :   ffff8000107ba3b8:       str     x4, [x2, #1992]
    0.00 :   ffff8000107ba3bc:       b       ffff8000107ba30c <nvme_setup_cmd+0x1c4>
         :                      t10_pi_ref_tag():
         :                      #define T10_PI_APP_ESCAPE cpu_to_be16(0xffff)
         :                      #define T10_PI_REF_ESCAPE cpu_to_be32(0xffffffff)
         :
         :                      static inline u32 t10_pi_ref_tag(struct request *rq)
         :                      {
         :                      unsigned int shift = ilog2(queue_logical_block_size(rq->q));
    0.00 :   ffff8000107ba3c0:       ldr     x4, [x20]
         :                      nvme_setup_rw():
         :                      control |= NVME_RW_PRINFO_PRCHK_GUARD |
    0.00 :   ffff8000107ba3c4:       mov     w0, #0x1400                     // #5120
    0.00 :   ffff8000107ba3c8:       orr     w1, w1, w0
         :                      queue_logical_block_size():
         :
         :                      static inline unsigned queue_logical_block_size(const struct request_queue *q)
         :                      {
         :                      int retval = 512;
         :
         :                      if (q && q->limits.logical_block_size)
    0.00 :   ffff8000107ba3cc:       mov     w0, #0x9                        // #9
         :                      nvme_setup_rw():
    0.00 :   ffff8000107ba3d0:       and     w1, w1, #0xffff
         :                      queue_logical_block_size():
    0.00 :   ffff8000107ba3d4:       cbz     x4, ffff8000107ba3f0 <nvme_setup_cmd+0x2a8>
    0.00 :   ffff8000107ba3d8:       ldr     w5, [x4, #1088]
    0.00 :   ffff8000107ba3dc:       clz     w2, w5
    0.00 :   ffff8000107ba3e0:       cmp     w5, #0x0
    0.00 :   ffff8000107ba3e4:       mvn     w2, w2
    0.00 :   ffff8000107ba3e8:       add     w2, w2, #0x20
    0.00 :   ffff8000107ba3ec:       csel    w0, w2, w0, ne  // ne = any
         :                      t10_pi_ref_tag():
         :
         :                      #ifdef CONFIG_BLK_DEV_INTEGRITY
         :                      if (rq->q->integrity.interval_exp)
    0.00 :   ffff8000107ba3f0:       ldrb    w4, [x4, #210]
         :                      shift = rq->q->integrity.interval_exp;
         :                      #endif
         :                      return blk_rq_pos(rq) >> (shift - SECTOR_SHIFT) & 0xffffffff;
    0.00 :   ffff8000107ba3f4:       ldr     x2, [x20, #48]
         :                      shift = rq->q->integrity.interval_exp;
    0.00 :   ffff8000107ba3f8:       cmp     w4, #0x0
    0.00 :   ffff8000107ba3fc:       csel    w0, w0, w4, eq  // eq = none
         :                      return blk_rq_pos(rq) >> (shift - SECTOR_SHIFT) & 0xffffffff;
    0.00 :   ffff8000107ba400:       sub     w0, w0, #0x9
    0.00 :   ffff8000107ba404:       lsr     x0, x2, x0
    0.00 :   ffff8000107ba408:       str     w0, [x19, #56]
    0.00 :   ffff8000107ba40c:       b       ffff8000107ba35c <nvme_setup_cmd+0x214>
         :                      nvme_assign_write_stream():
         :                      streamid = 0;
    0.00 :   ffff8000107ba410:       mov     w2, #0x0                        // #0
    0.00 :   ffff8000107ba414:       b       ffff8000107ba3a0 <nvme_setup_cmd+0x258>
         :                      if (WARN_ON_ONCE(streamid > ctrl->nr_streams))
    0.00 :   ffff8000107ba418:       brk     #0x800
    0.00 :   ffff8000107ba41c:       b       ffff8000107ba30c <nvme_setup_cmd+0x1c4>
 Percent |	Source code & Disassembly of vmlinux for cycles (732 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104753b8 <blkg_lookup_create>:
         :                      blkg_lookup_create():
         :                      * This looks up or creates the blkg representing the unique pair
         :                      * of the blkcg and the request_queue.
         :                      */
         :                      struct blkcg_gq *blkg_lookup_create(struct blkcg *blkcg,
         :                      struct request_queue *q)
         :                      {
    1.51 :   ffff8000104753b8:       stp     x29, x30, [sp, #-64]!
         :                      __blkg_lookup():
         :                      struct request_queue *q,
         :                      bool update_hint)
         :                      {
         :                      struct blkcg_gq *blkg;
         :
         :                      if (blkcg == &blkcg_root)
    0.00 :   ffff8000104753bc:       adrp    x2, ffff800011abc000 <drbg_algs+0x2b80>
    0.00 :   ffff8000104753c0:       add     x2, x2, #0xf00
         :                      blkg_lookup_create():
    0.00 :   ffff8000104753c4:       mov     x29, sp
    0.68 :   ffff8000104753c8:       stp     x19, x20, [sp, #16]
         :                      __blkg_lookup():
    0.00 :   ffff8000104753cc:       add     x2, x2, #0x28
         :                      blkg_lookup_create():
    0.00 :   ffff8000104753d0:       str     x21, [sp, #32]
    0.00 :   ffff8000104753d4:       mov     x20, x0
         :                      __blkg_lookup():
    0.00 :   ffff8000104753d8:       cmp     x0, x2
         :                      blkg_lookup_create():
    0.00 :   ffff8000104753dc:       mov     x21, x1
         :                      __blkg_lookup():
    0.41 :   ffff8000104753e0:       b.eq    ffff800010475424 <blkg_lookup_create+0x6c>  // b.none
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    1.91 :   ffff8000104753e4:       ldr     x19, [x0, #264]
         :                      __blkg_lookup():
         :                      return q->root_blkg;
         :
         :                      blkg = rcu_dereference(blkcg->blkg_hint);
         :                      if (blkg && blkg->q == q)
    0.00 :   ffff8000104753e8:       cbz     x19, ffff8000104753f8 <blkg_lookup_create+0x40>
    1.90 :   ffff8000104753ec:       ldr     x0, [x19]
    0.00 :   ffff8000104753f0:       cmp     x1, x0
    0.00 :   ffff8000104753f4:       b.eq    ffff800010475410 <blkg_lookup_create+0x58>  // b.none
         :                      return blkg;
         :
         :                      return blkg_lookup_slowpath(blkcg, q, update_hint);
    0.00 :   ffff8000104753f8:       mov     w2, #0x0                        // #0
    0.00 :   ffff8000104753fc:       mov     x1, x21
    0.00 :   ffff800010475400:       mov     x0, x20
    0.00 :   ffff800010475404:       bl      ffff800010473240 <blkg_lookup_slowpath>
    0.00 :   ffff800010475408:       mov     x19, x0
         :                      blkg_lookup_create():
         :                      struct blkcg_gq *blkg = blkg_lookup(blkcg, q);
         :
         :                      if (unlikely(!blkg)) {
    0.00 :   ffff80001047540c:       cbz     x19, ffff80001047542c <blkg_lookup_create+0x74>
         :                      blkg = __blkg_lookup_create(blkcg, q);
         :                      spin_unlock_irqrestore(&q->queue_lock, flags);
         :                      }
         :
         :                      return blkg;
         :                      }
   82.89 :   ffff800010475410:       mov     x0, x19
    3.43 :   ffff800010475414:       ldr     x21, [sp, #32]
    1.92 :   ffff800010475418:       ldp     x19, x20, [sp, #16]
    5.34 :   ffff80001047541c:       ldp     x29, x30, [sp], #64
    0.00 :   ffff800010475420:       ret
         :                      __blkg_lookup():
         :                      return q->root_blkg;
    0.00 :   ffff800010475424:       ldr     x19, [x1, #1016]
         :                      blkg_lookup_create():
         :                      if (unlikely(!blkg)) {
    0.00 :   ffff800010475428:       cbnz    x19, ffff800010475410 <blkg_lookup_create+0x58>
    0.00 :   ffff80001047542c:       stp     x22, x23, [x29, #40]
         :                      spinlock_check():
         :                      * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
         :                      */
         :
         :                      static __always_inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
         :                      {
         :                      return &lock->rlock;
    0.00 :   ffff800010475430:       add     x22, x21, #0x7c
         :                      blkg_lookup_create():
         :                      spin_lock_irqsave(&q->queue_lock, flags);
    0.00 :   ffff800010475434:       mov     x0, x22
    0.00 :   ffff800010475438:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
         :                      blkg = __blkg_lookup_create(blkcg, q);
    0.00 :   ffff80001047543c:       mov     x1, x21
         :                      spin_lock_irqsave(&q->queue_lock, flags);
    0.00 :   ffff800010475440:       mov     x23, x0
         :                      blkg = __blkg_lookup_create(blkcg, q);
    0.00 :   ffff800010475444:       mov     x0, x20
    0.00 :   ffff800010475448:       bl      ffff800010475248 <__blkg_lookup_create>
         :                      spin_unlock_irqrestore():
         :                      raw_spin_unlock_irq(&lock->rlock);
         :                      }
         :
         :                      static __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
         :                      {
         :                      raw_spin_unlock_irqrestore(&lock->rlock, flags);
    0.00 :   ffff80001047544c:       mov     x1, x23
         :                      blkg_lookup_create():
    0.00 :   ffff800010475450:       mov     x19, x0
         :                      spin_unlock_irqrestore():
    0.00 :   ffff800010475454:       mov     x0, x22
    0.00 :   ffff800010475458:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      blkg_lookup_create():
         :                      }
    0.00 :   ffff80001047545c:       mov     x0, x19
    0.00 :   ffff800010475460:       ldr     x21, [sp, #32]
         :                      spin_unlock_irqrestore():
    0.00 :   ffff800010475464:       ldp     x22, x23, [x29, #40]
         :                      blkg_lookup_create():
    0.00 :   ffff800010475468:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001047546c:       ldp     x29, x30, [sp], #64
    0.00 :   ffff800010475470:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (417 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001017a298 <tick_nohz_idle_exit>:
         :                      tick_nohz_idle_exit():
         :                      * Restart the idle tick when the CPU is woken up from idle
         :                      * This also exit the RCU extended quiescent state. The CPU
         :                      * can use RCU again after this function is called.
         :                      */
         :                      void tick_nohz_idle_exit(void)
         :                      {
    0.00 :   ffff80001017a298:       stp     x29, x30, [sp, #-48]!
         :                      arch_local_irq_disable():
         :                      u32 pmr = read_sysreg_s(SYS_ICC_PMR_EL1);
         :
         :                      WARN_ON_ONCE(pmr != GIC_PRIO_IRQON && pmr != GIC_PRIO_IRQOFF);
         :                      }
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001017a29c:       mov     x0, #0x60                       // #96
         :                      tick_nohz_idle_exit():
    0.00 :   ffff80001017a2a0:       mov     x29, sp
    0.00 :   ffff80001017a2a4:       stp     x19, x20, [sp, #16]
         :                      struct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);
    0.00 :   ffff80001017a2a8:       adrp    x19, ffff8000114d2000 <timer_bases+0x1c80>
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff80001017a2ac:       mrs     x1, tpidr_el1
         :                      tick_nohz_idle_exit():
    0.00 :   ffff80001017a2b0:       add     x19, x19, #0xdd0
         :                      {
    0.22 :   ffff80001017a2b4:       str     x21, [sp, #32]
         :                      struct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);
    0.00 :   ffff80001017a2b8:       add     x19, x19, x1
         :                      arch_local_irq_disable():
    0.00 :   ffff80001017a2bc:       msr     daifset, #0x2
         :                      tick_nohz_idle_exit():
         :                      bool idle_active, tick_stopped;
         :                      ktime_t now;
         :
         :                      local_irq_disable();
         :
         :                      WARN_ON_ONCE(!ts->inidle);
    0.00 :   ffff80001017a2c0:       ldrb    w0, [x19, #76]
    0.00 :   ffff80001017a2c4:       tbz     w0, #0, ffff80001017a340 <tick_nohz_idle_exit+0xa8>
         :                      WARN_ON_ONCE(ts->timer_expires_base);
    0.00 :   ffff80001017a2c8:       ldr     x0, [x19, #176]
    0.00 :   ffff80001017a2cc:       cbnz    x0, ffff80001017a338 <tick_nohz_idle_exit+0xa0>
         :
         :                      ts->inidle = 0;
    0.00 :   ffff80001017a2d0:       ldrb    w0, [x19, #76]
    0.00 :   ffff80001017a2d4:       and     w0, w0, #0xfffffffe
    0.00 :   ffff80001017a2d8:       strb    w0, [x19, #76]
         :                      idle_active = ts->idle_active;
    0.00 :   ffff80001017a2dc:       ubfx    x21, x0, #2, #1
         :                      tick_stopped = ts->tick_stopped;
    0.00 :   ffff80001017a2e0:       ubfx    x20, x0, #1, #1
         :
         :                      if (idle_active || tick_stopped)
    0.00 :   ffff80001017a2e4:       orr     w0, w21, w20
    0.00 :   ffff80001017a2e8:       cbnz    w0, ffff80001017a304 <tick_nohz_idle_exit+0x6c>
         :                      arch_local_irq_enable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001017a2ec:       mov     x0, #0xe0                       // #224
    0.00 :   ffff80001017a2f0:       msr     daifclr, #0x2
         :                      tick_nohz_idle_exit():
         :
         :                      if (tick_stopped)
         :                      __tick_nohz_idle_restart_tick(ts, now);
         :
         :                      local_irq_enable();
         :                      }
   97.56 :   ffff80001017a2f4:       ldp     x19, x20, [sp, #16]
    2.22 :   ffff80001017a2f8:       ldr     x21, [sp, #32]
    0.00 :   ffff80001017a2fc:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001017a300:       ret
    0.00 :   ffff80001017a304:       str     x22, [x29, #40]
         :                      now = ktime_get();
    0.00 :   ffff80001017a308:       bl      ffff80001016ad10 <ktime_get>
    0.00 :   ffff80001017a30c:       mov     x22, x0
         :                      if (idle_active)
    0.00 :   ffff80001017a310:       cbz     w21, ffff80001017a324 <tick_nohz_idle_exit+0x8c>
         :                      tick_nohz_stop_idle(ts, now);
    0.00 :   ffff80001017a314:       mov     x1, x0
    0.00 :   ffff80001017a318:       mov     x0, x19
    0.00 :   ffff80001017a31c:       bl      ffff800010179930 <tick_nohz_stop_idle>
         :                      if (tick_stopped)
    0.00 :   ffff80001017a320:       cbz     w20, ffff80001017a348 <tick_nohz_idle_exit+0xb0>
         :                      __tick_nohz_idle_restart_tick(ts, now);
    0.00 :   ffff80001017a324:       mov     x1, x22
    0.00 :   ffff80001017a328:       mov     x0, x19
    0.00 :   ffff80001017a32c:       bl      ffff800010179800 <__tick_nohz_idle_restart_tick>
    0.00 :   ffff80001017a330:       ldr     x22, [x29, #40]
    0.00 :   ffff80001017a334:       b       ffff80001017a2ec <tick_nohz_idle_exit+0x54>
         :                      WARN_ON_ONCE(ts->timer_expires_base);
    0.00 :   ffff80001017a338:       brk     #0x800
    0.00 :   ffff80001017a33c:       b       ffff80001017a2d0 <tick_nohz_idle_exit+0x38>
         :                      WARN_ON_ONCE(!ts->inidle);
    0.00 :   ffff80001017a340:       brk     #0x800
    0.00 :   ffff80001017a344:       b       ffff80001017a2c8 <tick_nohz_idle_exit+0x30>
    0.00 :   ffff80001017a348:       ldr     x22, [x29, #40]
    0.00 :   ffff80001017a34c:       b       ffff80001017a2ec <tick_nohz_idle_exit+0x54>
 Percent |	Source code & Disassembly of vmlinux for cycles (359 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000106fdda0 <iommu_dma_free_iova>:
         :                      iommu_dma_free_iova():
         :                      dma_addr_t iova, size_t size)
         :                      {
         :                      struct iova_domain *iovad = &cookie->iovad;
         :
         :                      /* The MSI case is only ever cleaning up its most recent allocation */
         :                      if (cookie->type == IOMMU_DMA_MSI_COOKIE)
    0.27 :   ffff8000106fdda0:       ldr     w3, [x0]
    0.00 :   ffff8000106fdda4:       cmp     w3, #0x1
    0.00 :   ffff8000106fdda8:       b.eq    ffff8000106fddf0 <iommu_dma_free_iova+0x50>  // b.none
         :                      {
   11.14 :   ffff8000106fddac:       stp     x29, x30, [sp, #-16]!
         :                      struct iova_domain *iovad = &cookie->iovad;
    0.00 :   ffff8000106fddb0:       add     x0, x0, #0x8
         :                      {
    0.00 :   ffff8000106fddb4:       mov     x29, sp
   49.18 :   ffff8000106fddb8:       ldr     x3, [x0, #32]
         :                      cookie->msi_iova -= size;
         :                      else if (cookie->fq_domain)     /* non-strict mode */
    9.78 :   ffff8000106fddbc:       ldr     x4, [x0, #1888]
    0.00 :   ffff8000106fddc0:       rbit    x3, x3
    0.00 :   ffff8000106fddc4:       clz     x3, x3
    0.00 :   ffff8000106fddc8:       lsr     x1, x1, x3
   29.62 :   ffff8000106fddcc:       lsr     x2, x2, x3
    0.00 :   ffff8000106fddd0:       cbz     x4, ffff8000106fdde4 <iommu_dma_free_iova+0x44>
         :                      queue_iova(iovad, iova_pfn(iovad, iova),
    0.00 :   ffff8000106fddd4:       mov     x3, #0x0                        // #0
    0.00 :   ffff8000106fddd8:       bl      ffff800010702430 <queue_iova>
         :                      size >> iova_shift(iovad), 0);
         :                      else
         :                      free_iova_fast(iovad, iova_pfn(iovad, iova),
         :                      size >> iova_shift(iovad));
         :                      }
    0.00 :   ffff8000106fdddc:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000106fdde0:       ret
         :                      free_iova_fast(iovad, iova_pfn(iovad, iova),
    0.00 :   ffff8000106fdde4:       bl      ffff800010702008 <free_iova_fast>
         :                      }
    0.00 :   ffff8000106fdde8:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000106fddec:       ret
         :                      cookie->msi_iova -= size;
    0.00 :   ffff8000106fddf0:       ldr     x1, [x0, #8]
    0.00 :   ffff8000106fddf4:       sub     x2, x1, x2
    0.00 :   ffff8000106fddf8:       str     x2, [x0, #8]
    0.00 :   ffff8000106fddfc:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (674 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102ca4e0 <fsnotify>:
         :                      fsnotify():
         :                      * out to all of the registered fsnotify_group.  Those groups can then use the
         :                      * notification event in whatever means they feel necessary.
         :                      */
         :                      int fsnotify(struct inode *to_tell, __u32 mask, const void *data, int data_is,
         :                      const struct qstr *file_name, u32 cookie)
         :                      {
    0.30 :   ffff8000102ca4e0:       stp     x29, x30, [sp, #-176]!
         :                      struct mount *mnt = NULL;
         :                      __u32 mnt_or_sb_mask = sb->s_fsnotify_mask;
         :                      int ret = 0;
         :                      __u32 test_mask = (mask & ALL_FSNOTIFY_EVENTS);
         :
         :                      if (data_is == FSNOTIFY_EVENT_PATH) {
    0.00 :   ffff8000102ca4e4:       cmp     w3, #0x1
         :                      {
    0.00 :   ffff8000102ca4e8:       mov     x29, sp
   17.54 :   ffff8000102ca4ec:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000102ca4f0:       mov     x20, x0
    1.64 :   ffff8000102ca4f4:       str     x21, [sp, #32]
    0.00 :   ffff8000102ca4f8:       adrp    x0, ffff800011899000 <page_wait_table+0x1500>
    1.33 :   ffff8000102ca4fc:       str     x23, [sp, #48]
    1.64 :   ffff8000102ca500:       add     x0, x0, #0x8c8
    1.33 :   ffff8000102ca504:       stp     x27, x28, [sp, #80]
         :                      __u32 test_mask = (mask & ALL_FSNOTIFY_EVENTS);
    0.00 :   ffff8000102ca508:       mov     w21, #0x1007ffff                // #268959743
         :                      {
    7.28 :   ffff8000102ca50c:       str     w5, [x29, #108]
    0.00 :   ffff8000102ca510:       mov     w19, w1
    0.59 :   ffff8000102ca514:       ldr     x6, [x0]
    1.63 :   ffff8000102ca518:       str     x6, [x29, #168]
    0.00 :   ffff8000102ca51c:       mov     x6, #0x0                        // #0
         :                      struct super_block *sb = to_tell->i_sb;
    0.89 :   ffff8000102ca520:       ldr     x28, [x20, #40]
         :                      {
    1.48 :   ffff8000102ca524:       stp     x4, x2, [x29, #112]
    0.00 :   ffff8000102ca528:       mov     x27, #0x0                       // #0
         :                      struct fsnotify_iter_info iter_info = {};
    1.93 :   ffff8000102ca52c:       stp     xzr, xzr, [x29, #136]
         :                      {
    0.00 :   ffff8000102ca530:       mov     w23, w3
         :                      struct fsnotify_iter_info iter_info = {};
    2.38 :   ffff8000102ca534:       stp     xzr, xzr, [x29, #152]
         :                      __u32 test_mask = (mask & ALL_FSNOTIFY_EVENTS);
    0.00 :   ffff8000102ca538:       and     w21, w1, w21
         :                      __u32 mnt_or_sb_mask = sb->s_fsnotify_mask;
    1.63 :   ffff8000102ca53c:       ldr     w0, [x28, #960]
         :                      if (data_is == FSNOTIFY_EVENT_PATH) {
    2.08 :   ffff8000102ca540:       b.ne    ffff8000102ca554 <fsnotify+0x74>  // b.any
         :                      mnt = real_mount(((const struct path *)data)->mnt);
    0.00 :   ffff8000102ca544:       ldr     x1, [x2]
         :                      real_mount():
         :
         :                      #define MNT_NS_INTERNAL ERR_PTR(-EINVAL) /* distinct from any mnt_namespace */
         :
         :                      static inline struct mount *real_mount(struct vfsmount *mnt)
         :                      {
         :                      return container_of(mnt, struct mount, mnt);
    0.00 :   ffff8000102ca548:       sub     x27, x1, #0x20
         :                      fsnotify():
         :                      mnt_or_sb_mask |= mnt->mnt_fsnotify_mask;
    8.35 :   ffff8000102ca54c:       ldr     w2, [x27, #280]
    0.00 :   ffff8000102ca550:       orr     w0, w0, w2
         :                      * be expensive.  It protects walking the *_fsnotify_marks lists.
         :                      * However, if we do not walk the lists, we do not have to do
         :                      * SRCU because we have no references to any objects and do not
         :                      * need SRCU to keep them "alive".
         :                      */
         :                      if (!to_tell->i_fsnotify_marks && !sb->s_fsnotify_marks &&
   28.30 :   ffff8000102ca554:       ldr     x2, [x20, #560]
         :                      mnt_or_sb_mask = 0;
    0.00 :   ffff8000102ca558:       tst     x19, #0x8000000
    0.00 :   ffff8000102ca55c:       csel    w0, w0, wzr, eq  // eq = none
         :                      if (!to_tell->i_fsnotify_marks && !sb->s_fsnotify_marks &&
    0.00 :   ffff8000102ca560:       cbz     x2, ffff8000102ca7f0 <fsnotify+0x310>
    0.00 :   ffff8000102ca564:       str     x22, [x29, #40]
         :                      /*
         :                      * if this is a modify event we may need to clear the ignored masks
         :                      * otherwise return if neither the inode nor the vfsmount/sb care about
         :                      * this type of event.
         :                      */
         :                      if (!(mask & FS_MODIFY) &&
    0.00 :   ffff8000102ca568:       and     w22, w19, #0x2
    0.00 :   ffff8000102ca56c:       tbnz    w19, #1, ffff8000102ca584 <fsnotify+0xa4>
         :                      !(test_mask & (to_tell->i_fsnotify_mask | mnt_or_sb_mask)))
    0.15 :   ffff8000102ca570:       ldr     w3, [x20, #556]
         :                      return 0;
    0.00 :   ffff8000102ca574:       mov     w2, #0x0                        // #0
         :                      !(test_mask & (to_tell->i_fsnotify_mask | mnt_or_sb_mask)))
    0.00 :   ffff8000102ca578:       orr     w0, w0, w3
         :                      if (!(mask & FS_MODIFY) &&
    0.00 :   ffff8000102ca57c:       tst     w0, w21
    0.00 :   ffff8000102ca580:       b.eq    ffff8000102ca7e8 <fsnotify+0x308>  // b.none
    0.00 :   ffff8000102ca584:       stp     x24, x25, [x29, #56]
         :                      srcu_read_lock():
         :                      */
         :                      static inline int srcu_read_lock(struct srcu_struct *ssp) __acquires(ssp)
         :                      {
         :                      int retval;
         :
         :                      retval = __srcu_read_lock(ssp);
    0.00 :   ffff8000102ca588:       adrp    x0, ffff800011ab4000 <in_lookup_hashtable+0x1c08>
    0.00 :   ffff8000102ca58c:       str     x26, [x29, #72]
    0.00 :   ffff8000102ca590:       add     x0, x0, #0x768
    0.00 :   ffff8000102ca594:       bl      ffff8000101598d8 <__srcu_read_lock>
         :                      fsnotify():
         :                      return 0;
         :
         :                      iter_info.srcu_idx = srcu_read_lock(&fsnotify_mark_srcu);
    0.00 :   ffff8000102ca598:       str     w0, [x29, #164]
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102ca59c:       ldr     x2, [x20, #560]
         :                      fsnotify_first_mark():
         :                      return hlist_entry_safe(node, struct fsnotify_mark, obj_list);
    0.00 :   ffff8000102ca5a0:       mov     x25, #0x0                       // #0
         :                      if (conn)
    0.00 :   ffff8000102ca5a4:       cbz     x2, ffff8000102ca5b8 <fsnotify+0xd8>
         :                      __read_once_size():
    0.00 :   ffff8000102ca5a8:       ldr     x0, [x2, #24]
         :                      fsnotify_first_mark():
         :                      return hlist_entry_safe(node, struct fsnotify_mark, obj_list);
    0.00 :   ffff8000102ca5ac:       sub     x3, x0, #0x28
    0.00 :   ffff8000102ca5b0:       cmp     x0, #0x0
    0.00 :   ffff8000102ca5b4:       csel    x25, x3, xzr, ne  // ne = any
         :                      __read_once_size():
    0.00 :   ffff8000102ca5b8:       ldr     x2, [x28, #968]
         :                      fsnotify_first_mark():
    0.00 :   ffff8000102ca5bc:       mov     x0, #0x0                        // #0
         :                      fsnotify():
         :
         :                      iter_info.marks[FSNOTIFY_OBJ_TYPE_INODE] =
    0.00 :   ffff8000102ca5c0:       str     x25, [x29, #136]
         :                      fsnotify_first_mark():
         :                      if (conn)
    0.00 :   ffff8000102ca5c4:       cbz     x2, ffff8000102ca5d8 <fsnotify+0xf8>
         :                      __read_once_size():
    0.00 :   ffff8000102ca5c8:       ldr     x2, [x2, #24]
         :                      fsnotify_first_mark():
         :                      return hlist_entry_safe(node, struct fsnotify_mark, obj_list);
    0.00 :   ffff8000102ca5cc:       sub     x0, x2, #0x28
    0.00 :   ffff8000102ca5d0:       cmp     x2, #0x0
    0.00 :   ffff8000102ca5d4:       csel    x0, x0, xzr, ne  // ne = any
         :                      fsnotify():
         :                      fsnotify_first_mark(&to_tell->i_fsnotify_marks);
         :                      iter_info.marks[FSNOTIFY_OBJ_TYPE_SB] =
    0.00 :   ffff8000102ca5d8:       str     x0, [x29, #152]
         :                      fsnotify_first_mark(&sb->s_fsnotify_marks);
         :                      if (mnt) {
    0.00 :   ffff8000102ca5dc:       cbz     x27, ffff8000102ca600 <fsnotify+0x120>
         :                      __read_once_size():
    0.00 :   ffff8000102ca5e0:       ldr     x1, [x27, #272]
         :                      fsnotify_first_mark():
         :                      return hlist_entry_safe(node, struct fsnotify_mark, obj_list);
    0.00 :   ffff8000102ca5e4:       mov     x0, #0x0                        // #0
         :                      if (conn)
    0.00 :   ffff8000102ca5e8:       cbz     x1, ffff8000102ca5fc <fsnotify+0x11c>
         :                      __read_once_size():
    0.00 :   ffff8000102ca5ec:       ldr     x1, [x1, #24]
         :                      fsnotify_first_mark():
         :                      return hlist_entry_safe(node, struct fsnotify_mark, obj_list);
    0.00 :   ffff8000102ca5f0:       sub     x0, x1, #0x28
    0.00 :   ffff8000102ca5f4:       cmp     x1, #0x0
    0.00 :   ffff8000102ca5f8:       csel    x0, x0, xzr, ne  // ne = any
         :                      fsnotify():
         :                      iter_info.marks[FSNOTIFY_OBJ_TYPE_VFSMOUNT] =
    0.00 :   ffff8000102ca5fc:       str     x0, [x29, #144]
    0.00 :   ffff8000102ca600:       add     x27, x29, #0x88
         :                      fsnotify_iter_set_report_type():
         :                      }
         :
         :                      static inline void fsnotify_iter_set_report_type(
         :                      struct fsnotify_iter_info *iter_info, int type)
         :                      {
         :                      iter_info->report_mask |= (1U << type);
    0.00 :   ffff8000102ca604:       mov     w28, #0x1                       // #1
         :                      fsnotify_next_mark():
         :                      return hlist_entry_safe(node, struct fsnotify_mark, obj_list);
    0.00 :   ffff8000102ca608:       mov     x24, #0x0                       // #0
         :                      fsnotify_iter_select_report_types():
         :                      struct fsnotify_group *max_prio_group = NULL;
    0.00 :   ffff8000102ca60c:       mov     x26, #0x0                       // #0
         :                      if (mark &&
    0.00 :   ffff8000102ca610:       cbz     x25, ffff8000102ca62c <fsnotify+0x14c>
         :                      fsnotify_compare_groups(max_prio_group, mark->group) > 0)
    0.00 :   ffff8000102ca614:       ldr     x1, [x25, #8]
    0.00 :   ffff8000102ca618:       mov     x0, x26
    0.00 :   ffff8000102ca61c:       bl      ffff8000102cbef8 <fsnotify_compare_groups>
         :                      if (mark &&
    0.00 :   ffff8000102ca620:       cmp     w0, #0x0
    0.00 :   ffff8000102ca624:       b.le    ffff8000102ca62c <fsnotify+0x14c>
         :                      max_prio_group = mark->group;
    0.00 :   ffff8000102ca628:       ldr     x26, [x25, #8]
    0.00 :   ffff8000102ca62c:       add     x24, x24, #0x8
         :                      fsnotify_foreach_obj_type(type) {
    0.00 :   ffff8000102ca630:       cmp     x24, #0x18
    0.00 :   ffff8000102ca634:       b.eq    ffff8000102ca640 <fsnotify+0x160>  // b.none
    0.00 :   ffff8000102ca638:       ldr     x25, [x27, x24]
    0.00 :   ffff8000102ca63c:       b       ffff8000102ca610 <fsnotify+0x130>
         :                      if (!max_prio_group)
    0.00 :   ffff8000102ca640:       cbz     x26, ffff8000102ca784 <fsnotify+0x2a4>
         :                      iter_info->report_mask = 0;
    0.00 :   ffff8000102ca644:       mov     x25, #0x0                       // #0
    0.00 :   ffff8000102ca648:       str     wzr, [x29, #160]
         :                      mark = iter_info->marks[type];
    0.00 :   ffff8000102ca64c:       ldr     x0, [x27, x25, lsl #3]
         :                      if (mark &&
    0.00 :   ffff8000102ca650:       cbz     x0, ffff8000102ca674 <fsnotify+0x194>
         :                      fsnotify_compare_groups(max_prio_group, mark->group) == 0)
    0.00 :   ffff8000102ca654:       ldr     x1, [x0, #8]
    0.00 :   ffff8000102ca658:       mov     x0, x26
    0.00 :   ffff8000102ca65c:       bl      ffff8000102cbef8 <fsnotify_compare_groups>
         :                      if (mark &&
    0.00 :   ffff8000102ca660:       cbnz    w0, ffff8000102ca674 <fsnotify+0x194>
         :                      fsnotify_iter_set_report_type():
    0.00 :   ffff8000102ca664:       ldr     w0, [x29, #160]
    0.00 :   ffff8000102ca668:       lsl     w3, w28, w25
    0.00 :   ffff8000102ca66c:       orr     w3, w0, w3
    0.00 :   ffff8000102ca670:       str     w3, [x29, #160]
    0.00 :   ffff8000102ca674:       add     x25, x25, #0x1
         :                      fsnotify_iter_select_report_types():
         :                      fsnotify_foreach_obj_type(type) {
    0.00 :   ffff8000102ca678:       cmp     x25, #0x3
    0.00 :   ffff8000102ca67c:       b.ne    ffff8000102ca64c <fsnotify+0x16c>  // b.any
         :                      return iter_info->report_mask;
    0.00 :   ffff8000102ca680:       ldr     w2, [x29, #160]
         :                      fsnotify():
         :                      /*
         :                      * We need to merge inode/vfsmount/sb mark lists so that e.g. inode mark
         :                      * ignore masks are properly reflected for mount/sb mark notifications.
         :                      * That's why this traversal is so complicated...
         :                      */
         :                      while (fsnotify_iter_select_report_types(&iter_info)) {
    0.00 :   ffff8000102ca684:       cbz     w2, ffff8000102ca784 <fsnotify+0x2a4>
         :                      send_to_group():
         :                      if (mask & FS_MODIFY) {
    0.00 :   ffff8000102ca688:       cbz     w22, ffff8000102ca6bc <fsnotify+0x1dc>
    0.00 :   ffff8000102ca68c:       mov     x0, #0x0                        // #0
         :                      fsnotify_iter_should_report_type():
         :                      return (iter_info->report_mask & (1U << type));
    0.00 :   ffff8000102ca690:       lsl     w1, w28, w0
         :                      send_to_group():
         :                      if (!fsnotify_iter_should_report_type(iter_info, type))
    0.00 :   ffff8000102ca694:       tst     w1, w2
    0.00 :   ffff8000102ca698:       b.eq    ffff8000102ca6b0 <fsnotify+0x1d0>  // b.none
         :                      mark = iter_info->marks[type];
    0.00 :   ffff8000102ca69c:       ldr     x1, [x27, x0, lsl #3]
         :                      if (mark &&
    0.00 :   ffff8000102ca6a0:       cbz     x1, ffff8000102ca6b0 <fsnotify+0x1d0>
         :                      !(mark->flags & FSNOTIFY_MARK_FLAG_IGNORED_SURV_MODIFY))
    0.00 :   ffff8000102ca6a4:       ldr     w3, [x1, #68]
         :                      if (mark &&
    0.00 :   ffff8000102ca6a8:       tbnz    w3, #0, ffff8000102ca6b0 <fsnotify+0x1d0>
         :                      mark->ignored_mask = 0;
    0.00 :   ffff8000102ca6ac:       str     wzr, [x1, #64]
    0.00 :   ffff8000102ca6b0:       add     x0, x0, #0x1
         :                      fsnotify_foreach_obj_type(type) {
    0.00 :   ffff8000102ca6b4:       cmp     x0, #0x3
    0.00 :   ffff8000102ca6b8:       b.ne    ffff8000102ca690 <fsnotify+0x1b0>  // b.any
         :                      if (mask & FS_MODIFY) {
    0.00 :   ffff8000102ca6bc:       mov     x1, #0x0                        // #0
    0.00 :   ffff8000102ca6c0:       mov     x0, #0x0                        // #0
    0.00 :   ffff8000102ca6c4:       mov     w5, #0x0                        // #0
    0.00 :   ffff8000102ca6c8:       mov     w4, #0x0                        // #0
         :                      fsnotify_iter_should_report_type():
    0.00 :   ffff8000102ca6cc:       lsl     w3, w28, w1
         :                      send_to_group():
         :                      if (!fsnotify_iter_should_report_type(iter_info, type))
    0.00 :   ffff8000102ca6d0:       tst     w3, w2
    0.00 :   ffff8000102ca6d4:       b.eq    ffff8000102ca6f4 <fsnotify+0x214>  // b.none
         :                      mark = iter_info->marks[type];
    0.00 :   ffff8000102ca6d8:       ldr     x3, [x27, x1, lsl #3]
         :                      if (mark) {
    0.00 :   ffff8000102ca6dc:       cbz     x3, ffff8000102ca6f4 <fsnotify+0x214>
         :                      marks_mask |= mark->mask;
    0.00 :   ffff8000102ca6e0:       ldr     w7, [x3]
         :                      marks_ignored_mask |= mark->ignored_mask;
    0.00 :   ffff8000102ca6e4:       ldr     w6, [x3, #64]
         :                      group = mark->group;
    0.00 :   ffff8000102ca6e8:       ldr     x0, [x3, #8]
         :                      marks_mask |= mark->mask;
    0.00 :   ffff8000102ca6ec:       orr     w4, w4, w7
         :                      marks_ignored_mask |= mark->ignored_mask;
    0.00 :   ffff8000102ca6f0:       orr     w5, w5, w6
    0.00 :   ffff8000102ca6f4:       add     x1, x1, #0x1
         :                      fsnotify_foreach_obj_type(type) {
    0.00 :   ffff8000102ca6f8:       cmp     x1, #0x3
    0.00 :   ffff8000102ca6fc:       b.ne    ffff8000102ca6cc <fsnotify+0x1ec>  // b.any
         :                      if (!(test_mask & marks_mask & ~marks_ignored_mask))
    0.00 :   ffff8000102ca700:       and     w4, w21, w4
    0.00 :   ffff8000102ca704:       bics    wzr, w4, w5
    0.00 :   ffff8000102ca708:       b.eq    ffff8000102ca744 <fsnotify+0x264>  // b.none
         :                      return group->ops->handle_event(group, to_tell, mask, data, data_is,
    0.00 :   ffff8000102ca70c:       ldr     x1, [x0]
    0.00 :   ffff8000102ca710:       mov     w2, w19
    0.00 :   ffff8000102ca714:       ldr     w6, [x29, #108]
    0.00 :   ffff8000102ca718:       mov     x7, x27
    0.00 :   ffff8000102ca71c:       ldp     x5, x3, [x29, #112]
    0.00 :   ffff8000102ca720:       mov     w4, w23
    0.00 :   ffff8000102ca724:       ldr     x10, [x1]
    0.00 :   ffff8000102ca728:       mov     x1, x20
    0.00 :   ffff8000102ca72c:       blr     x10
    0.00 :   ffff8000102ca730:       mov     w2, w0
         :                      fsnotify():
         :                      ret = send_to_group(to_tell, mask, data, data_is, cookie,
         :                      file_name, &iter_info);
         :
         :                      if (ret && (mask & ALL_FSNOTIFY_PERM_EVENTS))
    0.00 :   ffff8000102ca734:       cbz     w0, ffff8000102ca740 <fsnotify+0x260>
    0.00 :   ffff8000102ca738:       tst     w19, #0x70000
    0.00 :   ffff8000102ca73c:       b.ne    ffff8000102ca788 <fsnotify+0x2a8>  // b.any
    0.00 :   ffff8000102ca740:       ldr     w2, [x29, #160]
         :                      send_to_group():
         :                      if (mask & FS_MODIFY) {
    0.00 :   ffff8000102ca744:       mov     x0, #0x0                        // #0
         :                      fsnotify_iter_should_report_type():
    0.00 :   ffff8000102ca748:       lsl     w1, w28, w0
         :                      fsnotify_iter_next():
         :                      if (fsnotify_iter_should_report_type(iter_info, type))
    0.00 :   ffff8000102ca74c:       tst     w1, w2
    0.00 :   ffff8000102ca750:       b.eq    ffff8000102ca770 <fsnotify+0x290>  // b.none
         :                      fsnotify_next_mark(iter_info->marks[type]);
    0.00 :   ffff8000102ca754:       ldr     x1, [x27, x0, lsl #3]
         :                      fsnotify_next_mark():
         :                      if (mark)
    0.00 :   ffff8000102ca758:       cbz     x1, ffff8000102ca76c <fsnotify+0x28c>
         :                      __read_once_size():
    0.00 :   ffff8000102ca75c:       ldr     x3, [x1, #40]
         :                      fsnotify_next_mark():
         :                      return hlist_entry_safe(node, struct fsnotify_mark, obj_list);
    0.00 :   ffff8000102ca760:       sub     x1, x3, #0x28
    0.00 :   ffff8000102ca764:       cmp     x3, #0x0
    0.00 :   ffff8000102ca768:       csel    x1, x1, xzr, ne  // ne = any
         :                      fsnotify_iter_next():
         :                      iter_info->marks[type] =
    0.00 :   ffff8000102ca76c:       str     x1, [x27, x0, lsl #3]
    0.00 :   ffff8000102ca770:       add     x0, x0, #0x1
         :                      fsnotify_foreach_obj_type(type) {
    0.00 :   ffff8000102ca774:       cmp     x0, #0x3
    0.00 :   ffff8000102ca778:       b.ne    ffff8000102ca748 <fsnotify+0x268>  // b.any
    0.00 :   ffff8000102ca77c:       ldr     x25, [x29, #136]
    0.00 :   ffff8000102ca780:       b       ffff8000102ca608 <fsnotify+0x128>
         :                      fsnotify():
         :                      goto out;
         :
         :                      fsnotify_iter_next(&iter_info);
         :                      }
         :                      ret = 0;
    0.00 :   ffff8000102ca784:       mov     w2, #0x0                        // #0
         :                      out:
         :                      srcu_read_unlock(&fsnotify_mark_srcu, iter_info.srcu_idx);
    0.00 :   ffff8000102ca788:       ldr     w1, [x29, #164]
         :                      srcu_read_unlock():
         :                      * Exit an SRCU read-side critical section.
         :                      */
         :                      static inline void srcu_read_unlock(struct srcu_struct *ssp, int idx)
         :                      __releases(ssp)
         :                      {
         :                      WARN_ON_ONCE(idx & ~0x1);
    0.00 :   ffff8000102ca78c:       tst     w1, #0xfffffffe
    0.00 :   ffff8000102ca790:       b.ne    ffff8000102ca808 <fsnotify+0x328>  // b.any
    0.00 :   ffff8000102ca794:       str     w2, [x29, #120]
         :                      rcu_lock_release(&(ssp)->dep_map);
         :                      __srcu_read_unlock(ssp, idx);
    0.00 :   ffff8000102ca798:       adrp    x0, ffff800011ab4000 <in_lookup_hashtable+0x1c08>
    0.00 :   ffff8000102ca79c:       add     x0, x0, #0x768
    0.00 :   ffff8000102ca7a0:       bl      ffff800010159868 <__srcu_read_unlock>
    0.00 :   ffff8000102ca7a4:       ldr     w2, [x29, #120]
    0.00 :   ffff8000102ca7a8:       ldr     x22, [x29, #40]
    0.00 :   ffff8000102ca7ac:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff8000102ca7b0:       ldr     x26, [x29, #72]
         :                      fsnotify():
         :
         :                      return ret;
         :                      }
    7.26 :   ffff8000102ca7b4:       adrp    x0, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000102ca7b8:       add     x27, x0, #0x8c8
    0.00 :   ffff8000102ca7bc:       mov     w0, w2
    0.00 :   ffff8000102ca7c0:       ldr     x2, [x29, #168]
    6.51 :   ffff8000102ca7c4:       ldr     x1, [x27]
    0.00 :   ffff8000102ca7c8:       eor     x1, x2, x1
    0.00 :   ffff8000102ca7cc:       cbnz    x1, ffff8000102ca810 <fsnotify+0x330>
    0.44 :   ffff8000102ca7d0:       ldp     x19, x20, [sp, #16]
    4.30 :   ffff8000102ca7d4:       ldr     x21, [sp, #32]
    0.00 :   ffff8000102ca7d8:       ldr     x23, [sp, #48]
    0.00 :   ffff8000102ca7dc:       ldp     x27, x28, [sp, #80]
    0.00 :   ffff8000102ca7e0:       ldp     x29, x30, [sp], #176
    0.00 :   ffff8000102ca7e4:       ret
    1.04 :   ffff8000102ca7e8:       ldr     x22, [x29, #40]
    0.00 :   ffff8000102ca7ec:       b       ffff8000102ca7b4 <fsnotify+0x2d4>
         :                      if (!to_tell->i_fsnotify_marks && !sb->s_fsnotify_marks &&
    0.00 :   ffff8000102ca7f0:       ldr     x2, [x28, #968]
    0.00 :   ffff8000102ca7f4:       cbnz    x2, ffff8000102ca564 <fsnotify+0x84>
    0.00 :   ffff8000102ca7f8:       cbz     x27, ffff8000102ca7b4 <fsnotify+0x2d4>
         :                      (!mnt || !mnt->mnt_fsnotify_marks))
    0.00 :   ffff8000102ca7fc:       ldr     x3, [x27, #272]
    0.00 :   ffff8000102ca800:       cbnz    x3, ffff8000102ca564 <fsnotify+0x84>
    0.00 :   ffff8000102ca804:       b       ffff8000102ca7b4 <fsnotify+0x2d4>
         :                      srcu_read_unlock():
         :                      WARN_ON_ONCE(idx & ~0x1);
    0.00 :   ffff8000102ca808:       brk     #0x800
    0.00 :   ffff8000102ca80c:       b       ffff8000102ca794 <fsnotify+0x2b4>
    0.00 :   ffff8000102ca810:       str     x22, [x29, #40]
    0.00 :   ffff8000102ca814:       stp     x24, x25, [x29, #56]
    0.00 :   ffff8000102ca818:       str     x26, [x29, #72]
         :                      fsnotify():
         :                      }
    0.00 :   ffff8000102ca81c:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (695 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010460aa0 <__blk_mq_try_issue_directly>:
         :                      __blk_mq_try_issue_directly():
         :
         :                      static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
         :                      struct request *rq,
         :                      blk_qc_t *cookie,
         :                      bool bypass_insert, bool last)
         :                      {
    1.87 :   ffff800010460aa0:       stp     x29, x30, [sp, #-96]!
    0.00 :   ffff800010460aa4:       mov     x29, sp
   12.34 :   ffff800010460aa8:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010460aac:       adrp    x19, ffff800011899000 <page_wait_table+0x1500>
    1.01 :   ffff800010460ab0:       str     x22, [sp, #40]
    0.00 :   ffff800010460ab4:       add     x5, x19, #0x8c8
    0.00 :   ffff800010460ab8:       mov     x20, x1
    1.14 :   ffff800010460abc:       ldr     x6, [x5]
    1.00 :   ffff800010460ac0:       str     x6, [x29, #88]
    0.00 :   ffff800010460ac4:       mov     x6, #0x0                        // #0
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    3.18 :   ffff800010460ac8:       ldr     x1, [x0, #24]
         :                      __blk_mq_try_issue_directly():
         :                      *
         :                      * When queue is stopped or quiesced, ignore 'bypass_insert' from
         :                      * blk_mq_request_issue_directly(), and return BLK_STS_OK to caller,
         :                      * and avoid driver to try to dispatch again.
         :                      */
         :                      if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
    0.00 :   ffff800010460acc:       tbnz    w1, #0, ffff800010460ba8 <__blk_mq_try_issue_directly+0x108>
   15.44 :   ffff800010460ad0:       str     x23, [x29, #48]
    0.00 :   ffff800010460ad4:       and     w23, w3, #0xff
         :                      struct request_queue *q = rq->q;
    2.31 :   ffff800010460ad8:       ldr     x1, [x20]
         :                      test_bit():
    0.00 :   ffff800010460adc:       ldr     x3, [x1, #104]
         :                      __blk_mq_try_issue_directly():
         :                      if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
    0.00 :   ffff800010460ae0:       tst     w3, #0x1000000
    0.00 :   ffff800010460ae4:       b.eq    ffff800010460b24 <__blk_mq_try_issue_directly+0x84>  // b.none
         :                      run_queue = false;
    0.00 :   ffff800010460ae8:       ldr     x23, [x29, #48]
    0.00 :   ffff800010460aec:       mov     w1, #0x0                        // #0
         :                      insert:
         :                      if (bypass_insert)
         :                      return BLK_STS_RESOURCE;
         :
         :                      blk_mq_request_bypass_insert(rq, run_queue);
         :                      return BLK_STS_OK;
    0.00 :   ffff800010460af0:       mov     w22, #0x0                       // #0
         :                      blk_mq_request_bypass_insert(rq, run_queue);
    0.00 :   ffff800010460af4:       mov     x0, x20
    0.00 :   ffff800010460af8:       bl      ffff800010460a38 <blk_mq_request_bypass_insert>
         :                      }
    0.58 :   ffff800010460afc:       add     x19, x19, #0x8c8
    0.00 :   ffff800010460b00:       mov     w0, w22
    0.00 :   ffff800010460b04:       ldr     x2, [x29, #88]
    1.44 :   ffff800010460b08:       ldr     x1, [x19]
    0.00 :   ffff800010460b0c:       eor     x1, x2, x1
    0.00 :   ffff800010460b10:       cbnz    x1, ffff800010460c7c <__blk_mq_try_issue_directly+0x1dc>
    0.29 :   ffff800010460b14:       ldp     x19, x20, [sp, #16]
    0.87 :   ffff800010460b18:       ldr     x22, [sp, #40]
    0.00 :   ffff800010460b1c:       ldp     x29, x30, [sp], #96
    0.00 :   ffff800010460b20:       ret
         :                      if (q->elevator && !bypass_insert)
    0.59 :   ffff800010460b24:       ldr     x1, [x1, #8]
    0.00 :   ffff800010460b28:       cmp     x1, #0x0
    0.00 :   ffff800010460b2c:       ccmp    w23, #0x0, #0x0, ne  // ne = any
    0.00 :   ffff800010460b30:       b.eq    ffff800010460b9c <__blk_mq_try_issue_directly+0xfc>  // b.none
    2.16 :   ffff800010460b34:       str     x21, [x29, #32]
    0.00 :   ffff800010460b38:       and     w22, w4, #0xff
    2.87 :   ffff800010460b3c:       str     x24, [x29, #56]
    0.00 :   ffff800010460b40:       mov     x21, x0
    0.00 :   ffff800010460b44:       mov     x24, x2
         :                      blk_mq_get_dispatch_budget():
         :
         :                      static inline bool blk_mq_get_dispatch_budget(struct blk_mq_hw_ctx *hctx)
         :                      {
         :                      struct request_queue *q = hctx->queue;
         :
         :                      if (q->mq_ops->get_budget)
    0.00 :   ffff800010460b48:       ldr     x1, [x0, #208]
    1.15 :   ffff800010460b4c:       ldr     x1, [x1, #48]
    3.88 :   ffff800010460b50:       ldr     x1, [x1, #16]
    0.00 :   ffff800010460b54:       cbz     x1, ffff800010460b64 <__blk_mq_try_issue_directly+0xc4>
         :                      return q->mq_ops->get_budget(hctx);
    0.00 :   ffff800010460b58:       blr     x1
         :                      __blk_mq_try_issue_directly():
         :                      if (!blk_mq_get_dispatch_budget(hctx))
    0.00 :   ffff800010460b5c:       tst     w0, #0xff
    0.00 :   ffff800010460b60:       b.eq    ffff800010460b8c <__blk_mq_try_issue_directly+0xec>  // b.none
         :                      if (!blk_mq_get_driver_tag(rq)) {
    6.62 :   ffff800010460b64:       mov     x0, x20
    0.00 :   ffff800010460b68:       bl      ffff8000104602d0 <blk_mq_get_driver_tag>
    0.00 :   ffff800010460b6c:       tst     w0, #0xff
    0.00 :   ffff800010460b70:       b.ne    ffff800010460bb8 <__blk_mq_try_issue_directly+0x118>  // b.any
         :                      blk_mq_put_dispatch_budget():
         :                      if (q->mq_ops->put_budget)
    0.00 :   ffff800010460b74:       ldr     x0, [x21, #208]
    0.00 :   ffff800010460b78:       ldr     x0, [x0, #48]
    0.00 :   ffff800010460b7c:       ldr     x1, [x0, #24]
    0.00 :   ffff800010460b80:       cbz     x1, ffff800010460b8c <__blk_mq_try_issue_directly+0xec>
         :                      q->mq_ops->put_budget(hctx);
    0.00 :   ffff800010460b84:       mov     x0, x21
    0.00 :   ffff800010460b88:       blr     x1
         :                      __blk_mq_try_issue_directly():
         :                      return BLK_STS_RESOURCE;
    0.00 :   ffff800010460b8c:       mov     w22, #0x9                       // #9
    0.00 :   ffff800010460b90:       ldr     x21, [x29, #32]
         :                      if (bypass_insert)
    0.00 :   ffff800010460b94:       cbnz    w23, ffff800010460bb0 <__blk_mq_try_issue_directly+0x110>
    0.00 :   ffff800010460b98:       ldr     x24, [x29, #56]
         :                      run_queue = false;
    0.00 :   ffff800010460b9c:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010460ba0:       ldr     x23, [x29, #48]
    0.00 :   ffff800010460ba4:       b       ffff800010460af0 <__blk_mq_try_issue_directly+0x50>
    0.00 :   ffff800010460ba8:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010460bac:       b       ffff800010460af0 <__blk_mq_try_issue_directly+0x50>
    0.00 :   ffff800010460bb0:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff800010460bb4:       b       ffff800010460afc <__blk_mq_try_issue_directly+0x5c>
    4.33 :   ffff800010460bb8:       ldr     w0, [x21, #428]
         :                      __blk_mq_issue_directly():
         :                      new_cookie = request_to_qc_t(hctx, rq);
    9.08 :   ffff800010460bbc:       ldr     w1, [x20, #32]
         :                      struct blk_mq_queue_data bd = {
    2.00 :   ffff800010460bc0:       str     x20, [x29, #72]
    3.73 :   ffff800010460bc4:       strb    w22, [x29, #80]
         :                      request_to_qc_t():
         :                      ({ ctx = (hctx)->ctxs[(i)]; 1; }); (i)++)
         :
         :                      static inline blk_qc_t request_to_qc_t(struct blk_mq_hw_ctx *hctx,
         :                      struct request *rq)
         :                      {
         :                      if (rq->tag != -1)
    0.00 :   ffff800010460bc8:       cmn     w1, #0x1
    0.00 :   ffff800010460bcc:       lsl     w0, w0, #16
         :                      __blk_mq_issue_directly():
         :                      struct request_queue *q = rq->q;
    1.43 :   ffff800010460bd0:       ldr     x2, [x20]
         :                      request_to_qc_t():
    0.00 :   ffff800010460bd4:       b.ne    ffff800010460be0 <__blk_mq_try_issue_directly+0x140>  // b.any
         :                      return rq->tag | (hctx->queue_num << BLK_QC_T_SHIFT);
         :
         :                      return rq->internal_tag | (hctx->queue_num << BLK_QC_T_SHIFT) |
    0.00 :   ffff800010460bd8:       ldr     w1, [x20, #36]
    0.00 :   ffff800010460bdc:       orr     w0, w0, #0x80000000
         :                      __blk_mq_issue_directly():
         :                      ret = q->mq_ops->queue_rq(hctx, &bd);
    0.86 :   ffff800010460be0:       ldr     x2, [x2, #48]
         :                      request_to_qc_t():
    0.00 :   ffff800010460be4:       orr     w23, w0, w1
         :                      __blk_mq_issue_directly():
    0.00 :   ffff800010460be8:       add     x1, x29, #0x48
    0.00 :   ffff800010460bec:       mov     x0, x21
    8.05 :   ffff800010460bf0:       ldr     x2, [x2]
    0.00 :   ffff800010460bf4:       blr     x2
    0.72 :   ffff800010460bf8:       and     w22, w0, #0xff
         :                      blk_mq_update_dispatch_busy():
         :                      if (hctx->queue->elevator)
    2.15 :   ffff800010460bfc:       ldr     x1, [x21, #208]
    0.86 :   ffff800010460c00:       add     x0, x21, #0x108
         :                      __blk_mq_issue_directly():
         :                      switch (ret) {
    0.00 :   ffff800010460c04:       cmp     w22, #0x9
         :                      blk_mq_update_dispatch_busy():
         :                      if (hctx->queue->elevator)
    0.72 :   ffff800010460c08:       ldr     x1, [x1, #8]
         :                      __blk_mq_issue_directly():
         :                      switch (ret) {
    0.00 :   ffff800010460c0c:       b.eq    ffff800010460c34 <__blk_mq_try_issue_directly+0x194>  // b.none
    3.15 :   ffff800010460c10:       cmp     w22, #0xd
    0.00 :   ffff800010460c14:       b.eq    ffff800010460c34 <__blk_mq_try_issue_directly+0x194>  // b.none
    1.30 :   ffff800010460c18:       cbz     w22, ffff800010460c4c <__blk_mq_try_issue_directly+0x1ac>
         :                      blk_mq_update_dispatch_busy():
         :                      if (hctx->queue->elevator)
    0.00 :   ffff800010460c1c:       cbz     x1, ffff800010460c74 <__blk_mq_try_issue_directly+0x1d4>
         :                      __blk_mq_issue_directly():
         :                      *cookie = BLK_QC_T_NONE;
    0.00 :   ffff800010460c20:       mov     w0, #0xffffffff                 // #-1
    0.00 :   ffff800010460c24:       str     w0, [x24]
         :                      __blk_mq_try_issue_directly():
         :                      return __blk_mq_issue_directly(hctx, rq, cookie, last);
    0.00 :   ffff800010460c28:       ldr     x21, [x29, #32]
    0.00 :   ffff800010460c2c:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff800010460c30:       b       ffff800010460afc <__blk_mq_try_issue_directly+0x5c>
         :                      blk_mq_update_dispatch_busy():
         :                      if (hctx->queue->elevator)
    0.00 :   ffff800010460c34:       cbz     x1, ffff800010460c60 <__blk_mq_try_issue_directly+0x1c0>
         :                      __blk_mq_issue_directly():
         :                      __blk_mq_requeue_request(rq);
    0.00 :   ffff800010460c38:       mov     x0, x20
    0.00 :   ffff800010460c3c:       bl      ffff80001045eaa0 <__blk_mq_requeue_request>
    0.00 :   ffff800010460c40:       ldr     x21, [x29, #32]
    0.00 :   ffff800010460c44:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff800010460c48:       b       ffff800010460afc <__blk_mq_try_issue_directly+0x5c>
         :                      blk_mq_update_dispatch_busy():
         :                      if (hctx->queue->elevator)
    0.28 :   ffff800010460c4c:       cbz     x1, ffff800010460c6c <__blk_mq_try_issue_directly+0x1cc>
         :                      __blk_mq_issue_directly():
         :                      *cookie = new_cookie;
    0.00 :   ffff800010460c50:       str     w23, [x24]
    0.72 :   ffff800010460c54:       ldr     x21, [x29, #32]
    0.15 :   ffff800010460c58:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff800010460c5c:       b       ffff800010460afc <__blk_mq_try_issue_directly+0x5c>
         :                      blk_mq_update_dispatch_busy():
    0.00 :   ffff800010460c60:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010460c64:       bl      ffff80001045dd00 <blk_mq_update_dispatch_busy.isra.47.part.48>
    0.00 :   ffff800010460c68:       b       ffff800010460c38 <__blk_mq_try_issue_directly+0x198>
    1.15 :   ffff800010460c6c:       bl      ffff80001045dd00 <blk_mq_update_dispatch_busy.isra.47.part.48>
    0.57 :   ffff800010460c70:       b       ffff800010460c50 <__blk_mq_try_issue_directly+0x1b0>
    0.00 :   ffff800010460c74:       bl      ffff80001045dd00 <blk_mq_update_dispatch_busy.isra.47.part.48>
    0.00 :   ffff800010460c78:       b       ffff800010460c20 <__blk_mq_try_issue_directly+0x180>
    0.00 :   ffff800010460c7c:       str     x21, [x29, #32]
    0.00 :   ffff800010460c80:       stp     x23, x24, [x29, #48]
         :                      __blk_mq_try_issue_directly():
         :                      }
    0.00 :   ffff800010460c84:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (333 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001012e430 <__wake_up>:
         :                      __wake_up():
         :                      * If this function wakes up a task, it executes a full memory barrier before
         :                      * accessing the task state.
         :                      */
         :                      void __wake_up(struct wait_queue_head *wq_head, unsigned int mode,
         :                      int nr_exclusive, void *key)
         :                      {
   38.26 :   ffff80001012e430:       stp     x29, x30, [sp, #-16]!
         :                      __wake_up_common_lock(wq_head, mode, nr_exclusive, 0, key);
    0.00 :   ffff80001012e434:       mov     x4, x3
    0.00 :   ffff80001012e438:       mov     w3, #0x0                        // #0
         :                      {
    0.00 :   ffff80001012e43c:       mov     x29, sp
         :                      __wake_up_common_lock(wq_head, mode, nr_exclusive, 0, key);
   56.03 :   ffff80001012e440:       bl      ffff80001012e368 <__wake_up_common_lock>
         :                      }
    5.72 :   ffff80001012e444:       ldp     x29, x30, [sp], #16
    0.00 :   ffff80001012e448:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (584 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001044fbb0 <__bio_associate_blkg.isra.39>:
         :                      __bio_associate_blkg():
         :                      * alive blkg.
         :                      *
         :                      * A reference will be taken on the @blkg and will be released when @bio is
         :                      * freed.
         :                      */
         :                      static void __bio_associate_blkg(struct bio *bio, struct blkcg_gq *blkg)
    3.07 :   ffff80001044fbb0:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff80001044fbb4:       mov     x29, sp
    1.71 :   ffff80001044fbb8:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001044fbbc:       mov     x20, x0
    0.17 :   ffff80001044fbc0:       str     x21, [sp, #32]
    0.00 :   ffff80001044fbc4:       mov     x19, x1
         :                      bio_disassociate_blkg():
         :                      if (bio->bi_blkg) {
    3.09 :   ffff80001044fbc8:       ldr     x21, [x0]
    0.00 :   ffff80001044fbcc:       cbz     x21, ffff80001044fc30 <__bio_associate_blkg.isra.39+0x80>
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff80001044fbd0:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001044fbd4:       ldr     x0, [x21, #72]
         :                      __ref_is_percpu():
         :                      * Theoretically, the following could test just ATOMIC; however,
         :                      * then we'd have to mask off DEAD separately as DEAD may be
         :                      * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
         :                      * implies ATOMIC anyway.  Test them together.
         :                      */
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff80001044fbd8:       tst     x0, #0x3
    0.00 :   ffff80001044fbdc:       b.ne    ffff80001044fd14 <__bio_associate_blkg.isra.39+0x164>  // b.any
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80001044fbe0:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.00 :   ffff80001044fbe4:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff80001044fbe8:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001044fbec:       str     w2, [x1, #16]
         :                      __percpu_add_case_64():
         :
         :                      PERCPU_RW_OPS(8)
         :                      PERCPU_RW_OPS(16)
         :                      PERCPU_RW_OPS(32)
         :                      PERCPU_RW_OPS(64)
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff80001044fbf0:       mov     x3, #0xffffffffffffffff         // #-1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff80001044fbf4:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff80001044fbf8:       add     x0, x0, x2
    0.00 :   ffff80001044fbfc:       ldxr    x5, [x0]
    0.00 :   ffff80001044fc00:       add     x5, x5, x3
    0.00 :   ffff80001044fc04:       stxr    w4, x5, [x0]
    0.00 :   ffff80001044fc08:       cbnz    w4, ffff80001044fbfc <__bio_associate_blkg.isra.39+0x4c>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001044fc0c:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff80001044fc10:       add     x0, x0, x3
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001044fc14:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff80001044fc18:       cbz     x0, ffff80001044fc24 <__bio_associate_blkg.isra.39+0x74>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001044fc1c:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff80001044fc20:       cbnz    x0, ffff80001044fc28 <__bio_associate_blkg.isra.39+0x78>
         :                      percpu_ref_put_many():
         :                      unsigned long __percpu *percpu_count;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count))
         :                      this_cpu_sub(*percpu_count, nr);
    0.00 :   ffff80001044fc24:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.00 :   ffff80001044fc28:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      bio_disassociate_blkg():
         :                      bio->bi_blkg = NULL;
    0.00 :   ffff80001044fc2c:       str     xzr, [x20]
         :                      blkg_tryget_closest():
         :                      {
         :                      struct blkcg_gq *ret_blkg = NULL;
         :
         :                      WARN_ON_ONCE(!rcu_read_lock_held());
         :
         :                      while (blkg) {
    1.03 :   ffff80001044fc30:       cbz     x19, ffff80001044fcfc <__bio_associate_blkg.isra.39+0x14c>
    0.52 :   ffff80001044fc34:       nop
         :                      rcu_read_lock():
         :                      __rcu_read_lock();
    0.00 :   ffff80001044fc38:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
    0.00 :   ffff80001044fc3c:       ldr     x0, [x19, #72]
         :                      __ref_is_percpu():
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.50 :   ffff80001044fc40:       tst     x0, #0x3
    0.00 :   ffff80001044fc44:       b.ne    ffff80001044fcac <__bio_associate_blkg.isra.39+0xfc>  // b.any
         :                      get_current():
   17.78 :   ffff80001044fc48:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.17 :   ffff80001044fc4c:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff80001044fc50:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    2.40 :   ffff80001044fc54:       str     w2, [x1, #16]
         :                      __percpu_add_case_64():
    0.00 :   ffff80001044fc58:       mov     x3, #0x1                        // #1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.35 :   ffff80001044fc5c:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff80001044fc60:       add     x0, x0, x2
    0.69 :   ffff80001044fc64:       ldxr    x5, [x0]
   37.03 :   ffff80001044fc68:       add     x5, x5, x3
    0.00 :   ffff80001044fc6c:       stxr    w4, x5, [x0]
    0.34 :   ffff80001044fc70:       cbnz    w4, ffff80001044fc64 <__bio_associate_blkg.isra.39+0xb4>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    2.24 :   ffff80001044fc74:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff80001044fc78:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    3.95 :   ffff80001044fc7c:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff80001044fc80:       cbz     x0, ffff80001044fca4 <__bio_associate_blkg.isra.39+0xf4>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    4.79 :   ffff80001044fc84:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff80001044fc88:       cbz     x0, ffff80001044fca4 <__bio_associate_blkg.isra.39+0xf4>
         :                      rcu_read_unlock():
         :                      __rcu_read_unlock();
   16.06 :   ffff80001044fc8c:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      __bio_associate_blkg():
         :                      {
         :                      bio_disassociate_blkg(bio);
         :
         :                      bio->bi_blkg = blkg_tryget_closest(blkg);
    2.05 :   ffff80001044fc90:       str     x19, [x20]
         :                      }
    1.89 :   ffff80001044fc94:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001044fc98:       ldr     x21, [sp, #32]
    0.17 :   ffff80001044fc9c:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001044fca0:       ret
         :                      percpu_ref_tryget():
         :                      this_cpu_inc(*percpu_count);
    0.00 :   ffff80001044fca4:       bl      ffff800010cad668 <preempt_schedule_notrace>
    0.00 :   ffff80001044fca8:       b       ffff80001044fc8c <__bio_associate_blkg.isra.39+0xdc>
         :                      __read_once_size():
    0.00 :   ffff80001044fcac:       ldr     x3, [x19, #64]
         :                      atomic64_fetch_add_unless():
         :                      atomic64_fetch_add_unless(atomic64_t *v, s64 a, s64 u)
         :                      {
         :                      s64 c = atomic64_read(v);
         :
         :                      do {
         :                      if (unlikely(c == u))
    0.00 :   ffff80001044fcb0:       cbz     x3, ffff80001044fcf0 <__bio_associate_blkg.isra.39+0x140>
    0.00 :   ffff80001044fcb4:       add     x4, x19, #0x40
         :                      break;
         :                      } while (!atomic64_try_cmpxchg(v, &c, c + a));
    0.00 :   ffff80001044fcb8:       add     x2, x3, #0x1
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff80001044fcbc:       b       ffff80001044fcdc <__bio_associate_blkg.isra.39+0x12c>
    0.00 :   ffff80001044fcc0:       b       ffff80001044fcdc <__bio_associate_blkg.isra.39+0x12c>
         :                      __lse__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,  l, "memory")
         :                      __CMPXCHG_CASE(x,  , rel_, 64,  l, "memory")
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, al, "memory")
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, al, "memory")
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, al, "memory")
         :                      __CMPXCHG_CASE(x,  ,  mb_, 64, al, "memory")
    0.00 :   ffff80001044fcc4:       mov     x0, x4
    0.00 :   ffff80001044fcc8:       mov     x1, x3
    0.00 :   ffff80001044fccc:       mov     x5, x1
    0.00 :   ffff80001044fcd0:       casal   x5, x2, [x4]
    0.00 :   ffff80001044fcd4:       mov     x0, x5
    0.00 :   ffff80001044fcd8:       b       ffff80001044fce0 <__bio_associate_blkg.isra.39+0x130>
         :                      __ll_sc__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  , rel_, 64,        ,  , l, "memory", L)
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  ,  mb_, 64, dmb ish,  , l, "memory", L)
    0.00 :   ffff80001044fcdc:       b       ffff8000104520f8 <bio_associate_blkg_from_page+0xc0>
         :                      atomic64_try_cmpxchg():
         :                      if (unlikely(r != o))
    0.00 :   ffff80001044fce0:       cmp     x0, x3
    0.00 :   ffff80001044fce4:       b.eq    ffff80001044fc8c <__bio_associate_blkg.isra.39+0xdc>  // b.none
    0.00 :   ffff80001044fce8:       mov     x3, x0
         :                      atomic64_fetch_add_unless():
         :                      if (unlikely(c == u))
    0.00 :   ffff80001044fcec:       cbnz    x0, ffff80001044fcb8 <__bio_associate_blkg.isra.39+0x108>
         :                      rcu_read_unlock():
    0.00 :   ffff80001044fcf0:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blkg_tryget_closest():
         :                      if (blkg_tryget(blkg)) {
         :                      ret_blkg = blkg;
         :                      break;
         :                      }
         :                      blkg = blkg->parent;
    0.00 :   ffff80001044fcf4:       ldr     x19, [x19, #56]
         :                      while (blkg) {
    0.00 :   ffff80001044fcf8:       cbnz    x19, ffff80001044fc38 <__bio_associate_blkg.isra.39+0x88>
         :                      struct blkcg_gq *ret_blkg = NULL;
    0.00 :   ffff80001044fcfc:       mov     x19, #0x0                       // #0
         :                      __bio_associate_blkg():
         :                      bio->bi_blkg = blkg_tryget_closest(blkg);
    0.00 :   ffff80001044fd00:       str     x19, [x20]
         :                      }
    0.00 :   ffff80001044fd04:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001044fd08:       ldr     x21, [sp, #32]
    0.00 :   ffff80001044fd0c:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001044fd10:       ret
         :                      blkg_put():
         :                      * blkg_put - put a blkg reference
         :                      * @blkg: blkg to put
         :                      */
         :                      static inline void blkg_put(struct blkcg_gq *blkg)
         :                      {
         :                      percpu_ref_put(&blkg->refcnt);
    0.00 :   ffff80001044fd14:       add     x0, x21, #0x40
         :                      arch_static_branch_jump():
    0.00 :   ffff80001044fd18:       b       ffff80001044fd40 <__bio_associate_blkg.isra.39+0x190>
    0.00 :   ffff80001044fd1c:       b       ffff80001044fd40 <__bio_associate_blkg.isra.39+0x190>
         :                      __lse_atomic64_sub_return():
         :                      ATOMIC64_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff80001044fd20:       mov     x1, #0x1                        // #1
    0.00 :   ffff80001044fd24:       neg     x1, x1
    0.00 :   ffff80001044fd28:       ldaddal x1, x2, [x0]
    0.00 :   ffff80001044fd2c:       add     x1, x1, x2
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff80001044fd30:       cbnz    x1, ffff80001044fc28 <__bio_associate_blkg.isra.39+0x78>
         :                      ref->release(ref);
    0.00 :   ffff80001044fd34:       ldr     x1, [x0, #16]
    0.00 :   ffff80001044fd38:       blr     x1
    0.00 :   ffff80001044fd3c:       b       ffff80001044fc28 <__bio_associate_blkg.isra.39+0x78>
         :                      __ll_sc_atomic64_sub_return():
         :                      ATOMIC64_OPS(sub, sub, J)
    0.00 :   ffff80001044fd40:       mov     x2, #0x1                        // #1
    0.00 :   ffff80001044fd44:       add     x4, x21, #0x40
    0.00 :   ffff80001044fd48:       b       ffff800010452118 <bio_associate_blkg_from_page+0xe0>
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff80001044fd4c:       cbnz    x1, ffff80001044fc28 <__bio_associate_blkg.isra.39+0x78>
    0.00 :   ffff80001044fd50:       b       ffff80001044fd34 <__bio_associate_blkg.isra.39+0x184>
 Percent |	Source code & Disassembly of vmlinux for cycles (590 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010caec10 <mutex_unlock>:
         :                      mutex_unlock():
         :                      * of a not locked mutex is not allowed.
         :                      *
         :                      * This function is similar to (but not equivalent to) up().
         :                      */
         :                      void __sched mutex_unlock(struct mutex *lock)
         :                      {
    0.00 :   ffff800010caec10:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff800010caec14:       mov     x3, x0
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800010caec18:       mrs     x4, sp_el0
         :                      mutex_unlock():
    0.00 :   ffff800010caec1c:       mov     x29, sp
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    1.87 :   ffff800010caec20:       b       ffff800010caec40 <mutex_unlock+0x30>
    0.69 :   ffff800010caec24:       b       ffff800010caec40 <mutex_unlock+0x30>
         :                      __lse__cmpxchg_case_rel_64():
         :                      __CMPXCHG_CASE(w,  , acq_, 32,  a, "memory")
         :                      __CMPXCHG_CASE(x,  , acq_, 64,  a, "memory")
         :                      __CMPXCHG_CASE(w, b, rel_,  8,  l, "memory")
         :                      __CMPXCHG_CASE(w, h, rel_, 16,  l, "memory")
         :                      __CMPXCHG_CASE(w,  , rel_, 32,  l, "memory")
         :                      __CMPXCHG_CASE(x,  , rel_, 64,  l, "memory")
    0.00 :   ffff800010caec28:       mov     x2, #0x0                        // #0
    0.00 :   ffff800010caec2c:       mov     x1, x4
    0.00 :   ffff800010caec30:       mov     x5, x1
    0.00 :   ffff800010caec34:       casl    x5, x2, [x0]
   97.44 :   ffff800010caec38:       mov     x0, x5
    0.00 :   ffff800010caec3c:       b       ffff800010caec48 <mutex_unlock+0x38>
         :                      __ll_sc__cmpxchg_case_rel_64():
         :                      __CMPXCHG_CASE(w,  , acq_, 32,        , a,  , "memory", K)
         :                      __CMPXCHG_CASE( ,  , acq_, 64,        , a,  , "memory", L)
         :                      __CMPXCHG_CASE(w, b, rel_,  8,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w, h, rel_, 16,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w,  , rel_, 32,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  , rel_, 64,        ,  , l, "memory", L)
    0.00 :   ffff800010caec40:       mov     x1, #0x0                        // #0
    0.00 :   ffff800010caec44:       b       ffff800010cafeb0 <ww_mutex_lock_interruptible+0xf0>
         :                      __mutex_unlock_fast():
         :                      if (atomic_long_cmpxchg_release(&lock->owner, curr, 0UL) == curr)
    0.00 :   ffff800010caec48:       cmp     x4, x0
    0.00 :   ffff800010caec4c:       b.eq    ffff800010caec58 <mutex_unlock+0x48>  // b.none
    0.00 :   ffff800010caec50:       mov     x0, x3
         :                      mutex_unlock():
         :                      #ifndef CONFIG_DEBUG_LOCK_ALLOC
         :                      if (__mutex_unlock_fast(lock))
         :                      return;
         :                      #endif
         :                      __mutex_unlock_slowpath(lock, _RET_IP_);
    0.00 :   ffff800010caec54:       bl      ffff800010caea90 <__mutex_unlock_slowpath.isra.19>
         :                      }
    0.00 :   ffff800010caec58:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010caec5c:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (599 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010454dc0 <submit_bio>:
         :                      submit_bio():
         :                      * uses that function to do most of the work. Both are fairly rough
         :                      * interfaces; @bio must be presetup and ready for I/O.
         :                      *
         :                      */
         :                      blk_qc_t submit_bio(struct bio *bio)
         :                      {
    2.01 :   ffff800010454dc0:       stp     x29, x30, [sp, #-128]!
    0.00 :   ffff800010454dc4:       mov     x29, sp
    5.16 :   ffff800010454dc8:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010454dcc:       adrp    x19, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010454dd0:       add     x1, x19, #0x8c8
    0.00 :   ffff800010454dd4:       mov     x20, x0
    0.50 :   ffff800010454dd8:       ldr     x2, [x1]
    3.50 :   ffff800010454ddc:       str     x2, [x29, #120]
    0.00 :   ffff800010454de0:       mov     x2, #0x0                        // #0
         :                      blkcg_punt_bio_submit():
         :
         :                      bool __blkcg_punt_bio_submit(struct bio *bio);
         :
         :                      static inline bool blkcg_punt_bio_submit(struct bio *bio)
         :                      {
         :                      if (bio->bi_opf & REQ_CGROUP_PUNT)
    0.83 :   ffff800010454de4:       ldr     w1, [x0, #16]
    0.00 :   ffff800010454de8:       tbnz    w1, #23, ffff800010454e28 <submit_bio+0x68>
         :                      bio_has_data():
         :                      /*
         :                      * Check whether this bio carries any data or not. A NULL bio is allowed.
         :                      */
         :                      static inline bool bio_has_data(struct bio *bio)
         :                      {
         :                      if (bio &&
    0.50 :   ffff800010454dec:       cbz     x20, ffff800010454df8 <submit_bio+0x38>
         :                      bio->bi_iter.bi_size &&
    0.34 :   ffff800010454df0:       ldr     w0, [x20, #40]
         :                      if (bio &&
    0.00 :   ffff800010454df4:       cbnz    w0, ffff800010454e3c <submit_bio+0x7c>
         :                      submit_bio():
         :                      * submission can be a significant part of overall IO time.
         :                      */
         :                      if (workingset_read)
         :                      psi_memstall_enter(&pflags);
         :
         :                      ret = generic_make_request(bio);
    1.84 :   ffff800010454df8:       mov     x0, x20
    0.00 :   ffff800010454dfc:       bl      ffff800010454aa0 <generic_make_request>
    0.17 :   ffff800010454e00:       mov     w1, w0
         :
         :                      if (workingset_read)
         :                      psi_memstall_leave(&pflags);
         :
         :                      return ret;
         :                      }
    0.00 :   ffff800010454e04:       add     x19, x19, #0x8c8
    0.00 :   ffff800010454e08:       mov     w0, w1
    3.35 :   ffff800010454e0c:       ldr     x2, [x29, #120]
    5.67 :   ffff800010454e10:       ldr     x1, [x19]
    0.00 :   ffff800010454e14:       eor     x1, x2, x1
    0.00 :   ffff800010454e18:       cbnz    x1, ffff800010454fa0 <submit_bio+0x1e0>
    0.50 :   ffff800010454e1c:       ldp     x19, x20, [sp, #16]
    5.83 :   ffff800010454e20:       ldp     x29, x30, [sp], #128
    0.00 :   ffff800010454e24:       ret
         :                      blkcg_punt_bio_submit():
         :                      return __blkcg_punt_bio_submit(bio);
    0.00 :   ffff800010454e28:       bl      ffff800010475af0 <__blkcg_punt_bio_submit>
         :                      submit_bio():
         :                      if (blkcg_punt_bio_submit(bio))
    0.00 :   ffff800010454e2c:       tst     w0, #0xff
         :                      return BLK_QC_T_NONE;
    0.00 :   ffff800010454e30:       mov     w1, #0xffffffff                 // #-1
         :                      if (blkcg_punt_bio_submit(bio))
    0.00 :   ffff800010454e34:       b.eq    ffff800010454dec <submit_bio+0x2c>  // b.none
    0.00 :   ffff800010454e38:       b       ffff800010454e04 <submit_bio+0x44>
         :                      bio_has_data():
         :                      bio_op(bio) != REQ_OP_DISCARD &&
    2.68 :   ffff800010454e3c:       ldr     w2, [x20, #16]
    0.16 :   ffff800010454e40:       and     w1, w2, #0xff
         :                      bio->bi_iter.bi_size &&
    0.00 :   ffff800010454e44:       sub     w3, w1, #0x3
         :                      bio_op(bio) != REQ_OP_SECURE_ERASE &&
    0.00 :   ffff800010454e48:       tst     w3, #0xfffffffd
    1.84 :   ffff800010454e4c:       ccmp    w1, #0x9, #0x4, ne  // ne = any
    0.00 :   ffff800010454e50:       b.eq    ffff800010454df8 <submit_bio+0x38>  // b.none
    0.00 :   ffff800010454e54:       str     x21, [x29, #32]
         :                      submit_bio():
         :                      count = bio_sectors(bio);
    0.00 :   ffff800010454e58:       lsr     w21, w0, #9
         :                      if (unlikely(bio_op(bio) == REQ_OP_WRITE_SAME))
    0.00 :   ffff800010454e5c:       cmp     w1, #0x7
    0.00 :   ffff800010454e60:       mov     w1, w21
    3.81 :   ffff800010454e64:       b.eq    ffff800010454ef8 <submit_bio+0x138>  // b.none
         :                      if (op_is_write(bio_op(bio))) {
    1.35 :   ffff800010454e68:       tbnz    w2, #0, ffff800010454ed4 <submit_bio+0x114>
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.84 :   ffff800010454e6c:       mrs     x2, sp_el0
         :                      task_io_account_read():
         :                      #include <linux/sched.h>
         :
         :                      #ifdef CONFIG_TASK_IO_ACCOUNTING
         :                      static inline void task_io_account_read(size_t bytes)
         :                      {
         :                      current->ioac.read_bytes += bytes;
    0.00 :   ffff800010454e70:       ldr     x4, [x2, #1928]
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    5.17 :   ffff800010454e74:       ldr     w3, [x2, #16]
         :                      task_io_account_read():
    0.00 :   ffff800010454e78:       add     x0, x4, w0, uxtw
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff800010454e7c:       add     w3, w3, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    1.51 :   ffff800010454e80:       str     w3, [x2, #16]
         :                      task_io_account_read():
    3.32 :   ffff800010454e84:       str     x0, [x2, #1928]
         :                      count_vm_events():
         :                      raw_cpu_add(vm_event_states.event[item], delta);
         :                      }
         :
         :                      static inline void count_vm_events(enum vm_event_item item, long delta)
         :                      {
         :                      this_cpu_add(vm_event_states.event[item], delta);
    0.00 :   ffff800010454e88:       adrp    x0, ffff8000114d3000 <pmu_sb_events>
    0.00 :   ffff800010454e8c:       add     x0, x0, #0x8b8
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.17 :   ffff800010454e90:       mrs     x3, tpidr_el1
         :                      __percpu_add_case_64():
         :
         :                      PERCPU_RW_OPS(8)
         :                      PERCPU_RW_OPS(16)
         :                      PERCPU_RW_OPS(32)
         :                      PERCPU_RW_OPS(64)
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff800010454e94:       add     x0, x0, x3
    1.85 :   ffff800010454e98:       ldxr    x5, [x0]
   33.30 :   ffff800010454e9c:       add     x5, x5, x1
    0.00 :   ffff800010454ea0:       stxr    w4, x5, [x0]
    0.00 :   ffff800010454ea4:       cbnz    w4, ffff800010454e98 <submit_bio+0xd8>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.17 :   ffff800010454ea8:       ldr     x0, [x2, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800010454eac:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    2.32 :   ffff800010454eb0:       str     w0, [x2, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800010454eb4:       cbz     x0, ffff800010454ef0 <submit_bio+0x130>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010454eb8:       ldr     x0, [x2, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff800010454ebc:       cbz     x0, ffff800010454ef0 <submit_bio+0x130>
         :                      submit_bio():
         :                      if (unlikely(block_dump)) {
    3.02 :   ffff800010454ec0:       adrp    x0, ffff800011aaa000 <pmus_srcu+0x18>
    2.01 :   ffff800010454ec4:       ldr     w0, [x0, #2456]
    0.00 :   ffff800010454ec8:       cbnz    w0, ffff800010454f1c <submit_bio+0x15c>
    6.30 :   ffff800010454ecc:       ldr     x21, [x29, #32]
    0.00 :   ffff800010454ed0:       b       ffff800010454df8 <submit_bio+0x38>
         :                      get_current():
    0.00 :   ffff800010454ed4:       mrs     x2, sp_el0
         :                      __read_once_size():
    0.00 :   ffff800010454ed8:       ldr     w0, [x2, #16]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff800010454edc:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010454ee0:       str     w0, [x2, #16]
         :                      count_vm_events():
    0.00 :   ffff800010454ee4:       adrp    x0, ffff8000114d3000 <pmu_sb_events>
    0.00 :   ffff800010454ee8:       add     x0, x0, #0x8c0
    0.00 :   ffff800010454eec:       b       ffff800010454e90 <submit_bio+0xd0>
    0.00 :   ffff800010454ef0:       bl      ffff800010cad668 <preempt_schedule_notrace>
    0.00 :   ffff800010454ef4:       b       ffff800010454ec0 <submit_bio+0x100>
         :                      submit_bio():
         :                      count = queue_logical_block_size(bio->bi_disk->queue) >> 9;
    0.00 :   ffff800010454ef8:       ldr     x1, [x20, #8]
    0.00 :   ffff800010454efc:       ldr     x1, [x1, #1040]
         :                      queue_logical_block_size():
         :
         :                      static inline unsigned queue_logical_block_size(const struct request_queue *q)
         :                      {
         :                      int retval = 512;
         :
         :                      if (q && q->limits.logical_block_size)
    0.00 :   ffff800010454f00:       cbz     x1, ffff800010454f90 <submit_bio+0x1d0>
    0.00 :   ffff800010454f04:       ldr     w21, [x1, #1088]
    0.00 :   ffff800010454f08:       cbz     w21, ffff800010454f90 <submit_bio+0x1d0>
    0.00 :   ffff800010454f0c:       lsr     w1, w21, #9
    0.00 :   ffff800010454f10:       mov     x21, x1
         :                      submit_bio():
         :                      if (op_is_write(bio_op(bio))) {
    0.00 :   ffff800010454f14:       tbz     w2, #0, ffff800010454e6c <submit_bio+0xac>
    0.00 :   ffff800010454f18:       b       ffff800010454ed4 <submit_bio+0x114>
    0.00 :   ffff800010454f1c:       stp     x22, x23, [x29, #40]
         :                      printk(KERN_DEBUG "%s(%d): %s block %Lu on %s (%u sectors)\n",
    0.00 :   ffff800010454f20:       adrp    x1, ffff800011198000 <kallsyms_token_index+0x1e330>
    0.00 :   ffff800010454f24:       stp     x24, x25, [x29, #56]
    0.00 :   ffff800010454f28:       add     x1, x1, #0x9b8
    0.00 :   ffff800010454f2c:       adrp    x22, ffff800011188000 <kallsyms_token_index+0xe330>
    0.00 :   ffff800010454f30:       add     x22, x22, #0x2d0
         :                      op_is_write():
         :                      bio->bi_opf = op | op_flags;
         :                      }
         :
         :                      static inline bool op_is_write(unsigned int op)
         :                      {
         :                      return (op & 1);
    0.00 :   ffff800010454f34:       ldr     w2, [x20, #16]
         :                      get_current():
    0.00 :   ffff800010454f38:       mrs     x0, sp_el0
         :                      submit_bio():
         :                      (unsigned long long)bio->bi_iter.bi_sector,
    0.00 :   ffff800010454f3c:       ldr     x25, [x20, #32]
         :                      current->comm, task_pid_nr(current),
    0.00 :   ffff800010454f40:       add     x23, x0, #0x620
         :                      printk(KERN_DEBUG "%s(%d): %s block %Lu on %s (%u sectors)\n",
    0.00 :   ffff800010454f44:       tst     x2, #0x1
         :                      task_pid_nr():
         :                      */
         :                      pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type, struct pid_namespace *ns);
         :
         :                      static inline pid_t task_pid_nr(struct task_struct *tsk)
         :                      {
         :                      return tsk->pid;
    0.00 :   ffff800010454f48:       ldr     w24, [x0, #1128]
         :                      submit_bio():
    0.00 :   ffff800010454f4c:       csel    x22, x22, x1, eq  // eq = none
    0.00 :   ffff800010454f50:       mov     x0, x20
    0.00 :   ffff800010454f54:       add     x1, x29, #0x58
    0.00 :   ffff800010454f58:       bl      ffff80001046b940 <bio_devname>
    0.00 :   ffff800010454f5c:       mov     x5, x0
    0.00 :   ffff800010454f60:       adrp    x7, ffff8000111ab000 <kallsyms_token_index+0x31330>
    0.00 :   ffff800010454f64:       mov     w6, w21
    0.00 :   ffff800010454f68:       mov     x4, x25
    0.00 :   ffff800010454f6c:       mov     x3, x22
    0.00 :   ffff800010454f70:       mov     w2, w24
    0.00 :   ffff800010454f74:       mov     x1, x23
    0.00 :   ffff800010454f78:       add     x0, x7, #0x310
    0.00 :   ffff800010454f7c:       bl      ffff800010148e94 <printk>
    0.00 :   ffff800010454f80:       ldp     x21, x22, [x29, #32]
    0.00 :   ffff800010454f84:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff800010454f88:       ldr     x25, [x29, #64]
    0.00 :   ffff800010454f8c:       b       ffff800010454df8 <submit_bio+0x38>
         :                      queue_logical_block_size():
    0.00 :   ffff800010454f90:       mov     x1, #0x1                        // #1
    0.00 :   ffff800010454f94:       mov     w21, w1
         :                      submit_bio():
         :                      if (op_is_write(bio_op(bio))) {
    0.00 :   ffff800010454f98:       tbz     w2, #0, ffff800010454e6c <submit_bio+0xac>
    0.00 :   ffff800010454f9c:       b       ffff800010454ed4 <submit_bio+0x114>
    0.00 :   ffff800010454fa0:       stp     x21, x22, [x29, #32]
    0.00 :   ffff800010454fa4:       stp     x23, x24, [x29, #48]
    0.00 :   ffff800010454fa8:       str     x25, [x29, #64]
         :                      }
    0.00 :   ffff800010454fac:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (572 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001045c888 <blk_add_timer>:
         :                      blk_add_timer():
         :                      * Notes:
         :                      *    Each request has its own timer, and as it is added to the queue, we
         :                      *    set up the timer. When the request completes, we cancel the timer.
         :                      */
         :                      void blk_add_timer(struct request *req)
         :                      {
    0.00 :   ffff80001045c888:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff80001045c88c:       mov     x29, sp
    0.00 :   ffff80001045c890:       stp     x19, x20, [sp, #16]
    0.17 :   ffff80001045c894:       str     x21, [sp, #32]
         :
         :                      /*
         :                      * Some LLDs, like scsi, peek at the timeout to prevent a
         :                      * command from being retried forever.
         :                      */
         :                      if (!req->timeout)
    3.16 :   ffff80001045c898:       ldr     w1, [x0, #216]
         :                      struct request_queue *q = req->q;
    0.00 :   ffff80001045c89c:       ldr     x19, [x0]
         :                      if (!req->timeout)
    0.00 :   ffff80001045c8a0:       cbnz    w1, ffff80001045c8ac <blk_add_timer+0x24>
         :                      req->timeout = q->rq_timeout;
    0.00 :   ffff80001045c8a4:       ldr     w1, [x19, #264]
   20.45 :   ffff80001045c8a8:       str     w1, [x0, #216]
         :
         :                      req->rq_flags &= ~RQF_TIMED_OUT;
         :
         :                      expiry = jiffies + req->timeout;
    0.00 :   ffff80001045c8ac:       adrp    x21, ffff800011897000 <bit_wait_table+0xe80>
         :                      req->rq_flags &= ~RQF_TIMED_OUT;
    0.35 :   ffff80001045c8b0:       ldr     w2, [x0, #28]
         :                      expiry = jiffies + req->timeout;
    0.00 :   ffff80001045c8b4:       ldr     x3, [x21, #2432]
         :                      req->rq_flags &= ~RQF_TIMED_OUT;
    0.00 :   ffff80001045c8b8:       and     w2, w2, #0xffdfffff
   12.07 :   ffff80001045c8bc:       str     w2, [x0, #28]
         :                      expiry = jiffies + req->timeout;
    0.70 :   ffff80001045c8c0:       add     x1, x3, w1, uxtw
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    1.05 :   ffff80001045c8c4:       str     x1, [x0, #224]
         :                      blk_add_timer():
         :                      /*
         :                      * If the timer isn't already pending or this timeout is earlier
         :                      * than an existing one, modify the timer. Round up to next nearest
         :                      * second.
         :                      */
         :                      expiry = blk_rq_timeout(round_jiffies_up(expiry));
    0.00 :   ffff80001045c8c8:       mov     x0, x1
    0.00 :   ffff80001045c8cc:       bl      ffff800010167980 <round_jiffies_up>
    0.00 :   ffff80001045c8d0:       mov     x20, x0
         :                      blk_rq_timeout():
         :                      maxt = round_jiffies_up(jiffies + BLK_MAX_TIMEOUT);
    0.35 :   ffff80001045c8d4:       ldr     x0, [x21, #2432]
    0.00 :   ffff80001045c8d8:       add     x0, x0, #0x4e2
    0.17 :   ffff80001045c8dc:       bl      ffff800010167980 <round_jiffies_up>
         :                      blk_add_timer():
         :
         :                      if (!timer_pending(&q->timeout) ||
    0.52 :   ffff80001045c8e0:       ldr     x1, [x19, #928]
         :                      blk_rq_timeout():
         :                      if (time_after(timeout, maxt))
    0.00 :   ffff80001045c8e4:       sub     x2, x0, x20
    0.00 :   ffff80001045c8e8:       cmp     x2, #0x0
    0.00 :   ffff80001045c8ec:       csel    x0, x0, x20, lt  // lt = tstop
         :                      blk_add_timer():
         :                      if (!timer_pending(&q->timeout) ||
    3.13 :   ffff80001045c8f0:       cbz     x1, ffff80001045c91c <blk_add_timer+0x94>
         :                      time_before(expiry, q->timeout.expires)) {
    0.35 :   ffff80001045c8f4:       ldr     x1, [x19, #936]
    0.00 :   ffff80001045c8f8:       sub     x2, x0, x1
    0.00 :   ffff80001045c8fc:       tbnz    x2, #63, ffff80001045c910 <blk_add_timer+0x88>
         :                      */
         :                      if (!timer_pending(&q->timeout) || (diff >= HZ / 2))
         :                      mod_timer(&q->timeout, expiry);
         :                      }
         :
         :                      }
    1.75 :   ffff80001045c900:       ldp     x19, x20, [sp, #16]
   55.78 :   ffff80001045c904:       ldr     x21, [sp, #32]
    0.00 :   ffff80001045c908:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001045c90c:       ret
         :                      unsigned long diff = q->timeout.expires - expiry;
    0.00 :   ffff80001045c910:       sub     x1, x1, x0
         :                      if (!timer_pending(&q->timeout) || (diff >= HZ / 2))
    0.00 :   ffff80001045c914:       cmp     x1, #0x7c
    0.00 :   ffff80001045c918:       b.ls    ffff80001045c900 <blk_add_timer+0x78>  // b.plast
         :                      mod_timer(&q->timeout, expiry);
    0.00 :   ffff80001045c91c:       mov     x1, x0
    0.00 :   ffff80001045c920:       add     x0, x19, #0x398
    0.00 :   ffff80001045c924:       bl      ffff800010167fe0 <mod_timer>
         :                      }
    0.00 :   ffff80001045c928:       ldr     x21, [sp, #32]
    0.00 :   ffff80001045c92c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001045c930:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001045c934:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (590 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001045a9f8 <__blk_queue_split>:
         :                      __blk_queue_split():
         :                      * of the caller to ensure that @q is only released after processing of the
         :                      * split bio has finished.
         :                      */
         :                      void __blk_queue_split(struct request_queue *q, struct bio **bio,
         :                      unsigned int *nr_segs)
         :                      {
    4.55 :   ffff80001045a9f8:       stp     x29, x30, [sp, #-144]!
    0.00 :   ffff80001045a9fc:       mov     x29, sp
   18.47 :   ffff80001045aa00:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001045aa04:       mov     x20, x1
    2.38 :   ffff80001045aa08:       str     x21, [sp, #32]
    0.00 :   ffff80001045aa0c:       adrp    x19, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff80001045aa10:       stp     x25, x26, [sp, #64]
    0.00 :   ffff80001045aa14:       add     x1, x19, #0x8c8
    0.00 :   ffff80001045aa18:       mov     x26, x0
    0.00 :   ffff80001045aa1c:       mov     x21, x2
         :                      struct bio *split = NULL;
         :
         :                      switch (bio_op(*bio)) {
    1.52 :   ffff80001045aa20:       ldr     x25, [x20]
         :                      {
    1.53 :   ffff80001045aa24:       ldr     x0, [x1]
    0.68 :   ffff80001045aa28:       str     x0, [x29, #136]
    0.00 :   ffff80001045aa2c:       mov     x0, #0x0                        // #0
         :                      switch (bio_op(*bio)) {
    0.85 :   ffff80001045aa30:       ldrb    w0, [x25, #16]
    0.00 :   ffff80001045aa34:       cmp     w0, #0x5
    0.00 :   ffff80001045aa38:       b.eq    ffff80001045abf0 <__blk_queue_split+0x1f8>  // b.none
    6.60 :   ffff80001045aa3c:       b.ls    ffff80001045aab0 <__blk_queue_split+0xb8>  // b.plast
    0.00 :   ffff80001045aa40:       cmp     w0, #0x7
    0.00 :   ffff80001045aa44:       b.eq    ffff80001045aa90 <__blk_queue_split+0x98>  // b.none
    0.00 :   ffff80001045aa48:       cmp     w0, #0x9
    0.00 :   ffff80001045aa4c:       b.ne    ffff80001045aab8 <__blk_queue_split+0xc0>  // b.any
         :                      blk_bio_write_zeroes_split():
         :                      *nsegs = 0;
    0.00 :   ffff80001045aa50:       str     wzr, [x2]
         :                      if (!q->limits.max_write_zeroes_sectors)
    0.00 :   ffff80001045aa54:       ldr     w1, [x26, #1116]
    0.00 :   ffff80001045aa58:       cbz     w1, ffff80001045aa68 <__blk_queue_split+0x70>
         :                      blk_bio_write_same_split():
         :                      if (bio_sectors(bio) <= q->limits.max_write_same_sectors)
    0.00 :   ffff80001045aa5c:       ldr     w0, [x25, #40]
    0.00 :   ffff80001045aa60:       cmp     w1, w0, lsr #9
    0.00 :   ffff80001045aa64:       b.cc    ffff80001045ac9c <__blk_queue_split+0x2a4>  // b.lo, b.ul, b.last
         :                      __blk_queue_split():
         :                      bio_chain(split, *bio);
         :                      trace_block_split(q, split, (*bio)->bi_iter.bi_sector);
         :                      generic_make_request(*bio);
         :                      *bio = split;
         :                      }
         :                      }
    5.94 :   ffff80001045aa68:       add     x19, x19, #0x8c8
    1.02 :   ffff80001045aa6c:       ldr     x1, [x29, #136]
   19.15 :   ffff80001045aa70:       ldr     x0, [x19]
    0.00 :   ffff80001045aa74:       eor     x0, x1, x0
    0.00 :   ffff80001045aa78:       cbnz    x0, ffff80001045ae44 <__blk_queue_split+0x44c>
    9.49 :   ffff80001045aa7c:       ldp     x19, x20, [sp, #16]
    4.92 :   ffff80001045aa80:       ldr     x21, [sp, #32]
    0.00 :   ffff80001045aa84:       ldp     x25, x26, [sp, #64]
    0.00 :   ffff80001045aa88:       ldp     x29, x30, [sp], #144
    0.00 :   ffff80001045aa8c:       ret
         :                      blk_bio_write_same_split():
         :                      *nsegs = 1;
    0.00 :   ffff80001045aa90:       mov     w0, #0x1                        // #1
    0.00 :   ffff80001045aa94:       str     w0, [x2]
         :                      if (!q->limits.max_write_same_sectors)
    0.00 :   ffff80001045aa98:       ldr     w1, [x26, #1112]
    0.00 :   ffff80001045aa9c:       cbz     w1, ffff80001045aa68 <__blk_queue_split+0x70>
         :                      if (bio_sectors(bio) <= q->limits.max_write_same_sectors)
    0.00 :   ffff80001045aaa0:       ldr     w0, [x25, #40]
    0.00 :   ffff80001045aaa4:       cmp     w1, w0, lsr #9
    0.00 :   ffff80001045aaa8:       b.cs    ffff80001045aa68 <__blk_queue_split+0x70>  // b.hs, b.nlast
    0.00 :   ffff80001045aaac:       b       ffff80001045ac9c <__blk_queue_split+0x2a4>
         :                      __blk_queue_split():
         :                      switch (bio_op(*bio)) {
    2.21 :   ffff80001045aab0:       cmp     w0, #0x3
    0.00 :   ffff80001045aab4:       b.eq    ffff80001045abf0 <__blk_queue_split+0x1f8>  // b.none
         :                      if (!q->limits.chunk_sectors &&
    2.20 :   ffff80001045aab8:       ldr     w0, [x26, #1072]
    0.00 :   ffff80001045aabc:       cbnz    w0, ffff80001045acb4 <__blk_queue_split+0x2bc>
    4.24 :   ffff80001045aac0:       ldrh    w0, [x25, #96]
    0.00 :   ffff80001045aac4:       cmp     w0, #0x1
    0.00 :   ffff80001045aac8:       b.eq    ffff80001045ae28 <__blk_queue_split+0x430>  // b.none
    0.00 :   ffff80001045aacc:       stp     x22, x23, [x29, #40]
         :                      split = blk_bio_segment_split(q, *bio, &q->bio_split, nr_segs);
    0.00 :   ffff80001045aad0:       add     x18, x26, #0x610
    0.00 :   ffff80001045aad4:       str     x24, [x29, #56]
    0.00 :   ffff80001045aad8:       stp     x27, x28, [x29, #80]
         :                      blk_bio_segment_split():
         :                      unsigned nsegs = 0, sectors = 0;
    0.00 :   ffff80001045aadc:       stp     wzr, wzr, [x29, #96]
    0.00 :   ffff80001045aae0:       ldr     w4, [x25, #32]
         :                      get_max_io_size():
         :                      unsigned sectors = blk_max_size_offset(q, bio->bi_iter.bi_sector);
    0.00 :   ffff80001045aae4:       ldr     w1, [x26, #1076]
         :                      unsigned pbs = queue_physical_block_size(q) >> SECTOR_SHIFT;
    0.00 :   ffff80001045aae8:       ldr     w0, [x26, #1084]
         :                      queue_logical_block_size():
         :
         :                      static inline unsigned queue_logical_block_size(const struct request_queue *q)
         :                      {
         :                      int retval = 512;
         :
         :                      if (q && q->limits.logical_block_size)
    0.00 :   ffff80001045aaec:       mov     w2, #0x1                        // #1
         :                      get_max_io_size():
    0.00 :   ffff80001045aaf0:       lsr     w0, w0, #9
         :                      queue_logical_block_size():
    0.00 :   ffff80001045aaf4:       cbz     x26, ffff80001045ab08 <__blk_queue_split+0x110>
    0.00 :   ffff80001045aaf8:       ldr     w3, [x26, #1088]
    0.00 :   ffff80001045aafc:       cmp     w3, #0x0
    0.00 :   ffff80001045ab00:       lsr     w3, w3, #9
    0.00 :   ffff80001045ab04:       csel    w2, w3, w2, ne  // ne = any
         :                      get_max_io_size():
         :                      unsigned start_offset = bio->bi_iter.bi_sector & (pbs - 1);
    0.00 :   ffff80001045ab08:       sub     w3, w0, #0x1
         :                      max_sectors &= ~(pbs - 1);
    0.00 :   ffff80001045ab0c:       neg     w0, w0
         :                      unsigned start_offset = bio->bi_iter.bi_sector & (pbs - 1);
    0.00 :   ffff80001045ab10:       and     w3, w3, w4
         :                      return max_sectors - start_offset;
    0.00 :   ffff80001045ab14:       sub     w2, w2, #0x1
         :                      max_sectors += start_offset;
    0.00 :   ffff80001045ab18:       add     w4, w3, w1
         :                      blk_bio_segment_split():
         :                      const unsigned max_segs = queue_max_segments(q);
    0.00 :   ffff80001045ab1c:       ldrh    w27, [x26, #1128]
         :                      get_max_io_size():
         :                      max_sectors &= ~(pbs - 1);
    0.00 :   ffff80001045ab20:       and     w0, w0, w4
         :                      return max_sectors - start_offset;
    0.00 :   ffff80001045ab24:       and     w1, w2, w1
    0.00 :   ffff80001045ab28:       sub     w14, w0, w3
    0.00 :   ffff80001045ab2c:       cmp     w3, w0
         :                      blk_bio_segment_split():
         :                      bio_for_each_bvec(bv, bio, iter) {
    0.00 :   ffff80001045ab30:       ldp     w24, w23, [x25, #40]
         :                      get_max_io_size():
         :                      return max_sectors - start_offset;
    0.00 :   ffff80001045ab34:       csel    w14, w1, w14, cs  // cs = hs, nlast
         :                      blk_bio_segment_split():
         :                      bio_for_each_bvec(bv, bio, iter) {
    0.00 :   ffff80001045ab38:       ldr     w22, [x25, #48]
    0.00 :   ffff80001045ab3c:       mov     w2, #0x0                        // #0
         :                      struct bio_vec bv, bvprv, *bvprvp = NULL;
    0.00 :   ffff80001045ab40:       mov     x3, #0x0                        // #0
         :                      bio_no_advance_iter():
         :                      return false;
         :                      }
         :
         :                      static inline bool bio_no_advance_iter(struct bio *bio)
         :                      {
         :                      return bio_op(bio) == REQ_OP_DISCARD ||
    0.00 :   ffff80001045ab44:       mov     w15, #0xfb                      // #251
         :                      blk_bio_segment_split():
         :                      bio_for_each_bvec(bv, bio, iter) {
    0.00 :   ffff80001045ab48:       cbz     w24, ffff80001045ae0c <__blk_queue_split+0x414>
    0.00 :   ffff80001045ab4c:       ldr     x0, [x25, #104]
    0.00 :   ffff80001045ab50:       ubfiz   x28, x23, #4, #32
    0.00 :   ffff80001045ab54:       add     x1, x0, x28
    0.00 :   ffff80001045ab58:       ldr     x0, [x0, x28]
    0.00 :   ffff80001045ab5c:       str     x0, [x29, #104]
    0.00 :   ffff80001045ab60:       ldp     w0, w1, [x1, #8]
    0.00 :   ffff80001045ab64:       sub     w0, w0, w22
    0.00 :   ffff80001045ab68:       add     w1, w22, w1
    0.00 :   ffff80001045ab6c:       cmp     w0, w24
    0.00 :   ffff80001045ab70:       csel    w0, w0, w24, ls  // ls = plast
    0.00 :   ffff80001045ab74:       stp     w0, w1, [x29, #112]
         :                      if (bvprvp && bvec_gap_to_prev(q, bvprvp, bv.bv_offset))
    0.00 :   ffff80001045ab78:       cbz     x3, ffff80001045ad00 <__blk_queue_split+0x308>
         :                      bvec_gap_to_prev():
         :                      * the SG list. Most drivers don't care about this, but some do.
         :                      */
         :                      static inline bool bvec_gap_to_prev(struct request_queue *q,
         :                      struct bio_vec *bprv, unsigned int offset)
         :                      {
         :                      if (!queue_virt_boundary(q))
    0.00 :   ffff80001045ab7c:       ldr     x4, [x26, #1056]
    0.00 :   ffff80001045ab80:       cbz     x4, ffff80001045ad00 <__blk_queue_split+0x308>
         :                      __bvec_gap_to_prev():
         :                      return (offset & queue_virt_boundary(q)) ||
    0.00 :   ffff80001045ab84:       tst     w1, w4
    0.00 :   ffff80001045ab88:       b.eq    ffff80001045acec <__blk_queue_split+0x2f4>  // b.none
         :                      blk_bio_segment_split():
         :                      return bio_split(bio, sectors, GFP_NOIO, bs);
    0.00 :   ffff80001045ab8c:       ldr     w1, [x29, #100]
    0.00 :   ffff80001045ab90:       mov     x3, x18
         :                      *segs = nsegs;
    0.00 :   ffff80001045ab94:       str     w2, [x21]
         :                      return bio_split(bio, sectors, GFP_NOIO, bs);
    0.00 :   ffff80001045ab98:       mov     x0, x25
    0.00 :   ffff80001045ab9c:       mov     w2, #0xc00                      // #3072
    0.00 :   ffff80001045aba0:       bl      ffff8000104507c8 <bio_split>
    0.00 :   ffff80001045aba4:       ldr     x24, [x29, #56]
    0.00 :   ffff80001045aba8:       ldp     x22, x23, [x29, #40]
    0.00 :   ffff80001045abac:       mov     x21, x0
    0.00 :   ffff80001045abb0:       ldp     x27, x28, [x29, #80]
         :                      __blk_queue_split():
         :                      if (split) {
    0.00 :   ffff80001045abb4:       cbz     x21, ffff80001045aa68 <__blk_queue_split+0x70>
         :                      split->bi_opf |= REQ_NOMERGE;
    0.00 :   ffff80001045abb8:       ldr     w1, [x21, #16]
         :                      bio_chain(split, *bio);
    0.00 :   ffff80001045abbc:       mov     x0, x21
         :                      split->bi_opf |= REQ_NOMERGE;
    0.00 :   ffff80001045abc0:       orr     w1, w1, #0x4000
    0.00 :   ffff80001045abc4:       str     w1, [x21, #16]
         :                      bio_set_flag(*bio, BIO_QUEUE_ENTERED);
    0.00 :   ffff80001045abc8:       ldr     x2, [x20]
         :                      bio_set_flag():
         :                      return (bio->bi_flags & (1U << bit)) != 0;
         :                      }
         :
         :                      static inline void bio_set_flag(struct bio *bio, unsigned int bit)
         :                      {
         :                      bio->bi_flags |= (1U << bit);
    0.00 :   ffff80001045abcc:       ldrh    w1, [x2, #20]
    0.00 :   ffff80001045abd0:       orr     w1, w1, #0x800
    0.00 :   ffff80001045abd4:       strh    w1, [x2, #20]
         :                      __blk_queue_split():
         :                      bio_chain(split, *bio);
    0.00 :   ffff80001045abd8:       ldr     x1, [x20]
    0.00 :   ffff80001045abdc:       bl      ffff80001044f360 <bio_chain>
         :                      generic_make_request(*bio);
    0.00 :   ffff80001045abe0:       ldr     x0, [x20]
    0.00 :   ffff80001045abe4:       bl      ffff800010454aa0 <generic_make_request>
         :                      *bio = split;
    0.00 :   ffff80001045abe8:       str     x21, [x20]
         :                      }
    0.00 :   ffff80001045abec:       b       ffff80001045aa68 <__blk_queue_split+0x70>
         :                      blk_bio_discard_split():
         :                      *nsegs = 1;
    0.00 :   ffff80001045abf0:       mov     w2, #0x1                        // #1
    0.00 :   ffff80001045abf4:       str     w2, [x21]
         :                      __blk_queue_split():
         :                      split = blk_bio_discard_split(q, *bio, &q->bio_split, nr_segs);
    0.00 :   ffff80001045abf8:       add     x3, x26, #0x610
         :                      queue_logical_block_size():
    0.00 :   ffff80001045abfc:       mov     w0, #0x7fffff                   // #8388607
         :                      blk_bio_discard_split():
         :                      granularity = max(q->limits.discard_granularity >> 9, 1U);
    0.00 :   ffff80001045ac00:       ldr     w4, [x26, #1120]
         :                      max_discard_sectors = min(q->limits.max_discard_sectors,
    0.00 :   ffff80001045ac04:       ldr     w1, [x26, #1104]
         :                      granularity = max(q->limits.discard_granularity >> 9, 1U);
    0.00 :   ffff80001045ac08:       lsr     w4, w4, #9
    0.00 :   ffff80001045ac0c:       cmp     w4, #0x0
    0.00 :   ffff80001045ac10:       csel    w4, w4, w2, ne  // ne = any
         :                      queue_logical_block_size():
    0.00 :   ffff80001045ac14:       cbz     x26, ffff80001045ac2c <__blk_queue_split+0x234>
    0.00 :   ffff80001045ac18:       ldr     w5, [x26, #1088]
    0.00 :   ffff80001045ac1c:       neg     w2, w5
    0.00 :   ffff80001045ac20:       cmp     w5, #0x0
    0.00 :   ffff80001045ac24:       lsr     w2, w2, #9
    0.00 :   ffff80001045ac28:       csel    w0, w2, w0, ne  // ne = any
         :                      blk_bio_discard_split():
         :                      max_discard_sectors = min(q->limits.max_discard_sectors,
    0.00 :   ffff80001045ac2c:       cmp     w1, w0
    0.00 :   ffff80001045ac30:       csel    w1, w1, w0, ls  // ls = plast
         :                      max_discard_sectors -= max_discard_sectors % granularity;
    0.00 :   ffff80001045ac34:       udiv    w0, w1, w4
    0.00 :   ffff80001045ac38:       msub    w0, w0, w4, w1
         :                      if (unlikely(!max_discard_sectors)) {
    0.00 :   ffff80001045ac3c:       subs    w1, w1, w0
    0.00 :   ffff80001045ac40:       b.eq    ffff80001045aa68 <__blk_queue_split+0x70>  // b.none
         :                      if (bio_sectors(bio) <= max_discard_sectors)
    0.00 :   ffff80001045ac44:       ldr     w0, [x25, #40]
    0.00 :   ffff80001045ac48:       cmp     w1, w0, lsr #9
    0.00 :   ffff80001045ac4c:       b.cs    ffff80001045aa68 <__blk_queue_split+0x70>  // b.hs, b.nlast
         :                      alignment = (q->limits.discard_alignment >> 9) % granularity;
    0.00 :   ffff80001045ac50:       ldr     w6, [x26, #1124]
         :                      tmp = sector_div(tmp, granularity);
    0.00 :   ffff80001045ac54:       mov     w9, w4
         :                      tmp = bio->bi_iter.bi_sector + split_sectors - alignment;
    0.00 :   ffff80001045ac58:       ldr     x5, [x25, #32]
    0.00 :   ffff80001045ac5c:       mov     w8, w1
         :                      return bio_split(bio, split_sectors, GFP_NOIO, bs);
    0.00 :   ffff80001045ac60:       mov     w2, #0xc00                      // #3072
    0.00 :   ffff80001045ac64:       mov     x0, x25
         :                      alignment = (q->limits.discard_alignment >> 9) % granularity;
    0.00 :   ffff80001045ac68:       lsr     w6, w6, #9
    0.00 :   ffff80001045ac6c:       udiv    w7, w6, w4
         :                      tmp = bio->bi_iter.bi_sector + split_sectors - alignment;
    0.00 :   ffff80001045ac70:       msub    w4, w7, w4, w6
    0.00 :   ffff80001045ac74:       sub     x4, x5, x4
    0.00 :   ffff80001045ac78:       add     x5, x4, x8
         :                      tmp = sector_div(tmp, granularity);
    0.00 :   ffff80001045ac7c:       udiv    x4, x5, x9
    0.00 :   ffff80001045ac80:       msub    x4, x4, x9, x5
         :                      split_sectors -= tmp;
    0.00 :   ffff80001045ac84:       cmp     x8, x4
    0.00 :   ffff80001045ac88:       sub     w4, w1, w4
         :                      return bio_split(bio, split_sectors, GFP_NOIO, bs);
    0.00 :   ffff80001045ac8c:       csel    w1, w4, w1, hi  // hi = pmore
    0.00 :   ffff80001045ac90:       bl      ffff8000104507c8 <bio_split>
    0.00 :   ffff80001045ac94:       mov     x21, x0
    0.00 :   ffff80001045ac98:       b       ffff80001045abb4 <__blk_queue_split+0x1bc>
         :                      blk_bio_write_same_split():
         :                      return bio_split(bio, q->limits.max_write_same_sectors, GFP_NOIO, bs);
    0.00 :   ffff80001045ac9c:       add     x3, x26, #0x610
    0.00 :   ffff80001045aca0:       mov     w2, #0xc00                      // #3072
    0.00 :   ffff80001045aca4:       mov     x0, x25
    0.00 :   ffff80001045aca8:       bl      ffff8000104507c8 <bio_split>
    0.00 :   ffff80001045acac:       mov     x21, x0
         :                      __blk_queue_split():
         :                      break;
    0.00 :   ffff80001045acb0:       b       ffff80001045abb4 <__blk_queue_split+0x1bc>
    0.00 :   ffff80001045acb4:       stp     x22, x23, [x29, #40]
         :                      blk_max_size_offset():
         :                      return min(q->limits.max_sectors, (unsigned int)(q->limits.chunk_sectors -
    0.00 :   ffff80001045acb8:       sub     w1, w0, #0x1
    0.00 :   ffff80001045acbc:       str     x24, [x29, #56]
         :                      __blk_queue_split():
         :                      split = blk_bio_segment_split(q, *bio, &q->bio_split, nr_segs);
    0.00 :   ffff80001045acc0:       add     x18, x26, #0x610
    0.00 :   ffff80001045acc4:       stp     x27, x28, [x29, #80]
         :                      blk_bio_segment_split():
         :                      unsigned nsegs = 0, sectors = 0;
    0.00 :   ffff80001045acc8:       stp     wzr, wzr, [x29, #96]
         :                      blk_max_size_offset():
    0.00 :   ffff80001045accc:       ldr     x3, [x25, #32]
    0.00 :   ffff80001045acd0:       ldr     w2, [x26, #1076]
    0.00 :   ffff80001045acd4:       and     w1, w1, w3
    0.00 :   ffff80001045acd8:       mov     w4, w3
    0.00 :   ffff80001045acdc:       sub     w0, w0, w1
    0.00 :   ffff80001045ace0:       cmp     w0, w2
    0.00 :   ffff80001045ace4:       csel    w1, w0, w2, ls  // ls = plast
    0.00 :   ffff80001045ace8:       b       ffff80001045aae8 <__blk_queue_split+0xf0>
         :                      __bvec_gap_to_prev():
         :                      ((bprv->bv_offset + bprv->bv_len) & queue_virt_boundary(q));
    0.00 :   ffff80001045acec:       ldp     w5, w3, [x3, #8]
    0.00 :   ffff80001045acf0:       add     w3, w3, w5
         :                      return (offset & queue_virt_boundary(q)) ||
    0.00 :   ffff80001045acf4:       tst     x3, x4
    0.00 :   ffff80001045acf8:       b.ne    ffff80001045ab8c <__blk_queue_split+0x194>  // b.any
    0.00 :   ffff80001045acfc:       nop
         :                      blk_bio_segment_split():
         :                      if (nsegs < max_segs &&
    0.00 :   ffff80001045ad00:       cmp     w27, w2
    0.00 :   ffff80001045ad04:       b.ls    ffff80001045ad64 <__blk_queue_split+0x36c>  // b.plast
         :                      sectors + (bv.bv_len >> 9) <= max_sectors &&
    0.00 :   ffff80001045ad08:       ldr     w3, [x29, #100]
    0.00 :   ffff80001045ad0c:       add     w3, w3, w0, lsr #9
         :                      if (nsegs < max_segs &&
    0.00 :   ffff80001045ad10:       cmp     w3, w14
    0.00 :   ffff80001045ad14:       b.hi    ffff80001045ad64 <__blk_queue_split+0x36c>  // b.pmore
         :                      bv.bv_offset + bv.bv_len <= PAGE_SIZE) {
    0.00 :   ffff80001045ad18:       add     w1, w1, w0
         :                      sectors + (bv.bv_len >> 9) <= max_sectors &&
    0.00 :   ffff80001045ad1c:       cmp     w1, #0x1, lsl #12
    0.00 :   ffff80001045ad20:       b.hi    ffff80001045ad64 <__blk_queue_split+0x36c>  // b.pmore
         :                      nsegs++;
    0.00 :   ffff80001045ad24:       add     w2, w2, #0x1
         :                      sectors += bv.bv_len >> 9;
    0.00 :   ffff80001045ad28:       stp     w2, w3, [x29, #96]
    0.00 :   ffff80001045ad2c:       nop
         :                      bio_advance_iter():
         :                      if (bio_no_advance_iter(bio))
    0.00 :   ffff80001045ad30:       ldr     w1, [x25, #16]
         :                      blk_bio_segment_split():
         :                      bvprv = bv;
    0.00 :   ffff80001045ad34:       ldp     x2, x3, [x29, #104]
    0.00 :   ffff80001045ad38:       stp     x2, x3, [x29, #120]
         :                      bio_no_advance_iter():
         :                      return bio_op(bio) == REQ_OP_DISCARD ||
    0.00 :   ffff80001045ad3c:       and     w2, w1, #0xff
    0.00 :   ffff80001045ad40:       and     w1, w1, w15
    0.00 :   ffff80001045ad44:       sub     w2, w2, #0x5
         :                      bio_op(bio) == REQ_OP_WRITE_SAME ||
    0.00 :   ffff80001045ad48:       tst     w2, #0xfffffffb
    0.00 :   ffff80001045ad4c:       ccmp    w1, #0x3, #0x4, ne  // ne = any
    0.00 :   ffff80001045ad50:       b.ne    ffff80001045ad90 <__blk_queue_split+0x398>  // b.any
         :                      bio_advance_iter():
         :                      iter->bi_size -= bytes;
    0.00 :   ffff80001045ad54:       sub     w24, w24, w0
    0.00 :   ffff80001045ad58:       add     x3, x29, #0x78
    0.00 :   ffff80001045ad5c:       ldr     w2, [x29, #96]
    0.00 :   ffff80001045ad60:       b       ffff80001045ab48 <__blk_queue_split+0x150>
         :                      blk_bio_segment_split():
         :                      } else if (bvec_split_segs(q, &bv, &nsegs, &sectors, max_segs,
    0.00 :   ffff80001045ad64:       mov     w5, w14
    0.00 :   ffff80001045ad68:       mov     w4, w27
    0.00 :   ffff80001045ad6c:       add     x3, x29, #0x64
    0.00 :   ffff80001045ad70:       add     x2, x29, #0x60
    0.00 :   ffff80001045ad74:       add     x1, x29, #0x68
    0.00 :   ffff80001045ad78:       mov     x0, x26
    0.00 :   ffff80001045ad7c:       bl      ffff80001045a298 <bvec_split_segs>
    0.00 :   ffff80001045ad80:       tst     w0, #0xff
    0.00 :   ffff80001045ad84:       b.ne    ffff80001045ae20 <__blk_queue_split+0x428>  // b.any
    0.00 :   ffff80001045ad88:       ldr     w0, [x29, #112]
    0.00 :   ffff80001045ad8c:       b       ffff80001045ad30 <__blk_queue_split+0x338>
         :                      bvec_iter_advance():
         :                      static inline bool bvec_iter_advance(const struct bio_vec *bv,
         :                      struct bvec_iter *iter, unsigned bytes)
         :                      {
         :                      unsigned int idx = iter->bi_idx;
         :
         :                      if (WARN_ONCE(bytes > iter->bi_size,
    0.00 :   ffff80001045ad90:       cmp     w0, w24
         :                      bio_advance_iter():
         :                      bvec_iter_advance(bio->bi_io_vec, iter, bytes);
    0.00 :   ffff80001045ad94:       ldr     x1, [x25, #104]
         :                      bvec_iter_advance():
    0.00 :   ffff80001045ad98:       b.hi    ffff80001045ade0 <__blk_queue_split+0x3e8>  // b.pmore
         :                      "Attempted to advance past end of bvec iter\n")) {
         :                      iter->bi_size = 0;
         :                      return false;
         :                      }
         :
         :                      iter->bi_size -= bytes;
    0.00 :   ffff80001045ad9c:       sub     w24, w24, w0
         :                      bytes += iter->bi_bvec_done;
         :
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff80001045ada0:       adds    w22, w22, w0
    0.00 :   ffff80001045ada4:       b.eq    ffff80001045ad58 <__blk_queue_split+0x360>  // b.none
    0.00 :   ffff80001045ada8:       add     x28, x1, x28
    0.00 :   ffff80001045adac:       ldr     w0, [x28, #8]
    0.00 :   ffff80001045adb0:       cmp     w0, w22
    0.00 :   ffff80001045adb4:       b.ls    ffff80001045add0 <__blk_queue_split+0x3d8>  // b.plast
    0.00 :   ffff80001045adb8:       b       ffff80001045ad58 <__blk_queue_split+0x360>
    0.00 :   ffff80001045adbc:       ubfiz   x0, x23, #4, #32
    0.00 :   ffff80001045adc0:       add     x0, x1, x0
    0.00 :   ffff80001045adc4:       ldr     w0, [x0, #8]
    0.00 :   ffff80001045adc8:       cmp     w0, w22
    0.00 :   ffff80001045adcc:       b.hi    ffff80001045ad58 <__blk_queue_split+0x360>  // b.pmore
         :                      bytes -= bv[idx].bv_len;
         :                      idx++;
    0.00 :   ffff80001045add0:       add     w23, w23, #0x1
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff80001045add4:       subs    w22, w22, w0
    0.00 :   ffff80001045add8:       b.ne    ffff80001045adbc <__blk_queue_split+0x3c4>  // b.any
    0.00 :   ffff80001045addc:       b       ffff80001045ad58 <__blk_queue_split+0x360>
         :                      if (WARN_ONCE(bytes > iter->bi_size,
    0.00 :   ffff80001045ade0:       adrp    x1, ffff800011a5d000 <__print_once.28906>
    0.00 :   ffff80001045ade4:       ldrb    w0, [x1, #75]
    0.00 :   ffff80001045ade8:       cbnz    w0, ffff80001045ae08 <__blk_queue_split+0x410>
    0.00 :   ffff80001045adec:       mov     w2, #0x1                        // #1
    0.00 :   ffff80001045adf0:       adrp    x0, ffff8000111aa000 <kallsyms_token_index+0x30330>
    0.00 :   ffff80001045adf4:       strb    w2, [x1, #75]
    0.00 :   ffff80001045adf8:       add     x0, x0, #0xea8
    0.00 :   ffff80001045adfc:       bl      ffff8000100e5288 <__warn_printk>
    0.00 :   ffff80001045ae00:       brk     #0x800
    0.00 :   ffff80001045ae04:       nop
    0.00 :   ffff80001045ae08:       ldr     w2, [x29, #96]
         :                      blk_bio_segment_split():
         :                      *segs = nsegs;
    0.00 :   ffff80001045ae0c:       str     w2, [x21]
    0.00 :   ffff80001045ae10:       ldp     x22, x23, [x29, #40]
    0.00 :   ffff80001045ae14:       ldr     x24, [x29, #56]
    0.00 :   ffff80001045ae18:       ldp     x27, x28, [x29, #80]
    0.00 :   ffff80001045ae1c:       b       ffff80001045aa68 <__blk_queue_split+0x70>
    0.00 :   ffff80001045ae20:       ldr     w2, [x29, #96]
    0.00 :   ffff80001045ae24:       b       ffff80001045ab8c <__blk_queue_split+0x194>
         :                      __blk_queue_split():
         :                      ((*bio)->bi_io_vec[0].bv_len +
    3.91 :   ffff80001045ae28:       ldr     x2, [x25, #104]
    5.10 :   ffff80001045ae2c:       ldp     w1, w2, [x2, #8]
    0.00 :   ffff80001045ae30:       add     w1, w1, w2
         :                      (*bio)->bi_vcnt == 1 &&
    0.00 :   ffff80001045ae34:       cmp     w1, #0x1, lsl #12
    0.00 :   ffff80001045ae38:       b.hi    ffff80001045aacc <__blk_queue_split+0xd4>  // b.pmore
         :                      *nr_segs = 1;
    5.09 :   ffff80001045ae3c:       str     w0, [x21]
    0.17 :   ffff80001045ae40:       b       ffff80001045aa68 <__blk_queue_split+0x70>
    0.00 :   ffff80001045ae44:       stp     x22, x23, [x29, #40]
    0.00 :   ffff80001045ae48:       str     x24, [x29, #56]
    0.00 :   ffff80001045ae4c:       stp     x27, x28, [x29, #80]
         :                      }
    0.00 :   ffff80001045ae50:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (507 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104324e0 <security_file_permission>:
         :                      security_file_permission():
         :                      {
         :                      return call_int_hook(kernfs_init_security, 0, kn_dir, kn);
         :                      }
         :
         :                      int security_file_permission(struct file *file, int mask)
         :                      {
    0.00 :   ffff8000104324e0:       stp     x29, x30, [sp, #-64]!
         :                      int ret;
         :
         :                      ret = call_int_hook(file_permission, 0, file, mask);
    0.00 :   ffff8000104324e4:       adrp    x2, ffff8000112af000 <security_hook_heads+0x1e8>
         :                      {
    0.00 :   ffff8000104324e8:       mov     x29, sp
   13.86 :   ffff8000104324ec:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000104324f0:       mov     x20, x0
         :                      ret = call_int_hook(file_permission, 0, file, mask);
    2.37 :   ffff8000104324f4:       ldr     x19, [x2, #56]
         :                      {
   41.97 :   ffff8000104324f8:       str     x21, [sp, #32]
    0.00 :   ffff8000104324fc:       mov     w21, w1
         :                      ret = call_int_hook(file_permission, 0, file, mask);
    0.00 :   ffff800010432500:       cbz     x19, ffff800010432524 <security_file_permission+0x44>
    0.00 :   ffff800010432504:       nop
    0.00 :   ffff800010432508:       ldr     x2, [x19, #24]
    0.00 :   ffff80001043250c:       mov     w1, w21
    0.00 :   ffff800010432510:       mov     x0, x20
    0.00 :   ffff800010432514:       blr     x2
    0.00 :   ffff800010432518:       cbnz    w0, ffff800010432580 <security_file_permission+0xa0>
    0.00 :   ffff80001043251c:       ldr     x19, [x19]
    0.00 :   ffff800010432520:       cbnz    x19, ffff800010432508 <security_file_permission+0x28>
         :                      fsnotify_perm():
         :                      int ret;
         :                      const struct path *path = &file->f_path;
         :                      struct inode *inode = file_inode(file);
         :                      __u32 fsnotify_mask = 0;
         :
         :                      if (file->f_mode & FMODE_NONOTIFY)
    0.00 :   ffff800010432524:       ldr     w1, [x20, #68]
         :                      return 0;
         :                      if (!(mask & (MAY_READ | MAY_OPEN)))
         :                      return 0;
    0.00 :   ffff800010432528:       mov     w0, #0x0                        // #0
         :                      if (file->f_mode & FMODE_NONOTIFY)
    0.00 :   ffff80001043252c:       tbnz    w1, #26, ffff800010432580 <security_file_permission+0xa0>
         :                      if (!(mask & (MAY_READ | MAY_OPEN)))
    2.17 :   ffff800010432530:       mov     w0, #0x24                       // #36
    0.00 :   ffff800010432534:       ands    w0, w21, w0
    0.00 :   ffff800010432538:       b.eq    ffff800010432580 <security_file_permission+0xa0>  // b.none
    0.00 :   ffff80001043253c:       stp     x22, x23, [x29, #40]
         :                      const struct path *path = &file->f_path;
    0.00 :   ffff800010432540:       add     x22, x20, #0x10
         :                      struct inode *inode = file_inode(file);
    0.00 :   ffff800010432544:       ldp     x1, x23, [x20, #24]
         :                      if (mask & MAY_OPEN) {
    0.00 :   ffff800010432548:       tbz     w21, #5, ffff800010432590 <security_file_permission+0xb0>
         :                      fsnotify_mask = FS_OPEN_PERM;
         :
         :                      if (file->f_flags & __FMODE_EXEC) {
    0.00 :   ffff80001043254c:       ldr     w0, [x20, #64]
         :                      fsnotify_mask = FS_OPEN_PERM;
    0.00 :   ffff800010432550:       mov     w19, #0x10000                   // #65536
         :                      if (file->f_flags & __FMODE_EXEC) {
    0.00 :   ffff800010432554:       tbnz    w0, #5, ffff8000104325c0 <security_file_permission+0xe0>
         :                      }
         :                      } else if (mask & MAY_READ) {
         :                      fsnotify_mask = FS_ACCESS_PERM;
         :                      }
         :
         :                      if (S_ISDIR(inode->i_mode))
    0.20 :   ffff800010432558:       ldrh    w2, [x23]
         :                      fsnotify_mask |= FS_ISDIR;
    0.00 :   ffff80001043255c:       orr     w3, w19, #0x40000000
         :                      fsnotify_parent():
         :                      return __fsnotify_parent(path, dentry, mask);
    0.79 :   ffff800010432560:       mov     x0, x22
         :                      fsnotify_perm():
         :                      if (S_ISDIR(inode->i_mode))
    0.00 :   ffff800010432564:       and     w2, w2, #0xf000
         :                      fsnotify_mask |= FS_ISDIR;
   14.37 :   ffff800010432568:       cmp     w2, #0x4, lsl #12
    0.00 :   ffff80001043256c:       csel    w19, w3, w19, eq  // eq = none
         :                      fsnotify_parent():
         :                      return __fsnotify_parent(path, dentry, mask);
    0.00 :   ffff800010432570:       mov     w2, w19
    0.00 :   ffff800010432574:       bl      ffff8000102ca820 <__fsnotify_parent>
         :                      fsnotify_path():
         :                      if (ret)
    0.98 :   ffff800010432578:       cbz     w0, ffff80001043259c <security_file_permission+0xbc>
    0.00 :   ffff80001043257c:       ldp     x22, x23, [x29, #40]
         :                      security_file_permission():
         :                      if (ret)
         :                      return ret;
         :
         :                      return fsnotify_perm(file, mask);
         :                      }
   10.24 :   ffff800010432580:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010432584:       ldr     x21, [sp, #32]
    0.00 :   ffff800010432588:       ldp     x29, x30, [sp], #64
    0.00 :   ffff80001043258c:       ret
         :                      fsnotify_perm():
         :                      __u32 fsnotify_mask = 0;
    7.13 :   ffff800010432590:       ubfx    x19, x21, #2, #1
    0.00 :   ffff800010432594:       lsl     w19, w19, #17
    0.00 :   ffff800010432598:       b       ffff800010432558 <security_file_permission+0x78>
         :                      fsnotify_path():
         :                      return fsnotify(inode, mask, path, FSNOTIFY_EVENT_PATH, NULL, 0);
    2.16 :   ffff80001043259c:       mov     x2, x22
    0.00 :   ffff8000104325a0:       mov     x0, x23
    0.00 :   ffff8000104325a4:       mov     w5, #0x0                        // #0
    0.00 :   ffff8000104325a8:       mov     x4, #0x0                        // #0
    1.58 :   ffff8000104325ac:       mov     w3, #0x1                        // #1
    0.00 :   ffff8000104325b0:       mov     w1, w19
    0.00 :   ffff8000104325b4:       bl      ffff8000102ca4e0 <fsnotify>
    2.19 :   ffff8000104325b8:       ldp     x22, x23, [x29, #40]
    0.00 :   ffff8000104325bc:       b       ffff800010432580 <security_file_permission+0xa0>
         :                      fsnotify_parent():
         :                      return __fsnotify_parent(path, dentry, mask);
    0.00 :   ffff8000104325c0:       mov     w2, #0x40000                    // #262144
    0.00 :   ffff8000104325c4:       mov     x0, x22
    0.00 :   ffff8000104325c8:       bl      ffff8000102ca820 <__fsnotify_parent>
         :                      fsnotify_path():
         :                      if (ret)
    0.00 :   ffff8000104325cc:       cbnz    w0, ffff80001043257c <security_file_permission+0x9c>
         :                      return fsnotify(inode, mask, path, FSNOTIFY_EVENT_PATH, NULL, 0);
    0.00 :   ffff8000104325d0:       mov     w5, #0x0                        // #0
    0.00 :   ffff8000104325d4:       mov     x4, #0x0                        // #0
    0.00 :   ffff8000104325d8:       mov     w3, #0x1                        // #1
    0.00 :   ffff8000104325dc:       mov     x2, x22
    0.00 :   ffff8000104325e0:       mov     w1, #0x40000                    // #262144
    0.00 :   ffff8000104325e4:       mov     x0, x23
    0.00 :   ffff8000104325e8:       bl      ffff8000102ca4e0 <fsnotify>
         :                      fsnotify_perm():
         :                      if (ret)
    0.00 :   ffff8000104325ec:       cbnz    w0, ffff80001043257c <security_file_permission+0x9c>
    0.00 :   ffff8000104325f0:       ldr     x1, [x20, #24]
    0.00 :   ffff8000104325f4:       b       ffff800010432558 <security_file_permission+0x78>
 Percent |	Source code & Disassembly of vmlinux for cycles (511 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010451a00 <bio_set_pages_dirty>:
         :                      bio_set_pages_dirty():
         :
         :                      /*
         :                      * bio_set_pages_dirty() will mark all the bio's pages as dirty.
         :                      */
         :                      void bio_set_pages_dirty(struct bio *bio)
         :                      {
    9.40 :   ffff800010451a00:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff800010451a04:       mov     x29, sp
   36.20 :   ffff800010451a08:       stp     x19, x20, [sp, #16]
         :                      bvec_init_iter_all():
         :                      .bi_bvec_done   = 0,                                            \
         :                      }
         :
         :                      static inline struct bio_vec *bvec_init_iter_all(struct bvec_iter_all *iter_all)
         :                      {
         :                      iter_all->done = 0;
    0.00 :   ffff800010451a0c:       mov     w20, #0x0                       // #0
         :                      bio_set_pages_dirty():
    2.15 :   ffff800010451a10:       stp     x21, x22, [sp, #32]
    0.00 :   ffff800010451a14:       mov     x22, x0
    2.16 :   ffff800010451a18:       str     x23, [sp, #48]
         :                      bvec_init_iter_all():
         :                      iter_all->idx = 0;
    0.00 :   ffff800010451a1c:       mov     w21, #0x0                       // #0
    0.00 :   ffff800010451a20:       mov     w23, #0x1000                    // #4096
    2.36 :   ffff800010451a24:       ldrh    w4, [x0, #96]
         :                      bio_next_segment():
         :                      struct bvec_iter_all *iter)
         :                      {
         :                      if (iter->idx >= bio->bi_vcnt)
         :                      return false;
         :
         :                      bvec_advance(&bio->bi_io_vec[iter->idx], iter);
    0.00 :   ffff800010451a28:       sbfiz   x0, x21, #4, #32
         :                      if (iter->idx >= bio->bi_vcnt)
    0.00 :   ffff800010451a2c:       cmp     w4, w21
    0.00 :   ffff800010451a30:       b.le    ffff800010451aac <bio_set_pages_dirty+0xac>
         :                      bvec_advance(&bio->bi_io_vec[iter->idx], iter);
    0.58 :   ffff800010451a34:       ldr     x3, [x22, #104]
         :                      bvec_advance():
         :                      struct bvec_iter_all *iter_all)
         :                      {
         :                      struct bio_vec *bv = &iter_all->bv;
         :
         :                      if (iter_all->done) {
         :                      bv->bv_page++;
    0.00 :   ffff800010451a38:       add     x19, x19, #0x40
    0.00 :   ffff800010451a3c:       mov     w1, #0x1000                     // #4096
         :                      bio_next_segment():
    2.34 :   ffff800010451a40:       add     x2, x3, x0
         :                      bvec_advance():
         :                      if (iter_all->done) {
    0.00 :   ffff800010451a44:       cbnz    w20, ffff800010451a60 <bio_set_pages_dirty+0x60>
         :                      bv->bv_offset = 0;
         :                      } else {
         :                      bv->bv_page = bvec->bv_page + (bvec->bv_offset >> PAGE_SHIFT);
    0.00 :   ffff800010451a48:       ldr     x19, [x3, x0]
   13.49 :   ffff800010451a4c:       ldr     w0, [x2, #12]
         :                      bv->bv_offset = bvec->bv_offset & ~PAGE_MASK;
    0.00 :   ffff800010451a50:       and     w1, w0, #0xfff
         :                      bv->bv_page = bvec->bv_page + (bvec->bv_offset >> PAGE_SHIFT);
    0.00 :   ffff800010451a54:       lsr     w0, w0, #12
    0.00 :   ffff800010451a58:       sub     w1, w23, w1
    0.98 :   ffff800010451a5c:       add     x19, x19, x0, lsl #6
         :                      }
         :                      bv->bv_len = min_t(unsigned int, PAGE_SIZE - bv->bv_offset,
    0.00 :   ffff800010451a60:       ldr     w2, [x2, #8]
    0.00 :   ffff800010451a64:       sub     w0, w2, w20
    0.00 :   ffff800010451a68:       cmp     w0, w1
    0.00 :   ffff800010451a6c:       csel    w0, w0, w1, ls  // ls = plast
         :                      bvec->bv_len - iter_all->done);
         :                      iter_all->done += bv->bv_len;
    3.53 :   ffff800010451a70:       add     w20, w20, w0
         :
         :                      if (iter_all->done == bvec->bv_len) {
    0.00 :   ffff800010451a74:       cmp     w2, w20
    0.00 :   ffff800010451a78:       b.ne    ffff800010451a84 <bio_set_pages_dirty+0x84>  // b.any
         :                      iter_all->idx++;
    0.00 :   ffff800010451a7c:       add     w21, w21, #0x1
         :                      iter_all->done = 0;
    8.40 :   ffff800010451a80:       mov     w20, #0x0                       // #0
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    3.11 :   ffff800010451a84:       ldr     x0, [x19]
         :                      PageCompound():
         :                      return READ_ONCE(page->compound_head) & 1;
         :                      }
         :
         :                      static __always_inline int PageCompound(struct page *page)
         :                      {
         :                      return test_bit(PG_head, &page->flags) || PageTail(page);
    0.00 :   ffff800010451a88:       tbnz    w0, #16, ffff800010451a28 <bio_set_pages_dirty+0x28>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
   10.65 :   ffff800010451a8c:       ldr     x0, [x19, #8]
         :                      PageCompound():
    0.00 :   ffff800010451a90:       tbnz    w0, #0, ffff800010451a28 <bio_set_pages_dirty+0x28>
         :                      bio_set_pages_dirty():
         :                      struct bio_vec *bvec;
         :                      struct bvec_iter_all iter_all;
         :
         :                      bio_for_each_segment_all(bvec, bio, iter_all) {
         :                      if (!PageCompound(bvec->bv_page))
         :                      set_page_dirty_lock(bvec->bv_page);
    2.72 :   ffff800010451a94:       mov     x0, x19
    0.00 :   ffff800010451a98:       bl      ffff8000101db068 <set_page_dirty_lock>
    0.00 :   ffff800010451a9c:       ldrh    w4, [x22, #96]
         :                      bio_next_segment():
    0.00 :   ffff800010451aa0:       sbfiz   x0, x21, #4, #32
         :                      if (iter->idx >= bio->bi_vcnt)
    0.00 :   ffff800010451aa4:       cmp     w4, w21
    0.00 :   ffff800010451aa8:       b.gt    ffff800010451a34 <bio_set_pages_dirty+0x34>
         :                      bio_set_pages_dirty():
         :                      }
         :                      }
    0.96 :   ffff800010451aac:       ldp     x19, x20, [sp, #16]
    0.97 :   ffff800010451ab0:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010451ab4:       ldr     x23, [sp, #48]
    0.00 :   ffff800010451ab8:       ldp     x29, x30, [sp], #64
    0.00 :   ffff800010451abc:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (527 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101d3170 <generic_file_read_iter>:
         :                      generic_file_read_iter():
         :                      * * number of bytes copied, even for partial reads
         :                      * * negative error code if nothing was read
         :                      */
         :                      ssize_t
         :                      generic_file_read_iter(struct kiocb *iocb, struct iov_iter *iter)
         :                      {
    6.63 :   ffff8000101d3170:       stp     x29, x30, [sp, #-240]!
    0.00 :   ffff8000101d3174:       mov     x29, sp
    8.16 :   ffff8000101d3178:       stp     x27, x28, [sp, #80]
    0.00 :   ffff8000101d317c:       mov     x28, x1
    0.19 :   ffff8000101d3180:       str     x19, [sp, #16]
    0.00 :   ffff8000101d3184:       adrp    x1, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000101d3188:       add     x1, x1, #0x8c8
    0.00 :   ffff8000101d318c:       mov     x27, x0
         :                      size_t count = iov_iter_count(iter);
    0.00 :   ffff8000101d3190:       ldr     x19, [x28, #16]
         :                      {
    2.86 :   ffff8000101d3194:       ldr     x0, [x1]
    0.38 :   ffff8000101d3198:       str     x0, [x29, #232]
    0.00 :   ffff8000101d319c:       mov     x0, #0x0                        // #0
         :                      ssize_t retval = 0;
         :
         :                      if (!count)
    0.00 :   ffff8000101d31a0:       cbz     x19, ffff8000101d34f8 <generic_file_read_iter+0x388>
    0.19 :   ffff8000101d31a4:       str     x20, [x29, #24]
    0.38 :   ffff8000101d31a8:       str     x26, [x29, #72]
    1.31 :   ffff8000101d31ac:       ldp     x0, x1, [x27]
    0.00 :   ffff8000101d31b0:       mov     x2, x0
         :                      goto out; /* skip atime */
         :
         :                      if (iocb->ki_flags & IOCB_DIRECT) {
    0.19 :   ffff8000101d31b4:       ldr     w0, [x27, #32]
    0.57 :   ffff8000101d31b8:       str     x2, [x29, #160]
    0.38 :   ffff8000101d31bc:       ldr     x26, [x2, #240]
    4.75 :   ffff8000101d31c0:       ldr     x3, [x26]
   14.22 :   ffff8000101d31c4:       str     x3, [x29, #144]
    0.00 :   ffff8000101d31c8:       tbz     w0, #2, ffff8000101d354c <generic_file_read_iter+0x3dc>
    0.00 :   ffff8000101d31cc:       add     x2, x1, x19
         :                      struct file *file = iocb->ki_filp;
         :                      struct address_space *mapping = file->f_mapping;
         :                      struct inode *inode = mapping->host;
         :                      loff_t size;
         :
         :                      size = i_size_read(inode);
    0.00 :   ffff8000101d31d0:       ldr     x20, [x3, #80]
    0.00 :   ffff8000101d31d4:       sub     x2, x2, #0x1
         :                      if (iocb->ki_flags & IOCB_NOWAIT) {
    0.00 :   ffff8000101d31d8:       tbnz    w0, #7, ffff8000101d3528 <generic_file_read_iter+0x3b8>
         :                      if (filemap_range_has_page(mapping, iocb->ki_pos,
         :                      iocb->ki_pos + count - 1))
         :                      return -EAGAIN;
         :                      } else {
         :                      retval = filemap_write_and_wait_range(mapping,
   12.15 :   ffff8000101d31dc:       mov     x0, x26
    0.00 :   ffff8000101d31e0:       bl      ffff8000101d30d0 <filemap_write_and_wait_range>
    3.02 :   ffff8000101d31e4:       sxtw    x0, w0
    2.28 :   ffff8000101d31e8:       str     x0, [x29, #168]
         :                      iocb->ki_pos,
         :                      iocb->ki_pos + count - 1);
         :                      if (retval < 0)
    0.00 :   ffff8000101d31ec:       tbnz    x0, #63, ffff8000101d396c <generic_file_read_iter+0x7fc>
         :                      file_accessed():
         :
         :                      extern bool atime_needs_update(const struct path *, struct inode *);
         :                      extern void touch_atime(const struct path *);
         :                      static inline void file_accessed(struct file *file)
         :                      {
         :                      if (!(file->f_flags & O_NOATIME))
    4.78 :   ffff8000101d31f0:       ldr     x0, [x29, #160]
    0.56 :   ffff8000101d31f4:       ldr     w0, [x0, #64]
    0.00 :   ffff8000101d31f8:       tbz     w0, #18, ffff8000101d3a20 <generic_file_read_iter+0x8b0>
         :                      generic_file_read_iter():
         :                      goto out;
         :                      }
         :
         :                      file_accessed(file);
         :
         :                      retval = mapping->a_ops->direct_IO(iocb, iter);
    6.84 :   ffff8000101d31fc:       ldr     x2, [x26, #112]
    0.57 :   ffff8000101d3200:       mov     x1, x28
    0.00 :   ffff8000101d3204:       mov     x0, x27
    2.30 :   ffff8000101d3208:       ldr     x2, [x2, #88]
    0.00 :   ffff8000101d320c:       blr     x2
    0.00 :   ffff8000101d3210:       str     x0, [x29, #168]
    0.00 :   ffff8000101d3214:       mov     x2, x0
         :                      if (retval >= 0) {
    0.00 :   ffff8000101d3218:       tbnz    x0, #63, ffff8000101d3a04 <generic_file_read_iter+0x894>
         :                      iocb->ki_pos += retval;
    0.00 :   ffff8000101d321c:       ldr     x1, [x27, #8]
         :                      count -= retval;
    0.00 :   ffff8000101d3220:       sub     x19, x19, x0
         :                      }
         :                      iov_iter_revert(iter, count - iov_iter_count(iter));
    0.00 :   ffff8000101d3224:       mov     x0, x28
         :                      iocb->ki_pos += retval;
    0.00 :   ffff8000101d3228:       add     x1, x1, x2
    0.00 :   ffff8000101d322c:       str     x1, [x27, #8]
         :                      iov_iter_revert(iter, count - iov_iter_count(iter));
    0.00 :   ffff8000101d3230:       ldr     x1, [x28, #16]
    0.00 :   ffff8000101d3234:       sub     x1, x19, x1
    0.00 :   ffff8000101d3238:       bl      ffff800010482ae0 <iov_iter_revert>
         :                      * there was a short read because we hit EOF, go ahead
         :                      * and return.  Otherwise fallthrough to buffered io for
         :                      * the rest of the read.  Buffered reads will not work for
         :                      * DAX files, so don't bother trying.
         :                      */
         :                      if (retval < 0 || !count || iocb->ki_pos >= size ||
    0.00 :   ffff8000101d323c:       cbz     x19, ffff8000101d396c <generic_file_read_iter+0x7fc>
    0.00 :   ffff8000101d3240:       ldr     x1, [x27, #8]
    0.00 :   ffff8000101d3244:       cmp     x1, x20
    0.00 :   ffff8000101d3248:       b.ge    ffff8000101d396c <generic_file_read_iter+0x7fc>  // b.tcont
    0.00 :   ffff8000101d324c:       ldr     x0, [x27]
    0.00 :   ffff8000101d3250:       str     x0, [x29, #160]
    0.00 :   ffff8000101d3254:       ldr     x26, [x0, #240]
    0.00 :   ffff8000101d3258:       ldr     x0, [x26]
    0.00 :   ffff8000101d325c:       str     x0, [x29, #144]
         :                      generic_file_buffered_read():
         :                      if (unlikely(*ppos >= inode->i_sb->s_maxbytes))
    0.00 :   ffff8000101d3260:       ldr     x0, [x29, #144]
         :                      struct file_ra_state *ra = &filp->f_ra;
    0.00 :   ffff8000101d3264:       ldr     x2, [x29, #160]
         :                      if (unlikely(*ppos >= inode->i_sb->s_maxbytes))
    0.00 :   ffff8000101d3268:       ldr     x0, [x0, #40]
         :                      struct file_ra_state *ra = &filp->f_ra;
    0.00 :   ffff8000101d326c:       add     x2, x2, #0x98
    0.00 :   ffff8000101d3270:       str     x2, [x29, #136]
         :                      if (unlikely(*ppos >= inode->i_sb->s_maxbytes))
    0.00 :   ffff8000101d3274:       ldr     x2, [x0, #32]
    0.00 :   ffff8000101d3278:       cmp     x2, x1
    0.00 :   ffff8000101d327c:       b.le    ffff8000101d34f0 <generic_file_read_iter+0x380>
    0.00 :   ffff8000101d3280:       stp     x21, x22, [x29, #32]
    0.00 :   ffff8000101d3284:       stp     x23, x24, [x29, #48]
    0.00 :   ffff8000101d3288:       str     x25, [x29, #64]
         :                      iov_iter_truncate():
         :                      * count doesn't have to fit in size_t - comparison extends both
         :                      * operands to u64 here and any value that would be truncated by
         :                      * conversion in assignement is by definition greater than all
         :                      * values of size_t, including old i->count.
         :                      */
         :                      if (i->count > count)
    0.00 :   ffff8000101d328c:       ldr     x0, [x28, #16]
    0.00 :   ffff8000101d3290:       cmp     x2, x0
    0.00 :   ffff8000101d3294:       b.cs    ffff8000101d32a4 <generic_file_read_iter+0x134>  // b.hs, b.nlast
         :                      i->count = count;
    0.00 :   ffff8000101d3298:       str     x2, [x28, #16]
    0.00 :   ffff8000101d329c:       mov     x0, x2
    0.00 :   ffff8000101d32a0:       ldr     x1, [x27, #8]
         :                      generic_file_buffered_read():
         :                      prev_index = ra->prev_pos >> PAGE_SHIFT;
    0.00 :   ffff8000101d32a4:       ldr     x2, [x29, #136]
         :                      last_index = (*ppos + iter->count + PAGE_SIZE-1) >> PAGE_SHIFT;
    0.00 :   ffff8000101d32a8:       add     x0, x0, #0xfff
    0.00 :   ffff8000101d32ac:       add     x0, x0, x1
         :                      index = *ppos >> PAGE_SHIFT;
    0.00 :   ffff8000101d32b0:       asr     x3, x1, #12
         :                      INIT_LIST_HEAD():
         :                      #define LIST_HEAD(name) \
         :                      struct list_head name = LIST_HEAD_INIT(name)
         :
         :                      static inline void INIT_LIST_HEAD(struct list_head *list)
         :                      {
         :                      WRITE_ONCE(list->next, list);
    0.00 :   ffff8000101d32b4:       add     x21, x29, #0xd8
         :                      generic_file_buffered_read():
    0.00 :   ffff8000101d32b8:       str     x3, [x29, #152]
         :                      prev_index = ra->prev_pos >> PAGE_SHIFT;
    0.00 :   ffff8000101d32bc:       ldr     x2, [x2, #24]
         :                      last_index = (*ppos + iter->count + PAGE_SIZE-1) >> PAGE_SHIFT;
    0.00 :   ffff8000101d32c0:       lsr     x0, x0, #12
    0.00 :   ffff8000101d32c4:       str     x0, [x29, #112]
         :                      offset = *ppos & ~PAGE_MASK;
    0.00 :   ffff8000101d32c8:       and     x0, x1, #0xfff
         :                      prev_offset = ra->prev_pos & (PAGE_SIZE-1);
    0.00 :   ffff8000101d32cc:       and     w3, w2, #0xfff
    0.00 :   ffff8000101d32d0:       str     w3, [x29, #120]
         :                      prev_index = ra->prev_pos >> PAGE_SHIFT;
    0.00 :   ffff8000101d32d4:       asr     x2, x2, #12
    0.00 :   ffff8000101d32d8:       str     x2, [x29, #104]
         :                      offset = *ppos & ~PAGE_MASK;
    0.00 :   ffff8000101d32dc:       str     x0, [x29, #128]
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff8000101d32e0:       mrs     x0, sp_el0
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff8000101d32e4:       ldr     x1, [x0]
         :                      fatal_signal_pending():
         :                      return unlikely(sigismember(&p->pending.signal, SIGKILL));
         :                      }
         :
         :                      static inline int fatal_signal_pending(struct task_struct *p)
         :                      {
         :                      return signal_pending(p) && __fatal_signal_pending(p);
    0.00 :   ffff8000101d32e8:       tbnz    w1, #0, ffff8000101d3a30 <generic_file_read_iter+0x8c0>
         :                      find_get_page():
         :                      * Otherwise, %NULL is returned.
         :                      */
         :                      static inline struct page *find_get_page(struct address_space *mapping,
         :                      pgoff_t offset)
         :                      {
         :                      return pagecache_get_page(mapping, offset, 0, 0);
    0.00 :   ffff8000101d32ec:       ldr     x1, [x29, #152]
    0.00 :   ffff8000101d32f0:       mov     w3, #0x0                        // #0
    0.00 :   ffff8000101d32f4:       mov     w2, #0x0                        // #0
    0.00 :   ffff8000101d32f8:       mov     x0, x26
    0.00 :   ffff8000101d32fc:       bl      ffff8000101d1688 <pagecache_get_page>
    0.00 :   ffff8000101d3300:       mov     x22, x0
         :                      generic_file_buffered_read():
         :                      if (!page) {
    0.00 :   ffff8000101d3304:       cbz     x0, ffff8000101d3844 <generic_file_read_iter+0x6d4>
         :                      test_bit():
    0.00 :   ffff8000101d3308:       ldr     x0, [x22]
         :                      generic_file_buffered_read():
         :                      if (PageReadahead(page)) {
    0.00 :   ffff8000101d330c:       tbnz    w0, #18, ffff8000101d3820 <generic_file_read_iter+0x6b0>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000101d3310:       ldr     x0, [x22, #8]
         :                      compound_head():
         :                      static inline struct page *compound_head(struct page *page)
         :                      {
         :                      unsigned long head = READ_ONCE(page->compound_head);
         :
         :                      if (unlikely(head & 1))
         :                      return (struct page *) (head - 1);
    0.00 :   ffff8000101d3314:       tst     x0, #0x1
    0.00 :   ffff8000101d3318:       sub     x0, x0, #0x1
    0.00 :   ffff8000101d331c:       csel    x0, x0, x22, ne  // ne = any
         :                      test_bit():
    0.00 :   ffff8000101d3320:       ldr     x0, [x0]
         :                      PageUptodate():
         :                      * We can skip the barrier if the page is not uptodate, because
         :                      * we wouldn't be reading anything from it.
         :                      *
         :                      * See SetPageUptodate() for the other side of the story.
         :                      */
         :                      if (ret)
    0.00 :   ffff8000101d3324:       tbnz    w0, #2, ffff8000101d355c <generic_file_read_iter+0x3ec>
         :                      generic_file_buffered_read():
         :                      if (iocb->ki_flags & IOCB_NOWAIT) {
    0.00 :   ffff8000101d3328:       ldr     w0, [x27, #32]
    0.00 :   ffff8000101d332c:       tbnz    w0, #7, ffff8000101d3a50 <generic_file_read_iter+0x8e0>
         :                      __read_once_size():
    0.00 :   ffff8000101d3330:       ldr     x0, [x22, #8]
         :                      compound_head():
         :                      return (struct page *) (head - 1);
    0.00 :   ffff8000101d3334:       tst     x0, #0x1
    0.00 :   ffff8000101d3338:       sub     x0, x0, #0x1
    0.00 :   ffff8000101d333c:       csel    x0, x0, x22, ne  // ne = any
         :                      test_bit():
    0.00 :   ffff8000101d3340:       ldr     x0, [x0]
         :                      wait_on_page_locked_killable():
         :                      wait_on_page_bit(compound_head(page), PG_locked);
         :                      }
         :
         :                      static inline int wait_on_page_locked_killable(struct page *page)
         :                      {
         :                      if (!PageLocked(page))
    0.00 :   ffff8000101d3344:       tbz     w0, #0, ffff8000101d3684 <generic_file_read_iter+0x514>
         :                      __read_once_size():
    0.00 :   ffff8000101d3348:       ldr     x19, [x22, #8]
         :                      page_waitqueue():
         :                      return &page_wait_table[hash_ptr(page, PAGE_WAIT_TABLE_BITS)];
    0.00 :   ffff8000101d334c:       adrp    x24, ffff800011897000 <bit_wait_table+0xe80>
    0.00 :   ffff8000101d3350:       add     x20, x24, #0xb00
         :                      compound_head():
    0.00 :   ffff8000101d3354:       tst     x19, #0x1
    0.00 :   ffff8000101d3358:       sub     x19, x19, #0x1
    0.00 :   ffff8000101d335c:       csel    x19, x19, x22, ne  // ne = any
         :                      __read_once_size():
    0.00 :   ffff8000101d3360:       ldr     x0, [x19, #8]
         :                      compound_head():
    0.00 :   ffff8000101d3364:       tst     x0, #0x1
    0.00 :   ffff8000101d3368:       sub     x0, x0, #0x1
    0.00 :   ffff8000101d336c:       csel    x0, x0, x19, ne  // ne = any
         :                      test_bit():
    0.00 :   ffff8000101d3370:       ldr     x1, [x0]
         :                      hash_64_generic():
         :                      #endif
         :                      static __always_inline u32 hash_64_generic(u64 val, unsigned int bits)
         :                      {
         :                      #if BITS_PER_LONG == 64
         :                      /* 64x64-bit multiply is efficient on all 64-bit processors */
         :                      return val * GOLDEN_RATIO_64 >> (64 - bits);
    0.00 :   ffff8000101d3374:       mov     x0, #0x83eb                     // #33771
    0.00 :   ffff8000101d3378:       movk    x0, #0x80b5, lsl #16
    0.00 :   ffff8000101d337c:       movk    x0, #0x8646, lsl #32
    0.00 :   ffff8000101d3380:       movk    x0, #0x61c8, lsl #48
    0.00 :   ffff8000101d3384:       mul     x23, x19, x0
    0.00 :   ffff8000101d3388:       lsr     x23, x23, #56
    0.00 :   ffff8000101d338c:       add     x23, x23, x23, lsl #1
    0.00 :   ffff8000101d3390:       lsl     x23, x23, #3
         :                      page_waitqueue():
    0.00 :   ffff8000101d3394:       add     x20, x20, x23
         :                      PageUptodate():
         :                      if (ret)
    0.00 :   ffff8000101d3398:       tbz     w1, #2, ffff8000101d38f8 <generic_file_read_iter+0x788>
         :                      smp_rmb();
    0.00 :   ffff8000101d339c:       dmb     ishld
    0.00 :   ffff8000101d33a0:       str     wzr, [x29, #124]
         :                      __add_wait_queue_entry_tail():
         :                      __add_wait_queue(wq_head, wq_entry);
         :                      }
         :
         :                      static inline void __add_wait_queue_entry_tail(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
         :                      {
         :                      list_add_tail(&wq_entry->entry, &wq_head->head);
    0.00 :   ffff8000101d33a4:       add     x0, x24, #0xb00
    0.00 :   ffff8000101d33a8:       add     x23, x23, #0x8
    0.00 :   ffff8000101d33ac:       add     x23, x23, x0
         :                      __lse_atomic64_or():
         :                      : [i] "+r" (i), [v] "+Q" (v->counter)                           \
         :                      : "r" (v));                                                     \
         :                      }
         :
         :                      ATOMIC64_OP(andnot, stclr)
         :                      ATOMIC64_OP(or, stset)
    0.00 :   ffff8000101d33b0:       mov     x25, #0x80                      // #128
         :                      get_current():
    0.00 :   ffff8000101d33b4:       mrs     x0, sp_el0
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff8000101d33b8:       mov     x24, #0x102                     // #258
         :                      wait_on_page_bit_common():
         :                      wait_page.page = page;
    0.00 :   ffff8000101d33bc:       str     x19, [x29, #176]
         :                      wait_page.bit_nr = bit_nr;
    0.00 :   ffff8000101d33c0:       str     wzr, [x29, #184]
         :                      wait->flags = behavior == EXCLUSIVE ? WQ_FLAG_EXCLUSIVE : 0;
    0.00 :   ffff8000101d33c4:       str     wzr, [x29, #192]
         :                      init_wait(wait);
    0.00 :   ffff8000101d33c8:       str     x0, [x29, #200]
         :                      wait->func = wake_page_function;
    0.00 :   ffff8000101d33cc:       adrp    x0, ffff8000101ce000 <__rseq_handle_notify_resume+0x438>
         :                      __write_once_size():
    0.00 :   ffff8000101d33d0:       str     x21, [x29, #216]
         :                      wait_on_page_bit_common():
    0.00 :   ffff8000101d33d4:       add     x0, x0, #0x838
    0.00 :   ffff8000101d33d8:       str     x0, [x29, #208]
         :                      INIT_LIST_HEAD():
         :                      list->prev = list;
    0.00 :   ffff8000101d33dc:       str     x21, [x29, #224]
         :                      spin_lock_irq():
         :                      raw_spin_lock_nest_lock(spinlock_check(lock), nest_lock);       \
         :                      } while (0)
         :
         :                      static __always_inline void spin_lock_irq(spinlock_t *lock)
         :                      {
         :                      raw_spin_lock_irq(&lock->rlock);
    0.00 :   ffff8000101d33e0:       mov     x0, x20
    0.00 :   ffff8000101d33e4:       bl      ffff800010cb2cd0 <_raw_spin_lock_irq>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000101d33e8:       ldr     x0, [x29, #216]
         :                      wait_on_page_bit_common():
         :                      if (likely(list_empty(&wait->entry))) {
    0.00 :   ffff8000101d33ec:       cmp     x21, x0
    0.00 :   ffff8000101d33f0:       b.ne    ffff8000101d3414 <generic_file_read_iter+0x2a4>  // b.any
         :                      list_add_tail():
         :                      * Insert a new entry before the specified head.
         :                      * This is useful for implementing queues.
         :                      */
         :                      static inline void list_add_tail(struct list_head *new, struct list_head *head)
         :                      {
         :                      __list_add(new, head->prev, head);
    0.00 :   ffff8000101d33f4:       ldr     x0, [x20, #16]
         :                      __list_add():
         :                      new->prev = prev;
    0.00 :   ffff8000101d33f8:       stp     x23, x0, [x29, #216]
         :                      next->prev = new;
    0.00 :   ffff8000101d33fc:       str     x21, [x20, #16]
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff8000101d3400:       str     x21, [x0]
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000101d3404:       b       ffff8000101d3554 <generic_file_read_iter+0x3e4>
    0.00 :   ffff8000101d3408:       b       ffff8000101d3554 <generic_file_read_iter+0x3e4>
         :                      __lse_atomic64_or():
    0.00 :   ffff8000101d340c:       mov     x0, x25
    0.00 :   ffff8000101d3410:       stset   x0, [x19]
         :                      get_current():
    0.00 :   ffff8000101d3414:       mrs     x0, sp_el0
         :                      __write_once_size():
    0.00 :   ffff8000101d3418:       str     x24, [x0, #24]
         :                      wait_on_page_bit_common():
         :                      set_current_state(state);
    0.00 :   ffff8000101d341c:       dmb     ish
         :                      spin_unlock_irq():
         :                      raw_spin_unlock_bh(&lock->rlock);
         :                      }
         :
         :                      static __always_inline void spin_unlock_irq(spinlock_t *lock)
         :                      {
         :                      raw_spin_unlock_irq(&lock->rlock);
    0.00 :   ffff8000101d3420:       mov     x0, x20
    0.00 :   ffff8000101d3424:       bl      ffff800010cb2bf0 <_raw_spin_unlock_irq>
         :                      test_bit():
    0.00 :   ffff8000101d3428:       ldr     x0, [x19]
         :                      wait_on_page_bit_common():
         :                      if (likely(bit_is_set))
    0.00 :   ffff8000101d342c:       tbz     w0, #0, ffff8000101d3434 <generic_file_read_iter+0x2c4>
         :                      io_schedule();
    0.00 :   ffff8000101d3430:       bl      ffff800010cad7f0 <io_schedule>
         :                      test_bit():
    0.00 :   ffff8000101d3434:       ldr     x0, [x19]
         :                      wait_on_page_bit_common():
         :                      if (!test_bit(bit_nr, &page->flags))
    0.00 :   ffff8000101d3438:       tbz     w0, #0, ffff8000101d366c <generic_file_read_iter+0x4fc>
         :                      get_current():
    0.00 :   ffff8000101d343c:       mrs     x0, sp_el0
         :                      test_bit():
    0.00 :   ffff8000101d3440:       ldr     x1, [x0]
         :                      signal_pending_state():
         :
         :                      static inline int signal_pending_state(long state, struct task_struct *p)
         :                      {
         :                      if (!(state & (TASK_INTERRUPTIBLE | TASK_WAKEKILL)))
         :                      return 0;
         :                      if (!signal_pending(p))
    0.00 :   ffff8000101d3444:       tbz     w1, #0, ffff8000101d33e0 <generic_file_read_iter+0x270>
         :                      return 0;
         :
         :                      return (state & TASK_INTERRUPTIBLE) || __fatal_signal_pending(p);
    0.00 :   ffff8000101d3448:       ldr     x0, [x0, #1696]
    0.00 :   ffff8000101d344c:       tbz     w0, #8, ffff8000101d33e0 <generic_file_read_iter+0x270>
         :                      wait_on_page_bit_common():
         :                      finish_wait(q, wait);
    0.00 :   ffff8000101d3450:       mov     x0, x20
    0.00 :   ffff8000101d3454:       add     x1, x29, #0xc0
    0.00 :   ffff8000101d3458:       bl      ffff80001012e670 <finish_wait>
         :                      ret = -EINTR;
    0.00 :   ffff8000101d345c:       mov     w19, #0xfffffffc                // #-4
         :                      if (delayacct)
    0.00 :   ffff8000101d3460:       ldr     w0, [x29, #124]
    0.00 :   ffff8000101d3464:       cbnz    w0, ffff8000101d3c6c <generic_file_read_iter+0xafc>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000101d3468:       ldr     x0, [x22, #8]
         :                      compound_head():
         :                      return (struct page *) (head - 1);
    0.00 :   ffff8000101d346c:       sub     x1, x0, #0x1
    0.00 :   ffff8000101d3470:       tst     x0, #0x1
    0.00 :   ffff8000101d3474:       csel    x22, x1, x22, ne  // ne = any
         :                      page_ref_dec_and_test():
         :                      return ret;
         :                      }
         :
         :                      static inline int page_ref_dec_and_test(struct page *page)
         :                      {
         :                      int ret = atomic_dec_and_test(&page->_refcount);
    0.00 :   ffff8000101d3478:       add     x1, x22, #0x34
         :                      arch_static_branch_jump():
    0.00 :   ffff8000101d347c:       b       ffff8000101d3978 <generic_file_read_iter+0x808>
    0.00 :   ffff8000101d3480:       b       ffff8000101d3978 <generic_file_read_iter+0x808>
         :                      __lse_atomic_sub_return():
         :                      ATOMIC_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff8000101d3484:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000101d3488:       neg     w0, w0
    0.00 :   ffff8000101d348c:       ldaddal w0, w2, [x1]
    0.00 :   ffff8000101d3490:       add     w0, w0, w2
         :                      put_page():
         :                      * include/linux/memremap.h and HMM for details.
         :                      */
         :                      if (put_devmap_managed_page(page))
         :                      return;
         :
         :                      if (put_page_testzero(page))
    0.00 :   ffff8000101d3494:       cbz     w0, ffff8000101d3a84 <generic_file_read_iter+0x914>
         :                      generic_file_buffered_read():
         :                      *ppos = ((loff_t)index << PAGE_SHIFT) + offset;
    0.00 :   ffff8000101d3498:       ldr     x2, [x29, #128]
    0.00 :   ffff8000101d349c:       ldr     x1, [x29, #152]
         :                      ra->prev_pos |= prev_offset;
    0.00 :   ffff8000101d34a0:       ldr     w0, [x29, #120]
         :                      *ppos = ((loff_t)index << PAGE_SHIFT) + offset;
    0.00 :   ffff8000101d34a4:       add     x1, x2, x1, lsl #12
         :                      ra->prev_pos |= prev_offset;
    0.00 :   ffff8000101d34a8:       ldr     x2, [x29, #104]
    0.00 :   ffff8000101d34ac:       orr     x0, x0, x2, lsl #12
    0.00 :   ffff8000101d34b0:       ldr     x2, [x29, #136]
    0.00 :   ffff8000101d34b4:       str     x0, [x2, #24]
         :                      file_accessed():
    0.00 :   ffff8000101d34b8:       ldr     x0, [x29, #160]
         :                      generic_file_buffered_read():
         :                      *ppos = ((loff_t)index << PAGE_SHIFT) + offset;
    0.00 :   ffff8000101d34bc:       str     x1, [x27, #8]
         :                      file_accessed():
    0.00 :   ffff8000101d34c0:       ldr     w0, [x0, #64]
    0.00 :   ffff8000101d34c4:       tbz     w0, #18, ffff8000101d3a40 <generic_file_read_iter+0x8d0>
         :                      generic_file_buffered_read():
         :                      return written ? written : error;
    0.00 :   ffff8000101d34c8:       ldr     x0, [x29, #168]
    0.00 :   ffff8000101d34cc:       sxtw    x19, w19
    0.00 :   ffff8000101d34d0:       ldp     x20, x21, [x29, #24]
    0.00 :   ffff8000101d34d4:       cmp     x0, #0x0
    0.00 :   ffff8000101d34d8:       csel    x0, x19, x0, eq  // eq = none
    0.00 :   ffff8000101d34dc:       ldr     x26, [x29, #72]
    0.00 :   ffff8000101d34e0:       ldp     x22, x23, [x29, #40]
    0.00 :   ffff8000101d34e4:       str     x0, [x29, #168]
    0.00 :   ffff8000101d34e8:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff8000101d34ec:       b       ffff8000101d34fc <generic_file_read_iter+0x38c>
    0.00 :   ffff8000101d34f0:       ldr     x20, [x29, #24]
    0.00 :   ffff8000101d34f4:       ldr     x26, [x29, #72]
         :                      generic_file_read_iter():
         :                      ssize_t retval = 0;
    0.00 :   ffff8000101d34f8:       str     xzr, [x29, #168]
         :                      }
         :
         :                      retval = generic_file_buffered_read(iocb, iter, retval);
         :                      out:
         :                      return retval;
         :                      }
    2.46 :   ffff8000101d34fc:       adrp    x0, ffff800011899000 <page_wait_table+0x1500>
    1.50 :   ffff8000101d3500:       add     x1, x0, #0x8c8
    0.19 :   ffff8000101d3504:       ldr     x2, [x29, #232]
    6.25 :   ffff8000101d3508:       ldr     x1, [x1]
    0.00 :   ffff8000101d350c:       eor     x1, x2, x1
    1.52 :   ffff8000101d3510:       ldr     x0, [x29, #168]
    0.00 :   ffff8000101d3514:       cbnz    x1, ffff8000101d3c50 <generic_file_read_iter+0xae0>
    0.38 :   ffff8000101d3518:       ldr     x19, [sp, #16]
    0.95 :   ffff8000101d351c:       ldp     x27, x28, [sp, #80]
    0.57 :   ffff8000101d3520:       ldp     x29, x30, [sp], #240
    0.00 :   ffff8000101d3524:       ret
         :                      if (filemap_range_has_page(mapping, iocb->ki_pos,
    0.00 :   ffff8000101d3528:       mov     x0, x26
    0.00 :   ffff8000101d352c:       bl      ffff8000101ce750 <filemap_range_has_page>
    0.00 :   ffff8000101d3530:       tst     w0, #0xff
    0.00 :   ffff8000101d3534:       b.eq    ffff8000101d31f0 <generic_file_read_iter+0x80>  // b.none
         :                      return -EAGAIN;
    0.00 :   ffff8000101d3538:       mov     x0, #0xfffffffffffffff5         // #-11
    0.00 :   ffff8000101d353c:       ldr     x20, [x29, #24]
    0.00 :   ffff8000101d3540:       ldr     x26, [x29, #72]
    0.00 :   ffff8000101d3544:       str     x0, [x29, #168]
    0.00 :   ffff8000101d3548:       b       ffff8000101d34fc <generic_file_read_iter+0x38c>
         :                      ssize_t retval = 0;
    0.00 :   ffff8000101d354c:       str     xzr, [x29, #168]
    0.00 :   ffff8000101d3550:       b       ffff8000101d3260 <generic_file_read_iter+0xf0>
         :                      __ll_sc_atomic64_or():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(and, and, L)
         :                      ATOMIC64_OPS(or, orr, L)
    0.00 :   ffff8000101d3554:       b       ffff8000101d5824 <generic_file_write_iter+0x7ec>
    0.00 :   ffff8000101d3558:       b       ffff8000101d3414 <generic_file_read_iter+0x2a4>
         :                      PageUptodate():
         :                      smp_rmb();
    0.00 :   ffff8000101d355c:       dmb     ishld
         :                      generic_file_buffered_read():
         :                      isize = i_size_read(inode);
    0.00 :   ffff8000101d3560:       ldr     x0, [x29, #144]
    0.00 :   ffff8000101d3564:       ldr     x0, [x0, #80]
         :                      end_index = (isize - 1) >> PAGE_SHIFT;
    0.00 :   ffff8000101d3568:       sub     x2, x0, #0x1
         :                      if (unlikely(!isize || index > end_index)) {
    0.00 :   ffff8000101d356c:       cmp     x0, #0x0
    0.00 :   ffff8000101d3570:       ldr     x0, [x29, #152]
         :                      end_index = (isize - 1) >> PAGE_SHIFT;
    0.00 :   ffff8000101d3574:       asr     x1, x2, #12
         :                      if (unlikely(!isize || index > end_index)) {
    0.00 :   ffff8000101d3578:       ccmp    x0, x1, #0x2, ne  // ne = any
    0.00 :   ffff8000101d357c:       b.hi    ffff8000101d3bec <generic_file_read_iter+0xa7c>  // b.pmore
         :                      if (index == end_index) {
    0.00 :   ffff8000101d3580:       ldr     x0, [x29, #152]
         :                      nr = PAGE_SIZE;
    0.00 :   ffff8000101d3584:       mov     x19, #0x1000                    // #4096
         :                      if (index == end_index) {
    0.00 :   ffff8000101d3588:       cmp     x0, x1
    0.00 :   ffff8000101d358c:       b.eq    ffff8000101d3b50 <generic_file_read_iter+0x9e0>  // b.none
         :                      __read_once_size():
    0.00 :   ffff8000101d3590:       ldr     w0, [x26, #28]
         :                      generic_file_buffered_read():
         :                      nr = nr - offset;
    0.00 :   ffff8000101d3594:       ldr     x1, [x29, #128]
         :                      if (mapping_writably_mapped(mapping))
    0.00 :   ffff8000101d3598:       cmp     w0, #0x0
         :                      nr = nr - offset;
    0.00 :   ffff8000101d359c:       sub     x19, x19, x1
         :                      if (mapping_writably_mapped(mapping))
    0.00 :   ffff8000101d35a0:       b.gt    ffff8000101d3b44 <generic_file_read_iter+0x9d4>
         :                      if (prev_index != index || offset != prev_offset)
    0.00 :   ffff8000101d35a4:       ldr     x1, [x29, #104]
    0.00 :   ffff8000101d35a8:       ldr     x0, [x29, #152]
    0.00 :   ffff8000101d35ac:       cmp     x0, x1
    0.00 :   ffff8000101d35b0:       b.ne    ffff8000101d35c4 <generic_file_read_iter+0x454>  // b.any
    0.00 :   ffff8000101d35b4:       ldr     w0, [x29, #120]
    0.00 :   ffff8000101d35b8:       ldr     x1, [x29, #128]
    0.00 :   ffff8000101d35bc:       cmp     x1, x0
    0.00 :   ffff8000101d35c0:       b.eq    ffff8000101d35cc <generic_file_read_iter+0x45c>  // b.none
         :                      mark_page_accessed(page);
    0.00 :   ffff8000101d35c4:       mov     x0, x22
    0.00 :   ffff8000101d35c8:       bl      ffff8000101e00c0 <mark_page_accessed>
         :                      ret = copy_page_to_iter(page, offset, nr, iter);
    0.00 :   ffff8000101d35cc:       ldr     x20, [x29, #128]
    0.00 :   ffff8000101d35d0:       mov     x0, x22
    0.00 :   ffff8000101d35d4:       mov     x3, x28
    0.00 :   ffff8000101d35d8:       mov     x2, x19
    0.00 :   ffff8000101d35dc:       mov     x1, x20
    0.00 :   ffff8000101d35e0:       bl      ffff800010484f70 <copy_page_to_iter>
         :                      __read_once_size():
    0.00 :   ffff8000101d35e4:       ldr     x1, [x22, #8]
         :                      generic_file_buffered_read():
    0.00 :   ffff8000101d35e8:       mov     x23, x0
         :                      index += offset >> PAGE_SHIFT;
    0.00 :   ffff8000101d35ec:       ldr     x3, [x29, #152]
         :                      offset += ret;
    0.00 :   ffff8000101d35f0:       add     x0, x20, x23
         :                      offset &= ~PAGE_MASK;
    0.00 :   ffff8000101d35f4:       and     x2, x0, #0xfff
         :                      prev_offset = offset;
    0.00 :   ffff8000101d35f8:       str     w2, [x29, #120]
         :                      compound_head():
         :                      return (struct page *) (head - 1);
    0.00 :   ffff8000101d35fc:       tst     x1, #0x1
    0.00 :   ffff8000101d3600:       sub     x1, x1, #0x1
    0.00 :   ffff8000101d3604:       csel    x22, x1, x22, ne  // ne = any
         :                      generic_file_buffered_read():
         :                      index += offset >> PAGE_SHIFT;
    0.00 :   ffff8000101d3608:       add     x20, x3, x0, lsr #12
         :                      offset &= ~PAGE_MASK;
    0.00 :   ffff8000101d360c:       str     x2, [x29, #128]
         :                      page_ref_dec_and_test():
    0.00 :   ffff8000101d3610:       add     x0, x22, #0x34
         :                      arch_static_branch_jump():
    0.00 :   ffff8000101d3614:       b       ffff8000101d3b9c <generic_file_read_iter+0xa2c>
    0.00 :   ffff8000101d3618:       b       ffff8000101d3b9c <generic_file_read_iter+0xa2c>
         :                      __lse_atomic_sub_return():
    0.00 :   ffff8000101d361c:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000101d3620:       neg     w1, w1
    0.00 :   ffff8000101d3624:       ldaddal w1, w2, [x0]
    0.00 :   ffff8000101d3628:       add     w1, w1, w2
    0.00 :   ffff8000101d362c:       mov     w0, w1
         :                      put_page():
    0.00 :   ffff8000101d3630:       cbnz    w0, ffff8000101d363c <generic_file_read_iter+0x4cc>
         :                      __put_page(page);
    0.00 :   ffff8000101d3634:       mov     x0, x22
    0.00 :   ffff8000101d3638:       bl      ffff8000101de470 <__put_page>
         :                      generic_file_buffered_read():
         :                      written += ret;
    0.00 :   ffff8000101d363c:       ldr     x1, [x29, #168]
         :                      if (!iov_iter_count(iter))
    0.00 :   ffff8000101d3640:       ldr     x0, [x28, #16]
         :                      written += ret;
    0.00 :   ffff8000101d3644:       add     x1, x1, x23
    0.00 :   ffff8000101d3648:       str     x1, [x29, #168]
         :                      if (!iov_iter_count(iter))
    0.00 :   ffff8000101d364c:       cbz     x0, ffff8000101d3c3c <generic_file_read_iter+0xacc>
         :                      if (ret < nr) {
    0.00 :   ffff8000101d3650:       ldr     x0, [x29, #152]
    0.00 :   ffff8000101d3654:       cmp     x19, x23
    0.00 :   ffff8000101d3658:       str     x0, [x29, #104]
    0.00 :   ffff8000101d365c:       b.ls    ffff8000101d3bac <generic_file_read_iter+0xa3c>  // b.plast
         :                      error = -EFAULT;
    0.00 :   ffff8000101d3660:       mov     w19, #0xfffffff2                // #-14
         :                      index += offset >> PAGE_SHIFT;
    0.00 :   ffff8000101d3664:       str     x20, [x29, #152]
    0.00 :   ffff8000101d3668:       b       ffff8000101d3498 <generic_file_read_iter+0x328>
         :                      wait_on_page_bit_common():
         :                      finish_wait(q, wait);
    0.00 :   ffff8000101d366c:       mov     x0, x20
    0.00 :   ffff8000101d3670:       add     x1, x29, #0xc0
    0.00 :   ffff8000101d3674:       bl      ffff80001012e670 <finish_wait>
         :                      if (delayacct)
    0.00 :   ffff8000101d3678:       ldr     w0, [x29, #124]
    0.00 :   ffff8000101d367c:       cbz     w0, ffff8000101d3684 <generic_file_read_iter+0x514>
    0.00 :   ffff8000101d3680:       bl      ffff8000101cebc0 <wait_on_page_bit_common.part.44>
         :                      __read_once_size():
    0.00 :   ffff8000101d3684:       ldr     x0, [x22, #8]
         :                      compound_head():
    0.00 :   ffff8000101d3688:       tst     x0, #0x1
    0.00 :   ffff8000101d368c:       sub     x0, x0, #0x1
    0.00 :   ffff8000101d3690:       csel    x0, x0, x22, ne  // ne = any
         :                      test_bit():
    0.00 :   ffff8000101d3694:       ldr     x0, [x0]
         :                      PageUptodate():
         :                      if (ret)
    0.00 :   ffff8000101d3698:       tbnz    w0, #2, ffff8000101d355c <generic_file_read_iter+0x3ec>
         :                      generic_file_buffered_read():
         :                      if (inode->i_blkbits == PAGE_SHIFT ||
    0.00 :   ffff8000101d369c:       ldr     x0, [x29, #144]
    0.00 :   ffff8000101d36a0:       ldrb    w0, [x0, #142]
    0.00 :   ffff8000101d36a4:       cmp     w0, #0xc
    0.00 :   ffff8000101d36a8:       b.eq    ffff8000101d36e0 <generic_file_read_iter+0x570>  // b.none
         :                      !mapping->a_ops->is_partially_uptodate)
    0.00 :   ffff8000101d36ac:       ldr     x0, [x26, #112]
         :                      if (inode->i_blkbits == PAGE_SHIFT ||
    0.00 :   ffff8000101d36b0:       ldr     x0, [x0, #128]
    0.00 :   ffff8000101d36b4:       cbz     x0, ffff8000101d36e0 <generic_file_read_iter+0x570>
         :                      iov_iter_type():
         :                      return i->type & ~(READ | WRITE);
    0.00 :   ffff8000101d36b8:       ldr     w0, [x28]
    0.00 :   ffff8000101d36bc:       and     w0, w0, #0xfffffffe
         :                      generic_file_buffered_read():
         :                      if (unlikely(iov_iter_is_pipe(iter)))
    0.00 :   ffff8000101d36c0:       cmp     w0, #0x20
    0.00 :   ffff8000101d36c4:       b.eq    ffff8000101d36e0 <generic_file_read_iter+0x570>  // b.none
         :                      __read_once_size():
    0.00 :   ffff8000101d36c8:       ldr     x0, [x22, #8]
         :                      compound_head():
         :                      return (struct page *) (head - 1);
    0.00 :   ffff8000101d36cc:       tst     x0, #0x1
    0.00 :   ffff8000101d36d0:       sub     x0, x0, #0x1
    0.00 :   ffff8000101d36d4:       csel    x0, x0, x22, ne  // ne = any
         :                      __read_once_size():
    0.00 :   ffff8000101d36d8:       ldr     x1, [x0]
         :                      test_and_set_bit_lock():
         :                      {
         :                      long old;
         :                      unsigned long mask = BIT_MASK(nr);
         :
         :                      p += BIT_WORD(nr);
         :                      if (READ_ONCE(*p) & mask)
    0.00 :   ffff8000101d36dc:       tbz     w1, #0, ffff8000101d39b0 <generic_file_read_iter+0x840>
         :                      __read_once_size():
    0.00 :   ffff8000101d36e0:       ldr     x0, [x22, #8]
         :                      compound_head():
    0.00 :   ffff8000101d36e4:       tst     x0, #0x1
    0.00 :   ffff8000101d36e8:       sub     x0, x0, #0x1
    0.00 :   ffff8000101d36ec:       csel    x0, x0, x22, ne  // ne = any
         :                      __read_once_size():
    0.00 :   ffff8000101d36f0:       ldr     x1, [x0]
         :                      test_and_set_bit_lock():
    0.00 :   ffff8000101d36f4:       tbz     w1, #0, ffff8000101d3984 <generic_file_read_iter+0x814>
         :                      lock_page_killable():
         :                      return __lock_page_killable(page);
    0.00 :   ffff8000101d36f8:       mov     x0, x22
    0.00 :   ffff8000101d36fc:       bl      ffff8000101cfbe8 <__lock_page_killable>
    0.00 :   ffff8000101d3700:       mov     w19, w0
         :                      generic_file_buffered_read():
         :                      if (unlikely(error))
    0.00 :   ffff8000101d3704:       cbnz    w0, ffff8000101d3468 <generic_file_read_iter+0x2f8>
         :                      if (!page->mapping) {
    0.00 :   ffff8000101d3708:       ldr     x0, [x22, #24]
    0.00 :   ffff8000101d370c:       cbz     x0, ffff8000101d3a90 <generic_file_read_iter+0x920>
         :                      __read_once_size():
    0.00 :   ffff8000101d3710:       ldr     x0, [x22, #8]
         :                      compound_head():
    0.00 :   ffff8000101d3714:       tst     x0, #0x1
    0.00 :   ffff8000101d3718:       sub     x0, x0, #0x1
    0.00 :   ffff8000101d371c:       csel    x0, x0, x22, ne  // ne = any
         :                      test_bit():
    0.00 :   ffff8000101d3720:       ldr     x0, [x0]
         :                      PageUptodate():
         :                      if (ret)
    0.00 :   ffff8000101d3724:       tbz     w0, #2, ffff8000101d3738 <generic_file_read_iter+0x5c8>
         :                      smp_rmb();
    0.00 :   ffff8000101d3728:       dmb     ishld
         :                      generic_file_buffered_read():
         :                      unlock_page(page);
    0.00 :   ffff8000101d372c:       mov     x0, x22
    0.00 :   ffff8000101d3730:       bl      ffff8000101cfaf8 <unlock_page>
    0.00 :   ffff8000101d3734:       b       ffff8000101d3560 <generic_file_read_iter+0x3f0>
         :                      ClearPageError():
         :                      PAGEFLAG(Error, error, PF_NO_COMPOUND) TESTCLEARFLAG(Error, error, PF_NO_COMPOUND)
    0.00 :   ffff8000101d3738:       mov     x20, x22
         :                      arch_static_branch_jump():
    0.00 :   ffff8000101d373c:       b       ffff8000101d3808 <generic_file_read_iter+0x698>
    0.00 :   ffff8000101d3740:       b       ffff8000101d3808 <generic_file_read_iter+0x698>
         :                      __lse_atomic64_andnot():
         :                      ATOMIC64_OP(andnot, stclr)
    0.00 :   ffff8000101d3744:       mov     x0, #0x100                      // #256
    0.00 :   ffff8000101d3748:       stclr   x0, [x22]
         :                      generic_file_buffered_read():
         :                      error = mapping->a_ops->readpage(filp, page);
    0.00 :   ffff8000101d374c:       ldr     x0, [x26, #112]
    0.00 :   ffff8000101d3750:       mov     x1, x22
    0.00 :   ffff8000101d3754:       ldr     x2, [x0, #8]
    0.00 :   ffff8000101d3758:       ldr     x0, [x29, #160]
    0.00 :   ffff8000101d375c:       blr     x2
    0.00 :   ffff8000101d3760:       mov     w19, w0
         :                      if (unlikely(error)) {
    0.00 :   ffff8000101d3764:       cbnz    w0, ffff8000101d3ae4 <generic_file_read_iter+0x974>
         :                      __read_once_size():
    0.00 :   ffff8000101d3768:       ldr     x0, [x22, #8]
         :                      compound_head():
         :                      return (struct page *) (head - 1);
    0.00 :   ffff8000101d376c:       tst     x0, #0x1
    0.00 :   ffff8000101d3770:       sub     x0, x0, #0x1
    0.00 :   ffff8000101d3774:       csel    x0, x0, x22, ne  // ne = any
         :                      test_bit():
    0.00 :   ffff8000101d3778:       ldr     x0, [x0]
         :                      PageUptodate():
         :                      if (ret)
    0.00 :   ffff8000101d377c:       tbnz    w0, #2, ffff8000101d355c <generic_file_read_iter+0x3ec>
         :                      __read_once_size():
    0.00 :   ffff8000101d3780:       ldr     x0, [x22, #8]
         :                      compound_head():
         :                      if (unlikely(head & 1))
    0.00 :   ffff8000101d3784:       tbnz    w0, #0, ffff8000101d3b38 <generic_file_read_iter+0x9c8>
    0.00 :   ffff8000101d3788:       mov     x0, x22
         :                      __read_once_size():
    0.00 :   ffff8000101d378c:       ldr     x0, [x0]
         :                      test_and_set_bit_lock():
    0.00 :   ffff8000101d3790:       tbz     w0, #0, ffff8000101d3948 <generic_file_read_iter+0x7d8>
         :                      lock_page_killable():
    0.00 :   ffff8000101d3794:       mov     x0, x22
    0.00 :   ffff8000101d3798:       bl      ffff8000101cfbe8 <__lock_page_killable>
    0.00 :   ffff8000101d379c:       mov     w19, w0
         :                      generic_file_buffered_read():
         :                      if (unlikely(error))
    0.00 :   ffff8000101d37a0:       cbnz    w0, ffff8000101d3468 <generic_file_read_iter+0x2f8>
         :                      __read_once_size():
    0.00 :   ffff8000101d37a4:       ldr     x0, [x22, #8]
         :                      compound_head():
         :                      return (struct page *) (head - 1);
    0.00 :   ffff8000101d37a8:       tst     x0, #0x1
    0.00 :   ffff8000101d37ac:       sub     x0, x0, #0x1
    0.00 :   ffff8000101d37b0:       csel    x0, x0, x22, ne  // ne = any
         :                      test_bit():
    0.00 :   ffff8000101d37b4:       ldr     x0, [x0]
         :                      PageUptodate():
         :                      if (ret)
    0.00 :   ffff8000101d37b8:       tbnz    w0, #2, ffff8000101d3728 <generic_file_read_iter+0x5b8>
         :                      generic_file_buffered_read():
         :                      if (page->mapping == NULL) {
    0.00 :   ffff8000101d37bc:       ldr     x0, [x22, #24]
    0.00 :   ffff8000101d37c0:       cbnz    x0, ffff8000101d3bc0 <generic_file_read_iter+0xa50>
         :                      unlock_page(page);
    0.00 :   ffff8000101d37c4:       mov     x0, x22
    0.00 :   ffff8000101d37c8:       bl      ffff8000101cfaf8 <unlock_page>
         :                      __read_once_size():
    0.00 :   ffff8000101d37cc:       ldr     x0, [x22, #8]
         :                      compound_head():
         :                      return (struct page *) (head - 1);
    0.00 :   ffff8000101d37d0:       tst     x0, #0x1
    0.00 :   ffff8000101d37d4:       sub     x0, x0, #0x1
    0.00 :   ffff8000101d37d8:       csel    x22, x0, x22, ne  // ne = any
         :                      page_ref_dec_and_test():
    0.00 :   ffff8000101d37dc:       add     x0, x22, #0x34
         :                      arch_static_branch_jump():
    0.00 :   ffff8000101d37e0:       b       ffff8000101d3814 <generic_file_read_iter+0x6a4>
    0.00 :   ffff8000101d37e4:       b       ffff8000101d3814 <generic_file_read_iter+0x6a4>
         :                      __lse_atomic_sub_return():
         :                      ATOMIC_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff8000101d37e8:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000101d37ec:       neg     w1, w1
    0.00 :   ffff8000101d37f0:       ldaddal w1, w2, [x0]
    0.00 :   ffff8000101d37f4:       add     w1, w1, w2
         :                      put_page():
         :                      if (put_page_testzero(page))
    0.00 :   ffff8000101d37f8:       cbnz    w1, ffff8000101d32e0 <generic_file_read_iter+0x170>
         :                      __put_page(page);
    0.00 :   ffff8000101d37fc:       mov     x0, x22
    0.00 :   ffff8000101d3800:       bl      ffff8000101de470 <__put_page>
    0.00 :   ffff8000101d3804:       b       ffff8000101d32e0 <generic_file_read_iter+0x170>
         :                      __ll_sc_atomic64_andnot():
         :                      /*
         :                      * GAS converts the mysterious and undocumented BIC (immediate) alias to
         :                      * an AND (immediate) instruction with the immediate inverted. We don't
         :                      * have a constraint for this, so fall back to register.
         :                      */
         :                      ATOMIC64_OPS(andnot, bic, )
    0.00 :   ffff8000101d3808:       mov     x2, #0x100                      // #256
    0.00 :   ffff8000101d380c:       b       ffff8000101d583c <generic_file_write_iter+0x804>
    0.00 :   ffff8000101d3810:       b       ffff8000101d374c <generic_file_read_iter+0x5dc>
         :                      __ll_sc_atomic_sub_return():
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff8000101d3814:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000101d3818:       b       ffff8000101d5854 <generic_file_write_iter+0x81c>
    0.00 :   ffff8000101d381c:       b       ffff8000101d37f8 <generic_file_read_iter+0x688>
         :                      generic_file_buffered_read():
         :                      page_cache_async_readahead(mapping,
    0.00 :   ffff8000101d3820:       ldp     x0, x2, [x29, #152]
    0.00 :   ffff8000101d3824:       mov     x3, x22
    0.00 :   ffff8000101d3828:       ldr     x1, [x29, #112]
    0.00 :   ffff8000101d382c:       mov     x4, x0
    0.00 :   ffff8000101d3830:       sub     x5, x1, x0
    0.00 :   ffff8000101d3834:       ldr     x1, [x29, #136]
    0.00 :   ffff8000101d3838:       mov     x0, x26
    0.00 :   ffff8000101d383c:       bl      ffff8000101ddc00 <page_cache_async_readahead>
    0.00 :   ffff8000101d3840:       b       ffff8000101d3310 <generic_file_read_iter+0x1a0>
         :                      if (iocb->ki_flags & IOCB_NOWAIT)
    0.00 :   ffff8000101d3844:       ldr     w0, [x27, #32]
    0.00 :   ffff8000101d3848:       tbnz    w0, #7, ffff8000101d3c34 <generic_file_read_iter+0xac4>
         :                      page_cache_sync_readahead(mapping,
    0.00 :   ffff8000101d384c:       ldp     x19, x2, [x29, #152]
    0.00 :   ffff8000101d3850:       mov     x0, x26
    0.00 :   ffff8000101d3854:       ldr     x1, [x29, #112]
    0.00 :   ffff8000101d3858:       mov     x3, x19
    0.00 :   ffff8000101d385c:       sub     x4, x1, x19
    0.00 :   ffff8000101d3860:       ldr     x1, [x29, #136]
    0.00 :   ffff8000101d3864:       bl      ffff8000101dddf0 <page_cache_sync_readahead>
         :                      find_get_page():
         :                      return pagecache_get_page(mapping, offset, 0, 0);
    0.00 :   ffff8000101d3868:       mov     w3, #0x0                        // #0
    0.00 :   ffff8000101d386c:       mov     w2, #0x0                        // #0
    0.00 :   ffff8000101d3870:       mov     x1, x19
    0.00 :   ffff8000101d3874:       mov     x0, x26
    0.00 :   ffff8000101d3878:       bl      ffff8000101d1688 <pagecache_get_page>
    0.00 :   ffff8000101d387c:       mov     x22, x0
         :                      generic_file_buffered_read():
         :                      if (unlikely(page == NULL))
    0.00 :   ffff8000101d3880:       cbnz    x0, ffff8000101d3308 <generic_file_read_iter+0x198>
         :                      page_cache_alloc():
         :                      return __page_cache_alloc(mapping_gfp_mask(x));
    0.00 :   ffff8000101d3884:       ldr     w0, [x26, #24]
    0.00 :   ffff8000101d3888:       bl      ffff8000101ceb10 <__page_cache_alloc>
    0.00 :   ffff8000101d388c:       mov     x22, x0
         :                      generic_file_buffered_read():
         :                      if (!page) {
    0.00 :   ffff8000101d3890:       cbz     x0, ffff8000101d3c64 <generic_file_read_iter+0xaf4>
         :                      error = add_to_page_cache_lru(page, mapping, index,
    0.00 :   ffff8000101d3894:       ldr     x2, [x29, #152]
         :                      mapping_gfp_constraint():
         :                      return mapping_gfp_mask(mapping) & gfp_mask;
    0.00 :   ffff8000101d3898:       mov     w1, #0xcc0                      // #3264
    0.00 :   ffff8000101d389c:       ldr     w3, [x26, #24]
         :                      generic_file_buffered_read():
    0.00 :   ffff8000101d38a0:       and     w3, w3, w1
    0.00 :   ffff8000101d38a4:       mov     x1, x26
    0.00 :   ffff8000101d38a8:       bl      ffff8000101d0f98 <add_to_page_cache_lru>
    0.00 :   ffff8000101d38ac:       mov     w19, w0
         :                      if (error) {
    0.00 :   ffff8000101d38b0:       cbz     w0, ffff8000101d3738 <generic_file_read_iter+0x5c8>
         :                      __read_once_size():
    0.00 :   ffff8000101d38b4:       ldr     x0, [x22, #8]
         :                      compound_head():
    0.00 :   ffff8000101d38b8:       tst     x0, #0x1
    0.00 :   ffff8000101d38bc:       sub     x0, x0, #0x1
    0.00 :   ffff8000101d38c0:       csel    x22, x0, x22, ne  // ne = any
         :                      page_ref_dec_and_test():
    0.00 :   ffff8000101d38c4:       add     x1, x22, #0x34
         :                      arch_static_branch_jump():
    0.00 :   ffff8000101d38c8:       b       ffff8000101d3be0 <generic_file_read_iter+0xa70>
    0.00 :   ffff8000101d38cc:       b       ffff8000101d3be0 <generic_file_read_iter+0xa70>
         :                      __lse_atomic_sub_return():
    0.00 :   ffff8000101d38d0:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000101d38d4:       neg     w0, w0
    0.00 :   ffff8000101d38d8:       ldaddal w0, w2, [x1]
    0.00 :   ffff8000101d38dc:       add     w0, w0, w2
         :                      put_page():
         :                      if (put_page_testzero(page))
    0.00 :   ffff8000101d38e0:       cbnz    w0, ffff8000101d38ec <generic_file_read_iter+0x77c>
         :                      __put_page(page);
    0.00 :   ffff8000101d38e4:       mov     x0, x22
    0.00 :   ffff8000101d38e8:       bl      ffff8000101de470 <__put_page>
         :                      generic_file_buffered_read():
         :                      if (error == -EEXIST) {
    0.00 :   ffff8000101d38ec:       cmn     w19, #0x11
    0.00 :   ffff8000101d38f0:       b.eq    ffff8000101d32e0 <generic_file_read_iter+0x170>  // b.none
    0.00 :   ffff8000101d38f4:       b       ffff8000101d3498 <generic_file_read_iter+0x328>
         :                      __read_once_size():
    0.00 :   ffff8000101d38f8:       ldr     x0, [x19, #8]
         :                      wait_on_page_bit_common():
         :                      !PageUptodate(page) && PageWorkingset(page)) {
    0.00 :   ffff8000101d38fc:       str     wzr, [x29, #124]
         :                      compound_head():
         :                      return page;
    0.00 :   ffff8000101d3900:       tst     x0, #0x1
    0.00 :   ffff8000101d3904:       sub     x0, x0, #0x1
    0.00 :   ffff8000101d3908:       csel    x0, x0, x19, ne  // ne = any
         :                      test_bit():
    0.00 :   ffff8000101d390c:       ldr     x0, [x0]
         :                      wait_on_page_bit_common():
    0.00 :   ffff8000101d3910:       tbz     w0, #6, ffff8000101d33a4 <generic_file_read_iter+0x234>
         :                      __read_once_size():
    0.00 :   ffff8000101d3914:       ldr     x1, [x19, #8]
         :                      compound_head():
         :                      return (struct page *) (head - 1);
    0.00 :   ffff8000101d3918:       tst     x1, #0x1
    0.00 :   ffff8000101d391c:       sub     x1, x1, #0x1
    0.00 :   ffff8000101d3920:       csel    x1, x1, x19, ne  // ne = any
         :                      test_bit():
    0.00 :   ffff8000101d3924:       ldr     x0, [x1]
         :                      wait_on_page_bit_common():
         :                      if (!PageSwapBacked(page)) {
    0.00 :   ffff8000101d3928:       tbnz    w0, #19, ffff8000101d33a4 <generic_file_read_iter+0x234>
         :                      get_current():
    0.00 :   ffff8000101d392c:       mrs     x0, sp_el0
         :                      delayacct_thrashing_start():
         :                      __delayacct_freepages_end();
         :                      }
         :
         :                      static inline void delayacct_thrashing_start(void)
         :                      {
         :                      if (current->delays)
    0.00 :   ffff8000101d3930:       ldr     x0, [x0, #2368]
    0.00 :   ffff8000101d3934:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000101d3938:       str     w1, [x29, #124]
    0.00 :   ffff8000101d393c:       cbz     x0, ffff8000101d33a4 <generic_file_read_iter+0x234>
         :                      __delayacct_thrashing_start();
    0.00 :   ffff8000101d3940:       bl      ffff8000101b1958 <__delayacct_thrashing_start>
    0.00 :   ffff8000101d3944:       b       ffff8000101d33a4 <generic_file_read_iter+0x234>
         :                      arch_static_branch_jump():
    0.00 :   ffff8000101d3948:       b       ffff8000101d3960 <generic_file_read_iter+0x7f0>
    0.00 :   ffff8000101d394c:       b       ffff8000101d3960 <generic_file_read_iter+0x7f0>
         :                      __lse_atomic64_fetch_or_acquire():
         :                      ATOMIC64_FETCH_OP(_acquire,  a, op, asm_op, "memory")           \
         :                      ATOMIC64_FETCH_OP(_release,  l, op, asm_op, "memory")           \
         :                      ATOMIC64_FETCH_OP(        , al, op, asm_op, "memory")
         :
         :                      ATOMIC64_FETCH_OPS(andnot, ldclr)
         :                      ATOMIC64_FETCH_OPS(or, ldset)
    0.00 :   ffff8000101d3950:       mov     x0, #0x1                        // #1
    0.00 :   ffff8000101d3954:       ldseta  x0, x0, [x20]
         :                      lock_page_killable():
         :                      if (!trylock_page(page))
    0.00 :   ffff8000101d3958:       tbz     w0, #0, ffff8000101d37a4 <generic_file_read_iter+0x634>
    0.00 :   ffff8000101d395c:       b       ffff8000101d3794 <generic_file_read_iter+0x624>
         :                      __ll_sc_atomic64_fetch_or_acquire():
         :                      ATOMIC64_OPS(or, orr, L)
    0.00 :   ffff8000101d3960:       b       ffff8000101d5870 <generic_file_write_iter+0x838>
         :                      lock_page_killable():
    0.00 :   ffff8000101d3964:       tbz     w0, #0, ffff8000101d37a4 <generic_file_read_iter+0x634>
    0.00 :   ffff8000101d3968:       b       ffff8000101d3794 <generic_file_read_iter+0x624>
    0.00 :   ffff8000101d396c:       ldr     x20, [x29, #24]
    0.00 :   ffff8000101d3970:       ldr     x26, [x29, #72]
    0.00 :   ffff8000101d3974:       b       ffff8000101d34fc <generic_file_read_iter+0x38c>
         :                      __ll_sc_atomic_sub_return():
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff8000101d3978:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000101d397c:       b       ffff8000101d5888 <generic_file_write_iter+0x850>
    0.00 :   ffff8000101d3980:       b       ffff8000101d3494 <generic_file_read_iter+0x324>
         :                      arch_static_branch_jump():
    0.00 :   ffff8000101d3984:       b       ffff8000101d39a0 <generic_file_read_iter+0x830>
    0.00 :   ffff8000101d3988:       b       ffff8000101d39a0 <generic_file_read_iter+0x830>
         :                      __lse_atomic64_fetch_or_acquire():
    0.00 :   ffff8000101d398c:       mov     x1, #0x1                        // #1
    0.00 :   ffff8000101d3990:       ldseta  x1, x1, [x0]
    0.00 :   ffff8000101d3994:       mov     x0, x1
         :                      lock_page_killable():
    0.00 :   ffff8000101d3998:       tbz     w0, #0, ffff8000101d3708 <generic_file_read_iter+0x598>
    0.00 :   ffff8000101d399c:       b       ffff8000101d36f8 <generic_file_read_iter+0x588>
         :                      __ll_sc_atomic64_fetch_or_acquire():
         :                      ATOMIC64_OPS(or, orr, L)
    0.00 :   ffff8000101d39a0:       b       ffff8000101d58a4 <generic_file_write_iter+0x86c>
    0.00 :   ffff8000101d39a4:       mov     x0, x1
         :                      lock_page_killable():
    0.00 :   ffff8000101d39a8:       tbz     w0, #0, ffff8000101d3708 <generic_file_read_iter+0x598>
    0.00 :   ffff8000101d39ac:       b       ffff8000101d36f8 <generic_file_read_iter+0x588>
         :                      arch_static_branch_jump():
    0.00 :   ffff8000101d39b0:       b       ffff8000101d39f8 <generic_file_read_iter+0x888>
    0.00 :   ffff8000101d39b4:       b       ffff8000101d39f8 <generic_file_read_iter+0x888>
         :                      __lse_atomic64_fetch_or_acquire():
    0.00 :   ffff8000101d39b8:       mov     x1, #0x1                        // #1
    0.00 :   ffff8000101d39bc:       ldseta  x1, x1, [x0]
    0.00 :   ffff8000101d39c0:       mov     x0, x1
         :                      generic_file_buffered_read():
         :                      if (!trylock_page(page))
    0.00 :   ffff8000101d39c4:       tbnz    w0, #0, ffff8000101d36e0 <generic_file_read_iter+0x570>
         :                      if (!page->mapping)
    0.00 :   ffff8000101d39c8:       ldr     x0, [x22, #24]
    0.00 :   ffff8000101d39cc:       cbz     x0, ffff8000101d3a90 <generic_file_read_iter+0x920>
         :                      if (!mapping->a_ops->is_partially_uptodate(page,
    0.00 :   ffff8000101d39d0:       ldr     x0, [x26, #112]
    0.00 :   ffff8000101d39d4:       ldr     x2, [x28, #16]
    0.00 :   ffff8000101d39d8:       ldr     x1, [x29, #128]
    0.00 :   ffff8000101d39dc:       ldr     x3, [x0, #128]
    0.00 :   ffff8000101d39e0:       mov     x0, x22
    0.00 :   ffff8000101d39e4:       blr     x3
    0.00 :   ffff8000101d39e8:       cbz     w0, ffff8000101d3708 <generic_file_read_iter+0x598>
         :                      unlock_page(page);
    0.00 :   ffff8000101d39ec:       mov     x0, x22
    0.00 :   ffff8000101d39f0:       bl      ffff8000101cfaf8 <unlock_page>
    0.00 :   ffff8000101d39f4:       b       ffff8000101d3560 <generic_file_read_iter+0x3f0>
         :                      __ll_sc_atomic64_fetch_or_acquire():
    0.00 :   ffff8000101d39f8:       b       ffff8000101d58bc <generic_file_write_iter+0x884>
    0.00 :   ffff8000101d39fc:       mov     x0, x1
    0.00 :   ffff8000101d3a00:       b       ffff8000101d39c4 <generic_file_read_iter+0x854>
         :                      generic_file_read_iter():
         :                      iov_iter_revert(iter, count - iov_iter_count(iter));
    6.44 :   ffff8000101d3a04:       ldr     x1, [x28, #16]
    0.00 :   ffff8000101d3a08:       mov     x0, x28
    0.00 :   ffff8000101d3a0c:       sub     x1, x19, x1
    0.00 :   ffff8000101d3a10:       bl      ffff800010482ae0 <iov_iter_revert>
    2.28 :   ffff8000101d3a14:       ldr     x20, [x29, #24]
    4.75 :   ffff8000101d3a18:       ldr     x26, [x29, #72]
    0.00 :   ffff8000101d3a1c:       b       ffff8000101d34fc <generic_file_read_iter+0x38c>
         :                      file_accessed():
         :                      touch_atime(&file->f_path);
    0.00 :   ffff8000101d3a20:       ldr     x0, [x29, #160]
    0.00 :   ffff8000101d3a24:       add     x0, x0, #0x10
    0.00 :   ffff8000101d3a28:       bl      ffff80001029dae0 <touch_atime>
    0.00 :   ffff8000101d3a2c:       b       ffff8000101d31fc <generic_file_read_iter+0x8c>
         :                      fatal_signal_pending():
         :                      return signal_pending(p) && __fatal_signal_pending(p);
    0.00 :   ffff8000101d3a30:       ldr     x0, [x0, #1696]
    0.00 :   ffff8000101d3a34:       tbz     w0, #8, ffff8000101d32ec <generic_file_read_iter+0x17c>
         :                      generic_file_buffered_read():
         :                      error = -EINTR;
    0.00 :   ffff8000101d3a38:       mov     w19, #0xfffffffc                // #-4
    0.00 :   ffff8000101d3a3c:       b       ffff8000101d3498 <generic_file_read_iter+0x328>
         :                      file_accessed():
    0.00 :   ffff8000101d3a40:       ldr     x0, [x29, #160]
    0.00 :   ffff8000101d3a44:       add     x0, x0, #0x10
    0.00 :   ffff8000101d3a48:       bl      ffff80001029dae0 <touch_atime>
    0.00 :   ffff8000101d3a4c:       b       ffff8000101d34c8 <generic_file_read_iter+0x358>
         :                      __read_once_size():
    0.00 :   ffff8000101d3a50:       ldr     x0, [x22, #8]
         :                      compound_head():
    0.00 :   ffff8000101d3a54:       sub     x1, x0, #0x1
    0.00 :   ffff8000101d3a58:       tst     x0, #0x1
    0.00 :   ffff8000101d3a5c:       csel    x22, x1, x22, ne  // ne = any
         :                      page_ref_dec_and_test():
    0.00 :   ffff8000101d3a60:       add     x1, x22, #0x34
         :                      arch_static_branch_jump():
    0.00 :   ffff8000101d3a64:       b       ffff8000101d3bb4 <generic_file_read_iter+0xa44>
    0.00 :   ffff8000101d3a68:       b       ffff8000101d3bb4 <generic_file_read_iter+0xa44>
         :                      __lse_atomic_sub_return():
         :                      ATOMIC_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff8000101d3a6c:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000101d3a70:       neg     w0, w0
    0.00 :   ffff8000101d3a74:       ldaddal w0, w2, [x1]
    0.00 :   ffff8000101d3a78:       add     w0, w0, w2
         :                      generic_file_buffered_read():
         :                      error = -EAGAIN;
    0.00 :   ffff8000101d3a7c:       mov     w19, #0xfffffff5                // #-11
         :                      put_page():
         :                      if (put_page_testzero(page))
    0.00 :   ffff8000101d3a80:       cbnz    w0, ffff8000101d3498 <generic_file_read_iter+0x328>
         :                      __put_page(page);
    0.00 :   ffff8000101d3a84:       mov     x0, x22
    0.00 :   ffff8000101d3a88:       bl      ffff8000101de470 <__put_page>
    0.00 :   ffff8000101d3a8c:       b       ffff8000101d3498 <generic_file_read_iter+0x328>
         :                      generic_file_buffered_read():
         :                      unlock_page(page);
    0.00 :   ffff8000101d3a90:       mov     x0, x22
    0.00 :   ffff8000101d3a94:       bl      ffff8000101cfaf8 <unlock_page>
         :                      __read_once_size():
    0.00 :   ffff8000101d3a98:       ldr     x0, [x22, #8]
         :                      compound_head():
    0.00 :   ffff8000101d3a9c:       sub     x1, x0, #0x1
    0.00 :   ffff8000101d3aa0:       tst     x0, #0x1
    0.00 :   ffff8000101d3aa4:       csel    x22, x1, x22, ne  // ne = any
         :                      page_ref_dec_and_test():
    0.00 :   ffff8000101d3aa8:       add     x0, x22, #0x34
         :                      arch_static_branch_jump():
    0.00 :   ffff8000101d3aac:       b       ffff8000101d3ad0 <generic_file_read_iter+0x960>
    0.00 :   ffff8000101d3ab0:       b       ffff8000101d3ad0 <generic_file_read_iter+0x960>
         :                      __lse_atomic_sub_return():
    0.00 :   ffff8000101d3ab4:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000101d3ab8:       neg     w1, w1
    0.00 :   ffff8000101d3abc:       ldaddal w1, w2, [x0]
    0.00 :   ffff8000101d3ac0:       add     w1, w1, w2
    0.00 :   ffff8000101d3ac4:       mov     w0, w1
         :                      put_page():
         :                      if (put_page_testzero(page))
    0.00 :   ffff8000101d3ac8:       cbnz    w0, ffff8000101d32e0 <generic_file_read_iter+0x170>
    0.00 :   ffff8000101d3acc:       b       ffff8000101d37fc <generic_file_read_iter+0x68c>
         :                      __ll_sc_atomic_sub_return():
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff8000101d3ad0:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000101d3ad4:       b       ffff8000101d58d4 <generic_file_write_iter+0x89c>
    0.00 :   ffff8000101d3ad8:       mov     w0, w1
         :                      put_page():
    0.00 :   ffff8000101d3adc:       cbnz    w0, ffff8000101d32e0 <generic_file_read_iter+0x170>
    0.00 :   ffff8000101d3ae0:       b       ffff8000101d37fc <generic_file_read_iter+0x68c>
         :                      generic_file_buffered_read():
         :                      if (error == AOP_TRUNCATED_PAGE) {
    0.00 :   ffff8000101d3ae4:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000101d3ae8:       movk    w0, #0x8, lsl #16
    0.00 :   ffff8000101d3aec:       cmp     w19, w0
    0.00 :   ffff8000101d3af0:       b.ne    ffff8000101d3468 <generic_file_read_iter+0x2f8>  // b.any
         :                      __read_once_size():
    0.00 :   ffff8000101d3af4:       ldr     x0, [x22, #8]
         :                      compound_head():
    0.00 :   ffff8000101d3af8:       tst     x0, #0x1
    0.00 :   ffff8000101d3afc:       sub     x0, x0, #0x1
    0.00 :   ffff8000101d3b00:       csel    x22, x0, x22, ne  // ne = any
         :                      page_ref_dec_and_test():
    0.00 :   ffff8000101d3b04:       add     x1, x22, #0x34
         :                      arch_static_branch_jump():
    0.00 :   ffff8000101d3b08:       b       ffff8000101d3b28 <generic_file_read_iter+0x9b8>
    0.00 :   ffff8000101d3b0c:       b       ffff8000101d3b28 <generic_file_read_iter+0x9b8>
         :                      __lse_atomic_sub_return():
    0.00 :   ffff8000101d3b10:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000101d3b14:       neg     w0, w0
    0.00 :   ffff8000101d3b18:       ldaddal w0, w2, [x1]
    0.00 :   ffff8000101d3b1c:       add     w0, w0, w2
         :                      put_page():
    0.00 :   ffff8000101d3b20:       cbnz    w0, ffff8000101d32e0 <generic_file_read_iter+0x170>
    0.00 :   ffff8000101d3b24:       b       ffff8000101d37fc <generic_file_read_iter+0x68c>
         :                      __ll_sc_atomic_sub_return():
    0.00 :   ffff8000101d3b28:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000101d3b2c:       b       ffff8000101d58f0 <generic_file_write_iter+0x8b8>
         :                      put_page():
    0.00 :   ffff8000101d3b30:       cbnz    w0, ffff8000101d32e0 <generic_file_read_iter+0x170>
    0.00 :   ffff8000101d3b34:       b       ffff8000101d37fc <generic_file_read_iter+0x68c>
         :                      compound_head():
    0.00 :   ffff8000101d3b38:       sub     x20, x0, #0x1
    0.00 :   ffff8000101d3b3c:       mov     x0, x20
    0.00 :   ffff8000101d3b40:       b       ffff8000101d378c <generic_file_read_iter+0x61c>
         :                      generic_file_buffered_read():
         :                      flush_dcache_page(page);
    0.00 :   ffff8000101d3b44:       mov     x0, x22
    0.00 :   ffff8000101d3b48:       bl      ffff8000100a2608 <flush_dcache_page>
    0.00 :   ffff8000101d3b4c:       b       ffff8000101d35a4 <generic_file_read_iter+0x434>
         :                      nr = ((isize - 1) & ~PAGE_MASK) + 1;
    0.00 :   ffff8000101d3b50:       and     x2, x2, #0xfff
         :                      if (nr <= offset) {
    0.00 :   ffff8000101d3b54:       ldr     x0, [x29, #128]
         :                      nr = ((isize - 1) & ~PAGE_MASK) + 1;
    0.00 :   ffff8000101d3b58:       add     x19, x2, #0x1
         :                      if (nr <= offset) {
    0.00 :   ffff8000101d3b5c:       cmp     x0, x19
    0.00 :   ffff8000101d3b60:       b.cc    ffff8000101d3590 <generic_file_read_iter+0x420>  // b.lo, b.ul, b.last
         :                      __read_once_size():
    0.00 :   ffff8000101d3b64:       ldr     x0, [x22, #8]
         :                      compound_head():
         :                      if (unlikely(head & 1))
    0.00 :   ffff8000101d3b68:       tbz     w0, #0, ffff8000101d3b70 <generic_file_read_iter+0xa00>
         :                      return (struct page *) (head - 1);
    0.00 :   ffff8000101d3b6c:       sub     x22, x0, #0x1
         :                      page_ref_dec_and_test():
    0.00 :   ffff8000101d3b70:       add     x0, x22, #0x34
         :                      arch_static_branch_jump():
    0.00 :   ffff8000101d3b74:       b       ffff8000101d3c88 <generic_file_read_iter+0xb18>
    0.00 :   ffff8000101d3b78:       b       ffff8000101d3c88 <generic_file_read_iter+0xb18>
         :                      __lse_atomic_sub_return():
    0.00 :   ffff8000101d3b7c:       mov     w19, #0x1                       // #1
    0.00 :   ffff8000101d3b80:       neg     w19, w19
    0.00 :   ffff8000101d3b84:       ldaddal w19, w2, [x0]
    0.00 :   ffff8000101d3b88:       add     w19, w19, w2
         :                      put_page():
    0.00 :   ffff8000101d3b8c:       cbz     w19, ffff8000101d3c78 <generic_file_read_iter+0xb08>
         :                      generic_file_buffered_read():
         :                      end_index = (isize - 1) >> PAGE_SHIFT;
    0.00 :   ffff8000101d3b90:       str     x1, [x29, #152]
         :                      put_page():
    0.00 :   ffff8000101d3b94:       mov     w19, #0x0                       // #0
    0.00 :   ffff8000101d3b98:       b       ffff8000101d3498 <generic_file_read_iter+0x328>
         :                      __ll_sc_atomic_sub_return():
    0.00 :   ffff8000101d3b9c:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000101d3ba0:       b       ffff8000101d590c <generic_file_write_iter+0x8d4>
    0.00 :   ffff8000101d3ba4:       mov     w0, w1
    0.00 :   ffff8000101d3ba8:       b       ffff8000101d3630 <generic_file_read_iter+0x4c0>
         :                      generic_file_buffered_read():
         :                      index += offset >> PAGE_SHIFT;
    0.00 :   ffff8000101d3bac:       str     x20, [x29, #152]
    0.00 :   ffff8000101d3bb0:       b       ffff8000101d32e0 <generic_file_read_iter+0x170>
         :                      __ll_sc_atomic_sub_return():
    0.00 :   ffff8000101d3bb4:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000101d3bb8:       b       ffff8000101d5928 <generic_file_write_iter+0x8f0>
    0.00 :   ffff8000101d3bbc:       b       ffff8000101d3a7c <generic_file_read_iter+0x90c>
         :                      generic_file_buffered_read():
         :                      unlock_page(page);
    0.00 :   ffff8000101d3bc0:       mov     x0, x22
    0.00 :   ffff8000101d3bc4:       bl      ffff8000101cfaf8 <unlock_page>
         :                      shrink_readahead_size_eio():
         :                      ra->ra_pages /= 4;
    0.00 :   ffff8000101d3bc8:       ldr     x1, [x29, #160]
         :                      generic_file_buffered_read():
         :                      error = -EIO;
    0.00 :   ffff8000101d3bcc:       mov     w19, #0xfffffffb                // #-5
         :                      shrink_readahead_size_eio():
         :                      ra->ra_pages /= 4;
    0.00 :   ffff8000101d3bd0:       ldr     w0, [x1, #168]
    0.00 :   ffff8000101d3bd4:       lsr     w0, w0, #2
    0.00 :   ffff8000101d3bd8:       str     w0, [x1, #168]
    0.00 :   ffff8000101d3bdc:       b       ffff8000101d3468 <generic_file_read_iter+0x2f8>
         :                      __ll_sc_atomic_sub_return():
    0.00 :   ffff8000101d3be0:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000101d3be4:       b       ffff8000101d5944 <generic_file_write_iter+0x90c>
    0.00 :   ffff8000101d3be8:       b       ffff8000101d38e0 <generic_file_read_iter+0x770>
         :                      __read_once_size():
    0.00 :   ffff8000101d3bec:       ldr     x0, [x22, #8]
         :                      compound_head():
    0.00 :   ffff8000101d3bf0:       sub     x1, x0, #0x1
    0.00 :   ffff8000101d3bf4:       tst     x0, #0x1
    0.00 :   ffff8000101d3bf8:       csel    x22, x1, x22, ne  // ne = any
         :                      page_ref_dec_and_test():
    0.00 :   ffff8000101d3bfc:       add     x0, x22, #0x34
         :                      arch_static_branch_jump():
    0.00 :   ffff8000101d3c00:       b       ffff8000101d3c24 <generic_file_read_iter+0xab4>
    0.00 :   ffff8000101d3c04:       b       ffff8000101d3c24 <generic_file_read_iter+0xab4>
         :                      __lse_atomic_sub_return():
    0.00 :   ffff8000101d3c08:       mov     w19, #0x1                       // #1
    0.00 :   ffff8000101d3c0c:       neg     w19, w19
    0.00 :   ffff8000101d3c10:       ldaddal w19, w1, [x0]
    0.00 :   ffff8000101d3c14:       add     w19, w19, w1
         :                      put_page():
    0.00 :   ffff8000101d3c18:       cbz     w19, ffff8000101d3a84 <generic_file_read_iter+0x914>
    0.00 :   ffff8000101d3c1c:       mov     w19, #0x0                       // #0
    0.00 :   ffff8000101d3c20:       b       ffff8000101d3498 <generic_file_read_iter+0x328>
         :                      __ll_sc_atomic_sub_return():
    0.00 :   ffff8000101d3c24:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000101d3c28:       b       ffff8000101d5960 <generic_file_write_iter+0x928>
         :                      put_page():
    0.00 :   ffff8000101d3c2c:       cbnz    w19, ffff8000101d3c1c <generic_file_read_iter+0xaac>
    0.00 :   ffff8000101d3c30:       b       ffff8000101d3a84 <generic_file_read_iter+0x914>
         :                      generic_file_buffered_read():
         :                      error = -EAGAIN;
    0.00 :   ffff8000101d3c34:       mov     w19, #0xfffffff5                // #-11
    0.00 :   ffff8000101d3c38:       b       ffff8000101d3498 <generic_file_read_iter+0x328>
         :                      if (!iov_iter_count(iter))
    0.00 :   ffff8000101d3c3c:       ldr     x0, [x29, #152]
    0.00 :   ffff8000101d3c40:       mov     w19, #0x0                       // #0
    0.00 :   ffff8000101d3c44:       str     x0, [x29, #104]
         :                      index += offset >> PAGE_SHIFT;
    0.00 :   ffff8000101d3c48:       str     x20, [x29, #152]
    0.00 :   ffff8000101d3c4c:       b       ffff8000101d3498 <generic_file_read_iter+0x328>
    0.00 :   ffff8000101d3c50:       stp     x20, x21, [x29, #24]
    0.00 :   ffff8000101d3c54:       stp     x22, x23, [x29, #40]
    0.00 :   ffff8000101d3c58:       stp     x24, x25, [x29, #56]
    0.00 :   ffff8000101d3c5c:       str     x26, [x29, #72]
         :                      generic_file_read_iter():
         :                      }
    0.00 :   ffff8000101d3c60:       bl      ffff8000100e5630 <__stack_chk_fail>
         :                      generic_file_buffered_read():
         :                      error = -ENOMEM;
    0.00 :   ffff8000101d3c64:       mov     w19, #0xfffffff4                // #-12
    0.00 :   ffff8000101d3c68:       b       ffff8000101d3498 <generic_file_read_iter+0x328>
         :                      wait_on_page_bit_common():
    0.00 :   ffff8000101d3c6c:       bl      ffff8000101cebc0 <wait_on_page_bit_common.part.44>
         :                      ret = -EINTR;
    0.00 :   ffff8000101d3c70:       mov     w19, #0xfffffffc                // #-4
    0.00 :   ffff8000101d3c74:       b       ffff8000101d3468 <generic_file_read_iter+0x2f8>
         :                      put_page():
         :                      __put_page(page);
    0.00 :   ffff8000101d3c78:       mov     x0, x22
         :                      generic_file_buffered_read():
         :                      end_index = (isize - 1) >> PAGE_SHIFT;
    0.00 :   ffff8000101d3c7c:       str     x1, [x29, #152]
         :                      put_page():
    0.00 :   ffff8000101d3c80:       bl      ffff8000101de470 <__put_page>
    0.00 :   ffff8000101d3c84:       b       ffff8000101d3498 <generic_file_read_iter+0x328>
         :                      __ll_sc_atomic_sub_return():
    0.00 :   ffff8000101d3c88:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000101d3c8c:       b       ffff8000101d597c <generic_file_write_iter+0x944>
    0.00 :   ffff8000101d3c90:       b       ffff8000101d3b8c <generic_file_read_iter+0xa1c>
 Percent |	Source code & Disassembly of vmlinux for cycles (492 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102782f8 <rw_verify_area>:
         :                      rw_verify_area():
         :                      {
         :                      struct inode *inode;
         :                      int retval = -EINVAL;
         :
         :                      inode = file_inode(file);
         :                      if (unlikely((ssize_t) count < 0))
    0.40 :   ffff8000102782f8:       tbnz    x3, #63, ffff800010278354 <rw_verify_area+0x5c>
         :                      {
    1.43 :   ffff8000102782fc:       stp     x29, x30, [sp, #-32]!
   15.25 :   ffff800010278300:       mov     x29, sp
    0.20 :   ffff800010278304:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010278308:       mov     x19, x1
    0.00 :   ffff80001027830c:       mov     w20, w0
         :
         :                      /*
         :                      * ranged mandatory locking does not apply to streams - it makes sense
         :                      * only for files where position has a meaning.
         :                      */
         :                      if (ppos) {
    0.00 :   ffff800010278310:       cbz     x2, ffff800010278330 <rw_verify_area+0x38>
         :                      loff_t pos = *ppos;
   15.62 :   ffff800010278314:       ldr     x2, [x2]
         :
         :                      if (unlikely(pos < 0)) {
    0.00 :   ffff800010278318:       tbnz    x2, #63, ffff80001027835c <rw_verify_area+0x64>
         :                      if (!unsigned_offsets(file))
         :                      return retval;
         :                      if (count >= -pos) /* both values are in 0..LLONG_MAX */
         :                      return -EOVERFLOW;
         :                      } else if (unlikely((loff_t) (pos + count) < 0)) {
    3.24 :   ffff80001027831c:       cmn     x2, x3
    0.61 :   ffff800010278320:       b.mi    ffff8000102783b8 <rw_verify_area+0xc0>  // b.first
         :                      inode = file_inode(file);
    2.89 :   ffff800010278324:       ldr     x0, [x19, #32]
         :                      if (!unsigned_offsets(file))
         :                      return retval;
         :                      }
         :
         :                      if (unlikely(inode->i_flctx && mandatory_lock(inode))) {
    0.62 :   ffff800010278328:       ldr     x1, [x0, #360]
    0.00 :   ffff80001027832c:       cbnz    x1, ffff800010278378 <rw_verify_area+0x80>
         :                      if (retval < 0)
         :                      return retval;
         :                      }
         :                      }
         :
         :                      return security_file_permission(file,
   25.65 :   ffff800010278330:       cmp     w20, #0x0
    0.00 :   ffff800010278334:       mov     w2, #0x4                        // #4
    0.00 :   ffff800010278338:       mov     w1, #0x2                        // #2
    0.00 :   ffff80001027833c:       mov     x0, x19
    1.63 :   ffff800010278340:       csel    w1, w2, w1, eq  // eq = none
    0.00 :   ffff800010278344:       bl      ffff8000104324e0 <security_file_permission>
         :                      read_write == READ ? MAY_READ : MAY_WRITE);
         :                      }
    0.00 :   ffff800010278348:       ldp     x19, x20, [sp, #16]
    1.62 :   ffff80001027834c:       ldp     x29, x30, [sp], #32
    0.00 :   ffff800010278350:       ret
         :                      return retval;
    0.00 :   ffff800010278354:       mov     w0, #0xffffffea                 // #-22
         :                      }
    0.00 :   ffff800010278358:       ret
         :                      unsigned_offsets():
         :                      return file->f_mode & FMODE_UNSIGNED_OFFSET;
    0.00 :   ffff80001027835c:       ldr     w0, [x1, #68]
         :                      rw_verify_area():
         :                      if (!unsigned_offsets(file))
    0.00 :   ffff800010278360:       tbz     w0, #13, ffff8000102783c0 <rw_verify_area+0xc8>
         :                      if (count >= -pos) /* both values are in 0..LLONG_MAX */
    0.00 :   ffff800010278364:       neg     x0, x2
    0.00 :   ffff800010278368:       cmp     x0, x3
    0.00 :   ffff80001027836c:       b.hi    ffff800010278324 <rw_verify_area+0x2c>  // b.pmore
         :                      return -EOVERFLOW;
    0.00 :   ffff800010278370:       mov     w0, #0xffffffb5                 // #-75
    0.00 :   ffff800010278374:       b       ffff800010278348 <rw_verify_area+0x50>
         :                      mandatory_lock():
         :                      * otherwise these will be advisory locks
         :                      */
         :
         :                      static inline int mandatory_lock(struct inode *ino)
         :                      {
         :                      return IS_MANDLOCK(ino) && __mandatory_lock(ino);
    8.31 :   ffff800010278378:       ldr     x1, [x0, #40]
   10.36 :   ffff80001027837c:       ldr     x1, [x1, #80]
   12.15 :   ffff800010278380:       tbz     w1, #6, ffff800010278330 <rw_verify_area+0x38>
    0.00 :   ffff800010278384:       ldrh    w4, [x0]
    0.00 :   ffff800010278388:       mov     w1, #0x408                      // #1032
    0.00 :   ffff80001027838c:       and     w1, w1, w4
    0.00 :   ffff800010278390:       cmp     w1, #0x400
    0.00 :   ffff800010278394:       b.ne    ffff800010278330 <rw_verify_area+0x38>  // b.any
         :                      rw_verify_area():
         :                      retval = locks_mandatory_area(inode, file, pos, pos + count - 1,
    0.00 :   ffff800010278398:       cmp     w20, #0x0
    0.00 :   ffff80001027839c:       sub     x3, x3, #0x1
    0.00 :   ffff8000102783a0:       cset    w4, ne  // ne = any
    0.00 :   ffff8000102783a4:       add     x3, x3, x2
    0.00 :   ffff8000102783a8:       mov     x1, x19
    0.00 :   ffff8000102783ac:       bl      ffff8000102e8178 <locks_mandatory_area>
         :                      if (retval < 0)
    0.00 :   ffff8000102783b0:       tbz     w0, #31, ffff800010278330 <rw_verify_area+0x38>
    0.00 :   ffff8000102783b4:       b       ffff800010278348 <rw_verify_area+0x50>
         :                      unsigned_offsets():
         :                      return file->f_mode & FMODE_UNSIGNED_OFFSET;
    0.00 :   ffff8000102783b8:       ldr     w0, [x1, #68]
         :                      rw_verify_area():
         :                      if (!unsigned_offsets(file))
    0.00 :   ffff8000102783bc:       tbnz    w0, #13, ffff800010278324 <rw_verify_area+0x2c>
         :                      return retval;
    0.00 :   ffff8000102783c0:       mov     w0, #0xffffffea                 // #-22
    0.00 :   ffff8000102783c4:       b       ffff800010278348 <rw_verify_area+0x50>
 Percent |	Source code & Disassembly of vmlinux for cycles (499 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101db068 <set_page_dirty_lock>:
         :                      set_page_dirty_lock():
         :                      * holds a reference on the inode by having an open file.
         :                      *
         :                      * In other cases, the page should be locked before running set_page_dirty().
         :                      */
         :                      int set_page_dirty_lock(struct page *page)
         :                      {
    3.64 :   ffff8000101db068:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff8000101db06c:       mov     x29, sp
    0.00 :   ffff8000101db070:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000101db074:       adrp    x20, ffff800011899000 <page_wait_table+0x1500>
    2.79 :   ffff8000101db078:       str     x21, [sp, #32]
    0.00 :   ffff8000101db07c:       add     x1, x20, #0x8c8
    0.61 :   ffff8000101db080:       mov     x19, x0
    1.61 :   ffff8000101db084:       ldr     x0, [x1]
    4.99 :   ffff8000101db088:       str     x0, [x29, #56]
    0.00 :   ffff8000101db08c:       mov     x0, #0x0                        // #0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    1.41 :   ffff8000101db090:       ldr     x1, [x19, #8]
         :                      compound_head():
         :                      static inline struct page *compound_head(struct page *page)
         :                      {
         :                      unsigned long head = READ_ONCE(page->compound_head);
         :
         :                      if (unlikely(head & 1))
         :                      return (struct page *) (head - 1);
    0.00 :   ffff8000101db094:       sub     x0, x1, #0x1
    0.00 :   ffff8000101db098:       tst     x1, #0x1
    0.00 :   ffff8000101db09c:       csel    x0, x0, x19, ne  // ne = any
         :                      __read_once_size():
    0.80 :   ffff8000101db0a0:       ldr     x1, [x0]
         :                      test_and_set_bit_lock():
         :                      {
         :                      long old;
         :                      unsigned long mask = BIT_MASK(nr);
         :
         :                      p += BIT_WORD(nr);
         :                      if (READ_ONCE(*p) & mask)
    0.00 :   ffff8000101db0a4:       tbz     w1, #0, ffff8000101db0ec <set_page_dirty_lock+0x84>
         :                      lock_page():
         :                      */
         :                      static inline void lock_page(struct page *page)
         :                      {
         :                      might_sleep();
         :                      if (!trylock_page(page))
         :                      __lock_page(page);
    0.00 :   ffff8000101db0a8:       mov     x0, x19
    0.00 :   ffff8000101db0ac:       bl      ffff8000101cfe20 <__lock_page>
         :                      set_page_dirty_lock():
         :                      int ret;
         :
         :                      lock_page(page);
         :                      ret = set_page_dirty(page);
    0.00 :   ffff8000101db0b0:       mov     x0, x19
    0.00 :   ffff8000101db0b4:       bl      ffff8000101daf40 <set_page_dirty>
         :                      unlock_page(page);
         :                      return ret;
         :                      }
    0.20 :   ffff8000101db0b8:       add     x20, x20, #0x8c8
         :                      ret = set_page_dirty(page);
    0.00 :   ffff8000101db0bc:       mov     w21, w0
         :                      unlock_page(page);
    0.00 :   ffff8000101db0c0:       mov     x0, x19
    0.00 :   ffff8000101db0c4:       bl      ffff8000101cfaf8 <unlock_page>
         :                      }
    2.39 :   ffff8000101db0c8:       mov     w0, w21
    0.00 :   ffff8000101db0cc:       ldr     x2, [x29, #56]
    0.00 :   ffff8000101db0d0:       ldr     x1, [x20]
    0.00 :   ffff8000101db0d4:       eor     x1, x2, x1
    0.00 :   ffff8000101db0d8:       cbnz    x1, ffff8000101db118 <set_page_dirty_lock+0xb0>
    2.00 :   ffff8000101db0dc:       ldp     x19, x20, [sp, #16]
    3.01 :   ffff8000101db0e0:       ldr     x21, [sp, #32]
    0.00 :   ffff8000101db0e4:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000101db0e8:       ret
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
   17.84 :   ffff8000101db0ec:       b       ffff8000101db108 <set_page_dirty_lock+0xa0>
    2.79 :   ffff8000101db0f0:       b       ffff8000101db108 <set_page_dirty_lock+0xa0>
         :                      __lse_atomic64_fetch_or_acquire():
         :                      ATOMIC64_FETCH_OP(_acquire,  a, op, asm_op, "memory")           \
         :                      ATOMIC64_FETCH_OP(_release,  l, op, asm_op, "memory")           \
         :                      ATOMIC64_FETCH_OP(        , al, op, asm_op, "memory")
         :
         :                      ATOMIC64_FETCH_OPS(andnot, ldclr)
         :                      ATOMIC64_FETCH_OPS(or, ldset)
    0.00 :   ffff8000101db0f4:       mov     x1, #0x1                        // #1
    0.00 :   ffff8000101db0f8:       ldseta  x1, x1, [x0]
   55.92 :   ffff8000101db0fc:       mov     x0, x1
         :                      lock_page():
         :                      if (!trylock_page(page))
    0.00 :   ffff8000101db100:       tbz     w0, #0, ffff8000101db0b0 <set_page_dirty_lock+0x48>
    0.00 :   ffff8000101db104:       b       ffff8000101db0a8 <set_page_dirty_lock+0x40>
         :                      __ll_sc_atomic64_fetch_or_acquire():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(and, and, L)
         :                      ATOMIC64_OPS(or, orr, L)
    0.00 :   ffff8000101db108:       b       ffff8000101dd110 <test_clear_page_writeback+0x450>
    0.00 :   ffff8000101db10c:       mov     x0, x1
         :                      lock_page():
    0.00 :   ffff8000101db110:       tbz     w0, #0, ffff8000101db0b0 <set_page_dirty_lock+0x48>
    0.00 :   ffff8000101db114:       b       ffff8000101db0a8 <set_page_dirty_lock+0x40>
         :                      set_page_dirty_lock():
    0.00 :   ffff8000101db118:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (506 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010caf310 <mutex_lock>:
         :                      mutex_lock():
         :                      * deadlock debugging)
         :                      *
         :                      * This function is similar to (but not equivalent to) down().
         :                      */
         :                      void __sched mutex_lock(struct mutex *lock)
         :                      {
    0.80 :   ffff800010caf310:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff800010caf314:       mov     x3, x0
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    5.37 :   ffff800010caf318:       mrs     x2, sp_el0
         :                      mutex_lock():
    0.39 :   ffff800010caf31c:       mov     x29, sp
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010caf320:       b       ffff800010caf344 <mutex_lock+0x34>
    0.20 :   ffff800010caf324:       b       ffff800010caf344 <mutex_lock+0x34>
         :                      __lse__cmpxchg_case_acq_64():
         :                      __CMPXCHG_CASE(w,  ,     , 32,   )
         :                      __CMPXCHG_CASE(x,  ,     , 64,   )
         :                      __CMPXCHG_CASE(w, b, acq_,  8,  a, "memory")
         :                      __CMPXCHG_CASE(w, h, acq_, 16,  a, "memory")
         :                      __CMPXCHG_CASE(w,  , acq_, 32,  a, "memory")
         :                      __CMPXCHG_CASE(x,  , acq_, 64,  a, "memory")
    0.00 :   ffff800010caf328:       mov     x1, #0x0                        // #0
    0.00 :   ffff800010caf32c:       mov     x4, x1
    2.18 :   ffff800010caf330:       casa    x4, x2, [x0]
   79.00 :   ffff800010caf334:       mov     x0, x4
         :                      __mutex_trylock_fast():
         :                      if (atomic_long_try_cmpxchg_acquire(&lock->owner, &zero, curr))
    0.00 :   ffff800010caf338:       cbnz    x0, ffff800010caf350 <mutex_lock+0x40>
         :                      mutex_lock():
         :                      might_sleep();
         :
         :                      if (!__mutex_trylock_fast(lock))
         :                      __mutex_lock_slowpath(lock);
         :                      }
   11.48 :   ffff800010caf33c:       ldp     x29, x30, [sp], #16
    0.59 :   ffff800010caf340:       ret
         :                      __ll_sc__cmpxchg_case_acq_64():
         :                      __CMPXCHG_CASE(w,  ,     , 32,        ,  ,  ,         , K)
         :                      __CMPXCHG_CASE( ,  ,     , 64,        ,  ,  ,         , L)
         :                      __CMPXCHG_CASE(w, b, acq_,  8,        , a,  , "memory", K)
         :                      __CMPXCHG_CASE(w, h, acq_, 16,        , a,  , "memory", K)
         :                      __CMPXCHG_CASE(w,  , acq_, 32,        , a,  , "memory", K)
         :                      __CMPXCHG_CASE( ,  , acq_, 64,        , a,  , "memory", L)
    0.00 :   ffff800010caf344:       mov     x1, #0x0                        // #0
    0.00 :   ffff800010caf348:       b       ffff800010caffd8 <ww_mutex_lock_interruptible+0x218>
         :                      __mutex_trylock_fast():
         :                      if (atomic_long_try_cmpxchg_acquire(&lock->owner, &zero, curr))
    0.00 :   ffff800010caf34c:       cbz     x0, ffff800010caf33c <mutex_lock+0x2c>
    0.00 :   ffff800010caf350:       mov     x0, x3
         :                      mutex_lock():
         :                      __mutex_lock_slowpath(lock);
    0.00 :   ffff800010caf354:       bl      ffff800010caf2f8 <__mutex_lock_slowpath>
         :                      }
    0.00 :   ffff800010caf358:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010caf35c:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (499 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001046fbe8 <blk_queue_bounce>:
         :                      blk_queue_bounce():
         :                      mempool_t *pool;
         :
         :                      /*
         :                      * Data-less bio, nothing to bounce
         :                      */
         :                      if (!bio_has_data(*bio_orig))
    2.41 :   ffff80001046fbe8:       ldr     x3, [x1]
         :                      bio_has_data():
         :                      /*
         :                      * Check whether this bio carries any data or not. A NULL bio is allowed.
         :                      */
         :                      static inline bool bio_has_data(struct bio *bio)
         :                      {
         :                      if (bio &&
    0.00 :   ffff80001046fbec:       cbz     x3, ffff80001046fc28 <blk_queue_bounce+0x40>
         :                      bio->bi_iter.bi_size &&
    5.20 :   ffff80001046fbf0:       ldr     w6, [x3, #40]
         :                      if (bio &&
    0.00 :   ffff80001046fbf4:       cbz     w6, ffff80001046fc28 <blk_queue_bounce+0x40>
         :                      blk_queue_bounce():
         :                      {
   11.20 :   ffff80001046fbf8:       stp     x29, x30, [sp, #-112]!
    0.00 :   ffff80001046fbfc:       mov     x29, sp
    2.41 :   ffff80001046fc00:       str     x23, [sp, #48]
         :                      bio_has_data():
         :                      bio_op(bio) != REQ_OP_DISCARD &&
    0.60 :   ffff80001046fc04:       ldr     w23, [x3, #16]
    0.00 :   ffff80001046fc08:       and     w2, w23, #0xff
         :                      bio->bi_iter.bi_size &&
    0.00 :   ffff80001046fc0c:       sub     w4, w2, #0x3
         :                      bio_op(bio) != REQ_OP_SECURE_ERASE &&
    0.00 :   ffff80001046fc10:       tst     w4, #0xfffffffd
    0.00 :   ffff80001046fc14:       ccmp    w2, #0x9, #0x4, ne  // ne = any
    0.00 :   ffff80001046fc18:       b.ne    ffff80001046fc2c <blk_queue_bounce+0x44>  // b.any
         :                      blk_queue_bounce():
         :
         :                      /*
         :                      * slow path
         :                      */
         :                      __blk_queue_bounce(q, bio_orig, pool);
         :                      }
   42.69 :   ffff80001046fc1c:       ldr     x23, [sp, #48]
    1.01 :   ffff80001046fc20:       ldp     x29, x30, [sp], #112
    0.00 :   ffff80001046fc24:       ret
    0.00 :   ffff80001046fc28:       ret
    0.20 :   ffff80001046fc2c:       stp     x20, x21, [x29, #24]
         :                      if (!(q->bounce_gfp & GFP_DMA)) {
   11.42 :   ffff80001046fc30:       ldr     w4, [x0, #120]
    0.00 :   ffff80001046fc34:       tbnz    w4, #0, ffff80001046fc70 <blk_queue_bounce+0x88>
         :                      if (q->limits.bounce_pfn >= blk_max_pfn)
    0.20 :   ffff80001046fc38:       adrp    x4, ffff800011abc000 <drbg_algs+0x2b80>
    0.00 :   ffff80001046fc3c:       ldr     x5, [x0, #1040]
    1.00 :   ffff80001046fc40:       ldr     x4, [x4, #608]
    0.00 :   ffff80001046fc44:       cmp     x5, x4
    0.00 :   ffff80001046fc48:       b.cs    ffff80001046ff8c <blk_queue_bounce+0x3a4>  // b.hs, b.nlast
         :                      pool = &page_pool;
    0.00 :   ffff80001046fc4c:       adrp    x20, ffff800011abc000 <drbg_algs+0x2b80>
    0.00 :   ffff80001046fc50:       add     x21, x20, #0xa90
    0.00 :   ffff80001046fc54:       add     x21, x21, #0x48
    0.00 :   ffff80001046fc58:       str     x19, [x29, #16]
    0.00 :   ffff80001046fc5c:       str     x22, [x29, #40]
    0.00 :   ffff80001046fc60:       stp     x24, x25, [x29, #56]
    0.00 :   ffff80001046fc64:       stp     x26, x27, [x29, #72]
    0.00 :   ffff80001046fc68:       str     x28, [x29, #88]
    0.00 :   ffff80001046fc6c:       b       ffff80001046fc98 <blk_queue_bounce+0xb0>
         :                      BUG_ON(!mempool_initialized(&isa_page_pool));
    0.00 :   ffff80001046fc70:       adrp    x20, ffff800011abc000 <drbg_algs+0x2b80>
    0.00 :   ffff80001046fc74:       add     x4, x20, #0xa90
         :                      pool = &isa_page_pool;
    0.00 :   ffff80001046fc78:       mov     x21, x4
         :                      BUG_ON(!mempool_initialized(&isa_page_pool));
    0.00 :   ffff80001046fc7c:       ldr     x4, [x4, #16]
    0.00 :   ffff80001046fc80:       str     x19, [x29, #16]
    0.00 :   ffff80001046fc84:       str     x22, [x29, #40]
    0.00 :   ffff80001046fc88:       stp     x24, x25, [x29, #56]
    0.00 :   ffff80001046fc8c:       stp     x26, x27, [x29, #72]
    0.00 :   ffff80001046fc90:       str     x28, [x29, #88]
    0.00 :   ffff80001046fc94:       cbz     x4, ffff800010470288 <blk_queue_bounce+0x6a0>
    0.00 :   ffff80001046fc98:       ldr     w10, [x3, #16]
    0.00 :   ffff80001046fc9c:       mov     x22, x0
         :                      bio_is_passthrough():
         :
         :                      static inline bool bio_is_passthrough(struct bio *bio)
         :                      {
         :                      unsigned op = bio_op(bio);
         :
         :                      return blk_op_is_scsi(op) || blk_op_is_private(op);
    0.00 :   ffff80001046fca0:       sub     w2, w2, #0x20
         :                      bio_no_advance_iter():
         :                      return false;
         :                      }
         :
         :                      static inline bool bio_no_advance_iter(struct bio *bio)
         :                      {
         :                      return bio_op(bio) == REQ_OP_DISCARD ||
    0.00 :   ffff80001046fca4:       mov     w0, #0xfb                       // #251
    0.00 :   ffff80001046fca8:       and     w9, w10, #0xff
         :                      bio_is_passthrough():
    0.00 :   ffff80001046fcac:       cmp     w2, #0x3
         :                      bio_no_advance_iter():
    0.00 :   ffff80001046fcb0:       sub     w9, w9, #0x5
    0.00 :   ffff80001046fcb4:       mov     x19, x1
    0.00 :   ffff80001046fcb8:       and     w10, w10, w0
         :                      bio_is_passthrough():
    0.00 :   ffff80001046fcbc:       cset    w26, ls  // ls = plast
         :                      __blk_queue_bounce():
         :                      bool bounce = false;
    0.00 :   ffff80001046fcc0:       mov     w25, #0x0                       // #0
         :                      int sectors = 0;
    0.00 :   ffff80001046fcc4:       mov     w1, #0x0                        // #0
    0.00 :   ffff80001046fcc8:       adrp    x24, ffff8000112ae000 <cpu_ops+0x248>
         :                      bio_for_each_segment(from, *bio_orig, iter) {
    0.00 :   ffff80001046fccc:       mov     w13, #0x1000                    // #4096
         :                      bounce = true;
    0.00 :   ffff80001046fcd0:       mov     w11, #0x1                       // #1
         :                      bio_for_each_segment(from, *bio_orig, iter) {
    0.00 :   ffff80001046fcd4:       mov     w0, #0x0                        // #0
    0.00 :   ffff80001046fcd8:       ldp     w7, w2, [x3, #44]
         :                      bio_no_advance_iter():
    0.00 :   ffff80001046fcdc:       and     w9, w9, #0xfffffffb
    0.00 :   ffff80001046fce0:       ldr     x8, [x3, #104]
    0.00 :   ffff80001046fce4:       ldr     x12, [x22, #1040]
    0.00 :   ffff80001046fce8:       b       ffff80001046fcf8 <blk_queue_bounce+0x110>
         :                      bio_advance_iter():
         :                      unsigned bytes)
         :                      {
         :                      iter->bi_sector += bytes >> 9;
         :
         :                      if (bio_no_advance_iter(bio))
         :                      iter->bi_size -= bytes;
    0.00 :   ffff80001046fcec:       sub     w6, w6, w4
    0.00 :   ffff80001046fcf0:       add     w0, w0, #0x1
         :                      __blk_queue_bounce():
    0.00 :   ffff80001046fcf4:       cbz     w6, ffff80001046fdc0 <blk_queue_bounce+0x1d8>
    0.00 :   ffff80001046fcf8:       ubfiz   x4, x7, #4, #32
         :                      if (page_to_pfn(from.bv_page) > q->limits.bounce_pfn)
    0.00 :   ffff80001046fcfc:       ldr     x15, [x24, #1880]
         :                      bio_for_each_segment(from, *bio_orig, iter) {
    0.00 :   ffff80001046fd00:       add     x14, x8, x4
    0.00 :   ffff80001046fd04:       ldr     x16, [x8, x4]
    0.00 :   ffff80001046fd08:       ldp     w5, w3, [x14, #8]
    0.00 :   ffff80001046fd0c:       add     w3, w2, w3
    0.00 :   ffff80001046fd10:       sub     w4, w5, w2
    0.00 :   ffff80001046fd14:       cmp     w4, w6
    0.00 :   ffff80001046fd18:       and     w14, w3, #0xfff
    0.00 :   ffff80001046fd1c:       sub     w14, w13, w14
    0.00 :   ffff80001046fd20:       csel    w4, w4, w6, ls  // ls = plast
    0.00 :   ffff80001046fd24:       lsr     w3, w3, #12
    0.00 :   ffff80001046fd28:       cmp     w4, w14
    0.00 :   ffff80001046fd2c:       csel    w4, w4, w14, ls  // ls = plast
         :                      sectors += from.bv_len >> 9;
    0.00 :   ffff80001046fd30:       cmp     w0, #0x100
         :                      bio_for_each_segment(from, *bio_orig, iter) {
    0.00 :   ffff80001046fd34:       add     x3, x16, x3, lsl #6
         :                      if (page_to_pfn(from.bv_page) > q->limits.bounce_pfn)
    0.00 :   ffff80001046fd38:       sub     x3, x3, x15
         :                      sectors += from.bv_len >> 9;
    0.00 :   ffff80001046fd3c:       add     w14, w1, w4, lsr #9
    0.00 :   ffff80001046fd40:       csel    w1, w14, w1, cc  // cc = lo, ul, last
         :                      bounce = true;
    0.00 :   ffff80001046fd44:       cmp     x12, x3, asr #6
    0.00 :   ffff80001046fd48:       csel    w25, w25, w11, cs  // cs = hs, nlast
         :                      bio_no_advance_iter():
         :                      bio_op(bio) == REQ_OP_WRITE_SAME ||
    0.00 :   ffff80001046fd4c:       cmp     w9, #0x0
    0.00 :   ffff80001046fd50:       ccmp    w10, #0x3, #0x4, ne  // ne = any
    0.00 :   ffff80001046fd54:       b.eq    ffff80001046fcec <blk_queue_bounce+0x104>  // b.none
         :                      bvec_iter_advance():
         :                      static inline bool bvec_iter_advance(const struct bio_vec *bv,
         :                      struct bvec_iter *iter, unsigned bytes)
         :                      {
         :                      unsigned int idx = iter->bi_idx;
         :
         :                      if (WARN_ONCE(bytes > iter->bi_size,
    0.00 :   ffff80001046fd58:       cmp     w4, w6
    0.00 :   ffff80001046fd5c:       b.hi    ffff80001046fd94 <blk_queue_bounce+0x1ac>  // b.pmore
         :                      "Attempted to advance past end of bvec iter\n")) {
         :                      iter->bi_size = 0;
         :                      return false;
         :                      }
         :
         :                      iter->bi_size -= bytes;
    0.00 :   ffff80001046fd60:       sub     w6, w6, w4
         :                      bytes += iter->bi_bvec_done;
         :
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff80001046fd64:       adds    w2, w2, w4
    0.00 :   ffff80001046fd68:       b.ne    ffff80001046fd88 <blk_queue_bounce+0x1a0>  // b.any
    0.00 :   ffff80001046fd6c:       b       ffff80001046fcf0 <blk_queue_bounce+0x108>
         :                      bytes -= bv[idx].bv_len;
         :                      idx++;
    0.00 :   ffff80001046fd70:       add     w7, w7, #0x1
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff80001046fd74:       subs    w2, w2, w5
    0.00 :   ffff80001046fd78:       b.eq    ffff80001046fcf0 <blk_queue_bounce+0x108>  // b.none
    0.00 :   ffff80001046fd7c:       ubfiz   x3, x7, #4, #32
    0.00 :   ffff80001046fd80:       add     x3, x8, x3
    0.00 :   ffff80001046fd84:       ldr     w5, [x3, #8]
    0.00 :   ffff80001046fd88:       cmp     w5, w2
    0.00 :   ffff80001046fd8c:       b.ls    ffff80001046fd70 <blk_queue_bounce+0x188>  // b.plast
    0.00 :   ffff80001046fd90:       b       ffff80001046fcf0 <blk_queue_bounce+0x108>
         :                      if (WARN_ONCE(bytes > iter->bi_size,
    0.00 :   ffff80001046fd94:       adrp    x2, ffff800011a5d000 <__print_once.28906>
    0.00 :   ffff80001046fd98:       ldrb    w0, [x2, #76]
    0.00 :   ffff80001046fd9c:       cbnz    w0, ffff80001046fdc0 <blk_queue_bounce+0x1d8>
    0.00 :   ffff80001046fda0:       mov     w3, #0x1                        // #1
    0.00 :   ffff80001046fda4:       str     w1, [x29, #104]
    0.00 :   ffff80001046fda8:       strb    w3, [x2, #76]
    0.00 :   ffff80001046fdac:       adrp    x0, ffff8000111aa000 <kallsyms_token_index+0x30330>
    0.00 :   ffff80001046fdb0:       add     x0, x0, #0xea8
    0.00 :   ffff80001046fdb4:       bl      ffff8000100e5288 <__warn_printk>
    0.00 :   ffff80001046fdb8:       brk     #0x800
    0.00 :   ffff80001046fdbc:       ldr     w1, [x29, #104]
         :                      __blk_queue_bounce():
         :                      if (!bounce)
    0.00 :   ffff80001046fdc0:       cbz     w25, ffff80001046ff74 <blk_queue_bounce+0x38c>
         :                      bio = bounce_clone_bio(*bio_orig, GFP_NOIO, passthrough ? NULL :
    0.00 :   ffff80001046fdc4:       mov     x2, #0x0                        // #0
    0.00 :   ffff80001046fdc8:       ldr     x27, [x19]
         :                      if (!passthrough && sectors < bio_sectors(*bio_orig)) {
    0.00 :   ffff80001046fdcc:       cbnz    w26, ffff80001046fde4 <blk_queue_bounce+0x1fc>
    0.00 :   ffff80001046fdd0:       ldr     w0, [x27, #40]
    0.00 :   ffff80001046fdd4:       cmp     w1, w0, lsr #9
    0.00 :   ffff80001046fdd8:       b.cc    ffff800010470258 <blk_queue_bounce+0x670>  // b.lo, b.ul, b.last
         :                      bio = bounce_clone_bio(*bio_orig, GFP_NOIO, passthrough ? NULL :
    0.00 :   ffff80001046fddc:       add     x2, x20, #0xa90
    0.00 :   ffff80001046fde0:       add     x2, x2, #0x98
         :                      bio_segments():
         :                      /*
         :                      * We special case discard/write same/write zeroes, because they
         :                      * interpret bi_size differently:
         :                      */
         :
         :                      switch (bio_op(bio)) {
    0.00 :   ffff80001046fde4:       ldrb    w0, [x27, #16]
    0.00 :   ffff80001046fde8:       cmp     w0, #0x5
    0.00 :   ffff80001046fdec:       b.eq    ffff800010470070 <blk_queue_bounce+0x488>  // b.none
    0.00 :   ffff80001046fdf0:       b.ls    ffff800010470098 <blk_queue_bounce+0x4b0>  // b.plast
         :                      case REQ_OP_DISCARD:
         :                      case REQ_OP_SECURE_ERASE:
         :                      case REQ_OP_WRITE_ZEROES:
         :                      return 0;
         :                      case REQ_OP_WRITE_SAME:
         :                      return 1;
    0.00 :   ffff80001046fdf4:       mov     w1, #0x1                        // #1
         :                      switch (bio_op(bio)) {
    0.00 :   ffff80001046fdf8:       cmp     w0, #0x7
    0.00 :   ffff80001046fdfc:       b.ne    ffff800010470068 <blk_queue_bounce+0x480>  // b.any
         :                      bounce_clone_bio():
         :                      bio = bio_alloc_bioset(gfp_mask, bio_segments(bio_src), bs);
    0.00 :   ffff80001046fe00:       mov     w0, #0xc00                      // #3072
    0.00 :   ffff80001046fe04:       bl      ffff8000104504e0 <bio_alloc_bioset>
    0.00 :   ffff80001046fe08:       mov     x25, x0
         :                      if (!bio)
    0.00 :   ffff80001046fe0c:       cbz     x0, ffff80001046fed4 <blk_queue_bounce+0x2ec>
         :                      bio->bi_disk            = bio_src->bi_disk;
    0.00 :   ffff80001046fe10:       ldr     x0, [x27, #8]
    0.00 :   ffff80001046fe14:       str     x0, [x25, #8]
         :                      bio->bi_opf             = bio_src->bi_opf;
    0.00 :   ffff80001046fe18:       ldr     w0, [x27, #16]
    0.00 :   ffff80001046fe1c:       str     w0, [x25, #16]
         :                      switch (bio_op(bio)) {
    0.00 :   ffff80001046fe20:       and     w0, w0, #0xff
         :                      bio->bi_ioprio          = bio_src->bi_ioprio;
    0.00 :   ffff80001046fe24:       ldrh    w1, [x27, #22]
         :                      switch (bio_op(bio)) {
    0.00 :   ffff80001046fe28:       cmp     w0, #0x5
         :                      bio->bi_ioprio          = bio_src->bi_ioprio;
    0.00 :   ffff80001046fe2c:       strh    w1, [x25, #22]
         :                      bio->bi_write_hint      = bio_src->bi_write_hint;
    0.00 :   ffff80001046fe30:       ldrh    w1, [x27, #24]
    0.00 :   ffff80001046fe34:       strh    w1, [x25, #24]
         :                      bio->bi_iter.bi_sector  = bio_src->bi_iter.bi_sector;
    0.00 :   ffff80001046fe38:       ldr     x1, [x27, #32]
    0.00 :   ffff80001046fe3c:       str     x1, [x25, #32]
         :                      bio->bi_iter.bi_size    = bio_src->bi_iter.bi_size;
    0.00 :   ffff80001046fe40:       ldr     w1, [x27, #40]
    0.00 :   ffff80001046fe44:       str     w1, [x25, #40]
         :                      switch (bio_op(bio)) {
    0.00 :   ffff80001046fe48:       b.eq    ffff800010470060 <blk_queue_bounce+0x478>  // b.none
    0.00 :   ffff80001046fe4c:       b.ls    ffff800010470248 <blk_queue_bounce+0x660>  // b.plast
    0.00 :   ffff80001046fe50:       cmp     w0, #0x7
    0.00 :   ffff80001046fe54:       b.ne    ffff800010470158 <blk_queue_bounce+0x570>  // b.any
         :                      bio->bi_io_vec[bio->bi_vcnt++] = bio_src->bi_io_vec[0];
    0.00 :   ffff80001046fe58:       ldrh    w0, [x25, #96]
    0.00 :   ffff80001046fe5c:       ldr     x2, [x25, #104]
    0.00 :   ffff80001046fe60:       ldr     x1, [x27, #104]
    0.00 :   ffff80001046fe64:       add     w3, w0, #0x1
    0.00 :   ffff80001046fe68:       ubfiz   x0, x0, #4, #16
    0.00 :   ffff80001046fe6c:       strh    w3, [x25, #96]
    0.00 :   ffff80001046fe70:       add     x2, x2, x0
    0.00 :   ffff80001046fe74:       ldp     x0, x1, [x1]
    0.00 :   ffff80001046fe78:       stp     x0, x1, [x2]
    0.00 :   ffff80001046fe7c:       ldr     w2, [x27, #16]
         :                      bio_integrity():
         :
         :                      #if defined(CONFIG_BLK_DEV_INTEGRITY)
         :
         :                      static inline struct bio_integrity_payload *bio_integrity(struct bio *bio)
         :                      {
         :                      if (bio->bi_opf & REQ_INTEGRITY)
    0.00 :   ffff80001046fe80:       tbz     w2, #16, ffff80001046fea0 <blk_queue_bounce+0x2b8>
         :                      bounce_clone_bio():
         :                      if (bio_integrity(bio_src)) {
    0.00 :   ffff80001046fe84:       ldr     x0, [x27, #88]
    0.00 :   ffff80001046fe88:       cbz     x0, ffff80001046fea0 <blk_queue_bounce+0x2b8>
         :                      ret = bio_integrity_clone(bio, bio_src, gfp_mask);
    0.00 :   ffff80001046fe8c:       mov     w2, #0xc00                      // #3072
    0.00 :   ffff80001046fe90:       mov     x1, x27
    0.00 :   ffff80001046fe94:       mov     x0, x25
    0.00 :   ffff80001046fe98:       bl      ffff80001047a9c0 <bio_integrity_clone>
         :                      if (ret < 0) {
    0.00 :   ffff80001046fe9c:       tbnz    w0, #31, ffff8000104702c4 <blk_queue_bounce+0x6dc>
         :                      bio_clone_blkg_association(bio, bio_src);
    0.00 :   ffff80001046fea0:       mov     x1, x27
    0.00 :   ffff80001046fea4:       mov     x0, x25
    0.00 :   ffff80001046fea8:       bl      ffff80001044fd58 <bio_clone_blkg_association>
         :                      blkcg_bio_issue_init():
         :                      return false;
         :                      }
         :
         :                      static inline void blkcg_bio_issue_init(struct bio *bio)
         :                      {
         :                      bio_issue_init(&bio->bi_issue, bio_sectors(bio));
    0.00 :   ffff80001046feac:       ldr     w26, [x25, #40]
         :                      bio_issue_init():
         :
         :                      static inline void bio_issue_init(struct bio_issue *issue,
         :                      sector_t size)
         :                      {
         :                      size &= (1ULL << BIO_ISSUE_SIZE_BITS) - 1;
         :                      issue->value = ((issue->value & BIO_ISSUE_RES_MASK) |
    0.00 :   ffff80001046feb0:       ldr     x27, [x25, #80]
         :                      ktime_get_ns():
         :                      return ktime_mono_to_any(mono, TK_OFFS_REAL);
         :                      }
         :
         :                      static inline u64 ktime_get_ns(void)
         :                      {
         :                      return ktime_to_ns(ktime_get());
    0.00 :   ffff80001046feb4:       bl      ffff80001016ad10 <ktime_get>
         :                      bio_issue_init():
         :                      (ktime_get_ns() & BIO_ISSUE_TIME_MASK) |
    0.00 :   ffff80001046feb8:       and     x0, x0, #0x7ffffffffffff
         :                      blkcg_bio_issue_init():
    0.00 :   ffff80001046febc:       lsr     w26, w26, #9
         :                      bio_issue_init():
         :                      issue->value = ((issue->value & BIO_ISSUE_RES_MASK) |
    0.00 :   ffff80001046fec0:       and     x27, x27, #0x8000000000000000
         :                      ((u64)size << BIO_ISSUE_SIZE_SHIFT));
    0.00 :   ffff80001046fec4:       ubfiz   x26, x26, #51, #12
         :                      (ktime_get_ns() & BIO_ISSUE_TIME_MASK) |
    0.00 :   ffff80001046fec8:       orr     x26, x26, x27
    0.00 :   ffff80001046fecc:       orr     x0, x26, x0
         :                      issue->value = ((issue->value & BIO_ISSUE_RES_MASK) |
    0.00 :   ffff80001046fed0:       str     x0, [x25, #80]
         :                      __blk_queue_bounce():
         :                      for (i = 0, to = bio->bi_io_vec; i < bio->bi_vcnt; to++, i++) {
    0.00 :   ffff80001046fed4:       ldrh    w4, [x25, #96]
         :                      op_is_write():
         :                      bio->bi_opf = op | op_flags;
         :                      }
         :
         :                      static inline bool op_is_write(unsigned int op)
         :                      {
         :                      return (op & 1);
    0.00 :   ffff80001046fed8:       and     w23, w23, #0x1
         :                      __blk_queue_bounce():
    0.00 :   ffff80001046fedc:       ldr     x26, [x25, #104]
    0.00 :   ffff80001046fee0:       cbz     w4, ffff80001046ff30 <blk_queue_bounce+0x348>
    0.00 :   ffff80001046fee4:       mov     w27, #0x0                       // #0
         :                      if (page_to_pfn(page) <= q->limits.bounce_pfn)
    0.00 :   ffff80001046fee8:       ldr     x0, [x24, #1880]
         :                      struct page *page = to->bv_page;
    0.00 :   ffff80001046feec:       ldr     x28, [x26]
         :                      if (page_to_pfn(page) <= q->limits.bounce_pfn)
    0.00 :   ffff80001046fef0:       ldr     x1, [x22, #1040]
    0.00 :   ffff80001046fef4:       sub     x0, x28, x0
    0.00 :   ffff80001046fef8:       cmp     x1, x0, asr #6
    0.00 :   ffff80001046fefc:       b.cs    ffff80001046ff20 <blk_queue_bounce+0x338>  // b.hs, b.nlast
         :                      to->bv_page = mempool_alloc(pool, q->bounce_gfp);
    0.00 :   ffff80001046ff00:       ldr     w1, [x22, #120]
    0.00 :   ffff80001046ff04:       mov     x0, x21
    0.00 :   ffff80001046ff08:       bl      ffff8000101d5c38 <mempool_alloc>
    0.00 :   ffff80001046ff0c:       str     x0, [x26]
         :                      inc_zone_page_state(to->bv_page, NR_BOUNCE);
    0.00 :   ffff80001046ff10:       mov     w1, #0xa                        // #10
    0.00 :   ffff80001046ff14:       bl      ffff8000101f3b68 <inc_zone_page_state>
         :                      if (rw == WRITE) {
    0.00 :   ffff80001046ff18:       cbnz    w23, ffff80001046ff94 <blk_queue_bounce+0x3ac>
    0.00 :   ffff80001046ff1c:       ldrh    w4, [x25, #96]
         :                      for (i = 0, to = bio->bi_io_vec; i < bio->bi_vcnt; to++, i++) {
    0.00 :   ffff80001046ff20:       add     w27, w27, #0x1
    0.00 :   ffff80001046ff24:       add     x26, x26, #0x10
    0.00 :   ffff80001046ff28:       cmp     w27, w4
    0.00 :   ffff80001046ff2c:       b.cc    ffff80001046fee8 <blk_queue_bounce+0x300>  // b.lo, b.ul, b.last
         :                      bio->bi_flags |= (1 << BIO_BOUNCED);
    0.00 :   ffff80001046ff30:       ldrh    w0, [x25, #20]
         :                      if (pool == &page_pool) {
    0.00 :   ffff80001046ff34:       add     x20, x20, #0xa90
    0.00 :   ffff80001046ff38:       add     x20, x20, #0x48
         :                      bio->bi_flags |= (1 << BIO_BOUNCED);
    0.00 :   ffff80001046ff3c:       orr     w0, w0, #0x4
    0.00 :   ffff80001046ff40:       strh    w0, [x25, #20]
         :                      if (pool == &page_pool) {
    0.00 :   ffff80001046ff44:       cmp     x21, x20
    0.00 :   ffff80001046ff48:       b.eq    ffff800010470078 <blk_queue_bounce+0x490>  // b.none
         :                      bio->bi_end_io = bounce_end_io_write_isa;
    0.00 :   ffff80001046ff4c:       adrp    x1, ffff80001046f000 <__rq_qos_cleanup+0x8>
    0.00 :   ffff80001046ff50:       adrp    x0, ffff80001046f000 <__rq_qos_cleanup+0x8>
    0.00 :   ffff80001046ff54:       add     x1, x1, #0xa78
    0.00 :   ffff80001046ff58:       add     x0, x0, #0x7b0
    0.00 :   ffff80001046ff5c:       cmp     w23, #0x0
    0.00 :   ffff80001046ff60:       csel    x0, x0, x1, ne  // ne = any
    0.00 :   ffff80001046ff64:       str     x0, [x25, #56]
         :                      bio->bi_private = *bio_orig;
    0.00 :   ffff80001046ff68:       ldr     x0, [x19]
    0.00 :   ffff80001046ff6c:       str     x0, [x25, #64]
         :                      *bio_orig = bio;
    0.00 :   ffff80001046ff70:       str     x25, [x19]
    0.00 :   ffff80001046ff74:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff80001046ff78:       ldp     x21, x22, [x29, #32]
    0.00 :   ffff80001046ff7c:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff80001046ff80:       ldp     x26, x27, [x29, #72]
    0.00 :   ffff80001046ff84:       ldr     x28, [x29, #88]
    0.00 :   ffff80001046ff88:       b       ffff80001046fc1c <blk_queue_bounce+0x34>
   21.67 :   ffff80001046ff8c:       ldp     x20, x21, [x29, #24]
    0.00 :   ffff80001046ff90:       b       ffff80001046fc1c <blk_queue_bounce+0x34>
         :                      flush_dcache_page(page);
    0.00 :   ffff80001046ff94:       mov     x0, x28
    0.00 :   ffff80001046ff98:       bl      ffff8000100a2608 <flush_dcache_page>
         :                      lowmem_page_address():
         :                      */
         :                      #include <linux/vmstat.h>
         :
         :                      static __always_inline void *lowmem_page_address(const struct page *page)
         :                      {
         :                      return page_to_virt(page);
    0.00 :   ffff80001046ff9c:       ldr     x0, [x26]
    0.00 :   ffff80001046ffa0:       mov     x1, #0x200000                   // #2097152
    0.00 :   ffff80001046ffa4:       movk    x1, #0x200, lsl #32
         :                      __blk_queue_bounce():
         :                      vto = page_address(to->bv_page) + to->bv_offset;
    0.00 :   ffff80001046ffa8:       ldr     w5, [x26, #12]
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80001046ffac:       mrs     x4, sp_el0
         :                      lowmem_page_address():
    0.00 :   ffff80001046ffb0:       add     x0, x0, x1
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001046ffb4:       ldr     w1, [x4, #16]
         :                      lowmem_page_address():
    0.00 :   ffff80001046ffb8:       lsr     x0, x0, #6
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff80001046ffbc:       add     w1, w1, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001046ffc0:       str     w1, [x4, #16]
         :                      lowmem_page_address():
    0.00 :   ffff80001046ffc4:       mov     x1, #0xffff000000000000         // #-281474976710656
    0.00 :   ffff80001046ffc8:       add     x0, x1, x0, lsl #12
         :                      __blk_queue_bounce():
    0.00 :   ffff80001046ffcc:       add     x0, x0, x5
         :                      pagefault_disabled_inc():
         :                      }
         :                      #endif
         :
         :                      static __always_inline void pagefault_disabled_inc(void)
         :                      {
         :                      current->pagefault_disabled++;
    0.00 :   ffff80001046ffd0:       ldr     w1, [x4, #2448]
    0.00 :   ffff80001046ffd4:       str     x4, [x29, #104]
    0.00 :   ffff80001046ffd8:       add     w1, w1, #0x1
    0.00 :   ffff80001046ffdc:       str     w1, [x4, #2448]
         :                      lowmem_page_address():
    0.00 :   ffff80001046ffe0:       mov     x1, #0x200000                   // #2097152
    0.00 :   ffff80001046ffe4:       mov     x3, #0xffff000000000000         // #-281474976710656
    0.00 :   ffff80001046ffe8:       movk    x1, #0x200, lsl #32
    0.00 :   ffff80001046ffec:       add     x1, x28, x1
         :                      __blk_queue_bounce():
         :                      vfrom = kmap_atomic(page) + to->bv_offset;
    0.00 :   ffff80001046fff0:       ldp     w2, w5, [x26, #8]
         :                      lowmem_page_address():
    0.00 :   ffff80001046fff4:       lsr     x1, x1, #6
    0.00 :   ffff80001046fff8:       add     x1, x3, x1, lsl #12
         :                      __blk_queue_bounce():
         :                      memcpy(vto, vfrom, to->bv_len);
    0.00 :   ffff80001046fffc:       add     x1, x1, x5
    0.00 :   ffff800010470000:       bl      ffff800010c92540 <__memcpy>
         :                      pagefault_disabled_dec():
         :                      }
         :
         :                      static __always_inline void pagefault_disabled_dec(void)
         :                      {
         :                      current->pagefault_disabled--;
    0.00 :   ffff800010470004:       ldr     x4, [x29, #104]
    0.00 :   ffff800010470008:       ldr     w0, [x4, #2448]
    0.00 :   ffff80001047000c:       sub     w0, w0, #0x1
    0.00 :   ffff800010470010:       str     w0, [x4, #2448]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010470014:       ldr     x0, [x4, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800010470018:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001047001c:       str     w0, [x4, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800010470020:       cbz     x0, ffff80001047002c <blk_queue_bounce+0x444>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010470024:       ldr     x0, [x4, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff800010470028:       cbnz    x0, ffff80001046ff1c <blk_queue_bounce+0x334>
         :                      __kunmap_atomic():
         :                      #define kmap_atomic_prot(page, prot)    kmap_atomic(page)
         :
         :                      static inline void __kunmap_atomic(void *addr)
         :                      {
         :                      pagefault_enable();
         :                      preempt_enable();
    0.00 :   ffff80001047002c:       bl      ffff800010cad640 <preempt_schedule>
    0.00 :   ffff800010470030:       ldrh    w4, [x25, #96]
    0.00 :   ffff800010470034:       b       ffff80001046ff20 <blk_queue_bounce+0x338>
         :                      bvec_iter_advance():
    0.00 :   ffff800010470038:       adrp    x1, ffff800011a5d000 <__print_once.28906>
    0.00 :   ffff80001047003c:       ldrb    w0, [x1, #76]
    0.00 :   ffff800010470040:       cbnz    w0, ffff80001046fe80 <blk_queue_bounce+0x298>
    0.00 :   ffff800010470044:       mov     w2, #0x1                        // #1
    0.00 :   ffff800010470048:       adrp    x0, ffff8000111aa000 <kallsyms_token_index+0x30330>
    0.00 :   ffff80001047004c:       strb    w2, [x1, #76]
    0.00 :   ffff800010470050:       add     x0, x0, #0xea8
    0.00 :   ffff800010470054:       bl      ffff8000100e5288 <__warn_printk>
    0.00 :   ffff800010470058:       brk     #0x800
    0.00 :   ffff80001047005c:       nop
    0.00 :   ffff800010470060:       ldr     w2, [x27, #16]
    0.00 :   ffff800010470064:       b       ffff80001046fe80 <blk_queue_bounce+0x298>
         :                      bio_segments():
         :                      switch (bio_op(bio)) {
    0.00 :   ffff800010470068:       cmp     w0, #0x9
    0.00 :   ffff80001047006c:       b.ne    ffff8000104700a0 <blk_queue_bounce+0x4b8>  // b.any
         :                      return 0;
    0.00 :   ffff800010470070:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010470074:       b       ffff80001046fe00 <blk_queue_bounce+0x218>
         :                      __blk_queue_bounce():
         :                      bio->bi_end_io = bounce_end_io_write;
    0.00 :   ffff800010470078:       adrp    x1, ffff80001046f000 <__rq_qos_cleanup+0x8>
    0.00 :   ffff80001047007c:       adrp    x0, ffff80001046f000 <__rq_qos_cleanup+0x8>
    0.00 :   ffff800010470080:       add     x1, x1, #0xab8
    0.00 :   ffff800010470084:       add     x0, x0, #0x7d0
    0.00 :   ffff800010470088:       cmp     w23, #0x0
    0.00 :   ffff80001047008c:       csel    x0, x0, x1, ne  // ne = any
    0.00 :   ffff800010470090:       str     x0, [x25, #56]
    0.00 :   ffff800010470094:       b       ffff80001046ff68 <blk_queue_bounce+0x380>
         :                      bio_segments():
         :                      switch (bio_op(bio)) {
    0.00 :   ffff800010470098:       cmp     w0, #0x3
    0.00 :   ffff80001047009c:       b.eq    ffff800010470070 <blk_queue_bounce+0x488>  // b.none
         :                      bio_for_each_segment(bv, bio, iter)
    0.00 :   ffff8000104700a0:       ldp     w7, w5, [x27, #40]
         :                      unsigned segs = 0;
    0.00 :   ffff8000104700a4:       mov     w1, #0x0                        // #0
         :                      bio_for_each_segment(bv, bio, iter)
    0.00 :   ffff8000104700a8:       ldr     w3, [x27, #48]
    0.00 :   ffff8000104700ac:       mov     w10, #0x1000                    // #4096
         :                      bio_no_advance_iter():
         :                      return bio_op(bio) == REQ_OP_DISCARD ||
    0.00 :   ffff8000104700b0:       mov     w9, #0xfb                       // #251
    0.00 :   ffff8000104700b4:       nop
         :                      bio_segments():
         :                      bio_for_each_segment(bv, bio, iter)
    0.00 :   ffff8000104700b8:       cbz     w7, ffff80001046fe00 <blk_queue_bounce+0x218>
    0.00 :   ffff8000104700bc:       ldr     x8, [x27, #104]
    0.00 :   ffff8000104700c0:       ubfiz   x0, x5, #4, #32
         :                      segs++;
    0.00 :   ffff8000104700c4:       add     w1, w1, #0x1
         :                      bio_for_each_segment(bv, bio, iter)
    0.00 :   ffff8000104700c8:       add     x0, x8, x0
    0.00 :   ffff8000104700cc:       ldp     w4, w0, [x0, #8]
    0.00 :   ffff8000104700d0:       sub     w6, w4, w3
    0.00 :   ffff8000104700d4:       add     w0, w3, w0
    0.00 :   ffff8000104700d8:       cmp     w6, w7
    0.00 :   ffff8000104700dc:       and     w0, w0, #0xfff
    0.00 :   ffff8000104700e0:       csel    w6, w6, w7, ls  // ls = plast
    0.00 :   ffff8000104700e4:       sub     w0, w10, w0
    0.00 :   ffff8000104700e8:       cmp     w0, w6
    0.00 :   ffff8000104700ec:       csel    w0, w0, w6, ls  // ls = plast
         :                      bio_advance_iter():
         :                      if (bio_no_advance_iter(bio))
    0.00 :   ffff8000104700f0:       ldr     w6, [x27, #16]
         :                      bio_no_advance_iter():
         :                      return bio_op(bio) == REQ_OP_DISCARD ||
    0.00 :   ffff8000104700f4:       and     w11, w6, #0xff
    0.00 :   ffff8000104700f8:       and     w6, w6, w9
    0.00 :   ffff8000104700fc:       sub     w11, w11, #0x5
         :                      bio_op(bio) == REQ_OP_WRITE_SAME ||
    0.00 :   ffff800010470100:       tst     w11, #0xfffffffb
    0.00 :   ffff800010470104:       ccmp    w6, #0x3, #0x4, ne  // ne = any
    0.00 :   ffff800010470108:       b.ne    ffff800010470114 <blk_queue_bounce+0x52c>  // b.any
         :                      bio_advance_iter():
         :                      iter->bi_size -= bytes;
    0.00 :   ffff80001047010c:       sub     w7, w7, w0
    0.00 :   ffff800010470110:       b       ffff8000104700b8 <blk_queue_bounce+0x4d0>
         :                      bvec_iter_advance():
    0.00 :   ffff800010470114:       cmp     w0, w7
    0.00 :   ffff800010470118:       b.hi    ffff80001047028c <blk_queue_bounce+0x6a4>  // b.pmore
         :                      iter->bi_size -= bytes;
    0.00 :   ffff80001047011c:       sub     w7, w7, w0
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff800010470120:       adds    w3, w3, w0
    0.00 :   ffff800010470124:       b.eq    ffff8000104700b8 <blk_queue_bounce+0x4d0>  // b.none
    0.00 :   ffff800010470128:       cmp     w4, w3
    0.00 :   ffff80001047012c:       b.ls    ffff800010470148 <blk_queue_bounce+0x560>  // b.plast
    0.00 :   ffff800010470130:       b       ffff8000104700b8 <blk_queue_bounce+0x4d0>
    0.00 :   ffff800010470134:       ubfiz   x0, x5, #4, #32
    0.00 :   ffff800010470138:       add     x0, x8, x0
    0.00 :   ffff80001047013c:       ldr     w4, [x0, #8]
    0.00 :   ffff800010470140:       cmp     w4, w3
    0.00 :   ffff800010470144:       b.hi    ffff8000104700b8 <blk_queue_bounce+0x4d0>  // b.pmore
         :                      idx++;
    0.00 :   ffff800010470148:       add     w5, w5, #0x1
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff80001047014c:       subs    w3, w3, w4
    0.00 :   ffff800010470150:       b.ne    ffff800010470134 <blk_queue_bounce+0x54c>  // b.any
    0.00 :   ffff800010470154:       b       ffff8000104700b8 <blk_queue_bounce+0x4d0>
         :                      bounce_clone_bio():
         :                      switch (bio_op(bio)) {
    0.00 :   ffff800010470158:       cmp     w0, #0x9
    0.00 :   ffff80001047015c:       b.eq    ffff800010470060 <blk_queue_bounce+0x478>  // b.none
         :                      bio_for_each_segment(bv, bio_src, iter)
    0.00 :   ffff800010470160:       ldp     w5, w3, [x27, #40]
    0.00 :   ffff800010470164:       mov     w8, #0x1000                     // #4096
    0.00 :   ffff800010470168:       ldr     w1, [x27, #48]
         :                      bio_no_advance_iter():
         :                      return bio_op(bio) == REQ_OP_DISCARD ||
    0.00 :   ffff80001047016c:       mov     w7, #0xfb                       // #251
         :                      bounce_clone_bio():
    0.00 :   ffff800010470170:       cbz     w5, ffff800010470060 <blk_queue_bounce+0x478>
    0.00 :   ffff800010470174:       ldr     x2, [x27, #104]
    0.00 :   ffff800010470178:       ubfiz   x6, x3, #4, #32
         :                      bio->bi_io_vec[bio->bi_vcnt++] = bv;
    0.00 :   ffff80001047017c:       ldr     x11, [x25, #104]
         :                      bio_for_each_segment(bv, bio_src, iter)
    0.00 :   ffff800010470180:       add     x0, x2, x6
    0.00 :   ffff800010470184:       ldr     x9, [x2, x6]
    0.00 :   ffff800010470188:       ldr     w2, [x0, #12]
    0.00 :   ffff80001047018c:       ldr     w0, [x0, #8]
    0.00 :   ffff800010470190:       add     w2, w1, w2
    0.00 :   ffff800010470194:       sub     w0, w0, w1
    0.00 :   ffff800010470198:       and     w10, w2, #0xfff
    0.00 :   ffff80001047019c:       cmp     w0, w5
    0.00 :   ffff8000104701a0:       sub     w4, w8, w10
    0.00 :   ffff8000104701a4:       csel    w0, w0, w5, ls  // ls = plast
    0.00 :   ffff8000104701a8:       lsr     w2, w2, #12
    0.00 :   ffff8000104701ac:       cmp     w0, w4
    0.00 :   ffff8000104701b0:       csel    w0, w0, w4, ls  // ls = plast
         :                      bio->bi_io_vec[bio->bi_vcnt++] = bv;
    0.00 :   ffff8000104701b4:       ldrh    w4, [x25, #96]
         :                      bio_for_each_segment(bv, bio_src, iter)
    0.00 :   ffff8000104701b8:       add     x2, x9, x2, lsl #6
         :                      bio->bi_io_vec[bio->bi_vcnt++] = bv;
    0.00 :   ffff8000104701bc:       add     w9, w4, #0x1
    0.00 :   ffff8000104701c0:       strh    w9, [x25, #96]
    0.00 :   ffff8000104701c4:       ubfiz   x4, x4, #4, #16
    0.00 :   ffff8000104701c8:       add     x9, x11, x4
    0.00 :   ffff8000104701cc:       str     x2, [x11, x4]
    0.00 :   ffff8000104701d0:       stp     w0, w10, [x9, #8]
         :                      bio_advance_iter():
         :                      if (bio_no_advance_iter(bio))
    0.00 :   ffff8000104701d4:       ldr     w2, [x27, #16]
         :                      bio_no_advance_iter():
         :                      return bio_op(bio) == REQ_OP_DISCARD ||
    0.00 :   ffff8000104701d8:       and     w4, w2, #0xff
    0.00 :   ffff8000104701dc:       and     w9, w2, w7
    0.00 :   ffff8000104701e0:       sub     w4, w4, #0x5
         :                      bio_op(bio) == REQ_OP_WRITE_SAME ||
    0.00 :   ffff8000104701e4:       tst     w4, #0xfffffffb
    0.00 :   ffff8000104701e8:       ccmp    w9, #0x3, #0x4, ne  // ne = any
    0.00 :   ffff8000104701ec:       b.ne    ffff8000104701f8 <blk_queue_bounce+0x610>  // b.any
         :                      bio_advance_iter():
         :                      iter->bi_size -= bytes;
    0.00 :   ffff8000104701f0:       sub     w5, w5, w0
    0.00 :   ffff8000104701f4:       b       ffff800010470170 <blk_queue_bounce+0x588>
         :                      bvec_iter_advance():
         :                      if (WARN_ONCE(bytes > iter->bi_size,
    0.00 :   ffff8000104701f8:       cmp     w0, w5
         :                      bio_advance_iter():
         :                      bvec_iter_advance(bio->bi_io_vec, iter, bytes);
    0.00 :   ffff8000104701fc:       ldr     x4, [x27, #104]
         :                      bvec_iter_advance():
    0.00 :   ffff800010470200:       b.hi    ffff800010470038 <blk_queue_bounce+0x450>  // b.pmore
         :                      iter->bi_size -= bytes;
    0.00 :   ffff800010470204:       sub     w5, w5, w0
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff800010470208:       adds    w1, w1, w0
    0.00 :   ffff80001047020c:       b.eq    ffff800010470170 <blk_queue_bounce+0x588>  // b.none
    0.00 :   ffff800010470210:       add     x6, x4, x6
    0.00 :   ffff800010470214:       ldr     w0, [x6, #8]
    0.00 :   ffff800010470218:       cmp     w1, w0
    0.00 :   ffff80001047021c:       b.cs    ffff800010470238 <blk_queue_bounce+0x650>  // b.hs, b.nlast
    0.00 :   ffff800010470220:       b       ffff800010470170 <blk_queue_bounce+0x588>
    0.00 :   ffff800010470224:       ubfiz   x0, x3, #4, #32
    0.00 :   ffff800010470228:       add     x0, x4, x0
    0.00 :   ffff80001047022c:       ldr     w0, [x0, #8]
    0.00 :   ffff800010470230:       cmp     w0, w1
    0.00 :   ffff800010470234:       b.hi    ffff800010470170 <blk_queue_bounce+0x588>  // b.pmore
         :                      idx++;
    0.00 :   ffff800010470238:       add     w3, w3, #0x1
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff80001047023c:       subs    w1, w1, w0
    0.00 :   ffff800010470240:       b.ne    ffff800010470224 <blk_queue_bounce+0x63c>  // b.any
    0.00 :   ffff800010470244:       b       ffff800010470170 <blk_queue_bounce+0x588>
         :                      bounce_clone_bio():
         :                      switch (bio_op(bio)) {
    0.00 :   ffff800010470248:       cmp     w0, #0x3
    0.00 :   ffff80001047024c:       b.ne    ffff800010470160 <blk_queue_bounce+0x578>  // b.any
    0.00 :   ffff800010470250:       ldr     w2, [x27, #16]
    0.00 :   ffff800010470254:       b       ffff80001046fe80 <blk_queue_bounce+0x298>
         :                      __blk_queue_bounce():
         :                      bio = bio_split(*bio_orig, sectors, GFP_NOIO, &bounce_bio_split);
    0.00 :   ffff800010470258:       add     x3, x20, #0xa90
    0.00 :   ffff80001047025c:       mov     w2, #0xc00                      // #3072
    0.00 :   ffff800010470260:       add     x3, x3, #0x208
    0.00 :   ffff800010470264:       mov     x0, x27
    0.00 :   ffff800010470268:       bl      ffff8000104507c8 <bio_split>
    0.00 :   ffff80001047026c:       mov     x27, x0
         :                      bio_chain(bio, *bio_orig);
    0.00 :   ffff800010470270:       ldr     x1, [x19]
    0.00 :   ffff800010470274:       bl      ffff80001044f360 <bio_chain>
         :                      generic_make_request(*bio_orig);
    0.00 :   ffff800010470278:       ldr     x0, [x19]
    0.00 :   ffff80001047027c:       bl      ffff800010454aa0 <generic_make_request>
         :                      *bio_orig = bio;
    0.00 :   ffff800010470280:       str     x27, [x19]
    0.00 :   ffff800010470284:       b       ffff80001046fddc <blk_queue_bounce+0x1f4>
         :                      blk_queue_bounce():
         :                      BUG_ON(!mempool_initialized(&isa_page_pool));
    0.00 :   ffff800010470288:       brk     #0x800
         :                      bvec_iter_advance():
         :                      if (WARN_ONCE(bytes > iter->bi_size,
    0.00 :   ffff80001047028c:       adrp    x3, ffff800011a5d000 <__print_once.28906>
    0.00 :   ffff800010470290:       ldrb    w0, [x3, #76]
    0.00 :   ffff800010470294:       cbnz    w0, ffff80001046fe00 <blk_queue_bounce+0x218>
    0.00 :   ffff800010470298:       mov     w4, #0x1                        // #1
    0.00 :   ffff80001047029c:       str     w1, [x29, #100]
    0.00 :   ffff8000104702a0:       strb    w4, [x3, #76]
    0.00 :   ffff8000104702a4:       adrp    x0, ffff8000111aa000 <kallsyms_token_index+0x30330>
    0.00 :   ffff8000104702a8:       str     x2, [x29, #104]
    0.00 :   ffff8000104702ac:       add     x0, x0, #0xea8
    0.00 :   ffff8000104702b0:       bl      ffff8000100e5288 <__warn_printk>
    0.00 :   ffff8000104702b4:       brk     #0x800
    0.00 :   ffff8000104702b8:       ldr     w1, [x29, #100]
    0.00 :   ffff8000104702bc:       ldr     x2, [x29, #104]
    0.00 :   ffff8000104702c0:       b       ffff80001046fe00 <blk_queue_bounce+0x218>
         :                      bounce_clone_bio():
         :                      bio_put(bio);
    0.00 :   ffff8000104702c4:       mov     x0, x25
         :                      return NULL;
    0.00 :   ffff8000104702c8:       mov     x25, #0x0                       // #0
         :                      bio_put(bio);
    0.00 :   ffff8000104702cc:       bl      ffff800010450020 <bio_put>
    0.00 :   ffff8000104702d0:       b       ffff80001046fed4 <blk_queue_bounce+0x2ec>
 Percent |	Source code & Disassembly of vmlinux for cycles (504 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101f1160 <page_mapping>:
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
   31.55 :   ffff8000101f1160:       ldr     x1, [x0, #8]
         :                      compound_head():
         :                      static inline struct page *compound_head(struct page *page)
         :                      {
         :                      unsigned long head = READ_ONCE(page->compound_head);
         :
         :                      if (unlikely(head & 1))
         :                      return (struct page *) (head - 1);
    0.00 :   ffff8000101f1164:       sub     x2, x1, #0x1
    0.00 :   ffff8000101f1168:       tst     x1, #0x1
    0.00 :   ffff8000101f116c:       csel    x0, x2, x0, ne  // ne = any
         :                      __read_once_size():
   29.17 :   ffff8000101f1170:       ldr     x2, [x0, #8]
         :                      compound_head():
    0.00 :   ffff8000101f1174:       sub     x1, x2, #0x1
    0.00 :   ffff8000101f1178:       tst     x2, #0x1
    0.00 :   ffff8000101f117c:       csel    x1, x1, x0, ne  // ne = any
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
   14.85 :   ffff8000101f1180:       ldr     x1, [x1]
         :                      page_mapping():
         :                      struct address_space *mapping;
         :
         :                      page = compound_head(page);
         :
         :                      /* This happens if someone calls flush_dcache_page on slab page */
         :                      if (unlikely(PageSlab(page)))
    0.00 :   ffff8000101f1184:       tst     w1, #0x200
    0.00 :   ffff8000101f1188:       b.ne    ffff8000101f11ec <page_mapping+0x8c>  // b.any
         :                      __read_once_size():
   16.47 :   ffff8000101f118c:       ldr     x2, [x0, #8]
         :                      compound_head():
    0.00 :   ffff8000101f1190:       sub     x1, x2, #0x1
    0.00 :   ffff8000101f1194:       tst     x2, #0x1
    0.00 :   ffff8000101f1198:       csel    x1, x1, x0, ne  // ne = any
         :                      test_bit():
    2.01 :   ffff8000101f119c:       ldr     x1, [x1]
         :                      PageSwapCache():
         :                      static __always_inline int PageSwapCache(struct page *page)
         :                      {
         :                      #ifdef CONFIG_THP_SWAP
         :                      page = compound_head(page);
         :                      #endif
         :                      return PageSwapBacked(page) && test_bit(PG_swapcache, &page->flags);
    0.00 :   ffff8000101f11a0:       tst     w1, #0x80000
    0.00 :   ffff8000101f11a4:       b.eq    ffff8000101f11d8 <page_mapping+0x78>  // b.none
         :                      test_bit():
    3.57 :   ffff8000101f11a8:       ldr     x1, [x0]
         :                      PageSwapCache():
    0.00 :   ffff8000101f11ac:       tst     w1, #0x400
    0.00 :   ffff8000101f11b0:       b.eq    ffff8000101f11d8 <page_mapping+0x78>  // b.none
         :                      page_mapping():
         :                      return NULL;
         :
         :                      if (unlikely(PageSwapCache(page))) {
         :                      swp_entry_t entry;
         :
         :                      entry.val = page_private(page);
    0.00 :   ffff8000101f11b4:       ldr     x0, [x0, #40]
         :                      return swap_address_space(entry);
    0.00 :   ffff8000101f11b8:       adrp    x1, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff8000101f11bc:       add     x1, x1, #0x4a8
    0.00 :   ffff8000101f11c0:       lsr     x2, x0, #58
    0.00 :   ffff8000101f11c4:       ubfx    x0, x0, #14, #44
    0.00 :   ffff8000101f11c8:       add     x0, x0, x0, lsl #2
    0.00 :   ffff8000101f11cc:       ldr     x1, [x1, x2, lsl #3]
    0.00 :   ffff8000101f11d0:       add     x0, x1, x0, lsl #5
         :                      mapping = page->mapping;
         :                      if ((unsigned long)mapping & PAGE_MAPPING_ANON)
         :                      return NULL;
         :
         :                      return (void *)((unsigned long)mapping & ~PAGE_MAPPING_FLAGS);
         :                      }
    0.00 :   ffff8000101f11d4:       ret
         :                      if ((unsigned long)mapping & PAGE_MAPPING_ANON)
    2.18 :   ffff8000101f11d8:       ldr     x1, [x0, #24]
         :                      return (void *)((unsigned long)mapping & ~PAGE_MAPPING_FLAGS);
    0.00 :   ffff8000101f11dc:       and     x0, x1, #0xfffffffffffffffc
    0.20 :   ffff8000101f11e0:       tst     x1, #0x1
    0.00 :   ffff8000101f11e4:       csel    x0, x0, xzr, eq  // eq = none
         :                      }
    0.00 :   ffff8000101f11e8:       ret
         :                      return NULL;
    0.00 :   ffff8000101f11ec:       mov     x0, #0x0                        // #0
         :                      }
    0.00 :   ffff8000101f11f0:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (477 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101cfaf8 <unlock_page>:
         :                      unlock_page():
         :                      * clear the PG_locked bit and test PG_waiters at the same time fairly
         :                      * portably (architectures that do LL/SC can test any bit, while x86 can
         :                      * test the sign bit).
         :                      */
         :                      void unlock_page(struct page *page)
         :                      {
    0.00 :   ffff8000101cfaf8:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff8000101cfafc:       mov     x29, sp
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    4.39 :   ffff8000101cfb00:       ldr     x1, [x0, #8]
         :                      compound_head():
         :                      static inline struct page *compound_head(struct page *page)
         :                      {
         :                      unsigned long head = READ_ONCE(page->compound_head);
         :
         :                      if (unlikely(head & 1))
         :                      return (struct page *) (head - 1);
    0.00 :   ffff8000101cfb04:       sub     x2, x1, #0x1
    0.00 :   ffff8000101cfb08:       tst     x1, #0x1
    0.00 :   ffff8000101cfb0c:       csel    x0, x2, x0, ne  // ne = any
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    9.43 :   ffff8000101cfb10:       b       ffff8000101cfb34 <unlock_page+0x3c>
    0.00 :   ffff8000101cfb14:       b       ffff8000101cfb34 <unlock_page+0x3c>
         :                      __lse_atomic64_fetch_andnot_release():
         :                      ATOMIC64_FETCH_OP(_relaxed,   , op, asm_op)                     \
         :                      ATOMIC64_FETCH_OP(_acquire,  a, op, asm_op, "memory")           \
         :                      ATOMIC64_FETCH_OP(_release,  l, op, asm_op, "memory")           \
         :                      ATOMIC64_FETCH_OP(        , al, op, asm_op, "memory")
         :
         :                      ATOMIC64_FETCH_OPS(andnot, ldclr)
    0.00 :   ffff8000101cfb18:       mov     x1, #0x1                        // #1
    0.00 :   ffff8000101cfb1c:       ldclrl  x1, x1, [x0]
         :                      unlock_page():
         :                      BUILD_BUG_ON(PG_waiters != 7);
         :                      page = compound_head(page);
         :                      VM_BUG_ON_PAGE(!PageLocked(page), page);
         :                      if (clear_bit_unlock_is_negative_byte(PG_locked, &page->flags))
   86.17 :   ffff8000101cfb20:       tbz     w1, #7, ffff8000101cfb2c <unlock_page+0x34>
         :                      wake_up_page_bit(page, PG_locked);
    0.00 :   ffff8000101cfb24:       mov     w1, #0x0                        // #0
    0.00 :   ffff8000101cfb28:       bl      ffff8000101cf9a8 <wake_up_page_bit>
         :                      }
    0.00 :   ffff8000101cfb2c:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000101cfb30:       ret
         :                      __ll_sc_atomic64_fetch_andnot_release():
         :                      /*
         :                      * GAS converts the mysterious and undocumented BIC (immediate) alias to
         :                      * an AND (immediate) instruction with the immediate inverted. We don't
         :                      * have a constraint for this, so fall back to register.
         :                      */
         :                      ATOMIC64_OPS(andnot, bic, )
    0.00 :   ffff8000101cfb34:       mov     x2, #0x1                        // #1
    0.00 :   ffff8000101cfb38:       b       ffff8000101d5270 <generic_file_write_iter+0x238>
    0.00 :   ffff8000101cfb3c:       b       ffff8000101cfb20 <unlock_page+0x28>
 Percent |	Source code & Disassembly of vmlinux for cycles (453 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010482ae0 <iov_iter_revert>:
         :                      iov_iter_revert():
         :                      }
         :                      EXPORT_SYMBOL(iov_iter_advance);
         :
         :                      void iov_iter_revert(struct iov_iter *i, size_t unroll)
         :                      {
         :                      if (!unroll)
    8.82 :   ffff800010482ae0:       cbz     x1, ffff800010482b88 <iov_iter_revert+0xa8>
         :                      {
    0.00 :   ffff800010482ae4:       stp     x29, x30, [sp, #-64]!
         :                      return;
         :                      if (WARN_ON(unroll > MAX_RW_COUNT))
    0.00 :   ffff800010482ae8:       mov     x2, #0x7ffff000                 // #2147479552
    0.00 :   ffff800010482aec:       cmp     x1, x2
         :                      {
    0.00 :   ffff800010482af0:       mov     x29, sp
         :                      if (WARN_ON(unroll > MAX_RW_COUNT))
   11.72 :   ffff800010482af4:       b.hi    ffff800010482bc0 <iov_iter_revert+0xe0>  // b.pmore
         :                      return;
         :                      i->count += unroll;
    1.55 :   ffff800010482af8:       ldr     x2, [x0, #16]
         :                      iov_iter_type():
         :                      };
         :                      };
         :
         :                      static inline enum iter_type iov_iter_type(const struct iov_iter *i)
         :                      {
         :                      return i->type & ~(READ | WRITE);
    2.88 :   ffff800010482afc:       ldr     w3, [x0]
         :                      iov_iter_revert():
    1.32 :   ffff800010482b00:       add     x2, x2, x1
    0.44 :   ffff800010482b04:       str     x2, [x0, #16]
         :                      iov_iter_type():
    0.00 :   ffff800010482b08:       and     w3, w3, #0xfffffffe
         :                      iov_iter_revert():
         :                      if (unlikely(iov_iter_is_pipe(i))) {
    0.00 :   ffff800010482b0c:       cmp     w3, #0x20
    0.00 :   ffff800010482b10:       b.eq    ffff800010482bd8 <iov_iter_revert+0xf8>  // b.none
         :                      i->iov_offset = off;
         :                      i->head = i_head;
         :                      pipe_truncate(i);
         :                      return;
         :                      }
         :                      if (unlikely(iov_iter_is_discard(i)))
   20.72 :   ffff800010482b14:       cmp     w3, #0x40
    0.00 :   ffff800010482b18:       b.eq    ffff800010482b80 <iov_iter_revert+0xa0>  // b.none
         :                      return;
         :                      if (unroll <= i->iov_offset) {
    1.33 :   ffff800010482b1c:       ldr     x4, [x0, #8]
    0.00 :   ffff800010482b20:       cmp     x1, x4
    0.00 :   ffff800010482b24:       b.ls    ffff800010482bc8 <iov_iter_revert+0xe8>  // b.plast
    5.32 :   ffff800010482b28:       ldp     x5, x2, [x0, #24]
         :                      i->iov_offset -= unroll;
         :                      return;
         :                      }
         :                      unroll -= i->iov_offset;
         :                      if (iov_iter_is_bvec(i)) {
    0.00 :   ffff800010482b2c:       cmp     w3, #0x10
         :                      unroll -= i->iov_offset;
    0.00 :   ffff800010482b30:       sub     x1, x1, x4
    0.00 :   ffff800010482b34:       add     x6, x2, #0x1
   25.20 :   ffff800010482b38:       sub     x3, x5, #0x10
         :                      if (iov_iter_is_bvec(i)) {
    0.00 :   ffff800010482b3c:       b.eq    ffff800010482b8c <iov_iter_revert+0xac>  // b.none
         :                      unroll -= n;
         :                      }
         :                      } else { /* same logics for iovec and kvec */
         :                      const struct iovec *iov = i->iov;
         :                      while (1) {
         :                      size_t n = (--iov)->iov_len;
    0.00 :   ffff800010482b40:       ldur    x4, [x5, #-8]
    0.00 :   ffff800010482b44:       add     x2, x2, #0x2
         :                      i->nr_segs++;
   16.08 :   ffff800010482b48:       str     x6, [x0, #32]
         :                      if (unroll <= n) {
    0.00 :   ffff800010482b4c:       cmp     x1, x4
    0.00 :   ffff800010482b50:       b.ls    ffff800010482b74 <iov_iter_revert+0x94>  // b.plast
    0.00 :   ffff800010482b54:       nop
         :                      size_t n = (--iov)->iov_len;
    0.00 :   ffff800010482b58:       sub     x3, x3, #0x10
         :                      i->iov = iov;
         :                      i->iov_offset = n - unroll;
         :                      return;
         :                      }
         :                      unroll -= n;
    0.00 :   ffff800010482b5c:       sub     x1, x1, x4
         :                      size_t n = (--iov)->iov_len;
    0.00 :   ffff800010482b60:       ldr     x4, [x3, #8]
         :                      i->nr_segs++;
    0.00 :   ffff800010482b64:       str     x2, [x0, #32]
    0.00 :   ffff800010482b68:       add     x2, x2, #0x1
         :                      if (unroll <= n) {
    0.00 :   ffff800010482b6c:       cmp     x4, x1
    0.00 :   ffff800010482b70:       b.cc    ffff800010482b58 <iov_iter_revert+0x78>  // b.lo, b.ul, b.last
         :                      i->iov_offset = n - unroll;
    0.00 :   ffff800010482b74:       sub     x1, x4, x1
    4.63 :   ffff800010482b78:       str     x1, [x0, #8]
         :                      i->iov = iov;
    0.00 :   ffff800010482b7c:       str     x3, [x0, #24]
         :                      }
         :                      }
         :                      }
    0.00 :   ffff800010482b80:       ldp     x29, x30, [sp], #64
    0.00 :   ffff800010482b84:       ret
    0.00 :   ffff800010482b88:       ret
         :                      size_t n = (--bvec)->bv_len;
    0.00 :   ffff800010482b8c:       ldur    w4, [x5, #-8]
    0.00 :   ffff800010482b90:       add     x2, x2, #0x2
         :                      i->nr_segs++;
    0.00 :   ffff800010482b94:       str     x6, [x0, #32]
         :                      if (unroll <= n) {
    0.00 :   ffff800010482b98:       cmp     x4, x1
    0.00 :   ffff800010482b9c:       b.cs    ffff800010482b74 <iov_iter_revert+0x94>  // b.hs, b.nlast
         :                      size_t n = (--bvec)->bv_len;
    0.00 :   ffff800010482ba0:       sub     x3, x3, #0x10
         :                      unroll -= n;
    0.00 :   ffff800010482ba4:       sub     x1, x1, x4
         :                      size_t n = (--bvec)->bv_len;
    0.00 :   ffff800010482ba8:       ldr     w4, [x3, #8]
         :                      i->nr_segs++;
    0.00 :   ffff800010482bac:       str     x2, [x0, #32]
    0.00 :   ffff800010482bb0:       add     x2, x2, #0x1
         :                      if (unroll <= n) {
    0.00 :   ffff800010482bb4:       cmp     x4, x1
    0.00 :   ffff800010482bb8:       b.cc    ffff800010482ba0 <iov_iter_revert+0xc0>  // b.lo, b.ul, b.last
    0.00 :   ffff800010482bbc:       b       ffff800010482b74 <iov_iter_revert+0x94>
         :                      if (WARN_ON(unroll > MAX_RW_COUNT))
    0.00 :   ffff800010482bc0:       brk     #0x800
    0.00 :   ffff800010482bc4:       b       ffff800010482b80 <iov_iter_revert+0xa0>
         :                      i->iov_offset -= unroll;
    0.00 :   ffff800010482bc8:       sub     x1, x4, x1
    0.00 :   ffff800010482bcc:       str     x1, [x0, #8]
         :                      }
    0.00 :   ffff800010482bd0:       ldp     x29, x30, [sp], #64
    0.00 :   ffff800010482bd4:       ret
    0.00 :   ffff800010482bd8:       stp     x19, x20, [x29, #16]
         :                      struct pipe_buffer *b = &pipe->bufs[i_head & p_mask];
    0.00 :   ffff800010482bdc:       mov     w7, #0x28                       // #40
    0.00 :   ffff800010482be0:       stp     x21, x22, [x29, #32]
         :                      b = &pipe->bufs[i_head & p_mask];
    0.00 :   ffff800010482be4:       mov     w6, w7
         :                      struct pipe_inode_info *pipe = i->pipe;
    0.00 :   ffff800010482be8:       ldr     x21, [x0, #24]
         :                      unsigned int i_head = i->head;
    0.00 :   ffff800010482bec:       ldr     w19, [x0, #32]
         :                      size_t off = i->iov_offset;
    0.00 :   ffff800010482bf0:       ldr     x3, [x0, #8]
         :                      unsigned int p_mask = pipe->ring_size - 1;
    0.00 :   ffff800010482bf4:       ldr     w4, [x21, #68]
         :                      struct pipe_buffer *b = &pipe->bufs[i_head & p_mask];
    0.00 :   ffff800010482bf8:       ldr     x5, [x21, #120]
         :                      unsigned int p_mask = pipe->ring_size - 1;
    0.00 :   ffff800010482bfc:       sub     w4, w4, #0x1
         :                      struct pipe_buffer *b = &pipe->bufs[i_head & p_mask];
    0.00 :   ffff800010482c00:       and     w2, w4, w19
         :                      size_t n = off - b->offset;
    0.00 :   ffff800010482c04:       umaddl  x2, w2, w7, x5
    0.00 :   ffff800010482c08:       ldr     w2, [x2, #8]
    0.00 :   ffff800010482c0c:       sub     x2, x3, x2
         :                      if (unroll < n) {
    0.00 :   ffff800010482c10:       cmp     x1, x2
    0.00 :   ffff800010482c14:       b.cc    ffff800010482c4c <iov_iter_revert+0x16c>  // b.lo, b.ul, b.last
         :                      if (!unroll && i_head == i->start_head) {
    0.00 :   ffff800010482c18:       subs    x1, x1, x2
    0.00 :   ffff800010482c1c:       b.ne    ffff800010482c2c <iov_iter_revert+0x14c>  // b.any
    0.00 :   ffff800010482c20:       ldr     w2, [x0, #36]
    0.00 :   ffff800010482c24:       cmp     w2, w19
    0.00 :   ffff800010482c28:       b.eq    ffff800010482ce4 <iov_iter_revert+0x204>  // b.none
         :                      i_head--;
    0.00 :   ffff800010482c2c:       sub     w19, w19, #0x1
         :                      b = &pipe->bufs[i_head & p_mask];
    0.00 :   ffff800010482c30:       and     w3, w4, w19
    0.00 :   ffff800010482c34:       umaddl  x3, w3, w6, x5
         :                      off = b->offset + b->len;
    0.00 :   ffff800010482c38:       ldp     w2, w3, [x3, #8]
    0.00 :   ffff800010482c3c:       add     w3, w2, w3
         :                      size_t n = off - b->offset;
    0.00 :   ffff800010482c40:       sub     x2, x3, w2, uxtw
         :                      if (unroll < n) {
    0.00 :   ffff800010482c44:       cmp     x2, x1
    0.00 :   ffff800010482c48:       b.ls    ffff800010482c18 <iov_iter_revert+0x138>  // b.plast
         :                      off -= unroll;
    0.00 :   ffff800010482c4c:       sub     x1, x3, x1
         :                      i->iov_offset = off;
    0.00 :   ffff800010482c50:       str     x1, [x0, #8]
         :                      i->head = i_head;
    0.00 :   ffff800010482c54:       str     w19, [x0, #32]
         :                      pipe_truncate():
         :                      if (!pipe_empty(p_head, p_tail)) {
    0.00 :   ffff800010482c58:       ldp     w20, w0, [x21, #56]
    0.00 :   ffff800010482c5c:       cmp     w0, w20
    0.00 :   ffff800010482c60:       b.eq    ffff800010482d08 <iov_iter_revert+0x228>  // b.none
    0.00 :   ffff800010482c64:       str     x23, [x29, #48]
         :                      unsigned int p_mask = pipe->ring_size - 1;
    0.00 :   ffff800010482c68:       ldr     w22, [x21, #68]
    0.00 :   ffff800010482c6c:       sub     w22, w22, #0x1
         :                      if (off) {
    0.00 :   ffff800010482c70:       cbz     x1, ffff800010482c94 <iov_iter_revert+0x1b4>
         :                      buf = &pipe->bufs[i_head & p_mask];
    0.00 :   ffff800010482c74:       ldr     x2, [x21, #120]
    0.00 :   ffff800010482c78:       and     w0, w19, w22
    0.00 :   ffff800010482c7c:       mov     w3, #0x28                       // #40
    0.00 :   ffff800010482c80:       add     w19, w19, #0x1
    0.00 :   ffff800010482c84:       umaddl  x0, w0, w3, x2
         :                      buf->len = off - buf->offset;
    0.00 :   ffff800010482c88:       ldr     w2, [x0, #8]
    0.00 :   ffff800010482c8c:       sub     w1, w1, w2
    0.00 :   ffff800010482c90:       str     w1, [x0, #12]
         :                      pipe_buf_release(pipe, &pipe->bufs[p_head & p_mask]);
    0.00 :   ffff800010482c94:       mov     w23, #0x28                      // #40
         :                      while (p_head != i_head) {
    0.00 :   ffff800010482c98:       cmp     w19, w20
    0.00 :   ffff800010482c9c:       b.eq    ffff800010482cd0 <iov_iter_revert+0x1f0>  // b.none
         :                      pipe_buf_release(pipe, &pipe->bufs[p_head & p_mask]);
    0.00 :   ffff800010482ca0:       ldr     x1, [x21, #120]
         :                      p_head--;
    0.00 :   ffff800010482ca4:       sub     w20, w20, #0x1
         :                      pipe_buf_release(pipe, &pipe->bufs[p_head & p_mask]);
    0.00 :   ffff800010482ca8:       and     w2, w20, w22
         :                      pipe_buf_release():
         :                      struct pipe_buffer *buf)
         :                      {
         :                      const struct pipe_buf_operations *ops = buf->ops;
         :
         :                      buf->ops = NULL;
         :                      ops->release(pipe, buf);
    0.00 :   ffff800010482cac:       mov     x0, x21
         :                      pipe_truncate():
    0.00 :   ffff800010482cb0:       umaddl  x2, w2, w23, x1
         :                      pipe_buf_release():
    0.00 :   ffff800010482cb4:       mov     x1, x2
         :                      const struct pipe_buf_operations *ops = buf->ops;
    0.00 :   ffff800010482cb8:       ldr     x3, [x2, #16]
         :                      buf->ops = NULL;
    0.00 :   ffff800010482cbc:       str     xzr, [x2, #16]
         :                      ops->release(pipe, buf);
    0.00 :   ffff800010482cc0:       ldr     x2, [x3, #8]
    0.00 :   ffff800010482cc4:       blr     x2
         :                      pipe_truncate():
         :                      while (p_head != i_head) {
    0.00 :   ffff800010482cc8:       cmp     w20, w19
    0.00 :   ffff800010482ccc:       b.ne    ffff800010482ca0 <iov_iter_revert+0x1c0>  // b.any
         :                      pipe->head = p_head;
    0.00 :   ffff800010482cd0:       str     w19, [x21, #56]
    0.00 :   ffff800010482cd4:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff800010482cd8:       ldp     x21, x22, [x29, #32]
    0.00 :   ffff800010482cdc:       ldr     x23, [x29, #48]
    0.00 :   ffff800010482ce0:       b       ffff800010482b80 <iov_iter_revert+0xa0>
         :                      iov_iter_revert():
         :                      i->iov_offset = off;
    0.00 :   ffff800010482ce4:       str     xzr, [x0, #8]
         :                      i->head = i_head;
    0.00 :   ffff800010482ce8:       str     w19, [x0, #32]
         :                      pipe_truncate():
         :                      if (!pipe_empty(p_head, p_tail)) {
    0.00 :   ffff800010482cec:       ldp     w20, w0, [x21, #56]
    0.00 :   ffff800010482cf0:       cmp     w0, w20
    0.00 :   ffff800010482cf4:       b.eq    ffff800010482d08 <iov_iter_revert+0x228>  // b.none
    0.00 :   ffff800010482cf8:       str     x23, [x29, #48]
         :                      unsigned int p_mask = pipe->ring_size - 1;
    0.00 :   ffff800010482cfc:       ldr     w22, [x21, #68]
    0.00 :   ffff800010482d00:       sub     w22, w22, #0x1
    0.00 :   ffff800010482d04:       b       ffff800010482c94 <iov_iter_revert+0x1b4>
    0.00 :   ffff800010482d08:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff800010482d0c:       ldp     x21, x22, [x29, #32]
    0.00 :   ffff800010482d10:       b       ffff800010482b80 <iov_iter_revert+0xa0>
 Percent |	Source code & Disassembly of vmlinux for cycles (462 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010483cf8 <iov_iter_npages>:
         :                      iov_iter_npages():
         :                      #endif
         :                      }
         :                      EXPORT_SYMBOL(hash_and_copy_to_iter);
         :
         :                      int iov_iter_npages(const struct iov_iter *i, int maxpages)
         :                      {
    3.92 :   ffff800010483cf8:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff800010483cfc:       mov     x29, sp
    3.25 :   ffff800010483d00:       str     x19, [sp, #16]
         :                      size_t size = i->count;
    1.72 :   ffff800010483d04:       ldr     x7, [x0, #16]
         :                      int npages = 0;
         :
         :                      if (!size)
    0.00 :   ffff800010483d08:       cbz     x7, ffff800010483da8 <iov_iter_npages+0xb0>
    0.87 :   ffff800010483d0c:       str     x20, [x29, #24]
    0.00 :   ffff800010483d10:       mov     x20, x0
         :                      iov_iter_is_discard():
         :                      return iov_iter_type(i) == ITER_PIPE;
         :                      }
         :
         :                      static inline bool iov_iter_is_discard(const struct iov_iter *i)
         :                      {
         :                      return iov_iter_type(i) == ITER_DISCARD;
    2.39 :   ffff800010483d14:       ldr     w3, [x0]
         :                      iov_iter_type():
         :                      return i->type & ~(READ | WRITE);
    0.00 :   ffff800010483d18:       and     w4, w3, #0xfffffffe
         :                      iov_iter_npages():
         :                      return 0;
         :                      if (unlikely(iov_iter_is_discard(i)))
    0.00 :   ffff800010483d1c:       cmp     w4, #0x40
    0.00 :   ffff800010483d20:       b.eq    ffff800010483ebc <iov_iter_npages+0x1c4>  // b.none
    0.00 :   ffff800010483d24:       str     x21, [x29, #32]
         :                      return 0;
         :
         :                      if (unlikely(iov_iter_is_pipe(i))) {
    0.00 :   ffff800010483d28:       cmp     w4, #0x20
    0.00 :   ffff800010483d2c:       mov     w21, w1
    0.00 :   ffff800010483d30:       b.eq    ffff800010483ec4 <iov_iter_npages+0x1cc>  // b.none
         :                      data_start(i, &iter_head, &off);
         :                      /* some of this one + all after this one */
         :                      npages = pipe_space_for_user(iter_head, pipe->tail, pipe);
         :                      if (npages >= maxpages)
         :                      return maxpages;
         :                      } else iterate_all_kinds(i, size, v, ({
    3.02 :   ffff800010483d34:       ldr     x2, [x0, #8]
    0.00 :   ffff800010483d38:       mov     w4, #0x0                        // #0
         :                      int npages = 0;
    0.00 :   ffff800010483d3c:       mov     w19, #0x0                       // #0
         :                      } else iterate_all_kinds(i, size, v, ({
    1.52 :   ffff800010483d40:       mov     w8, #0x1000                     // #4096
    1.53 :   ffff800010483d44:       tbnz    w3, #4, ffff800010483dec <iov_iter_npages+0xf4>
    1.09 :   ffff800010483d48:       tbnz    w3, #3, ffff800010483f54 <iov_iter_npages+0x25c>
    4.32 :   ffff800010483d4c:       tbnz    w3, #6, ffff800010483da4 <iov_iter_npages+0xac>
    0.65 :   ffff800010483d50:       ldr     x1, [x0, #24]
    7.15 :   ffff800010483d54:       ldr     x0, [x1, #8]
    0.00 :   ffff800010483d58:       sub     x0, x0, x2
    0.00 :   ffff800010483d5c:       cmp     x0, x7
    0.00 :   ffff800010483d60:       csel    x0, x0, x7, ls  // ls = plast
    9.11 :   ffff800010483d64:       cbz     x0, ffff800010483e64 <iov_iter_npages+0x16c>
    0.00 :   ffff800010483d68:       ldr     x19, [x1]
    0.00 :   ffff800010483d6c:       add     x2, x19, x2
    0.00 :   ffff800010483d70:       add     x19, x0, x2
    0.00 :   ffff800010483d74:       add     x19, x19, #0xfff
   15.12 :   ffff800010483d78:       lsr     x2, x2, #12
    0.00 :   ffff800010483d7c:       lsr     x19, x19, #12
    0.00 :   ffff800010483d80:       sub     w19, w19, w2
    0.00 :   ffff800010483d84:       cmp     w19, w21
    0.00 :   ffff800010483d88:       b.lt    ffff800010483ea0 <iov_iter_npages+0x1a8>  // b.tstop
    0.00 :   ffff800010483d8c:       mov     w19, w21
         :                      if (npages >= maxpages)
         :                      return maxpages;
         :                      })
         :                      )
         :                      return npages;
         :                      }
    0.00 :   ffff800010483d90:       mov     w0, w19
    0.00 :   ffff800010483d94:       ldr     x19, [sp, #16]
    0.00 :   ffff800010483d98:       ldp     x20, x21, [x29, #24]
    0.00 :   ffff800010483d9c:       ldp     x29, x30, [sp], #48
    0.00 :   ffff800010483da0:       ret
    0.00 :   ffff800010483da4:       ldp     x20, x21, [x29, #24]
         :                      return 0;
    0.87 :   ffff800010483da8:       mov     w19, #0x0                       // #0
         :                      }
    0.00 :   ffff800010483dac:       mov     w0, w19
    2.38 :   ffff800010483db0:       ldr     x19, [sp, #16]
    0.00 :   ffff800010483db4:       ldp     x29, x30, [sp], #48
    0.00 :   ffff800010483db8:       ret
         :                      bvec_iter_advance():
         :                      "Attempted to advance past end of bvec iter\n")) {
         :                      iter->bi_size = 0;
         :                      return false;
         :                      }
         :
         :                      iter->bi_size -= bytes;
    0.00 :   ffff800010483dbc:       sub     w7, w7, w5
         :                      bytes += iter->bi_bvec_done;
         :
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff800010483dc0:       adds    w2, w2, w5
    0.00 :   ffff800010483dc4:       b.ne    ffff800010483de4 <iov_iter_npages+0xec>  // b.any
    0.00 :   ffff800010483dc8:       b       ffff800010483dec <iov_iter_npages+0xf4>
    0.00 :   ffff800010483dcc:       subs    w2, w2, w3
         :                      bytes -= bv[idx].bv_len;
         :                      idx++;
    0.00 :   ffff800010483dd0:       add     w4, w4, #0x1
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff800010483dd4:       b.eq    ffff800010483dec <iov_iter_npages+0xf4>  // b.none
    0.00 :   ffff800010483dd8:       ubfiz   x3, x4, #4, #32
    0.00 :   ffff800010483ddc:       add     x3, x0, x3
    0.00 :   ffff800010483de0:       ldr     w3, [x3, #8]
    0.00 :   ffff800010483de4:       cmp     w3, w2
    0.00 :   ffff800010483de8:       b.ls    ffff800010483dcc <iov_iter_npages+0xd4>  // b.plast
         :                      iov_iter_npages():
         :                      } else iterate_all_kinds(i, size, v, ({
    0.00 :   ffff800010483dec:       cbz     w7, ffff800010483ea8 <iov_iter_npages+0x1b0>
    0.00 :   ffff800010483df0:       ldr     x0, [x20, #24]
    0.00 :   ffff800010483df4:       ubfiz   x1, x4, #4, #32
    0.00 :   ffff800010483df8:       add     x1, x0, x1
    0.00 :   ffff800010483dfc:       ldp     w3, w5, [x1, #8]
    0.00 :   ffff800010483e00:       sub     w6, w3, w2
    0.00 :   ffff800010483e04:       add     w5, w2, w5
    0.00 :   ffff800010483e08:       cmp     w6, w7
    0.00 :   ffff800010483e0c:       and     w5, w5, #0xfff
    0.00 :   ffff800010483e10:       sub     w5, w8, w5
    0.00 :   ffff800010483e14:       csel    w6, w6, w7, ls  // ls = plast
    0.00 :   ffff800010483e18:       cmp     w5, w6
    0.00 :   ffff800010483e1c:       csel    w5, w5, w6, ls  // ls = plast
    0.00 :   ffff800010483e20:       cbz     w5, ffff800010483dbc <iov_iter_npages+0xc4>
    0.00 :   ffff800010483e24:       add     w19, w19, #0x1
    0.00 :   ffff800010483e28:       cmp     w21, w19
    0.00 :   ffff800010483e2c:       b.le    ffff800010483d8c <iov_iter_npages+0x94>
         :                      bvec_iter_advance():
         :                      if (WARN_ONCE(bytes > iter->bi_size,
    0.00 :   ffff800010483e30:       cmp     w7, w5
    0.00 :   ffff800010483e34:       b.cs    ffff800010483dbc <iov_iter_npages+0xc4>  // b.hs, b.nlast
    0.00 :   ffff800010483e38:       adrp    x1, ffff800011a5d000 <__print_once.28906>
    0.00 :   ffff800010483e3c:       ldrb    w0, [x1, #82]
    0.00 :   ffff800010483e40:       cbnz    w0, ffff800010483ea8 <iov_iter_npages+0x1b0>
    0.00 :   ffff800010483e44:       mov     w2, #0x1                        // #1
    0.00 :   ffff800010483e48:       adrp    x0, ffff8000111aa000 <kallsyms_token_index+0x30330>
    0.00 :   ffff800010483e4c:       strb    w2, [x1, #82]
    0.00 :   ffff800010483e50:       add     x0, x0, #0xea8
    0.00 :   ffff800010483e54:       bl      ffff8000100e5288 <__warn_printk>
    0.00 :   ffff800010483e58:       brk     #0x800
    0.00 :   ffff800010483e5c:       ldp     x20, x21, [x29, #24]
    0.00 :   ffff800010483e60:       b       ffff800010483dac <iov_iter_npages+0xb4>
         :                      iov_iter_npages():
         :                      int npages = 0;
    0.00 :   ffff800010483e64:       mov     w19, #0x0                       // #0
         :                      } else iterate_all_kinds(i, size, v, ({
    0.00 :   ffff800010483e68:       add     x1, x1, #0x10
    0.00 :   ffff800010483e6c:       ldr     x0, [x1, #8]
    0.00 :   ffff800010483e70:       cmp     x0, x7
    0.00 :   ffff800010483e74:       csel    x0, x0, x7, ls  // ls = plast
    0.00 :   ffff800010483e78:       cbz     x0, ffff800010483e68 <iov_iter_npages+0x170>
    0.00 :   ffff800010483e7c:       ldr     x2, [x1]
    0.00 :   ffff800010483e80:       add     x3, x2, #0xfff
    0.00 :   ffff800010483e84:       add     x3, x3, x0
    0.00 :   ffff800010483e88:       lsr     x2, x2, #12
    0.00 :   ffff800010483e8c:       sub     w19, w19, w2
    0.00 :   ffff800010483e90:       lsr     x2, x3, #12
    0.00 :   ffff800010483e94:       add     w19, w19, w2
    0.00 :   ffff800010483e98:       cmp     w21, w19
    0.00 :   ffff800010483e9c:       b.le    ffff800010483d8c <iov_iter_npages+0x94>
    0.64 :   ffff800010483ea0:       subs    x7, x7, x0
    0.00 :   ffff800010483ea4:       b.ne    ffff800010483e68 <iov_iter_npages+0x170>  // b.any
         :                      }
   12.34 :   ffff800010483ea8:       mov     w0, w19
    3.44 :   ffff800010483eac:       ldr     x19, [sp, #16]
    5.63 :   ffff800010483eb0:       ldp     x20, x21, [x29, #24]
   19.04 :   ffff800010483eb4:       ldp     x29, x30, [sp], #48
    0.00 :   ffff800010483eb8:       ret
    0.00 :   ffff800010483ebc:       ldr     x20, [x29, #24]
    0.00 :   ffff800010483ec0:       b       ffff800010483da8 <iov_iter_npages+0xb0>
    0.00 :   ffff800010483ec4:       str     x22, [x29, #40]
         :                      struct pipe_inode_info *pipe = i->pipe;
    0.00 :   ffff800010483ec8:       ldr     x22, [x0, #24]
         :                      if (!sanity(i))
    0.00 :   ffff800010483ecc:       bl      ffff8000104822e0 <sanity>
    0.00 :   ffff800010483ed0:       tst     w0, #0xff
    0.00 :   ffff800010483ed4:       b.eq    ffff800010483fd4 <iov_iter_npages+0x2dc>  // b.none
         :                      data_start():
         :                      size_t off = i->iov_offset;
    0.00 :   ffff800010483ed8:       ldr     x2, [x20, #8]
         :                      unsigned int iter_head = i->head;
    0.00 :   ffff800010483edc:       ldr     w0, [x20, #32]
         :                      if (off && (!allocated(&i->pipe->bufs[iter_head & p_mask]) ||
    0.00 :   ffff800010483ee0:       cbz     x2, ffff800010483f1c <iov_iter_npages+0x224>
         :                      unsigned int p_mask = i->pipe->ring_size - 1;
    0.00 :   ffff800010483ee4:       ldr     x4, [x20, #24]
         :                      if (off && (!allocated(&i->pipe->bufs[iter_head & p_mask]) ||
    0.00 :   ffff800010483ee8:       mov     w6, #0x28                       // #40
    0.00 :   ffff800010483eec:       adrp    x3, ffff800010cf9000 <empty_dir_inode_operations+0x40>
    0.00 :   ffff800010483ef0:       add     x3, x3, #0x280
    0.00 :   ffff800010483ef4:       mov     x5, #0x1000                     // #4096
         :                      unsigned int p_mask = i->pipe->ring_size - 1;
    0.00 :   ffff800010483ef8:       ldr     w1, [x4, #68]
         :                      if (off && (!allocated(&i->pipe->bufs[iter_head & p_mask]) ||
    0.00 :   ffff800010483efc:       ldr     x4, [x4, #120]
         :                      unsigned int p_mask = i->pipe->ring_size - 1;
    0.00 :   ffff800010483f00:       sub     w1, w1, #0x1
         :                      if (off && (!allocated(&i->pipe->bufs[iter_head & p_mask]) ||
    0.00 :   ffff800010483f04:       and     w1, w1, w0
    0.00 :   ffff800010483f08:       umaddl  x1, w1, w6, x4
    0.00 :   ffff800010483f0c:       ldr     x1, [x1, #16]
    0.00 :   ffff800010483f10:       cmp     x1, x3
         :                      iter_head++;
    0.00 :   ffff800010483f14:       ccmp    x2, x5, #0x4, eq  // eq = none
    0.00 :   ffff800010483f18:       cinc    w0, w0, eq  // eq = none
         :                      iov_iter_npages():
         :                      npages = pipe_space_for_user(iter_head, pipe->tail, pipe);
    0.00 :   ffff800010483f1c:       ldp     w2, w1, [x22, #60]
         :                      pipe_space_for_user():
         :                      struct pipe_inode_info *pipe)
         :                      {
         :                      unsigned int p_occupancy, p_space;
         :
         :                      p_occupancy = pipe_occupancy(head, tail);
         :                      if (p_occupancy >= pipe->max_usage)
    0.00 :   ffff800010483f20:       mov     w19, #0x0                       // #0
         :                      pipe_occupancy():
         :                      return head - tail;
    0.00 :   ffff800010483f24:       sub     w0, w0, w2
         :                      pipe_space_for_user():
         :                      if (p_occupancy >= pipe->max_usage)
    0.00 :   ffff800010483f28:       cmp     w1, w0
    0.00 :   ffff800010483f2c:       b.ls    ffff800010483f40 <iov_iter_npages+0x248>  // b.plast
         :                      return 0;
         :                      p_space = pipe->ring_size - p_occupancy;
    0.00 :   ffff800010483f30:       ldr     w19, [x22, #68]
    0.00 :   ffff800010483f34:       sub     w0, w19, w0
    0.00 :   ffff800010483f38:       cmp     w0, w1
    0.00 :   ffff800010483f3c:       csel    w19, w0, w1, ls  // ls = plast
    0.00 :   ffff800010483f40:       cmp     w21, w19
         :                      iov_iter_npages():
    0.00 :   ffff800010483f44:       ldr     x22, [x29, #40]
    0.00 :   ffff800010483f48:       csel    w19, w21, w19, le
    0.00 :   ffff800010483f4c:       ldp     x20, x21, [x29, #24]
    0.00 :   ffff800010483f50:       b       ffff800010483dac <iov_iter_npages+0xb4>
         :                      } else iterate_all_kinds(i, size, v, ({
    0.00 :   ffff800010483f54:       ldr     x3, [x0, #24]
    0.00 :   ffff800010483f58:       ldr     x0, [x3, #8]
    0.00 :   ffff800010483f5c:       sub     x0, x0, x2
    0.00 :   ffff800010483f60:       cmp     x0, x7
    0.00 :   ffff800010483f64:       csel    x0, x0, x7, ls  // ls = plast
    0.00 :   ffff800010483f68:       cbz     x0, ffff800010483fe0 <iov_iter_npages+0x2e8>
    0.00 :   ffff800010483f6c:       ldr     x1, [x3]
    0.00 :   ffff800010483f70:       add     x2, x1, x2
    0.00 :   ffff800010483f74:       add     x19, x0, x2
    0.00 :   ffff800010483f78:       add     x19, x19, #0xfff
    0.00 :   ffff800010483f7c:       lsr     x2, x2, #12
    0.00 :   ffff800010483f80:       lsr     x19, x19, #12
    0.00 :   ffff800010483f84:       sub     w19, w19, w2
    0.00 :   ffff800010483f88:       cmp     w21, w19
    0.00 :   ffff800010483f8c:       b.le    ffff800010483d8c <iov_iter_npages+0x94>
    0.00 :   ffff800010483f90:       subs    x7, x7, x0
    0.00 :   ffff800010483f94:       b.eq    ffff800010483ea8 <iov_iter_npages+0x1b0>  // b.none
    0.00 :   ffff800010483f98:       add     x3, x3, #0x10
    0.00 :   ffff800010483f9c:       ldr     x0, [x3, #8]
    0.00 :   ffff800010483fa0:       cmp     x0, x7
    0.00 :   ffff800010483fa4:       csel    x0, x0, x7, ls  // ls = plast
    0.00 :   ffff800010483fa8:       cbz     x0, ffff800010483f98 <iov_iter_npages+0x2a0>
    0.00 :   ffff800010483fac:       ldr     x1, [x3]
    0.00 :   ffff800010483fb0:       add     x2, x1, #0xfff
    0.00 :   ffff800010483fb4:       add     x2, x2, x0
    0.00 :   ffff800010483fb8:       lsr     x1, x1, #12
    0.00 :   ffff800010483fbc:       sub     w19, w19, w1
    0.00 :   ffff800010483fc0:       lsr     x1, x2, #12
    0.00 :   ffff800010483fc4:       add     w19, w19, w1
    0.00 :   ffff800010483fc8:       cmp     w21, w19
    0.00 :   ffff800010483fcc:       b.gt    ffff800010483f90 <iov_iter_npages+0x298>
    0.00 :   ffff800010483fd0:       b       ffff800010483d8c <iov_iter_npages+0x94>
    0.00 :   ffff800010483fd4:       ldp     x20, x21, [x29, #24]
    0.00 :   ffff800010483fd8:       ldr     x22, [x29, #40]
    0.00 :   ffff800010483fdc:       b       ffff800010483da8 <iov_iter_npages+0xb0>
         :                      int npages = 0;
    0.00 :   ffff800010483fe0:       mov     w19, #0x0                       // #0
    0.00 :   ffff800010483fe4:       b       ffff800010483f98 <iov_iter_npages+0x2a0>
 Percent |	Source code & Disassembly of vmlinux for cycles (416 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001045da10 <blk_mq_start_request>:
         :                      blk_mq_start_request():
         :                      return true;
         :                      }
         :                      EXPORT_SYMBOL(blk_mq_complete_request);
         :
         :                      void blk_mq_start_request(struct request *rq)
         :                      {
   11.49 :   ffff80001045da10:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff80001045da14:       mov     x29, sp
   25.93 :   ffff80001045da18:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001045da1c:       mov     x19, x0
         :                      struct request_queue *q = rq->q;
    3.85 :   ffff80001045da20:       ldr     x20, [x0]
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    4.56 :   ffff80001045da24:       ldr     x0, [x20, #104]
         :                      blk_mq_start_request():
         :
         :                      trace_block_rq_issue(q, rq);
         :
         :                      if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
    0.00 :   ffff80001045da28:       tst     w0, #0x100000
    0.00 :   ffff80001045da2c:       b.ne    ffff80001045da98 <blk_mq_start_request+0x88>  // b.any
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
   33.26 :   ffff80001045da30:       ldr     w0, [x19, #208]
         :                      blk_mq_start_request():
         :                      rq->stats_sectors = blk_rq_sectors(rq);
         :                      rq->rq_flags |= RQF_STATS;
         :                      rq_qos_issue(q, rq);
         :                      }
         :
         :                      WARN_ON_ONCE(blk_mq_rq_state(rq) != MQ_RQ_IDLE);
    0.00 :   ffff80001045da34:       cbnz    w0, ffff80001045dad0 <blk_mq_start_request+0xc0>
         :
         :                      blk_add_timer(rq);
    2.89 :   ffff80001045da38:       mov     x0, x19
    0.00 :   ffff80001045da3c:       bl      ffff80001045c888 <blk_add_timer>
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001045da40:       mov     w0, #0x1                        // #1
    0.00 :   ffff80001045da44:       str     w0, [x19, #208]
         :                      blk_mq_start_request():
         :                      WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
         :
         :                      if (q->dma_drain_size && blk_rq_bytes(rq)) {
    4.59 :   ffff80001045da48:       ldr     w0, [x20, #240]
    0.00 :   ffff80001045da4c:       cbz     w0, ffff80001045da58 <blk_mq_start_request+0x48>
    0.00 :   ffff80001045da50:       ldr     w0, [x19, #40]
    0.00 :   ffff80001045da54:       cbnz    w0, ffff80001045da88 <blk_mq_start_request+0x78>
         :                      */
         :                      rq->nr_phys_segments++;
         :                      }
         :
         :                      #ifdef CONFIG_BLK_DEV_INTEGRITY
         :                      if (blk_integrity_rq(rq) && req_op(rq) == REQ_OP_WRITE)
    0.72 :   ffff80001045da58:       ldr     w0, [x19, #24]
    0.00 :   ffff80001045da5c:       tbz     w0, #16, ffff80001045da7c <blk_mq_start_request+0x6c>
    0.00 :   ffff80001045da60:       and     w0, w0, #0xff
    0.00 :   ffff80001045da64:       cmp     w0, #0x1
    0.00 :   ffff80001045da68:       b.ne    ffff80001045da7c <blk_mq_start_request+0x6c>  // b.any
         :                      q->integrity.profile->prepare_fn(rq);
    0.00 :   ffff80001045da6c:       ldr     x1, [x20, #200]
    0.00 :   ffff80001045da70:       mov     x0, x19
    0.00 :   ffff80001045da74:       ldr     x1, [x1, #16]
    0.00 :   ffff80001045da78:       blr     x1
         :                      #endif
         :                      }
    6.23 :   ffff80001045da7c:       ldp     x19, x20, [sp, #16]
    6.46 :   ffff80001045da80:       ldp     x29, x30, [sp], #32
    0.00 :   ffff80001045da84:       ret
         :                      rq->nr_phys_segments++;
    0.00 :   ffff80001045da88:       ldrh    w0, [x19, #194]
    0.00 :   ffff80001045da8c:       add     w0, w0, #0x1
    0.00 :   ffff80001045da90:       strh    w0, [x19, #194]
    0.00 :   ffff80001045da94:       b       ffff80001045da58 <blk_mq_start_request+0x48>
         :                      ktime_get_ns():
         :                      return ktime_mono_to_any(mono, TK_OFFS_REAL);
         :                      }
         :
         :                      static inline u64 ktime_get_ns(void)
         :                      {
         :                      return ktime_to_ns(ktime_get());
    0.00 :   ffff80001045da98:       bl      ffff80001016ad10 <ktime_get>
         :                      blk_mq_start_request():
         :                      rq->io_start_time_ns = ktime_get_ns();
    0.00 :   ffff80001045da9c:       str     x0, [x19, #184]
         :                      blk_rq_sectors():
         :
         :                      extern unsigned int blk_rq_err_bytes(const struct request *rq);
         :
         :                      static inline unsigned int blk_rq_sectors(const struct request *rq)
         :                      {
         :                      return blk_rq_bytes(rq) >> SECTOR_SHIFT;
    0.00 :   ffff80001045daa0:       ldr     w1, [x19, #40]
         :                      blk_mq_start_request():
         :                      rq->rq_flags |= RQF_STATS;
    0.00 :   ffff80001045daa4:       ldr     w2, [x19, #28]
         :                      blk_rq_sectors():
    0.00 :   ffff80001045daa8:       lsr     w1, w1, #9
         :                      blk_mq_start_request():
    0.00 :   ffff80001045daac:       orr     w2, w2, #0x20000
    0.00 :   ffff80001045dab0:       str     w2, [x19, #28]
         :                      rq->stats_sectors = blk_rq_sectors(rq);
    0.00 :   ffff80001045dab4:       strh    w1, [x19, #192]
         :                      rq_qos_issue(q, rq);
    0.00 :   ffff80001045dab8:       ldr     x0, [x20, #24]
         :                      rq_qos_issue():
         :                      __rq_qos_done(q->rq_qos, rq);
         :                      }
         :
         :                      static inline void rq_qos_issue(struct request_queue *q, struct request *rq)
         :                      {
         :                      if (q->rq_qos)
    0.00 :   ffff80001045dabc:       cbz     x0, ffff80001045da30 <blk_mq_start_request+0x20>
         :                      __rq_qos_issue(q->rq_qos, rq);
    0.00 :   ffff80001045dac0:       mov     x1, x19
    0.00 :   ffff80001045dac4:       bl      ffff80001046f088 <__rq_qos_issue>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001045dac8:       ldr     w0, [x19, #208]
         :                      blk_mq_start_request():
         :                      WARN_ON_ONCE(blk_mq_rq_state(rq) != MQ_RQ_IDLE);
    0.00 :   ffff80001045dacc:       cbz     w0, ffff80001045da38 <blk_mq_start_request+0x28>
    0.00 :   ffff80001045dad0:       brk     #0x800
    0.00 :   ffff80001045dad4:       b       ffff80001045da38 <blk_mq_start_request+0x28>
 Percent |	Source code & Disassembly of vmlinux for cycles (347 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001024fe88 <kfree>:
         :                      kfree():
         :                      struct page *page;
         :                      void *object = (void *)x;
         :
         :                      trace_kfree(_RET_IP_, x);
         :
         :                      if (unlikely(ZERO_OR_NULL_PTR(x)))
   22.15 :   ffff80001024fe88:       cmp     x0, #0x10
    0.00 :   ffff80001024fe8c:       b.ls    ffff800010250060 <kfree+0x1d8>  // b.plast
         :                      {
    0.00 :   ffff80001024fe90:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff80001024fe94:       mov     x29, sp
    0.00 :   ffff80001024fe98:       stp     x21, x22, [sp, #32]
         :                      virt_to_head_page():
         :                      }
         :                      #endif
         :
         :                      static inline struct page *virt_to_head_page(const void *x)
         :                      {
         :                      struct page *page = virt_to_page(x);
    0.00 :   ffff80001024fe9c:       mov     x21, #0x1000000000000           // #281474976710656
    0.00 :   ffff80001024fea0:       add     x21, x0, x21
         :                      kfree():
    0.00 :   ffff80001024fea4:       str     x19, [sp, #16]
    0.00 :   ffff80001024fea8:       mov     x19, x0
         :                      virt_to_head_page():
    0.00 :   ffff80001024feac:       mov     x0, #0xffffffffffe00000         // #-2097152
    0.00 :   ffff80001024feb0:       lsr     x21, x21, #12
    0.00 :   ffff80001024feb4:       movk    x0, #0xfdff, lsl #32
    0.00 :   ffff80001024feb8:       mov     x22, x30
    0.00 :   ffff80001024febc:       add     x21, x0, x21, lsl #6
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001024fec0:       ldr     x0, [x21, #8]
         :                      compound_head():
         :                      static inline struct page *compound_head(struct page *page)
         :                      {
         :                      unsigned long head = READ_ONCE(page->compound_head);
         :
         :                      if (unlikely(head & 1))
         :                      return (struct page *) (head - 1);
    0.00 :   ffff80001024fec4:       sub     x1, x0, #0x1
    0.00 :   ffff80001024fec8:       tst     x0, #0x1
    0.00 :   ffff80001024fecc:       csel    x21, x1, x21, ne  // ne = any
         :                      __read_once_size():
    0.00 :   ffff80001024fed0:       ldr     x1, [x21, #8]
         :                      compound_head():
    0.00 :   ffff80001024fed4:       sub     x0, x1, #0x1
    0.00 :   ffff80001024fed8:       tst     x1, #0x1
    0.00 :   ffff80001024fedc:       csel    x0, x0, x21, ne  // ne = any
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff80001024fee0:       ldr     x0, [x0]
         :                      kfree():
         :                      return;
         :
         :                      page = virt_to_head_page(x);
         :                      if (unlikely(!PageSlab(page))) {
    0.00 :   ffff80001024fee4:       tst     w0, #0x200
    0.00 :   ffff80001024fee8:       b.eq    ffff800010250064 <kfree+0x1dc>  // b.none
    0.00 :   ffff80001024feec:       str     x20, [x29, #24]
         :                      mod_node_page_state(page_pgdat(page), NR_SLAB_UNRECLAIMABLE,
         :                      -(1 << order));
         :                      __free_pages(page, order);
         :                      return;
         :                      }
         :                      slab_free(page->slab_cache, page, object, NULL, 1, _RET_IP_);
    0.00 :   ffff80001024fef0:       ldr     x20, [x21, #24]
         :                      arch_static_branch():
         :                      #define JUMP_LABEL_NOP_SIZE             AARCH64_INSN_SIZE
         :
         :                      static __always_inline bool arch_static_branch(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff80001024fef4:       nop
         :                      set_freepointer():
         :                      unsigned long freeptr_addr = (unsigned long)object + s->offset;
    0.00 :   ffff80001024fef8:       ldr     w0, [x20, #32]
         :                      *(void **)freeptr_addr = freelist_ptr(s, fp, freeptr_addr);
    0.00 :   ffff80001024fefc:       str     xzr, [x19, x0]
         :                      slab_free():
         :                      if (slab_free_freelist_hook(s, &head, &tail))
    0.00 :   ffff80001024ff00:       cbz     x19, ffff800010250038 <kfree+0x1b0>
    0.00 :   ffff80001024ff04:       str     x23, [x29, #48]
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80001024ff08:       mrs     x2, sp_el0
         :                      __read_once_size():
    0.00 :   ffff80001024ff0c:       ldr     w0, [x2, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff80001024ff10:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001024ff14:       str     w0, [x2, #16]
         :                      do_slab_free():
         :                      tid = this_cpu_read(s->cpu_slab->tid);
    0.00 :   ffff80001024ff18:       ldr     x1, [x20]
    0.00 :   ffff80001024ff1c:       add     x1, x1, #0x8
    0.00 :   ffff80001024ff20:       bl      ffff80001024af98 <__my_cpu_offset>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001024ff24:       ldr     x23, [x1, x0]
    0.00 :   ffff80001024ff28:       ldr     x0, [x2, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff80001024ff2c:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001024ff30:       str     w0, [x2, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff80001024ff34:       cbnz    x0, ffff80001025004c <kfree+0x1c4>
         :                      do_slab_free():
    0.00 :   ffff80001024ff38:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      c = raw_cpu_ptr(s->cpu_slab);
    0.00 :   ffff80001024ff3c:       ldr     x1, [x20]
    0.00 :   ffff80001024ff40:       bl      ffff80001024af98 <__my_cpu_offset>
    0.00 :   ffff80001024ff44:       mov     x2, x0
    0.00 :   ffff80001024ff48:       add     x0, x1, x0
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001024ff4c:       ldr     x3, [x0, #8]
         :                      do_slab_free():
         :                      unlikely(tid != READ_ONCE(c->tid)));
    0.00 :   ffff80001024ff50:       cmp     x3, x23
    0.00 :   ffff80001024ff54:       b.ne    ffff80001024ff08 <kfree+0x80>  // b.any
         :                      if (likely(page == c->page)) {
    0.00 :   ffff80001024ff58:       ldr     x0, [x0, #16]
    0.00 :   ffff80001024ff5c:       cmp     x0, x21
    0.00 :   ffff80001024ff60:       b.ne    ffff8000102500d4 <kfree+0x24c>  // b.any
         :                      set_freepointer():
         :                      unsigned long freeptr_addr = (unsigned long)object + s->offset;
    0.00 :   ffff80001024ff64:       ldr     w0, [x20, #32]
         :                      do_slab_free():
         :                      set_freepointer(s, tail_obj, c->freelist);
    0.00 :   ffff80001024ff68:       ldr     x3, [x1, x2]
         :                      get_current():
    0.00 :   ffff80001024ff6c:       mrs     x7, sp_el0
         :                      set_freepointer():
         :                      *(void **)freeptr_addr = freelist_ptr(s, fp, freeptr_addr);
    0.00 :   ffff80001024ff70:       str     x3, [x19, x0]
         :                      __read_once_size():
    0.00 :   ffff80001024ff74:       ldr     w0, [x7, #16]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff80001024ff78:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001024ff7c:       str     w0, [x7, #16]
         :                      do_slab_free():
         :                      if (unlikely(!this_cpu_cmpxchg_double(
    0.00 :   ffff80001024ff80:       ldr     x4, [x20]
    0.00 :   ffff80001024ff84:       bl      ffff80001024af98 <__my_cpu_offset>
    0.00 :   ffff80001024ff88:       add     x4, x4, x0
    0.00 :   ffff80001024ff8c:       ldr     x0, [x1, x2]
    0.00 :   ffff80001024ff90:       add     x3, x23, #0x100
    0.00 :   ffff80001024ff94:       mov     x1, x23
    0.00 :   ffff80001024ff98:       mov     x2, x19
    0.00 :   ffff80001024ff9c:       bl      ffff80001024d338 <__cmpxchg_double>
    0.00 :   ffff80001024ffa0:       mov     x23, x0
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001024ffa4:       ldr     x1, [x7, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff80001024ffa8:       sub     x1, x1, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001024ffac:       str     w1, [x7, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff80001024ffb0:       cbz     x1, ffff800010250058 <kfree+0x1d0>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001024ffb4:       ldr     x0, [x7, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff80001024ffb8:       cbz     x0, ffff800010250058 <kfree+0x1d0>
         :                      do_slab_free():
    0.00 :   ffff80001024ffbc:       cbnz    x23, ffff80001024ff08 <kfree+0x80>
    0.00 :   ffff80001024ffc0:       ldr     x20, [x29, #24]
    0.00 :   ffff80001024ffc4:       ldr     x23, [x29, #48]
         :                      kfree():
         :                      }
    0.00 :   ffff80001024ffc8:       ldr     x19, [sp, #16]
    0.00 :   ffff80001024ffcc:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff80001024ffd0:       ldp     x29, x30, [sp], #64
    0.00 :   ffff80001024ffd4:       ret
         :                      slab_want_init_on_free():
         :                      }
         :
         :                      static inline bool slab_want_init_on_free(struct kmem_cache *c)
         :                      {
         :                      if (static_branch_unlikely(&init_on_free))
         :                      return !(c->ctor ||
    0.00 :   ffff80001024ffd8:       ldr     x0, [x20, #64]
    0.00 :   ffff80001024ffdc:       cbnz    x0, ffff80001024fef8 <kfree+0x70>
         :                      (c->flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON)));
    0.00 :   ffff80001024ffe0:       ldr     w0, [x20, #8]
    0.00 :   ffff80001024ffe4:       and     w0, w0, #0xff800
    0.00 :   ffff80001024ffe8:       and     w0, w0, #0xfff80fff
         :                      return !(c->ctor ||
    0.00 :   ffff80001024ffec:       cbnz    w0, ffff80001024fef8 <kfree+0x70>
         :                      slab_free_freelist_hook():
         :                      memset(object, 0, s->object_size);
    0.00 :   ffff80001024fff0:       ldr     w2, [x20, #28]
    0.00 :   ffff80001024fff4:       mov     w1, #0x0                        // #0
    0.00 :   ffff80001024fff8:       mov     x0, x19
    0.00 :   ffff80001024fffc:       bl      ffff800010c92840 <__memset>
         :                      rsize = (s->flags & SLAB_RED_ZONE) ? s->red_left_pad
    0.00 :   ffff800010250000:       ldr     w0, [x20, #8]
         :                      : 0;
    0.00 :   ffff800010250004:       mov     w3, #0x0                        // #0
    0.00 :   ffff800010250008:       tbz     w0, #10, ffff800010250010 <kfree+0x188>
         :                      rsize = (s->flags & SLAB_RED_ZONE) ? s->red_left_pad
    0.00 :   ffff80001025000c:       ldr     w3, [x20, #80]
         :                      memset((char *)object + s->inuse, 0,
    0.00 :   ffff800010250010:       ldr     w0, [x20, #72]
    0.00 :   ffff800010250014:       mov     w1, #0x0                        // #0
         :                      s->size - s->inuse - rsize);
    0.00 :   ffff800010250018:       ldr     w2, [x20, #24]
    0.00 :   ffff80001025001c:       sub     w2, w2, w0
         :                      memset((char *)object + s->inuse, 0,
    0.00 :   ffff800010250020:       add     x0, x19, w0, uxtw
    0.00 :   ffff800010250024:       sub     w2, w2, w3
    0.00 :   ffff800010250028:       bl      ffff800010c92840 <__memset>
         :                      set_freepointer():
         :                      unsigned long freeptr_addr = (unsigned long)object + s->offset;
    0.00 :   ffff80001025002c:       ldr     w0, [x20, #32]
         :                      *(void **)freeptr_addr = freelist_ptr(s, fp, freeptr_addr);
    0.00 :   ffff800010250030:       str     xzr, [x19, x0]
         :                      slab_free():
         :                      if (slab_free_freelist_hook(s, &head, &tail))
    0.00 :   ffff800010250034:       cbnz    x19, ffff80001024ff04 <kfree+0x7c>
    0.00 :   ffff800010250038:       ldr     x20, [x29, #24]
         :                      kfree():
         :                      }
    0.00 :   ffff80001025003c:       ldr     x19, [sp, #16]
    0.00 :   ffff800010250040:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010250044:       ldp     x29, x30, [sp], #64
    0.00 :   ffff800010250048:       ret
         :                      __read_once_size():
    0.00 :   ffff80001025004c:       ldr     x0, [x2, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff800010250050:       cbz     x0, ffff80001024ff38 <kfree+0xb0>
    0.00 :   ffff800010250054:       b       ffff80001024ff3c <kfree+0xb4>
         :                      do_slab_free():
         :                      if (unlikely(!this_cpu_cmpxchg_double(
    0.00 :   ffff800010250058:       bl      ffff800010cad668 <preempt_schedule_notrace>
    0.00 :   ffff80001025005c:       b       ffff80001024ffbc <kfree+0x134>
   77.85 :   ffff800010250060:       ret
         :                      test_bit():
    0.00 :   ffff800010250064:       ldr     x0, [x21]
         :                      compound_order():
         :                      }
         :
         :                      static inline unsigned int compound_order(struct page *page)
         :                      {
         :                      if (!PageHead(page))
         :                      return 0;
    0.00 :   ffff800010250068:       mov     w19, #0x0                       // #0
         :                      if (!PageHead(page))
    0.00 :   ffff80001025006c:       tst     w0, #0x10000
    0.00 :   ffff800010250070:       b.ne    ffff8000102500cc <kfree+0x244>  // b.any
         :                      test_bit():
    0.00 :   ffff800010250074:       ldr     x0, [x21]
         :                      PageCompound():
         :                      return READ_ONCE(page->compound_head) & 1;
         :                      }
         :
         :                      static __always_inline int PageCompound(struct page *page)
         :                      {
         :                      return test_bit(PG_head, &page->flags) || PageTail(page);
    0.00 :   ffff800010250078:       tst     w0, #0x10000
    0.00 :   ffff80001025007c:       b.ne    ffff800010250094 <kfree+0x20c>  // b.any
         :                      __read_once_size():
    0.00 :   ffff800010250080:       ldr     x0, [x21, #8]
         :                      PageCompound():
    0.00 :   ffff800010250084:       tbnz    w0, #0, ffff800010250094 <kfree+0x20c>
    0.00 :   ffff800010250088:       str     x20, [x29, #24]
    0.00 :   ffff80001025008c:       str     x23, [x29, #48]
         :                      kfree():
         :                      BUG_ON(!PageCompound(page));
    0.00 :   ffff800010250090:       brk     #0x800
         :                      page_to_nid():
         :                      #else
         :                      static inline int page_to_nid(const struct page *page)
         :                      {
         :                      struct page *p = (struct page *)page;
         :
         :                      return (PF_POISONED_CHECK(p)->flags >> NODES_PGSHIFT) & NODES_MASK;
    0.00 :   ffff800010250094:       ldr     x3, [x21]
         :                      page_pgdat():
         :                      return &NODE_DATA(page_to_nid(page))->node_zones[page_zonenum(page)];
         :                      }
         :
         :                      static inline pg_data_t *page_pgdat(const struct page *page)
         :                      {
         :                      return NODE_DATA(page_to_nid(page));
    0.00 :   ffff800010250098:       adrp    x0, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff80001025009c:       add     x0, x0, #0xe8
         :                      kfree():
         :                      -(1 << order));
    0.00 :   ffff8000102500a0:       mov     w2, #0xffffffff                 // #-1
    0.00 :   ffff8000102500a4:       lsl     w2, w2, w19
         :                      mod_node_page_state(page_pgdat(page), NR_SLAB_UNRECLAIMABLE,
    0.00 :   ffff8000102500a8:       mov     w1, #0x6                        // #6
         :                      page_pgdat():
    0.00 :   ffff8000102500ac:       lsr     x3, x3, #62
         :                      kfree():
    0.00 :   ffff8000102500b0:       sxtw    x2, w2
    0.00 :   ffff8000102500b4:       ldr     x0, [x0, x3, lsl #3]
    0.00 :   ffff8000102500b8:       bl      ffff8000101f4418 <mod_node_page_state>
         :                      __free_pages(page, order);
    0.00 :   ffff8000102500bc:       mov     w1, w19
    0.00 :   ffff8000102500c0:       mov     x0, x21
    0.00 :   ffff8000102500c4:       bl      ffff8000102265b8 <__free_pages>
         :                      return;
    0.00 :   ffff8000102500c8:       b       ffff80001025003c <kfree+0x1b4>
         :                      compound_order():
         :                      return page[1].compound_order;
    0.00 :   ffff8000102500cc:       ldrb    w19, [x21, #81]
    0.00 :   ffff8000102500d0:       b       ffff800010250074 <kfree+0x1ec>
         :                      do_slab_free():
         :                      __slab_free(s, page, head, tail_obj, cnt, addr);
    0.00 :   ffff8000102500d4:       mov     x0, x20
    0.00 :   ffff8000102500d8:       mov     x5, x22
    0.00 :   ffff8000102500dc:       mov     w4, #0x1                        // #1
    0.00 :   ffff8000102500e0:       mov     x3, x19
    0.00 :   ffff8000102500e4:       mov     x2, x19
    0.00 :   ffff8000102500e8:       mov     x1, x21
    0.00 :   ffff8000102500ec:       bl      ffff80001024fa68 <__slab_free>
    0.00 :   ffff8000102500f0:       ldr     x20, [x29, #24]
    0.00 :   ffff8000102500f4:       ldr     x23, [x29, #48]
    0.00 :   ffff8000102500f8:       b       ffff80001025003c <kfree+0x1b4>
 Percent |	Source code & Disassembly of vmlinux for cycles (441 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104561e0 <blk_flush_plug_list>:
         :                      blk_flush_plug_list():
         :                      return cb;
         :                      }
         :                      EXPORT_SYMBOL(blk_check_plugged);
         :
         :                      void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
         :                      {
   12.01 :   ffff8000104561e0:       stp     x29, x30, [sp, #-112]!
    0.00 :   ffff8000104561e4:       mov     x29, sp
   33.12 :   ffff8000104561e8:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000104561ec:       and     w20, w1, #0xff
    4.10 :   ffff8000104561f0:       stp     x21, x22, [sp, #32]
         :                      list_del():
         :                      }
         :
         :                      static inline void list_del(struct list_head *entry)
         :                      {
         :                      __list_del_entry(entry);
         :                      entry->next = LIST_POISON1;
    0.00 :   ffff8000104561f4:       mov     x22, #0x100                     // #256
         :                      blk_flush_plug_list():
    4.09 :   ffff8000104561f8:       stp     x23, x24, [sp, #48]
    0.00 :   ffff8000104561fc:       mov     x23, x0
    4.07 :   ffff800010456200:       str     x25, [sp, #64]
    0.00 :   ffff800010456204:       add     x24, x0, #0x10
    0.00 :   ffff800010456208:       adrp    x25, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff80001045620c:       add     x0, x25, #0x8c8
         :                      list_del():
         :                      entry->prev = LIST_POISON2;
    9.51 :   ffff800010456210:       mov     x21, #0x122                     // #290
         :                      blk_flush_plug_list():
    0.00 :   ffff800010456214:       ldr     x1, [x0]
    2.93 :   ffff800010456218:       str     x1, [x29, #104]
    0.00 :   ffff80001045621c:       mov     x1, #0x0                        // #0
         :                      flush_plug_callbacks():
         :                      LIST_HEAD(callbacks);
    0.00 :   ffff800010456220:       add     x19, x29, #0x58
         :                      list_del():
         :                      entry->next = LIST_POISON1;
    0.00 :   ffff800010456224:       movk    x22, #0xdead, lsl #48
         :                      entry->prev = LIST_POISON2;
    0.00 :   ffff800010456228:       movk    x21, #0xdead, lsl #48
         :                      flush_plug_callbacks():
    0.00 :   ffff80001045622c:       stp     x19, x19, [x29, #88]
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    5.00 :   ffff800010456230:       ldr     x0, [x23, #16]
         :                      flush_plug_callbacks():
         :                      while (!list_empty(&plug->cb_list)) {
    0.00 :   ffff800010456234:       cmp     x24, x0
    0.00 :   ffff800010456238:       b.eq    ffff80001045629c <blk_flush_plug_list+0xbc>  // b.none
         :                      __read_once_size():
    0.00 :   ffff80001045623c:       ldr     x0, [x23, #16]
         :                      list_splice_init():
         :                      * The list at @list is reinitialised
         :                      */
         :                      static inline void list_splice_init(struct list_head *list,
         :                      struct list_head *head)
         :                      {
         :                      if (!list_empty(list)) {
    0.00 :   ffff800010456240:       cmp     x24, x0
    0.00 :   ffff800010456244:       b.eq    ffff800010456268 <blk_flush_plug_list+0x88>  // b.none
         :                      __list_splice():
         :                      struct list_head *last = list->prev;
    0.00 :   ffff800010456248:       ldp     x2, x1, [x23, #16]
         :                      first->prev = prev;
    0.00 :   ffff80001045624c:       str     x19, [x2, #8]
         :                      list_splice_init():
         :                      __list_splice(list, head, head->next);
    0.00 :   ffff800010456250:       ldr     x0, [x29, #88]
         :                      __list_splice():
         :                      prev->next = first;
    0.00 :   ffff800010456254:       str     x2, [x29, #88]
         :                      last->next = next;
    0.00 :   ffff800010456258:       str     x0, [x1]
         :                      next->prev = last;
    0.00 :   ffff80001045625c:       str     x1, [x0, #8]
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff800010456260:       str     x24, [x23, #16]
         :                      INIT_LIST_HEAD():
         :                      list->prev = list;
    0.00 :   ffff800010456264:       str     x24, [x23, #24]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010456268:       ldr     x0, [x29, #88]
         :                      flush_plug_callbacks():
         :                      while (!list_empty(&callbacks)) {
    0.00 :   ffff80001045626c:       cmp     x19, x0
    0.00 :   ffff800010456270:       b.eq    ffff800010456230 <blk_flush_plug_list+0x50>  // b.none
         :                      struct blk_plug_cb *cb = list_first_entry(&callbacks,
    0.00 :   ffff800010456274:       ldr     x2, [x29, #88]
         :                      cb->callback(cb, from_schedule);
    0.00 :   ffff800010456278:       mov     w1, w20
    0.00 :   ffff80001045627c:       mov     x0, x2
         :                      __list_del_entry():
         :                      __list_del(entry->prev, entry->next);
    0.00 :   ffff800010456280:       ldp     x4, x3, [x2]
         :                      __list_del():
         :                      next->prev = prev;
    0.00 :   ffff800010456284:       str     x3, [x4, #8]
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff800010456288:       str     x4, [x3]
         :                      list_del():
         :                      entry->prev = LIST_POISON2;
    0.00 :   ffff80001045628c:       stp     x22, x21, [x2]
         :                      flush_plug_callbacks():
    0.00 :   ffff800010456290:       ldr     x2, [x2, #16]
    0.00 :   ffff800010456294:       blr     x2
    0.00 :   ffff800010456298:       b       ffff800010456268 <blk_flush_plug_list+0x88>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    3.61 :   ffff80001045629c:       ldr     x0, [x23]
         :                      blk_flush_plug_list():
         :                      flush_plug_callbacks(plug, from_schedule);
         :
         :                      if (!list_empty(&plug->mq_list))
    2.50 :   ffff8000104562a0:       cmp     x23, x0
    0.00 :   ffff8000104562a4:       b.eq    ffff8000104562b4 <blk_flush_plug_list+0xd4>  // b.none
         :                      blk_mq_flush_plug_list(plug, from_schedule);
    2.50 :   ffff8000104562a8:       mov     w1, w20
    0.00 :   ffff8000104562ac:       mov     x0, x23
    0.00 :   ffff8000104562b0:       bl      ffff800010461398 <blk_mq_flush_plug_list>
         :                      }
    2.05 :   ffff8000104562b4:       add     x25, x25, #0x8c8
    2.49 :   ffff8000104562b8:       ldr     x1, [x29, #104]
    4.54 :   ffff8000104562bc:       ldr     x0, [x25]
    0.00 :   ffff8000104562c0:       eor     x0, x1, x0
    0.00 :   ffff8000104562c4:       cbnz    x0, ffff8000104562e0 <blk_flush_plug_list+0x100>
    0.00 :   ffff8000104562c8:       ldp     x19, x20, [sp, #16]
    3.16 :   ffff8000104562cc:       ldp     x21, x22, [sp, #32]
    3.65 :   ffff8000104562d0:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff8000104562d4:       ldr     x25, [sp, #64]
    0.69 :   ffff8000104562d8:       ldp     x29, x30, [sp], #112
    0.00 :   ffff8000104562dc:       ret
    0.00 :   ffff8000104562e0:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (371 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102d9db0 <__arm64_sys_io_getevents>:
         :                      __arm64_sys_io_getevents():
         :                      *       specifies an infinite timeout. Note that the timeout pointed to by
         :                      *       timeout is relative.  Will fail with -ENOSYS if not implemented.
         :                      */
         :                      #ifdef CONFIG_64BIT
         :
         :                      SYSCALL_DEFINE5(io_getevents, aio_context_t, ctx_id,
   33.44 :   ffff8000102d9db0:       stp     x29, x30, [sp, #-96]!
    0.00 :   ffff8000102d9db4:       mov     x29, sp
    7.22 :   ffff8000102d9db8:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000102d9dbc:       adrp    x19, ffff800011899000 <page_wait_table+0x1500>
    2.45 :   ffff8000102d9dc0:       stp     x21, x22, [sp, #32]
    0.00 :   ffff8000102d9dc4:       add     x1, x19, #0x8c8
    4.56 :   ffff8000102d9dc8:       str     x23, [sp, #48]
    2.15 :   ffff8000102d9dcc:       ldr     x2, [x1]
   31.54 :   ffff8000102d9dd0:       str     x2, [x29, #88]
    0.00 :   ffff8000102d9dd4:       mov     x2, #0x0                        // #0
    0.54 :   ffff8000102d9dd8:       ldr     x1, [x0, #32]
    0.27 :   ffff8000102d9ddc:       ldp     x20, x21, [x0]
         :                      __se_sys_io_getevents():
    2.43 :   ffff8000102d9de0:       ldp     x22, x23, [x0, #16]
         :                      __do_sys_io_getevents():
         :                      struct __kernel_timespec __user *, timeout)
         :                      {
         :                      struct timespec64       ts;
         :                      int                     ret;
         :
         :                      if (timeout && unlikely(get_timespec64(&ts, timeout)))
    0.00 :   ffff8000102d9de4:       cbz     x1, ffff8000102d9e54 <__arm64_sys_io_getevents+0xa4>
    0.00 :   ffff8000102d9de8:       add     x0, x29, #0x48
    0.00 :   ffff8000102d9dec:       bl      ffff8000101659a0 <get_timespec64>
    0.00 :   ffff8000102d9df0:       cbnz    w0, ffff8000102d9e5c <__arm64_sys_io_getevents+0xac>
         :                      return -EFAULT;
         :
         :                      ret = do_io_getevents(ctx_id, min_nr, nr, events, timeout ? &ts : NULL);
    0.00 :   ffff8000102d9df4:       add     x4, x29, #0x48
    0.00 :   ffff8000102d9df8:       mov     x3, x23
    0.00 :   ffff8000102d9dfc:       mov     x2, x22
    0.54 :   ffff8000102d9e00:       mov     x1, x21
    0.00 :   ffff8000102d9e04:       mov     x0, x20
    0.00 :   ffff8000102d9e08:       bl      ffff8000102d6c18 <do_io_getevents>
         :                      if (!ret && signal_pending(current))
    0.00 :   ffff8000102d9e0c:       cbz     w0, ffff8000102d9e3c <__arm64_sys_io_getevents+0x8c>
    0.00 :   ffff8000102d9e10:       sxtw    x0, w0
         :                      __arm64_sys_io_getevents():
         :                      SYSCALL_DEFINE5(io_getevents, aio_context_t, ctx_id,
    0.00 :   ffff8000102d9e14:       add     x19, x19, #0x8c8
    5.43 :   ffff8000102d9e18:       ldr     x2, [x29, #88]
    1.07 :   ffff8000102d9e1c:       ldr     x1, [x19]
    0.00 :   ffff8000102d9e20:       eor     x1, x2, x1
    0.00 :   ffff8000102d9e24:       cbnz    x1, ffff8000102d9e64 <__arm64_sys_io_getevents+0xb4>
    0.54 :   ffff8000102d9e28:       ldp     x19, x20, [sp, #16]
    0.27 :   ffff8000102d9e2c:       ldp     x21, x22, [sp, #32]
    4.58 :   ffff8000102d9e30:       ldr     x23, [sp, #48]
    0.00 :   ffff8000102d9e34:       ldp     x29, x30, [sp], #96
    0.00 :   ffff8000102d9e38:       ret
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff8000102d9e3c:       mrs     x0, sp_el0
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff8000102d9e40:       ldr     x1, [x0]
         :                      __do_sys_io_getevents():
         :                      if (!ret && signal_pending(current))
    0.00 :   ffff8000102d9e44:       mov     x0, #0xfffffffffffffffc         // #-4
    0.00 :   ffff8000102d9e48:       tst     x1, #0x1
    0.00 :   ffff8000102d9e4c:       csel    x0, x0, xzr, ne  // ne = any
    0.00 :   ffff8000102d9e50:       b       ffff8000102d9e14 <__arm64_sys_io_getevents+0x64>
         :                      ret = do_io_getevents(ctx_id, min_nr, nr, events, timeout ? &ts : NULL);
    2.97 :   ffff8000102d9e54:       mov     x4, #0x0                        // #0
    0.00 :   ffff8000102d9e58:       b       ffff8000102d9df8 <__arm64_sys_io_getevents+0x48>
         :                      return -EFAULT;
    0.00 :   ffff8000102d9e5c:       mov     x0, #0xfffffffffffffff2         // #-14
         :                      __arm64_sys_io_getevents():
         :                      SYSCALL_DEFINE5(io_getevents, aio_context_t, ctx_id,
    0.00 :   ffff8000102d9e60:       b       ffff8000102d9e14 <__arm64_sys_io_getevents+0x64>
    0.00 :   ffff8000102d9e64:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (414 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104547d8 <blk_queue_enter>:
         :                      blk_queue_enter():
         :                      * blk_queue_enter() - try to increase q->q_usage_counter
         :                      * @q: request queue pointer
         :                      * @flags: BLK_MQ_REQ_NOWAIT and/or BLK_MQ_REQ_PREEMPT
         :                      */
         :                      int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)
         :                      {
    5.33 :   ffff8000104547d8:       stp     x29, x30, [sp, #-128]!
    0.00 :   ffff8000104547dc:       mov     x29, sp
    0.72 :   ffff8000104547e0:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000104547e4:       mov     x19, x0
    0.00 :   ffff8000104547e8:       stp     x21, x22, [sp, #32]
         :                      const bool pm = flags & BLK_MQ_REQ_PREEMPT;
    0.00 :   ffff8000104547ec:       and     w21, w1, #0x8
         :                      {
    2.65 :   ffff8000104547f0:       stp     x23, x24, [sp, #48]
    0.00 :   ffff8000104547f4:       add     x24, x0, #0x5c0
    4.35 :   ffff8000104547f8:       str     x25, [sp, #64]
    0.00 :   ffff8000104547fc:       adrp    x25, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010454800:       add     x0, x25, #0x8c8
         :                      rcu_read_unlock();
         :
         :                      if (success)
         :                      return 0;
         :
         :                      if (flags & BLK_MQ_REQ_NOWAIT)
    0.00 :   ffff800010454804:       and     w23, w1, #0x1
         :                      {
    2.90 :   ffff800010454808:       ldr     x2, [x0]
    0.00 :   ffff80001045480c:       str     x2, [x29, #120]
    0.00 :   ffff800010454810:       mov     x2, #0x0                        // #0
         :                      __percpu_add_case_64():
         :
         :                      PERCPU_RW_OPS(8)
         :                      PERCPU_RW_OPS(16)
         :                      PERCPU_RW_OPS(32)
         :                      PERCPU_RW_OPS(64)
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff800010454814:       mov     x22, #0xffffffffffffffff        // #-1
         :                      __ll_sc_atomic64_sub_return():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(add, add, I)
         :                      ATOMIC64_OPS(sub, sub, J)
    0.00 :   ffff800010454818:       mov     x20, #0x1                       // #1
    4.82 :   ffff80001045481c:       nop
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff800010454820:       bl      ffff80001015a698 <__rcu_read_lock>
    1.68 :   ffff800010454824:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010454828:       ldr     x0, [x19, #1480]
         :                      __ref_is_percpu():
         :                      * Theoretically, the following could test just ATOMIC; however,
         :                      * then we'd have to mask off DEAD separately as DEAD may be
         :                      * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
         :                      * implies ATOMIC anyway.  Test them together.
         :                      */
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff80001045482c:       tst     x0, #0x3
    0.00 :   ffff800010454830:       b.ne    ffff800010454a14 <blk_queue_enter+0x23c>  // b.any
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800010454834:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.24 :   ffff800010454838:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff80001045483c:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    2.17 :   ffff800010454840:       str     w2, [x1, #16]
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800010454844:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff800010454848:       add     x0, x0, x2
    0.00 :   ffff80001045484c:       ldxr    x4, [x0]
   42.75 :   ffff800010454850:       add     x4, x4, x20
    0.00 :   ffff800010454854:       stxr    w3, x4, [x0]
    0.00 :   ffff800010454858:       cbnz    w3, ffff80001045484c <blk_queue_enter+0x74>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.24 :   ffff80001045485c:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800010454860:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    4.86 :   ffff800010454864:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800010454868:       cbz     x0, ffff800010454874 <blk_queue_enter+0x9c>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001045486c:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff800010454870:       cbnz    x0, ffff800010454978 <blk_queue_enter+0x1a0>
         :                      percpu_ref_tryget_live():
         :                      bool ret = false;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count)) {
         :                      this_cpu_inc(*percpu_count);
    0.00 :   ffff800010454874:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.00 :   ffff800010454878:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_queue_enter():
         :                      if (pm || !blk_queue_pm_only(q)) {
    0.00 :   ffff80001045487c:       cbnz    w21, ffff800010454980 <blk_queue_enter+0x1a8>
         :                      __read_once_size():
    1.21 :   ffff800010454880:       ldr     w0, [x19, #112]
         :                      blk_queue_enter():
    0.00 :   ffff800010454884:       cbz     w0, ffff800010454980 <blk_queue_enter+0x1a8>
         :                      rcu_read_lock():
         :                      __rcu_read_lock();
    0.00 :   ffff800010454888:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
    0.00 :   ffff80001045488c:       ldr     x0, [x19, #1480]
         :                      __ref_is_percpu():
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff800010454890:       tst     x0, #0x3
    0.00 :   ffff800010454894:       b.ne    ffff800010454a48 <blk_queue_enter+0x270>  // b.any
         :                      get_current():
    0.00 :   ffff800010454898:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.00 :   ffff80001045489c:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff8000104548a0:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000104548a4:       str     w2, [x1, #16]
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000104548a8:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff8000104548ac:       add     x0, x0, x2
    0.00 :   ffff8000104548b0:       ldxr    x4, [x0]
    0.00 :   ffff8000104548b4:       add     x4, x4, x22
    0.00 :   ffff8000104548b8:       stxr    w3, x4, [x0]
    0.00 :   ffff8000104548bc:       cbnz    w3, ffff8000104548b0 <blk_queue_enter+0xd8>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000104548c0:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000104548c4:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000104548c8:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000104548cc:       cbz     x0, ffff800010454954 <blk_queue_enter+0x17c>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000104548d0:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000104548d4:       cbz     x0, ffff800010454954 <blk_queue_enter+0x17c>
         :                      rcu_read_unlock():
         :                      __rcu_read_unlock();
    0.00 :   ffff8000104548d8:       bl      ffff80001015eee0 <__rcu_read_unlock>
    0.00 :   ffff8000104548dc:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_queue_enter():
         :                      if (flags & BLK_MQ_REQ_NOWAIT)
    0.00 :   ffff8000104548e0:       cbnz    w23, ffff800010454964 <blk_queue_enter+0x18c>
         :                      * we need to order reading __PERCPU_REF_DEAD flag of
         :                      * .q_usage_counter and reading .mq_freeze_depth or
         :                      * queue dying flag, otherwise the following wait may
         :                      * never return if the two reads are reordered.
         :                      */
         :                      smp_rmb();
    0.00 :   ffff8000104548e4:       dmb     ishld
         :
         :                      wait_event(q->mq_freeze_wq,
    0.00 :   ffff8000104548e8:       ldr     w0, [x19, #1364]
    0.00 :   ffff8000104548ec:       cbnz    w0, ffff800010454914 <blk_queue_enter+0x13c>
    0.00 :   ffff8000104548f0:       cbnz    w21, ffff80001045491c <blk_queue_enter+0x144>
    0.00 :   ffff8000104548f4:       ldr     x0, [x19, #216]
         :                      blk_pm_request_resume():
         :                      #include <linux/pm_runtime.h>
         :
         :                      #ifdef CONFIG_PM
         :                      static inline void blk_pm_request_resume(struct request_queue *q)
         :                      {
         :                      if (q->dev && (q->rpm_status == RPM_SUSPENDED ||
    0.00 :   ffff8000104548f8:       cbz     x0, ffff80001045490c <blk_queue_enter+0x134>
    0.00 :   ffff8000104548fc:       ldr     w1, [x19, #224]
    0.00 :   ffff800010454900:       sub     w1, w1, #0x2
    0.00 :   ffff800010454904:       cmp     w1, #0x1
    0.00 :   ffff800010454908:       b.ls    ffff800010454a80 <blk_queue_enter+0x2a8>  // b.plast
         :                      __read_once_size():
    0.00 :   ffff80001045490c:       ldr     w0, [x19, #112]
         :                      blk_queue_enter():
    0.00 :   ffff800010454910:       cbz     w0, ffff80001045491c <blk_queue_enter+0x144>
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff800010454914:       ldr     x0, [x19, #104]
         :                      blk_queue_enter():
    0.00 :   ffff800010454918:       tbz     w0, #1, ffff80001045498c <blk_queue_enter+0x1b4>
         :                      test_bit():
    0.00 :   ffff80001045491c:       ldr     x0, [x19, #104]
         :                      blk_queue_enter():
         :                      (!q->mq_freeze_depth &&
         :                      (pm || (blk_pm_request_resume(q),
         :                      !blk_queue_pm_only(q)))) ||
         :                      blk_queue_dying(q));
         :                      if (blk_queue_dying(q))
    0.00 :   ffff800010454920:       tbz     w0, #1, ffff800010454820 <blk_queue_enter+0x48>
         :                      return -ENODEV;
    0.00 :   ffff800010454924:       mov     w0, #0xffffffed                 // #-19
         :                      }
         :                      }
    0.00 :   ffff800010454928:       add     x25, x25, #0x8c8
    0.97 :   ffff80001045492c:       ldr     x2, [x29, #120]
    1.70 :   ffff800010454930:       ldr     x1, [x25]
    0.00 :   ffff800010454934:       eor     x1, x2, x1
    0.00 :   ffff800010454938:       cbnz    x1, ffff800010454a98 <blk_queue_enter+0x2c0>
    0.00 :   ffff80001045493c:       ldp     x19, x20, [sp, #16]
    2.65 :   ffff800010454940:       ldp     x21, x22, [sp, #32]
    4.84 :   ffff800010454944:       ldp     x23, x24, [sp, #48]
    3.61 :   ffff800010454948:       ldr     x25, [sp, #64]
    2.64 :   ffff80001045494c:       ldp     x29, x30, [sp], #128
    0.00 :   ffff800010454950:       ret
         :                      percpu_ref_put_many():
         :                      unsigned long __percpu *percpu_count;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count))
         :                      this_cpu_sub(*percpu_count, nr);
    0.00 :   ffff800010454954:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      rcu_read_unlock():
    0.00 :   ffff800010454958:       bl      ffff80001015eee0 <__rcu_read_unlock>
    0.00 :   ffff80001045495c:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_queue_enter():
         :                      if (flags & BLK_MQ_REQ_NOWAIT)
    0.00 :   ffff800010454960:       cbz     w23, ffff8000104548e4 <blk_queue_enter+0x10c>
         :                      return -EBUSY;
    0.00 :   ffff800010454964:       mov     w0, #0xfffffff0                 // #-16
    0.00 :   ffff800010454968:       b       ffff800010454928 <blk_queue_enter+0x150>
         :                      __ll_sc__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  , rel_, 64,        ,  , l, "memory", L)
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  ,  mb_, 64, dmb ish,  , l, "memory", L)
    0.00 :   ffff80001045496c:       b       ffff800010456420 <blk_finish_plug+0x138>
         :                      atomic64_try_cmpxchg():
         :                      static inline bool
         :                      atomic64_try_cmpxchg(atomic64_t *v, s64 *old, s64 new)
         :                      {
         :                      s64 r, o = *old;
         :                      r = atomic64_cmpxchg(v, o, new);
         :                      if (unlikely(r != o))
    0.00 :   ffff800010454970:       cmp     x0, x3
    0.00 :   ffff800010454974:       b.ne    ffff800010454a8c <blk_queue_enter+0x2b4>  // b.any
         :                      rcu_read_unlock():
    4.84 :   ffff800010454978:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_queue_enter():
         :                      if (pm || !blk_queue_pm_only(q)) {
    2.18 :   ffff80001045497c:       cbz     w21, ffff800010454880 <blk_queue_enter+0xa8>
         :                      rcu_read_unlock():
    1.93 :   ffff800010454980:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_queue_enter():
         :                      return 0;
    0.73 :   ffff800010454984:       mov     w0, #0x0                        // #0
    0.00 :   ffff800010454988:       b       ffff800010454928 <blk_queue_enter+0x150>
    0.00 :   ffff80001045498c:       str     x26, [x29, #72]
         :                      wait_event(q->mq_freeze_wq,
    0.00 :   ffff800010454990:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010454994:       add     x0, x29, #0x50
    0.00 :   ffff800010454998:       add     x26, x19, #0x588
    0.00 :   ffff80001045499c:       bl      ffff80001012e648 <init_wait_entry>
    0.00 :   ffff8000104549a0:       b       ffff8000104549b8 <blk_queue_enter+0x1e0>
         :                      __read_once_size():
    0.00 :   ffff8000104549a4:       ldr     w0, [x19, #112]
         :                      blk_queue_enter():
    0.00 :   ffff8000104549a8:       cbz     w0, ffff800010454a00 <blk_queue_enter+0x228>
         :                      test_bit():
    0.00 :   ffff8000104549ac:       ldr     x0, [x19, #104]
         :                      blk_queue_enter():
    0.00 :   ffff8000104549b0:       tbnz    w0, #1, ffff800010454a00 <blk_queue_enter+0x228>
    0.00 :   ffff8000104549b4:       bl      ffff800010cad270 <schedule>
    0.00 :   ffff8000104549b8:       mov     w2, #0x2                        // #2
    0.00 :   ffff8000104549bc:       add     x1, x29, #0x50
    0.00 :   ffff8000104549c0:       mov     x0, x26
    0.00 :   ffff8000104549c4:       bl      ffff80001012e740 <prepare_to_wait_event>
    0.00 :   ffff8000104549c8:       ldr     w0, [x19, #1364]
    0.00 :   ffff8000104549cc:       cbnz    w0, ffff8000104549ac <blk_queue_enter+0x1d4>
    0.00 :   ffff8000104549d0:       cbnz    w21, ffff800010454a00 <blk_queue_enter+0x228>
    0.00 :   ffff8000104549d4:       ldr     x0, [x19, #216]
         :                      blk_pm_request_resume():
    0.00 :   ffff8000104549d8:       cbz     x0, ffff8000104549a4 <blk_queue_enter+0x1cc>
    0.00 :   ffff8000104549dc:       ldr     w1, [x19, #224]
    0.00 :   ffff8000104549e0:       sub     w1, w1, #0x2
    0.00 :   ffff8000104549e4:       cmp     w1, #0x1
    0.00 :   ffff8000104549e8:       b.hi    ffff8000104549a4 <blk_queue_enter+0x1cc>  // b.pmore
         :                      pm_request_resume():
         :                      return __pm_runtime_idle(dev, RPM_ASYNC);
         :                      }
         :
         :                      static inline int pm_request_resume(struct device *dev)
         :                      {
         :                      return __pm_runtime_resume(dev, RPM_ASYNC);
    0.00 :   ffff8000104549ec:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000104549f0:       bl      ffff8000107295d0 <__pm_runtime_resume>
         :                      __read_once_size():
    0.00 :   ffff8000104549f4:       ldr     w0, [x19, #112]
         :                      blk_queue_enter():
    0.00 :   ffff8000104549f8:       cbnz    w0, ffff8000104549ac <blk_queue_enter+0x1d4>
    0.00 :   ffff8000104549fc:       nop
    0.00 :   ffff800010454a00:       mov     x0, x26
    0.00 :   ffff800010454a04:       add     x1, x29, #0x50
    0.00 :   ffff800010454a08:       bl      ffff80001012e670 <finish_wait>
    0.00 :   ffff800010454a0c:       ldr     x26, [x29, #72]
    0.00 :   ffff800010454a10:       b       ffff80001045491c <blk_queue_enter+0x144>
         :                      percpu_ref_tryget_live():
         :                      } else if (!(ref->percpu_count_ptr & __PERCPU_REF_DEAD)) {
    0.00 :   ffff800010454a14:       ldr     x0, [x24, #8]
    0.00 :   ffff800010454a18:       tbnz    w0, #1, ffff8000104548d8 <blk_queue_enter+0x100>
         :                      __read_once_size():
    0.00 :   ffff800010454a1c:       ldr     x3, [x19, #1472]
         :                      atomic64_fetch_add_unless():
         :                      atomic64_fetch_add_unless(atomic64_t *v, s64 a, s64 u)
         :                      {
         :                      s64 c = atomic64_read(v);
         :
         :                      do {
         :                      if (unlikely(c == u))
    0.00 :   ffff800010454a20:       cbz     x3, ffff8000104548d8 <blk_queue_enter+0x100>
         :                      break;
         :                      } while (!atomic64_try_cmpxchg(v, &c, c + a));
    0.00 :   ffff800010454a24:       add     x2, x3, #0x1
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010454a28:       b       ffff80001045496c <blk_queue_enter+0x194>
    0.00 :   ffff800010454a2c:       b       ffff80001045496c <blk_queue_enter+0x194>
         :                      __lse__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,  l, "memory")
         :                      __CMPXCHG_CASE(x,  , rel_, 64,  l, "memory")
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, al, "memory")
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, al, "memory")
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, al, "memory")
         :                      __CMPXCHG_CASE(x,  ,  mb_, 64, al, "memory")
    0.00 :   ffff800010454a30:       mov     x0, x24
    0.00 :   ffff800010454a34:       mov     x1, x3
    0.00 :   ffff800010454a38:       mov     x4, x1
    0.00 :   ffff800010454a3c:       casal   x4, x2, [x24]
    0.00 :   ffff800010454a40:       mov     x0, x4
    0.00 :   ffff800010454a44:       b       ffff800010454970 <blk_queue_enter+0x198>
         :                      arch_static_branch_jump():
    0.00 :   ffff800010454a48:       b       ffff800010454a74 <blk_queue_enter+0x29c>
    0.00 :   ffff800010454a4c:       b       ffff800010454a74 <blk_queue_enter+0x29c>
         :                      __lse_atomic64_sub_return():
         :                      ATOMIC64_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff800010454a50:       mov     x0, x20
    0.00 :   ffff800010454a54:       neg     x0, x0
    0.00 :   ffff800010454a58:       ldaddal x0, x1, [x24]
    0.00 :   ffff800010454a5c:       add     x0, x0, x1
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff800010454a60:       cbnz    x0, ffff8000104548d8 <blk_queue_enter+0x100>
         :                      ref->release(ref);
    0.00 :   ffff800010454a64:       ldr     x1, [x24, #16]
    0.00 :   ffff800010454a68:       mov     x0, x24
    0.00 :   ffff800010454a6c:       blr     x1
    0.00 :   ffff800010454a70:       b       ffff8000104548d8 <blk_queue_enter+0x100>
         :                      __ll_sc_atomic64_sub_return():
         :                      ATOMIC64_OPS(sub, sub, J)
    0.00 :   ffff800010454a74:       b       ffff800010456440 <blk_finish_plug+0x158>
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff800010454a78:       cbnz    x0, ffff8000104548d8 <blk_queue_enter+0x100>
    0.00 :   ffff800010454a7c:       b       ffff800010454a64 <blk_queue_enter+0x28c>
         :                      pm_request_resume():
    0.00 :   ffff800010454a80:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010454a84:       bl      ffff8000107295d0 <__pm_runtime_resume>
    0.00 :   ffff800010454a88:       b       ffff80001045490c <blk_queue_enter+0x134>
    0.00 :   ffff800010454a8c:       mov     x3, x0
         :                      atomic64_fetch_add_unless():
         :                      if (unlikely(c == u))
    0.00 :   ffff800010454a90:       cbnz    x0, ffff800010454a24 <blk_queue_enter+0x24c>
    0.00 :   ffff800010454a94:       b       ffff8000104548d8 <blk_queue_enter+0x100>
    0.00 :   ffff800010454a98:       str     x26, [x29, #72]
         :                      blk_queue_enter():
         :                      }
    0.00 :   ffff800010454a9c:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (256 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010117488 <idle_cpu>:
         :                      idle_cpu():
         :                      *
         :                      * Return: 1 if the CPU is currently idle. 0 otherwise.
         :                      */
         :                      int idle_cpu(int cpu)
         :                      {
         :                      struct rq *rq = cpu_rq(cpu);
    3.55 :   ffff800010117488:       adrp    x2, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff80001011748c:       add     x2, x2, #0x8e8
    0.00 :   ffff800010117490:       adrp    x1, ffff8000114d5000 <tegra_to+0x180>
    0.00 :   ffff800010117494:       add     x1, x1, #0xd80
    0.44 :   ffff800010117498:       ldr     x0, [x2, w0, sxtw #3]
    0.00 :   ffff80001011749c:       add     x1, x1, x0
         :
         :                      if (rq->curr != rq->idle)
    5.01 :   ffff8000101174a0:       ldr     x2, [x1, #2352]
   55.15 :   ffff8000101174a4:       ldr     x0, [x1, #2360]
    0.39 :   ffff8000101174a8:       cmp     x2, x0
    0.00 :   ffff8000101174ac:       b.eq    ffff8000101174b8 <idle_cpu+0x30>  // b.none
         :                      return 0;
    0.00 :   ffff8000101174b0:       mov     w0, #0x0                        // #0
         :                      if (!llist_empty(&rq->wake_list))
         :                      return 0;
         :                      #endif
         :
         :                      return 1;
         :                      }
    0.36 :   ffff8000101174b4:       ret
         :                      if (rq->nr_running)
    4.55 :   ffff8000101174b8:       ldr     w2, [x1, #4]
         :                      return 0;
    0.42 :   ffff8000101174bc:       mov     w0, #0x0                        // #0
         :                      if (rq->nr_running)
    2.38 :   ffff8000101174c0:       cbnz    w2, ffff8000101174b4 <idle_cpu+0x2c>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
   25.33 :   ffff8000101174c4:       ldr     x0, [x1, #2976]
         :                      idle_cpu():
         :                      if (!llist_empty(&rq->wake_list))
    0.78 :   ffff8000101174c8:       cmp     x0, #0x0
    1.23 :   ffff8000101174cc:       cset    w0, eq  // eq = none
         :                      }
    0.42 :   ffff8000101174d0:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (410 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102c2c10 <blkdev_read_iter>:
         :                      blkdev_read_iter():
         :                      ssize_t blkdev_read_iter(struct kiocb *iocb, struct iov_iter *to)
         :                      {
         :                      struct file *file = iocb->ki_filp;
         :                      struct inode *bd_inode = bdev_file_inode(file);
         :                      loff_t size = i_size_read(bd_inode);
         :                      loff_t pos = iocb->ki_pos;
   12.91 :   ffff8000102c2c10:       ldp     x2, x3, [x0]
         :                      bdev_file_inode():
         :                      return file->f_mapping->host;
   25.12 :   ffff8000102c2c14:       ldr     x2, [x2, #240]
         :                      blkdev_read_iter():
         :                      loff_t size = i_size_read(bd_inode);
   19.35 :   ffff8000102c2c18:       ldr     x2, [x2]
   15.34 :   ffff8000102c2c1c:       ldr     x2, [x2, #80]
         :
         :                      if (pos >= size)
   13.90 :   ffff8000102c2c20:       cmp     x3, x2
    0.00 :   ffff8000102c2c24:       b.ge    ffff8000102c2c50 <blkdev_read_iter+0x40>  // b.tcont
         :                      {
    0.00 :   ffff8000102c2c28:       stp     x29, x30, [sp, #-16]!
         :                      return 0;
         :
         :                      size -= pos;
    0.00 :   ffff8000102c2c2c:       sub     x2, x2, x3
         :                      {
    0.00 :   ffff8000102c2c30:       mov     x29, sp
         :                      iov_iter_truncate():
         :                      * count doesn't have to fit in size_t - comparison extends both
         :                      * operands to u64 here and any value that would be truncated by
         :                      * conversion in assignement is by definition greater than all
         :                      * values of size_t, including old i->count.
         :                      */
         :                      if (i->count > count)
    3.66 :   ffff8000102c2c34:       ldr     x4, [x1, #16]
    0.00 :   ffff8000102c2c38:       cmp     x2, x4
    0.00 :   ffff8000102c2c3c:       b.cs    ffff8000102c2c44 <blkdev_read_iter+0x34>  // b.hs, b.nlast
         :                      i->count = count;
    0.00 :   ffff8000102c2c40:       str     x2, [x1, #16]
         :                      blkdev_read_iter():
         :                      iov_iter_truncate(to, size);
         :                      return generic_file_read_iter(iocb, to);
    5.82 :   ffff8000102c2c44:       bl      ffff8000101d3170 <generic_file_read_iter>
         :                      }
    3.91 :   ffff8000102c2c48:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000102c2c4c:       ret
         :                      return 0;
    0.00 :   ffff8000102c2c50:       mov     x0, #0x0                        // #0
         :                      }
    0.00 :   ffff8000102c2c54:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (424 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104615d0 <blk_mq_try_issue_list_directly>:
         :                      blk_mq_try_issue_list_directly():
         :                      return ret;
         :                      }
         :
         :                      void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
         :                      struct list_head *list)
         :                      {
    0.00 :   ffff8000104615d0:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff8000104615d4:       mov     x29, sp
    3.54 :   ffff8000104615d8:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000104615dc:       mov     x19, x1
    1.41 :   ffff8000104615e0:       str     x21, [sp, #32]
    0.00 :   ffff8000104615e4:       mov     x21, x0
         :                      while (!list_empty(list)) {
    0.00 :   ffff8000104615e8:       b       ffff800010461624 <blk_mq_try_issue_list_directly+0x54>
         :                      blk_status_t ret;
         :                      struct request *rq = list_first_entry(list, struct request,
   10.63 :   ffff8000104615ec:       ldr     x1, [x19]
    0.00 :   ffff8000104615f0:       sub     x20, x1, #0x48
         :                      __list_del_entry():
         :                      static inline void __list_del_entry(struct list_head *entry)
         :                      {
         :                      if (!__list_del_entry_valid(entry))
         :                      return;
         :
         :                      __list_del(entry->prev, entry->next);
    1.90 :   ffff8000104615f4:       ldp     x3, x2, [x1]
         :                      __list_del():
         :                      next->prev = prev;
   25.50 :   ffff8000104615f8:       str     x2, [x3, #8]
         :                      blk_mq_try_issue_list_directly():
         :                      queuelist);
         :
         :                      list_del_init(&rq->queuelist);
         :                      ret = blk_mq_request_issue_directly(rq, list_empty(list));
    0.00 :   ffff8000104615fc:       mov     x0, x20
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
   12.50 :   ffff800010461600:       str     x3, [x2]
    0.24 :   ffff800010461604:       str     x1, [x1]
         :                      INIT_LIST_HEAD():
         :                      list->prev = list;
    0.00 :   ffff800010461608:       str     x1, [x1, #8]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
   23.35 :   ffff80001046160c:       ldr     x1, [x19]
         :                      list_empty():
         :                      * list_empty - tests whether a list is empty
         :                      * @head: the list to test.
         :                      */
         :                      static inline int list_empty(const struct list_head *head)
         :                      {
         :                      return READ_ONCE(head->next) == head;
    0.00 :   ffff800010461610:       cmp     x19, x1
         :                      blk_mq_try_issue_list_directly():
    0.00 :   ffff800010461614:       cset    w1, eq  // eq = none
    0.00 :   ffff800010461618:       bl      ffff800010461540 <blk_mq_request_issue_directly>
         :                      if (ret != BLK_STS_OK) {
    4.01 :   ffff80001046161c:       ands    w0, w0, #0xff
    0.24 :   ffff800010461620:       b.ne    ffff800010461664 <blk_mq_try_issue_list_directly+0x94>  // b.any
         :                      __read_once_size():
    2.34 :   ffff800010461624:       ldr     x0, [x19]
         :                      blk_mq_try_issue_list_directly():
         :                      while (!list_empty(list)) {
    0.00 :   ffff800010461628:       cmp     x19, x0
    0.00 :   ffff80001046162c:       b.ne    ffff8000104615ec <blk_mq_try_issue_list_directly+0x1c>  // b.any
         :                      __read_once_size():
    7.99 :   ffff800010461630:       ldr     x0, [x19]
         :                      blk_mq_try_issue_list_directly():
         :                      /*
         :                      * If we didn't flush the entire list, we could have told
         :                      * the driver there was more coming, but that turned out to
         :                      * be a lie.
         :                      */
         :                      if (!list_empty(list) && hctx->queue->mq_ops->commit_rqs)
    0.23 :   ffff800010461634:       cmp     x19, x0
    0.00 :   ffff800010461638:       b.eq    ffff800010461654 <blk_mq_try_issue_list_directly+0x84>  // b.none
    0.00 :   ffff80001046163c:       ldr     x0, [x21, #208]
    0.00 :   ffff800010461640:       ldr     x0, [x0, #48]
    0.00 :   ffff800010461644:       ldr     x1, [x0, #8]
    0.00 :   ffff800010461648:       cbz     x1, ffff800010461654 <blk_mq_try_issue_list_directly+0x84>
         :                      hctx->queue->mq_ops->commit_rqs(hctx);
    0.00 :   ffff80001046164c:       mov     x0, x21
    0.00 :   ffff800010461650:       blr     x1
         :                      }
    1.87 :   ffff800010461654:       ldp     x19, x20, [sp, #16]
    3.77 :   ffff800010461658:       ldr     x21, [sp, #32]
    0.47 :   ffff80001046165c:       ldp     x29, x30, [sp], #48
    0.00 :   ffff800010461660:       ret
         :                      if (ret == BLK_STS_RESOURCE ||
    0.00 :   ffff800010461664:       and     w2, w0, #0xfffffffb
         :                      blk_mq_end_request(rq, ret);
    0.00 :   ffff800010461668:       mov     w1, w0
         :                      if (ret == BLK_STS_RESOURCE ||
    0.00 :   ffff80001046166c:       cmp     w2, #0x9
    0.00 :   ffff800010461670:       b.eq    ffff800010461680 <blk_mq_try_issue_list_directly+0xb0>  // b.none
         :                      blk_mq_end_request(rq, ret);
    0.00 :   ffff800010461674:       mov     x0, x20
    0.00 :   ffff800010461678:       bl      ffff80001045e968 <blk_mq_end_request>
    0.00 :   ffff80001046167c:       b       ffff800010461624 <blk_mq_try_issue_list_directly+0x54>
         :                      __read_once_size():
    0.00 :   ffff800010461680:       ldr     x1, [x19]
         :                      blk_mq_try_issue_list_directly():
         :                      blk_mq_request_bypass_insert(rq,
    0.00 :   ffff800010461684:       mov     x0, x20
         :                      list_empty():
    0.00 :   ffff800010461688:       cmp     x19, x1
         :                      blk_mq_try_issue_list_directly():
    0.00 :   ffff80001046168c:       cset    w1, eq  // eq = none
    0.00 :   ffff800010461690:       bl      ffff800010460a38 <blk_mq_request_bypass_insert>
         :                      break;
    0.00 :   ffff800010461694:       b       ffff800010461630 <blk_mq_try_issue_list_directly+0x60>
 Percent |	Source code & Disassembly of vmlinux for cycles (395 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102d6c18 <do_io_getevents>:
         :                      do_io_getevents():
         :                      static long do_io_getevents(aio_context_t ctx_id,
         :                      long min_nr,
         :                      long nr,
         :                      struct io_event __user *events,
         :                      struct timespec64 *ts)
         :                      {
    1.50 :   ffff8000102d6c18:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff8000102d6c1c:       mov     x29, sp
    3.03 :   ffff8000102d6c20:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000102d6c24:       mov     x20, x1
    2.03 :   ffff8000102d6c28:       stp     x21, x22, [sp, #32]
         :                      ktime_t until = ts ? timespec64_to_ktime(*ts) : KTIME_MAX;
    0.00 :   ffff8000102d6c2c:       mov     x19, #0x7fffffffffffffff        // #9223372036854775807
         :                      {
    2.79 :   ffff8000102d6c30:       str     x23, [sp, #48]
    0.00 :   ffff8000102d6c34:       mov     x22, x2
    0.00 :   ffff8000102d6c38:       mov     x23, x3
         :                      ktime_t until = ts ? timespec64_to_ktime(*ts) : KTIME_MAX;
    0.00 :   ffff8000102d6c3c:       cbz     x4, ffff8000102d6c64 <do_io_getevents+0x4c>
    0.00 :   ffff8000102d6c40:       ldp     x1, x3, [x4]
         :                      ktime_set():
         :                      *
         :                      * Return: The ktime_t representation of the value.
         :                      */
         :                      static inline ktime_t ktime_set(const s64 secs, const unsigned long nsecs)
         :                      {
         :                      if (unlikely(secs >= KTIME_SEC_MAX))
    0.00 :   ffff8000102d6c44:       mov     x2, #0x7d03                     // #32003
    0.00 :   ffff8000102d6c48:       movk    x2, #0x25c1, lsl #16
    0.00 :   ffff8000102d6c4c:       movk    x2, #0x2, lsl #32
    0.00 :   ffff8000102d6c50:       cmp     x1, x2
    0.00 :   ffff8000102d6c54:       b.gt    ffff8000102d6c64 <do_io_getevents+0x4c>
         :                      return KTIME_MAX;
         :
         :                      return secs * NSEC_PER_SEC + (s64)nsecs;
    0.00 :   ffff8000102d6c58:       mov     x19, #0xca00                    // #51712
    0.00 :   ffff8000102d6c5c:       movk    x19, #0x3b9a, lsl #16
    0.00 :   ffff8000102d6c60:       madd    x19, x1, x19, x3
         :                      do_io_getevents():
         :                      struct kioctx *ioctx = lookup_ioctx(ctx_id);
    1.02 :   ffff8000102d6c64:       bl      ffff8000102d66c8 <lookup_ioctx>
    1.00 :   ffff8000102d6c68:       mov     x21, x0
         :                      long ret = -EINVAL;
         :
         :                      if (likely(ioctx)) {
    0.00 :   ffff8000102d6c6c:       cbz     x0, ffff8000102d6d10 <do_io_getevents+0xf8>
         :                      if (likely(min_nr <= nr && min_nr >= 0))
    1.27 :   ffff8000102d6c70:       cmp     x20, #0x0
    0.00 :   ffff8000102d6c74:       ccmp    x20, x22, #0x0, ge  // ge = tcont
    0.00 :   ffff8000102d6c78:       b.gt    ffff8000102d6d2c <do_io_getevents+0x114>
         :                      ret = read_events(ioctx, min_nr, nr, events, until);
    0.00 :   ffff8000102d6c7c:       mov     x4, x19
    0.50 :   ffff8000102d6c80:       mov     x3, x23
    0.00 :   ffff8000102d6c84:       mov     x2, x22
    0.00 :   ffff8000102d6c88:       mov     x1, x20
    2.78 :   ffff8000102d6c8c:       bl      ffff8000102d64c8 <read_events>
    0.00 :   ffff8000102d6c90:       mov     x19, x0
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff8000102d6c94:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d6c98:       ldr     x0, [x21, #8]
         :                      __ref_is_percpu():
         :                      * Theoretically, the following could test just ATOMIC; however,
         :                      * then we'd have to mask off DEAD separately as DEAD may be
         :                      * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
         :                      * implies ATOMIC anyway.  Test them together.
         :                      */
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff8000102d6c9c:       tst     x0, #0x3
    0.00 :   ffff8000102d6ca0:       b.ne    ffff8000102d6d34 <do_io_getevents+0x11c>  // b.any
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.50 :   ffff8000102d6ca4:       mrs     x1, sp_el0
         :                      __read_once_size():
    3.29 :   ffff8000102d6ca8:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff8000102d6cac:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    1.26 :   ffff8000102d6cb0:       str     w2, [x1, #16]
         :                      __percpu_add_case_64():
         :
         :                      PERCPU_RW_OPS(8)
         :                      PERCPU_RW_OPS(16)
         :                      PERCPU_RW_OPS(32)
         :                      PERCPU_RW_OPS(64)
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff8000102d6cb4:       mov     x3, #0xffffffffffffffff         // #-1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000102d6cb8:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.51 :   ffff8000102d6cbc:       add     x0, x0, x2
    4.06 :   ffff8000102d6cc0:       ldxr    x5, [x0]
   35.94 :   ffff8000102d6cc4:       add     x5, x5, x3
    0.00 :   ffff8000102d6cc8:       stxr    w4, x5, [x0]
    0.00 :   ffff8000102d6ccc:       cbnz    w4, ffff8000102d6cc0 <do_io_getevents+0xa8>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102d6cd0:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000102d6cd4:       add     x0, x0, x3
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    4.84 :   ffff8000102d6cd8:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000102d6cdc:       cbnz    x0, ffff8000102d6d00 <do_io_getevents+0xe8>
         :                      percpu_ref_put_many():
         :                      unsigned long __percpu *percpu_count;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count))
         :                      this_cpu_sub(*percpu_count, nr);
    0.00 :   ffff8000102d6ce0:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
   28.11 :   ffff8000102d6ce4:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      do_io_getevents():
         :                      percpu_ref_put(&ioctx->users);
         :                      }
         :
         :                      return ret;
         :                      }
    0.00 :   ffff8000102d6ce8:       ldr     x23, [sp, #48]
    0.00 :   ffff8000102d6cec:       mov     x0, x19
    0.00 :   ffff8000102d6cf0:       ldp     x19, x20, [sp, #16]
    5.33 :   ffff8000102d6cf4:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000102d6cf8:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000102d6cfc:       ret
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.25 :   ffff8000102d6d00:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000102d6d04:       cbnz    x0, ffff8000102d6ce4 <do_io_getevents+0xcc>
         :                      percpu_ref_put_many():
    0.00 :   ffff8000102d6d08:       bl      ffff800010cad668 <preempt_schedule_notrace>
    0.00 :   ffff8000102d6d0c:       b       ffff8000102d6ce4 <do_io_getevents+0xcc>
         :                      do_io_getevents():
         :                      long ret = -EINVAL;
    0.00 :   ffff8000102d6d10:       mov     x19, #0xffffffffffffffea        // #-22
         :                      }
    0.00 :   ffff8000102d6d14:       ldr     x23, [sp, #48]
    0.00 :   ffff8000102d6d18:       mov     x0, x19
    0.00 :   ffff8000102d6d1c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000102d6d20:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000102d6d24:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000102d6d28:       ret
         :                      long ret = -EINVAL;
    0.00 :   ffff8000102d6d2c:       mov     x19, #0xffffffffffffffea        // #-22
    0.00 :   ffff8000102d6d30:       b       ffff8000102d6c94 <do_io_getevents+0x7c>
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000102d6d34:       b       ffff8000102d6d60 <do_io_getevents+0x148>
    0.00 :   ffff8000102d6d38:       b       ffff8000102d6d60 <do_io_getevents+0x148>
         :                      __lse_atomic64_sub_return():
         :                      }
         :
         :                      ATOMIC64_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC64_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC64_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff8000102d6d3c:       mov     x0, #0x1                        // #1
    0.00 :   ffff8000102d6d40:       neg     x0, x0
    0.00 :   ffff8000102d6d44:       ldaddal x0, x1, [x21]
    0.00 :   ffff8000102d6d48:       add     x0, x0, x1
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff8000102d6d4c:       cbnz    x0, ffff8000102d6ce4 <do_io_getevents+0xcc>
         :                      ref->release(ref);
    0.00 :   ffff8000102d6d50:       ldr     x1, [x21, #16]
    0.00 :   ffff8000102d6d54:       mov     x0, x21
    0.00 :   ffff8000102d6d58:       blr     x1
    0.00 :   ffff8000102d6d5c:       b       ffff8000102d6ce4 <do_io_getevents+0xcc>
         :                      __ll_sc_atomic64_sub_return():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(add, add, I)
         :                      ATOMIC64_OPS(sub, sub, J)
    0.00 :   ffff8000102d6d60:       mov     x1, #0x1                        // #1
    0.00 :   ffff8000102d6d64:       b       ffff8000102da598 <__arm64_compat_sys_io_pgetevents_time64+0x2d8>
         :                      percpu_ref_put_many():
         :                      else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
    0.00 :   ffff8000102d6d68:       cbnz    x0, ffff8000102d6ce4 <do_io_getevents+0xcc>
    0.00 :   ffff8000102d6d6c:       b       ffff8000102d6d50 <do_io_getevents+0x138>
 Percent |	Source code & Disassembly of vmlinux for cycles (410 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000106fcb28 <__iommu_map>:
         :                      __iommu_map():
         :                      return pgsize;
         :                      }
         :
         :                      int __iommu_map(struct iommu_domain *domain, unsigned long iova,
         :                      phys_addr_t paddr, size_t size, int prot, gfp_t gfp)
         :                      {
    3.42 :   ffff8000106fcb28:       stp     x29, x30, [sp, #-112]!
    0.00 :   ffff8000106fcb2c:       mov     x29, sp
    0.00 :   ffff8000106fcb30:       stp     x21, x22, [sp, #32]
    0.00 :   ffff8000106fcb34:       mov     x21, x0
    0.96 :   ffff8000106fcb38:       str     x28, [sp, #88]
    0.00 :   ffff8000106fcb3c:       str     x2, [x29, #104]
         :                      const struct iommu_ops *ops = domain->ops;
    3.91 :   ffff8000106fcb40:       ldr     x22, [x0, #8]
         :                      unsigned int min_pagesz;
         :                      size_t orig_size = size;
         :                      phys_addr_t orig_paddr = paddr;
         :                      int ret = 0;
         :
         :                      if (unlikely(ops->map == NULL ||
    0.24 :   ffff8000106fcb44:       ldr     x0, [x22, #40]
    0.00 :   ffff8000106fcb48:       cbz     x0, ffff8000106fcc90 <__iommu_map+0x168>
    2.18 :   ffff8000106fcb4c:       ldr     x0, [x21, #16]
    0.00 :   ffff8000106fcb50:       cbz     x0, ffff8000106fcc90 <__iommu_map+0x168>
         :                      domain->pgsize_bitmap == 0UL))
         :                      return -ENODEV;
         :
         :                      if (unlikely(!(domain->type & __IOMMU_DOMAIN_PAGING)))
    1.23 :   ffff8000106fcb54:       ldr     w6, [x21]
    0.00 :   ffff8000106fcb58:       tbz     w6, #0, ffff8000106fcca8 <__iommu_map+0x180>
         :                      phys_addr_t orig_paddr = paddr;
    2.67 :   ffff8000106fcb5c:       mov     x6, x2
         :                      __ffs():
         :                      *
         :                      * Undefined if no bit exists, so code should check against 0 first.
         :                      */
         :                      static __always_inline unsigned long __ffs(unsigned long word)
         :                      {
         :                      return __builtin_ctzl(word);
    0.00 :   ffff8000106fcb60:       rbit    x2, x0
    0.00 :   ffff8000106fcb64:       clz     x2, x2
    0.49 :   ffff8000106fcb68:       stp     x23, x24, [x29, #48]
    0.00 :   ffff8000106fcb6c:       stp     x25, x26, [x29, #64]
    0.00 :   ffff8000106fcb70:       mov     w24, w5
    0.00 :   ffff8000106fcb74:       mov     w23, w4
         :                      __iommu_map():
         :                      /*
         :                      * both the virtual address and the physical one, as well as
         :                      * the size of the mapping, must be aligned (at least) to the
         :                      * size of the smallest page supported by the hardware
         :                      */
         :                      if (!IS_ALIGNED(iova | paddr | size, min_pagesz)) {
    0.00 :   ffff8000106fcb78:       orr     x5, x3, x6
         :                      min_pagesz = 1 << __ffs(domain->pgsize_bitmap);
    0.98 :   ffff8000106fcb7c:       mov     w4, #0x1                        // #1
         :                      if (!IS_ALIGNED(iova | paddr | size, min_pagesz)) {
    0.00 :   ffff8000106fcb80:       orr     x5, x5, x1
         :                      min_pagesz = 1 << __ffs(domain->pgsize_bitmap);
    0.00 :   ffff8000106fcb84:       lsl     w2, w4, w2
    0.00 :   ffff8000106fcb88:       mov     x4, x2
         :                      if (!IS_ALIGNED(iova | paddr | size, min_pagesz)) {
    2.19 :   ffff8000106fcb8c:       sub     x2, x2, #0x1
    0.00 :   ffff8000106fcb90:       mov     x26, x1
    0.00 :   ffff8000106fcb94:       mov     x25, x3
    0.00 :   ffff8000106fcb98:       tst     x5, x2
    0.74 :   ffff8000106fcb9c:       b.ne    ffff8000106fccb0 <__iommu_map+0x188>  // b.any
    0.00 :   ffff8000106fcba0:       str     x20, [x29, #24]
         :                      return -EINVAL;
         :                      }
         :
         :                      pr_debug("map: iova 0x%lx pa %pa size 0x%zx\n", iova, &paddr, size);
         :
         :                      while (size) {
    0.00 :   ffff8000106fcba4:       mov     x20, x1
    2.44 :   ffff8000106fcba8:       str     x27, [x29, #80]
    0.00 :   ffff8000106fcbac:       mov     x27, x3
    0.00 :   ffff8000106fcbb0:       cbz     x3, ffff8000106fcc58 <__iommu_map+0x130>
    2.20 :   ffff8000106fcbb4:       str     x19, [x29, #16]
    0.00 :   ffff8000106fcbb8:       b       ffff8000106fcbd4 <__iommu_map+0xac>
         :
         :                      if (ret)
         :                      break;
         :
         :                      iova += pgsize;
         :                      paddr += pgsize;
    1.22 :   ffff8000106fcbbc:       ldr     x6, [x29, #104]
         :                      while (size) {
    1.70 :   ffff8000106fcbc0:       subs    x27, x27, x19
         :                      paddr += pgsize;
    0.00 :   ffff8000106fcbc4:       add     x6, x19, x6
    8.56 :   ffff8000106fcbc8:       str     x6, [x29, #104]
         :                      while (size) {
    0.00 :   ffff8000106fcbcc:       b.eq    ffff8000106fcc54 <__iommu_map+0x12c>  // b.none
    0.00 :   ffff8000106fcbd0:       ldr     x0, [x21, #16]
         :                      size_t pgsize = iommu_pgsize(domain, iova | paddr, size);
    0.25 :   ffff8000106fcbd4:       orr     x1, x6, x20
    0.00 :   ffff8000106fcbd8:       mov     x2, x27
    0.00 :   ffff8000106fcbdc:       bl      ffff8000106fb958 <iommu_pgsize.isra.21>
    0.00 :   ffff8000106fcbe0:       mov     x19, x0
         :                      ret = ops->map(domain, iova, paddr, pgsize, prot, gfp);
    0.00 :   ffff8000106fcbe4:       ldr     x7, [x22, #40]
    0.00 :   ffff8000106fcbe8:       mov     x1, x20
    0.00 :   ffff8000106fcbec:       mov     x0, x21
    0.00 :   ffff8000106fcbf0:       mov     x2, x6
    1.46 :   ffff8000106fcbf4:       mov     w5, w24
    0.00 :   ffff8000106fcbf8:       mov     w4, w23
    0.00 :   ffff8000106fcbfc:       mov     x3, x19
         :                      iova += pgsize;
    0.00 :   ffff8000106fcc00:       add     x20, x20, x19
         :                      ret = ops->map(domain, iova, paddr, pgsize, prot, gfp);
    2.20 :   ffff8000106fcc04:       blr     x7
    8.30 :   ffff8000106fcc08:       mov     w28, w0
         :                      if (ret)
    0.00 :   ffff8000106fcc0c:       cbz     w0, ffff8000106fcbbc <__iommu_map+0x94>
         :                      size -= pgsize;
         :                      }
         :
         :                      if (ops->iotlb_sync_map)
    0.00 :   ffff8000106fcc10:       ldr     x1, [x22, #64]
    0.00 :   ffff8000106fcc14:       cbz     x1, ffff8000106fcc20 <__iommu_map+0xf8>
         :                      ops->iotlb_sync_map(domain);
    0.00 :   ffff8000106fcc18:       mov     x0, x21
    0.00 :   ffff8000106fcc1c:       blr     x1
         :
         :                      /* unroll mapping in case something went wrong */
         :                      if (ret)
         :                      iommu_unmap(domain, orig_iova, orig_size - size);
    0.00 :   ffff8000106fcc20:       sub     x2, x25, x27
    0.00 :   ffff8000106fcc24:       mov     x1, x26
    0.00 :   ffff8000106fcc28:       mov     x0, x21
    0.00 :   ffff8000106fcc2c:       bl      ffff8000106fbac8 <iommu_unmap>
    0.00 :   ffff8000106fcc30:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff8000106fcc34:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff8000106fcc38:       ldp     x25, x26, [x29, #64]
    0.00 :   ffff8000106fcc3c:       ldr     x27, [x29, #80]
         :                      else
         :                      trace_map(orig_iova, orig_paddr, orig_size);
         :
         :                      return ret;
         :                      }
    0.98 :   ffff8000106fcc40:       mov     w0, w28
    0.00 :   ffff8000106fcc44:       ldr     x28, [sp, #88]
    1.71 :   ffff8000106fcc48:       ldp     x21, x22, [sp, #32]
    1.98 :   ffff8000106fcc4c:       ldp     x29, x30, [sp], #112
    0.00 :   ffff8000106fcc50:       ret
    0.24 :   ffff8000106fcc54:       ldr     x19, [x29, #16]
         :                      if (ops->iotlb_sync_map)
    4.63 :   ffff8000106fcc58:       ldr     x1, [x22, #64]
    0.00 :   ffff8000106fcc5c:       cbz     x1, ffff8000106fccd0 <__iommu_map+0x1a8>
         :                      ops->iotlb_sync_map(domain);
    0.00 :   ffff8000106fcc60:       mov     x0, x21
    0.00 :   ffff8000106fcc64:       mov     w28, #0x0                       // #0
    0.00 :   ffff8000106fcc68:       blr     x1
    0.00 :   ffff8000106fcc6c:       ldr     x20, [x29, #24]
    0.00 :   ffff8000106fcc70:       ldp     x23, x24, [x29, #48]
         :                      }
    0.00 :   ffff8000106fcc74:       mov     w0, w28
    0.00 :   ffff8000106fcc78:       ldp     x25, x26, [x29, #64]
    0.00 :   ffff8000106fcc7c:       ldr     x27, [x29, #80]
    0.00 :   ffff8000106fcc80:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000106fcc84:       ldr     x28, [sp, #88]
    0.00 :   ffff8000106fcc88:       ldp     x29, x30, [sp], #112
    0.00 :   ffff8000106fcc8c:       ret
         :                      return -ENODEV;
    0.00 :   ffff8000106fcc90:       mov     w28, #0xffffffed                // #-19
         :                      }
    0.00 :   ffff8000106fcc94:       mov     w0, w28
    0.00 :   ffff8000106fcc98:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000106fcc9c:       ldr     x28, [sp, #88]
    0.00 :   ffff8000106fcca0:       ldp     x29, x30, [sp], #112
    0.00 :   ffff8000106fcca4:       ret
         :                      return -EINVAL;
    0.00 :   ffff8000106fcca8:       mov     w28, #0xffffffea                // #-22
    0.00 :   ffff8000106fccac:       b       ffff8000106fcc40 <__iommu_map+0x118>
         :                      pr_err("unaligned: iova 0x%lx pa %pa size 0x%zx min_pagesz 0x%x\n",
    0.00 :   ffff8000106fccb0:       adrp    x0, ffff800011228000 <kallsyms_token_index+0xae330>
    0.00 :   ffff8000106fccb4:       add     x2, x29, #0x68
    0.00 :   ffff8000106fccb8:       add     x0, x0, #0x48
         :                      return -EINVAL;
    0.00 :   ffff8000106fccbc:       mov     w28, #0xffffffea                // #-22
         :                      pr_err("unaligned: iova 0x%lx pa %pa size 0x%zx min_pagesz 0x%x\n",
    0.00 :   ffff8000106fccc0:       bl      ffff800010148e94 <printk>
         :                      return -EINVAL;
    0.00 :   ffff8000106fccc4:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff8000106fccc8:       ldp     x25, x26, [x29, #64]
    0.00 :   ffff8000106fcccc:       b       ffff8000106fcc40 <__iommu_map+0x118>
         :                      if (ops->iotlb_sync_map)
   21.43 :   ffff8000106fccd0:       mov     w28, #0x0                       // #0
    2.19 :   ffff8000106fccd4:       ldr     x20, [x29, #24]
   15.35 :   ffff8000106fccd8:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff8000106fccdc:       ldp     x25, x26, [x29, #64]
    4.15 :   ffff8000106fcce0:       ldr     x27, [x29, #80]
    0.00 :   ffff8000106fcce4:       b       ffff8000106fcc40 <__iommu_map+0x118>
 Percent |	Source code & Disassembly of vmlinux for cycles (401 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104602d0 <blk_mq_get_driver_tag>:
         :                      blk_mq_get_driver_tag():
         :
         :                      return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
         :                      }
         :
         :                      bool blk_mq_get_driver_tag(struct request *rq)
         :                      {
    3.73 :   ffff8000104602d0:       stp     x29, x30, [sp, #-96]!
    0.00 :   ffff8000104602d4:       mov     x29, sp
         :                      struct blk_mq_alloc_data data = {
    0.00 :   ffff8000104602d8:       add     x1, x29, #0x3c
         :                      {
   13.99 :   ffff8000104602dc:       stp     x19, x20, [sp, #16]
    2.25 :   ffff8000104602e0:       mov     x19, x0
    0.00 :   ffff8000104602e4:       adrp    x20, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000104602e8:       add     x0, x20, #0x8c8
         :                      struct blk_mq_alloc_data data = {
    1.01 :   ffff8000104602ec:       stp     xzr, xzr, [x1]
         :                      {
    3.01 :   ffff8000104602f0:       ldr     x1, [x0]
    0.50 :   ffff8000104602f4:       str     x1, [x29, #88]
    0.00 :   ffff8000104602f8:       mov     x1, #0x0                        // #0
         :                      struct blk_mq_alloc_data data = {
    7.00 :   ffff8000104602fc:       ldr     w3, [x19, #24]
    0.50 :   ffff800010460300:       mov     w0, #0x1                        // #1
    0.74 :   ffff800010460304:       ldr     x4, [x19]
         :                      .q = rq->q,
         :                      .hctx = rq->mq_hctx,
   22.40 :   ffff800010460308:       ldr     x1, [x19, #16]
         :                      .flags = BLK_MQ_REQ_NOWAIT,
         :                      .cmd_flags = rq->cmd_flags,
         :                      };
         :                      bool shared;
         :
         :                      if (rq->tag != -1)
    0.74 :   ffff80001046030c:       ldr     w2, [x19, #32]
         :                      struct blk_mq_alloc_data data = {
    2.97 :   ffff800010460310:       str     x4, [x29, #48]
    0.00 :   ffff800010460314:       str     w0, [x29, #56]
         :                      if (rq->tag != -1)
    0.00 :   ffff800010460318:       cmn     w2, #0x1
         :                      struct blk_mq_alloc_data data = {
    1.49 :   ffff80001046031c:       str     w3, [x29, #64]
    0.24 :   ffff800010460320:       str     wzr, [x29, #76]
    3.50 :   ffff800010460324:       str     x1, [x29, #80]
         :                      if (rq->tag != -1)
    0.00 :   ffff800010460328:       b.eq    ffff80001046034c <blk_mq_get_driver_tag+0x7c>  // b.none
         :                      }
         :                      data.hctx->tags->rqs[rq->tag] = rq;
         :                      }
         :
         :                      return rq->tag != -1;
         :                      }
    0.00 :   ffff80001046032c:       add     x20, x20, #0x8c8
    0.75 :   ffff800010460330:       ldr     x2, [x29, #88]
    3.71 :   ffff800010460334:       ldr     x1, [x20]
    0.00 :   ffff800010460338:       eor     x1, x2, x1
    0.00 :   ffff80001046033c:       cbnz    x1, ffff800010460424 <blk_mq_get_driver_tag+0x154>
   12.98 :   ffff800010460340:       ldp     x19, x20, [sp, #16]
   18.49 :   ffff800010460344:       ldp     x29, x30, [sp], #96
    0.00 :   ffff800010460348:       ret
         :                      if (blk_mq_tag_is_reserved(data.hctx->sched_tags, rq->internal_tag))
    0.00 :   ffff80001046034c:       ldr     x0, [x1, #344]
    0.00 :   ffff800010460350:       ldr     w2, [x19, #36]
    0.00 :   ffff800010460354:       ldr     w0, [x0, #4]
    0.00 :   ffff800010460358:       cmp     w2, w0
    0.00 :   ffff80001046035c:       b.cc    ffff800010460398 <blk_mq_get_driver_tag+0xc8>  // b.lo, b.ul, b.last
         :                      blk_mq_tag_busy():
         :                      extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
         :                      extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
         :
         :                      static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
         :                      {
         :                      if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
    0.00 :   ffff800010460360:       ldr     x0, [x1, #192]
    0.00 :   ffff800010460364:       tbnz    w0, #1, ffff8000104603a8 <blk_mq_get_driver_tag+0xd8>
         :                      blk_mq_get_driver_tag():
         :                      rq->tag = blk_mq_get_tag(&data);
    0.00 :   ffff800010460368:       add     x0, x29, #0x30
    0.00 :   ffff80001046036c:       bl      ffff800010463b98 <blk_mq_get_tag>
    0.00 :   ffff800010460370:       str     w0, [x19, #32]
         :                      if (rq->tag >= 0) {
    0.00 :   ffff800010460374:       tbnz    w0, #31, ffff80001046038c <blk_mq_get_driver_tag+0xbc>
    0.00 :   ffff800010460378:       ldr     x1, [x29, #80]
         :                      data.hctx->tags->rqs[rq->tag] = rq;
    0.00 :   ffff80001046037c:       ldr     x1, [x1, #336]
    0.00 :   ffff800010460380:       ldr     x1, [x1, #144]
    0.00 :   ffff800010460384:       str     x19, [x1, w0, sxtw #3]
    0.00 :   ffff800010460388:       ldr     w0, [x19, #32]
         :                      return rq->tag != -1;
    0.00 :   ffff80001046038c:       cmn     w0, #0x1
    0.00 :   ffff800010460390:       cset    w0, ne  // ne = any
    0.00 :   ffff800010460394:       b       ffff80001046032c <blk_mq_get_driver_tag+0x5c>
         :                      data.flags |= BLK_MQ_REQ_RESERVED;
    0.00 :   ffff800010460398:       mov     w0, #0x3                        // #3
    0.00 :   ffff80001046039c:       str     w0, [x29, #56]
         :                      blk_mq_tag_busy():
    0.00 :   ffff8000104603a0:       ldr     x0, [x1, #192]
    0.00 :   ffff8000104603a4:       tbz     w0, #1, ffff800010460368 <blk_mq_get_driver_tag+0x98>
         :                      return false;
         :
         :                      return __blk_mq_tag_busy(hctx);
    0.00 :   ffff8000104603a8:       mov     x0, x1
    0.00 :   ffff8000104603ac:       str     x21, [x29, #32]
    0.00 :   ffff8000104603b0:       bl      ffff800010463a08 <__blk_mq_tag_busy>
    0.00 :   ffff8000104603b4:       and     w21, w0, #0xff
         :                      blk_mq_get_driver_tag():
         :                      rq->tag = blk_mq_get_tag(&data);
    0.00 :   ffff8000104603b8:       add     x0, x29, #0x30
    0.00 :   ffff8000104603bc:       bl      ffff800010463b98 <blk_mq_get_tag>
    0.00 :   ffff8000104603c0:       str     w0, [x19, #32]
         :                      if (rq->tag >= 0) {
    0.00 :   ffff8000104603c4:       tbnz    w0, #31, ffff800010460414 <blk_mq_get_driver_tag+0x144>
    0.00 :   ffff8000104603c8:       ldr     x1, [x29, #80]
         :                      if (shared) {
    0.00 :   ffff8000104603cc:       cbz     w21, ffff80001046041c <blk_mq_get_driver_tag+0x14c>
         :                      rq->rq_flags |= RQF_MQ_INFLIGHT;
    0.00 :   ffff8000104603d0:       ldr     w0, [x19, #28]
         :                      atomic_inc(&data.hctx->nr_active);
    0.00 :   ffff8000104603d4:       add     x1, x1, #0x1b0
         :                      rq->rq_flags |= RQF_MQ_INFLIGHT;
    0.00 :   ffff8000104603d8:       orr     w0, w0, #0x40
    0.00 :   ffff8000104603dc:       str     w0, [x19, #28]
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000104603e0:       b       ffff800010460400 <blk_mq_get_driver_tag+0x130>
    0.00 :   ffff8000104603e4:       b       ffff800010460400 <blk_mq_get_driver_tag+0x130>
         :                      __lse_atomic_add():
         :                      }
         :
         :                      ATOMIC_OP(andnot, stclr)
         :                      ATOMIC_OP(or, stset)
         :                      ATOMIC_OP(xor, steor)
         :                      ATOMIC_OP(add, stadd)
    0.00 :   ffff8000104603e8:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000104603ec:       stadd   w0, [x1]
    0.00 :   ffff8000104603f0:       ldr     w0, [x19, #32]
    0.00 :   ffff8000104603f4:       ldr     x21, [x29, #32]
    0.00 :   ffff8000104603f8:       ldr     x1, [x29, #80]
    0.00 :   ffff8000104603fc:       b       ffff80001046037c <blk_mq_get_driver_tag+0xac>
         :                      __ll_sc_atomic_add():
         :                      ATOMIC_FETCH_OP (        , dmb ish,  , l, "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
    0.00 :   ffff800010460400:       b       ffff8000104634fc <blk_mq_update_nr_requests+0x324>
    0.00 :   ffff800010460404:       ldr     w0, [x19, #32]
    0.00 :   ffff800010460408:       ldr     x21, [x29, #32]
    0.00 :   ffff80001046040c:       ldr     x1, [x29, #80]
    0.00 :   ffff800010460410:       b       ffff80001046037c <blk_mq_get_driver_tag+0xac>
    0.00 :   ffff800010460414:       ldr     x21, [x29, #32]
    0.00 :   ffff800010460418:       b       ffff80001046038c <blk_mq_get_driver_tag+0xbc>
    0.00 :   ffff80001046041c:       ldr     x21, [x29, #32]
    0.00 :   ffff800010460420:       b       ffff80001046037c <blk_mq_get_driver_tag+0xac>
    0.00 :   ffff800010460424:       str     x21, [x29, #32]
         :                      blk_mq_get_driver_tag():
         :                      }
    0.00 :   ffff800010460428:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (198 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101d5be0 <mempool_free_slab>:
         :                      mempool_free_slab():
         :                      return kmem_cache_alloc(mem, gfp_mask);
         :                      }
         :                      EXPORT_SYMBOL(mempool_alloc_slab);
         :
         :                      void mempool_free_slab(void *element, void *pool_data)
         :                      {
    0.52 :   ffff8000101d5be0:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff8000101d5be4:       mov     x2, x1
         :                      struct kmem_cache *mem = pool_data;
         :                      kmem_cache_free(mem, element);
    0.00 :   ffff8000101d5be8:       mov     x1, x0
    0.00 :   ffff8000101d5bec:       mov     x0, x2
         :                      {
   87.91 :   ffff8000101d5bf0:       mov     x29, sp
         :                      kmem_cache_free(mem, element);
    0.00 :   ffff8000101d5bf4:       bl      ffff800010250100 <kmem_cache_free>
         :                      }
   11.57 :   ffff8000101d5bf8:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000101d5bfc:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (379 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104abbb0 <__sbitmap_queue_get>:
         :                      __sbitmap_queue_get():
         :                      sbitmap_resize(&sbq->sb, depth);
         :                      }
         :                      EXPORT_SYMBOL_GPL(sbitmap_queue_resize);
         :
         :                      int __sbitmap_queue_get(struct sbitmap_queue *sbq)
         :                      {
    0.00 :   ffff8000104abbb0:       stp     x29, x30, [sp, #-64]!
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    8.94 :   ffff8000104abbb4:       mrs     x1, sp_el0
         :                      __sbitmap_queue_get():
    0.27 :   ffff8000104abbb8:       mov     x29, sp
    0.00 :   ffff8000104abbbc:       stp     x19, x20, [sp, #16]
    2.38 :   ffff8000104abbc0:       mov     x19, x0
    1.86 :   ffff8000104abbc4:       str     x21, [sp, #32]
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
   10.55 :   ffff8000104abbc8:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff8000104abbcc:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
   10.30 :   ffff8000104abbd0:       str     w2, [x1, #16]
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    1.57 :   ffff8000104abbd4:       mrs     x2, tpidr_el1
         :                      __sbitmap_queue_get():
         :                      unsigned int hint, depth;
         :                      int nr;
         :
         :                      hint = this_cpu_read(*sbq->alloc_hint);
    0.79 :   ffff8000104abbd8:       ldr     x0, [x0, #24]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    7.39 :   ffff8000104abbdc:       ldr     w20, [x0, x2]
   17.90 :   ffff8000104abbe0:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000104abbe4:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    8.71 :   ffff8000104abbe8:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000104abbec:       cbz     x0, ffff8000104abc3c <__sbitmap_queue_get+0x8c>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000104abbf0:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000104abbf4:       cbz     x0, ffff8000104abc3c <__sbitmap_queue_get+0x8c>
         :                      __read_once_size():
   17.71 :   ffff8000104abbf8:       ldr     w21, [x19]
         :                      __sbitmap_queue_get():
         :                      depth = READ_ONCE(sbq->sb.depth);
         :                      if (unlikely(hint >= depth)) {
    0.00 :   ffff8000104abbfc:       cmp     w20, w21
    5.28 :   ffff8000104abc00:       b.cs    ffff8000104abc4c <__sbitmap_queue_get+0x9c>  // b.hs, b.nlast
         :                      hint = depth ? prandom_u32() % depth : 0;
         :                      this_cpu_write(*sbq->alloc_hint, hint);
         :                      }
         :                      nr = sbitmap_get(&sbq->sb, hint, sbq->round_robin);
    0.00 :   ffff8000104abc04:       ldrb    w2, [x19, #52]
    0.00 :   ffff8000104abc08:       mov     w1, w20
    0.00 :   ffff8000104abc0c:       mov     x0, x19
    0.00 :   ffff8000104abc10:       bl      ffff8000104ab160 <sbitmap_get>
         :
         :                      if (nr == -1) {
    2.65 :   ffff8000104abc14:       cmn     w0, #0x1
    0.00 :   ffff8000104abc18:       b.eq    ffff8000104abcf0 <__sbitmap_queue_get+0x140>  // b.none
         :                      /* If the map is full, a hint won't do us much good. */
         :                      this_cpu_write(*sbq->alloc_hint, 0);
         :                      } else if (nr == hint || unlikely(sbq->round_robin)) {
    0.00 :   ffff8000104abc1c:       cmp     w0, w20
    0.00 :   ffff8000104abc20:       b.eq    ffff8000104abc90 <__sbitmap_queue_get+0xe0>  // b.none
    0.00 :   ffff8000104abc24:       ldrb    w1, [x19, #52]
    0.00 :   ffff8000104abc28:       cbnz    w1, ffff8000104abc90 <__sbitmap_queue_get+0xe0>
         :                      hint = 0;
         :                      this_cpu_write(*sbq->alloc_hint, hint);
         :                      }
         :
         :                      return nr;
         :                      }
    1.32 :   ffff8000104abc2c:       ldp     x19, x20, [sp, #16]
    2.38 :   ffff8000104abc30:       ldr     x21, [sp, #32]
    0.00 :   ffff8000104abc34:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000104abc38:       ret
         :                      hint = this_cpu_read(*sbq->alloc_hint);
    0.00 :   ffff8000104abc3c:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      __read_once_size():
    0.00 :   ffff8000104abc40:       ldr     w21, [x19]
         :                      __sbitmap_queue_get():
         :                      if (unlikely(hint >= depth)) {
    0.00 :   ffff8000104abc44:       cmp     w20, w21
    0.00 :   ffff8000104abc48:       b.cc    ffff8000104abc04 <__sbitmap_queue_get+0x54>  // b.lo, b.ul, b.last
         :                      hint = depth ? prandom_u32() % depth : 0;
    0.00 :   ffff8000104abc4c:       mov     w20, #0x0                       // #0
    0.00 :   ffff8000104abc50:       cbnz    w21, ffff8000104abd10 <__sbitmap_queue_get+0x160>
         :                      get_current():
    0.00 :   ffff8000104abc54:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.00 :   ffff8000104abc58:       ldr     w0, [x1, #16]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff8000104abc5c:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000104abc60:       str     w0, [x1, #16]
         :                      __sbitmap_queue_get():
         :                      this_cpu_write(*sbq->alloc_hint, hint);
    0.00 :   ffff8000104abc64:       ldr     x0, [x19, #24]
         :                      __my_cpu_offset():
    0.00 :   ffff8000104abc68:       mrs     x2, tpidr_el1
         :                      __write_once_size():
    0.00 :   ffff8000104abc6c:       str     w20, [x0, x2]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000104abc70:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000104abc74:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000104abc78:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000104abc7c:       cbz     x0, ffff8000104abc88 <__sbitmap_queue_get+0xd8>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000104abc80:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000104abc84:       cbnz    x0, ffff8000104abc04 <__sbitmap_queue_get+0x54>
         :                      __sbitmap_queue_get():
    0.00 :   ffff8000104abc88:       bl      ffff800010cad668 <preempt_schedule_notrace>
    0.00 :   ffff8000104abc8c:       b       ffff8000104abc04 <__sbitmap_queue_get+0x54>
         :                      hint = nr + 1;
    0.00 :   ffff8000104abc90:       add     w3, w0, #0x1
         :                      if (hint >= depth - 1)
    0.00 :   ffff8000104abc94:       sub     w21, w21, #0x1
         :                      get_current():
    0.00 :   ffff8000104abc98:       mrs     x2, sp_el0
         :                      __read_once_size():
    0.00 :   ffff8000104abc9c:       ldr     w1, [x2, #16]
         :                      __sbitmap_queue_get():
         :                      hint = 0;
    0.00 :   ffff8000104abca0:       cmp     w3, w21
    0.00 :   ffff8000104abca4:       csel    w3, w3, wzr, cc  // cc = lo, ul, last
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff8000104abca8:       add     w1, w1, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000104abcac:       str     w1, [x2, #16]
         :                      __sbitmap_queue_get():
         :                      this_cpu_write(*sbq->alloc_hint, hint);
    0.00 :   ffff8000104abcb0:       ldr     x1, [x19, #24]
         :                      __my_cpu_offset():
    0.00 :   ffff8000104abcb4:       mrs     x4, tpidr_el1
         :                      __write_once_size():
    0.00 :   ffff8000104abcb8:       str     w3, [x1, x4]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000104abcbc:       ldr     x1, [x2, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000104abcc0:       sub     x1, x1, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000104abcc4:       str     w1, [x2, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000104abcc8:       cbz     x1, ffff8000104abcd4 <__sbitmap_queue_get+0x124>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000104abccc:       ldr     x1, [x2, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000104abcd0:       cbnz    x1, ffff8000104abc2c <__sbitmap_queue_get+0x7c>
    0.00 :   ffff8000104abcd4:       str     w0, [x29, #60]
         :                      __sbitmap_queue_get():
         :                      this_cpu_write(*sbq->alloc_hint, 0);
    0.00 :   ffff8000104abcd8:       bl      ffff800010cad668 <preempt_schedule_notrace>
    0.00 :   ffff8000104abcdc:       ldr     w0, [x29, #60]
         :                      }
    0.00 :   ffff8000104abce0:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000104abce4:       ldr     x21, [sp, #32]
    0.00 :   ffff8000104abce8:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000104abcec:       ret
         :                      get_current():
    0.00 :   ffff8000104abcf0:       mrs     x2, sp_el0
         :                      __read_once_size():
    0.00 :   ffff8000104abcf4:       ldr     w1, [x2, #16]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff8000104abcf8:       add     w1, w1, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000104abcfc:       str     w1, [x2, #16]
         :                      __sbitmap_queue_get():
         :                      this_cpu_write(*sbq->alloc_hint, 0);
    0.00 :   ffff8000104abd00:       ldr     x1, [x19, #24]
         :                      __my_cpu_offset():
    0.00 :   ffff8000104abd04:       mrs     x3, tpidr_el1
         :                      __write_once_size():
    0.00 :   ffff8000104abd08:       str     wzr, [x1, x3]
    0.00 :   ffff8000104abd0c:       b       ffff8000104abcbc <__sbitmap_queue_get+0x10c>
         :                      __sbitmap_queue_get():
         :                      hint = depth ? prandom_u32() % depth : 0;
    0.00 :   ffff8000104abd10:       bl      ffff80001047f070 <prandom_u32>
    0.00 :   ffff8000104abd14:       udiv    w20, w0, w21
    0.00 :   ffff8000104abd18:       msub    w20, w20, w21, w0
    0.00 :   ffff8000104abd1c:       b       ffff8000104abc54 <__sbitmap_queue_get+0xa4>
 Percent |	Source code & Disassembly of vmlinux for cycles (364 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001044fe58 <bio_associate_blkg_from_css>:
         :                      bio_associate_blkg_from_css():
         :                      * request_queue of the @bio.  This falls back to the queue's root_blkg if
         :                      * the association fails with the css.
         :                      */
         :                      void bio_associate_blkg_from_css(struct bio *bio,
         :                      struct cgroup_subsys_state *css)
         :                      {
    0.00 :   ffff80001044fe58:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff80001044fe5c:       mov     x29, sp
    0.27 :   ffff80001044fe60:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001044fe64:       mov     x20, x0
    0.55 :   ffff80001044fe68:       str     x21, [sp, #32]
    0.00 :   ffff80001044fe6c:       mov     x19, x1
         :                      struct request_queue *q = bio->bi_disk->queue;
    3.28 :   ffff80001044fe70:       ldr     x0, [x0, #8]
    0.28 :   ffff80001044fe74:       ldr     x21, [x0, #1040]
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff80001044fe78:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      bio_associate_blkg_from_css():
         :                      struct blkcg_gq *blkg;
         :
         :                      rcu_read_lock();
         :
         :                      if (!css || !css->parent)
    0.00 :   ffff80001044fe7c:       cbz     x19, ffff80001044feb4 <bio_associate_blkg_from_css+0x5c>
    0.27 :   ffff80001044fe80:       ldr     x0, [x19, #232]
    0.00 :   ffff80001044fe84:       cbz     x0, ffff80001044feb4 <bio_associate_blkg_from_css+0x5c>
         :                      blkg = q->root_blkg;
         :                      else
         :                      blkg = blkg_lookup_create(css_to_blkcg(css), q);
   81.63 :   ffff80001044fe88:       mov     x1, x21
    0.00 :   ffff80001044fe8c:       mov     x0, x19
    0.00 :   ffff80001044fe90:       bl      ffff8000104753b8 <blkg_lookup_create>
    7.67 :   ffff80001044fe94:       mov     x1, x0
         :
         :                      __bio_associate_blkg(bio, blkg);
    0.00 :   ffff80001044fe98:       add     x0, x20, #0x48
    0.00 :   ffff80001044fe9c:       bl      ffff80001044fbb0 <__bio_associate_blkg.isra.39>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    2.20 :   ffff80001044fea0:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      bio_associate_blkg_from_css():
         :
         :                      rcu_read_unlock();
         :                      }
    2.75 :   ffff80001044fea4:       ldr     x21, [sp, #32]
    1.09 :   ffff80001044fea8:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001044feac:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001044feb0:       ret
         :                      blkg = q->root_blkg;
    0.00 :   ffff80001044feb4:       ldr     x1, [x21, #1016]
         :                      __bio_associate_blkg(bio, blkg);
    0.00 :   ffff80001044feb8:       add     x0, x20, #0x48
    0.00 :   ffff80001044febc:       bl      ffff80001044fbb0 <__bio_associate_blkg.isra.39>
         :                      rcu_read_unlock():
    0.00 :   ffff80001044fec0:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      bio_associate_blkg_from_css():
         :                      }
    0.00 :   ffff80001044fec4:       ldr     x21, [sp, #32]
    0.00 :   ffff80001044fec8:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001044fecc:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001044fed0:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (193 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104ab7f8 <__sbq_wake_up>:
         :                      __sbq_wake_up():
         :
         :                      return NULL;
         :                      }
         :
         :                      static bool __sbq_wake_up(struct sbitmap_queue *sbq)
         :                      {
    0.00 :   ffff8000104ab7f8:       mov     x7, x0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    4.17 :   ffff8000104ab7fc:       ldr     w0, [x0, #48]
         :                      sbq_wake_ptr():
         :                      if (!atomic_read(&sbq->ws_active))
    0.53 :   ffff8000104ab800:       cbz     w0, ffff8000104ab83c <__sbq_wake_up+0x44>
         :                      __read_once_size():
    0.00 :   ffff8000104ab804:       ldr     w0, [x7, #36]
         :                      sbq_wake_ptr():
         :                      struct sbq_wait_state *ws = &sbq->ws[wake_index];
    0.00 :   ffff8000104ab808:       mov     w1, #0x8                        // #8
    0.00 :   ffff8000104ab80c:       ldr     x8, [x7, #40]
    0.00 :   ffff8000104ab810:       sbfiz   x3, x0, #6, #32
         :                      sbq_index_inc():
         :                      void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
         :                      unsigned int cpu);
         :
         :                      static inline int sbq_index_inc(int index)
         :                      {
         :                      return (index + 1) & (SBQ_WAIT_QUEUES - 1);
    0.00 :   ffff8000104ab814:       add     w6, w0, #0x1
         :                      sbq_wake_ptr():
    0.00 :   ffff8000104ab818:       add     x3, x8, x3
         :                      waitqueue_active():
         :                      * Also note that this 'optimization' trades a spin_lock() for an smp_mb(),
         :                      * which (when the lock is uncontended) are of roughly equal cost.
         :                      */
         :                      static inline int waitqueue_active(struct wait_queue_head *wq_head)
         :                      {
         :                      return !list_empty(&wq_head->head);
    0.00 :   ffff8000104ab81c:       add     x4, x3, #0x8
    0.00 :   ffff8000104ab820:       add     x5, x4, #0x8
         :                      __read_once_size():
    0.00 :   ffff8000104ab824:       ldr     x2, [x3, #16]
         :                      sbq_wake_ptr():
         :                      if (waitqueue_active(&ws->wait)) {
    0.00 :   ffff8000104ab828:       cmp     x5, x2
    0.00 :   ffff8000104ab82c:       b.ne    ffff8000104ab844 <__sbq_wake_up+0x4c>  // b.any
         :                      sbq_index_inc():
    0.00 :   ffff8000104ab830:       and     w0, w6, #0x7
         :                      sbq_wake_ptr():
         :                      for (i = 0; i < SBQ_WAIT_QUEUES; i++) {
    0.00 :   ffff8000104ab834:       subs    w1, w1, #0x1
    0.00 :   ffff8000104ab838:       b.ne    ffff8000104ab810 <__sbq_wake_up+0x18>  // b.any
         :                      __sbq_wake_up():
         :                      unsigned int wake_batch;
         :                      int wait_cnt;
         :
         :                      ws = sbq_wake_ptr(sbq);
         :                      if (!ws)
         :                      return false;
   95.31 :   ffff8000104ab83c:       mov     w0, #0x0                        // #0
         :
         :                      return true;
         :                      }
         :
         :                      return false;
         :                      }
    0.00 :   ffff8000104ab840:       ret
         :                      __read_once_size():
    0.00 :   ffff8000104ab844:       ldr     w1, [x7, #36]
         :                      sbq_wake_ptr():
         :                      if (wake_index != atomic_read(&sbq->wake_index))
    0.00 :   ffff8000104ab848:       cmp     w1, w0
    0.00 :   ffff8000104ab84c:       b.eq    ffff8000104ab854 <__sbq_wake_up+0x5c>  // b.none
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000104ab850:       str     w0, [x7, #36]
         :                      __sbq_wake_up():
         :                      if (!ws)
    0.00 :   ffff8000104ab854:       cbz     x3, ffff8000104ab83c <__sbq_wake_up+0x44>
         :                      {
    0.00 :   ffff8000104ab858:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff8000104ab85c:       mov     x29, sp
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000104ab860:       b       ffff8000104ab88c <__sbq_wake_up+0x94>
    0.00 :   ffff8000104ab864:       b       ffff8000104ab88c <__sbq_wake_up+0x94>
         :                      __lse_atomic_sub_return():
         :                      }
         :
         :                      ATOMIC_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff8000104ab868:       mov     w5, #0x1                        // #1
    0.00 :   ffff8000104ab86c:       neg     w5, w5
    0.00 :   ffff8000104ab870:       ldaddal w5, w0, [x3]
    0.00 :   ffff8000104ab874:       add     w5, w5, w0
         :                      __sbq_wake_up():
         :                      if (wait_cnt <= 0) {
    0.00 :   ffff8000104ab878:       cmp     w5, #0x0
    0.00 :   ffff8000104ab87c:       b.le    ffff8000104ab89c <__sbq_wake_up+0xa4>
         :                      return false;
    0.00 :   ffff8000104ab880:       mov     w0, #0x0                        // #0
         :                      }
    0.00 :   ffff8000104ab884:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000104ab888:       ret
         :                      __ll_sc_atomic_sub_return():
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff8000104ab88c:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000104ab890:       b       ffff8000104abdf8 <__sbitmap_queue_get+0x248>
         :                      __sbq_wake_up():
         :                      if (wait_cnt <= 0) {
    0.00 :   ffff8000104ab894:       cmp     w5, #0x0
    0.00 :   ffff8000104ab898:       b.gt    ffff8000104ab880 <__sbq_wake_up+0x88>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000104ab89c:       ldr     w6, [x7, #32]
         :                      __sbq_wake_up():
         :                      smp_mb__before_atomic();
    0.00 :   ffff8000104ab8a0:       dmb     ish
         :                      atomic_cmpxchg():
         :                      #if !defined(arch_atomic_cmpxchg_relaxed) || defined(arch_atomic_cmpxchg)
         :                      static inline int
         :                      atomic_cmpxchg(atomic_t *v, int old, int new)
         :                      {
         :                      kasan_check_write(v, sizeof(*v));
         :                      return arch_atomic_cmpxchg(v, old, new);
    0.00 :   ffff8000104ab8a4:       sxtw    x1, w5
         :                      arch_static_branch_jump():
    0.00 :   ffff8000104ab8a8:       b       ffff8000104ab8d0 <__sbq_wake_up+0xd8>
    0.00 :   ffff8000104ab8ac:       b       ffff8000104ab8d0 <__sbq_wake_up+0xd8>
         :                      __lse__cmpxchg_case_mb_32():
         :                      __CMPXCHG_CASE(w, h, rel_, 16,  l, "memory")
         :                      __CMPXCHG_CASE(w,  , rel_, 32,  l, "memory")
         :                      __CMPXCHG_CASE(x,  , rel_, 64,  l, "memory")
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, al, "memory")
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, al, "memory")
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, al, "memory")
    0.00 :   ffff8000104ab8b0:       mov     x0, x3
    0.00 :   ffff8000104ab8b4:       mov     w1, w5
    0.00 :   ffff8000104ab8b8:       mov     w2, w6
    0.00 :   ffff8000104ab8bc:       mov     w8, w1
    0.00 :   ffff8000104ab8c0:       casal   w8, w2, [x3]
    0.00 :   ffff8000104ab8c4:       mov     w0, w8
    0.00 :   ffff8000104ab8c8:       mov     w1, w0
    0.00 :   ffff8000104ab8cc:       b       ffff8000104ab8dc <__sbq_wake_up+0xe4>
         :                      __ll_sc__cmpxchg_case_mb_32():
         :                      __CMPXCHG_CASE(w, h, rel_, 16,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w,  , rel_, 32,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  , rel_, 64,        ,  , l, "memory", L)
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, dmb ish,  , l, "memory", K)
    0.00 :   ffff8000104ab8d0:       and     x1, x1, #0xffffffff
    0.00 :   ffff8000104ab8d4:       b       ffff8000104abe14 <__sbitmap_queue_get+0x264>
    0.00 :   ffff8000104ab8d8:       mov     w1, w0
         :                      __sbq_wake_up():
         :                      return true;
    0.00 :   ffff8000104ab8dc:       mov     w0, #0x1                        // #1
         :                      if (ret == wait_cnt) {
    0.00 :   ffff8000104ab8e0:       cmp     w1, w5
    0.00 :   ffff8000104ab8e4:       b.ne    ffff8000104ab884 <__sbq_wake_up+0x8c>  // b.any
         :                      __read_once_size():
    0.00 :   ffff8000104ab8e8:       mov     x0, x7
    0.00 :   ffff8000104ab8ec:       ldr     w1, [x0, #36]!
         :                      sbq_index_inc():
    0.00 :   ffff8000104ab8f0:       add     w2, w1, #0x1
         :                      atomic_cmpxchg():
    0.00 :   ffff8000104ab8f4:       sxtw    x3, w1
         :                      sbq_index_inc():
    0.00 :   ffff8000104ab8f8:       and     w2, w2, #0x7
         :                      arch_static_branch_jump():
    0.00 :   ffff8000104ab8fc:       b       ffff8000104ab92c <__sbq_wake_up+0x134>
    0.00 :   ffff8000104ab900:       b       ffff8000104ab92c <__sbq_wake_up+0x134>
         :                      __lse__cmpxchg_case_mb_32():
    0.00 :   ffff8000104ab904:       mov     x3, x0
    0.00 :   ffff8000104ab908:       mov     w5, w1
    0.00 :   ffff8000104ab90c:       casal   w5, w2, [x3]
    0.00 :   ffff8000104ab910:       mov     w0, w5
         :                      __sbq_wake_up():
         :                      wake_up_nr(&ws->wait, wake_batch);
    0.00 :   ffff8000104ab914:       mov     x3, #0x0                        // #0
    0.00 :   ffff8000104ab918:       mov     w2, w6
    0.00 :   ffff8000104ab91c:       mov     w1, #0x3                        // #3
    0.00 :   ffff8000104ab920:       mov     x0, x4
    0.00 :   ffff8000104ab924:       bl      ffff80001012e430 <__wake_up>
    0.00 :   ffff8000104ab928:       b       ffff8000104ab880 <__sbq_wake_up+0x88>
         :                      __ll_sc__cmpxchg_case_mb_32():
    0.00 :   ffff8000104ab92c:       and     x0, x3, #0xffffffff
    0.00 :   ffff8000104ab930:       add     x5, x7, #0x24
    0.00 :   ffff8000104ab934:       b       ffff8000104abe34 <__sbitmap_queue_get+0x284>
    0.00 :   ffff8000104ab938:       b       ffff8000104ab914 <__sbq_wake_up+0x11c>
 Percent |	Source code & Disassembly of vmlinux for cycles (359 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010463b98 <blk_mq_get_tag>:
         :                      blk_mq_get_tag():
         :                      else
         :                      return __sbitmap_queue_get(bt);
         :                      }
         :
         :                      unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
         :                      {
    5.28 :   ffff800010463b98:       stp     x29, x30, [sp, #-144]!
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.83 :   ffff800010463b9c:       mrs     x5, sp_el0
         :                      blk_mq_get_tag():
    0.00 :   ffff800010463ba0:       mov     x29, sp
    0.00 :   ffff800010463ba4:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010463ba8:       mov     x20, x0
    4.77 :   ffff800010463bac:       str     x22, [sp, #40]
    0.00 :   ffff800010463bb0:       adrp    x22, ffff800011899000 <page_wait_table+0x1500>
    0.56 :   ffff800010463bb4:       str     x24, [sp, #56]
    0.00 :   ffff800010463bb8:       add     x0, x22, #0x8c8
         :                      struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
         :                      struct sbitmap_queue *bt;
         :                      struct sbq_wait_state *ws;
         :                      DEFINE_SBQ_WAIT(wait);
    0.00 :   ffff800010463bbc:       add     x2, x29, #0x78
    1.12 :   ffff800010463bc0:       stp     xzr, xzr, [x29, #88]
         :                      {
   40.68 :   ffff800010463bc4:       ldr     x1, [x0]
    1.39 :   ffff800010463bc8:       str     x1, [x29, #136]
    0.00 :   ffff800010463bcc:       mov     x1, #0x0                        // #0
         :                      struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
    2.24 :   ffff800010463bd0:       ldr     x3, [x20, #32]
    1.67 :   ffff800010463bd4:       ldr     w1, [x20, #8]
         :                      DEFINE_SBQ_WAIT(wait);
    0.00 :   ffff800010463bd8:       adrp    x0, ffff80001012e000 <dl_cpu_busy+0x200>
    1.69 :   ffff800010463bdc:       str     x5, [x29, #104]
    0.00 :   ffff800010463be0:       add     x0, x0, #0xa78
         :                      blk_mq_tags_from_data():
         :                      };
         :
         :                      static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data *data)
         :                      {
         :                      if (data->flags & BLK_MQ_REQ_INTERNAL)
         :                      return data->hctx->sched_tags;
    1.94 :   ffff800010463be4:       ldp     x4, x19, [x3, #336]
         :
         :                      return data->hctx->tags;
    0.00 :   ffff800010463be8:       tst     x1, #0x4
         :                      blk_mq_get_tag():
   16.15 :   ffff800010463bec:       str     x0, [x29, #112]
    0.28 :   ffff800010463bf0:       stp     x2, x2, [x29, #120]
         :                      blk_mq_tags_from_data():
    0.00 :   ffff800010463bf4:       csel    x19, x19, x4, ne  // ne = any
         :                      blk_mq_get_tag():
         :                      unsigned int tag_offset;
         :                      int tag;
         :
         :                      if (data->flags & BLK_MQ_REQ_RESERVED) {
    0.00 :   ffff800010463bf8:       tbz     w1, #1, ffff800010463dfc <blk_mq_get_tag+0x264>
         :                      if (unlikely(!tags->nr_reserved_tags)) {
    0.00 :   ffff800010463bfc:       ldr     w0, [x19, #4]
         :                      WARN_ON_ONCE(1);
         :                      return BLK_MQ_TAG_FAIL;
         :                      }
         :                      bt = &tags->breserved_tags;
         :                      tag_offset = 0;
    0.00 :   ffff800010463c00:       mov     w24, #0x0                       // #0
         :                      bt = &tags->breserved_tags;
    0.00 :   ffff800010463c04:       add     x19, x19, #0x50
         :                      if (unlikely(!tags->nr_reserved_tags)) {
    0.00 :   ffff800010463c08:       cbz     w0, ffff800010463e64 <blk_mq_get_tag+0x2cc>
    0.00 :   ffff800010463c0c:       str     x26, [x29, #72]
         :                      } else {
         :                      bt = &tags->bitmap_tags;
         :                      tag_offset = tags->nr_reserved_tags;
         :                      }
         :
         :                      tag = __blk_mq_get_tag(data, bt);
    0.00 :   ffff800010463c10:       mov     x1, x19
    0.00 :   ffff800010463c14:       mov     x0, x20
    0.00 :   ffff800010463c18:       bl      ffff800010463688 <__blk_mq_get_tag>
    0.00 :   ffff800010463c1c:       mov     w26, w0
         :                      if (tag != -1)
    0.00 :   ffff800010463c20:       cmn     w0, #0x1
    0.00 :   ffff800010463c24:       b.ne    ffff800010463e20 <blk_mq_get_tag+0x288>  // b.any
         :                      goto found_tag;
         :
         :                      if (data->flags & BLK_MQ_REQ_NOWAIT)
    0.00 :   ffff800010463c28:       ldr     w1, [x20, #8]
    0.00 :   ffff800010463c2c:       tbnz    w1, #0, ffff800010463e80 <blk_mq_get_tag+0x2e8>
    0.00 :   ffff800010463c30:       str     x21, [x29, #32]
    0.00 :   ffff800010463c34:       str     x23, [x29, #48]
    0.00 :   ffff800010463c38:       str     x25, [x29, #64]
         :                      return BLK_MQ_TAG_FAIL;
         :
         :                      ws = bt_wait_ptr(bt, data->hctx);
    0.00 :   ffff800010463c3c:       ldr     x0, [x20, #32]
    0.00 :   ffff800010463c40:       ldr     x21, [x19, #40]
         :                      bt_wait_ptr():
         :                      void *priv);
         :
         :                      static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
         :                      struct blk_mq_hw_ctx *hctx)
         :                      {
         :                      if (!hctx)
    0.00 :   ffff800010463c44:       cbz     x0, ffff800010463c84 <blk_mq_get_tag+0xec>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010463c48:       ldr     w4, [x0, #328]
         :                      atomic_cmpxchg():
         :                      #if !defined(arch_atomic_cmpxchg_relaxed) || defined(arch_atomic_cmpxchg)
         :                      static inline int
         :                      atomic_cmpxchg(atomic_t *v, int old, int new)
         :                      {
         :                      kasan_check_write(v, sizeof(*v));
         :                      return arch_atomic_cmpxchg(v, old, new);
    0.00 :   ffff800010463c4c:       add     x3, x0, #0x148
         :                      __read_once_size():
    0.00 :   ffff800010463c50:       ldr     w1, [x0, #328]
         :                      sbq_wait_ptr():
         :                      static inline struct sbq_wait_state *sbq_wait_ptr(struct sbitmap_queue *sbq,
         :                      atomic_t *wait_index)
         :                      {
         :                      struct sbq_wait_state *ws;
         :
         :                      ws = &sbq->ws[atomic_read(wait_index)];
    0.00 :   ffff800010463c54:       sbfiz   x4, x4, #6, #32
         :                      sbq_index_inc():
         :                      return (index + 1) & (SBQ_WAIT_QUEUES - 1);
    0.00 :   ffff800010463c58:       add     w2, w1, #0x1
         :                      atomic_cmpxchg():
    0.00 :   ffff800010463c5c:       sxtw    x0, w1
         :                      sbq_wait_ptr():
         :                      ws = &sbq->ws[atomic_read(wait_index)];
    0.00 :   ffff800010463c60:       add     x21, x21, x4
         :                      sbq_index_inc():
         :                      return (index + 1) & (SBQ_WAIT_QUEUES - 1);
    0.00 :   ffff800010463c64:       and     w2, w2, #0x7
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010463c68:       b       ffff800010463e70 <blk_mq_get_tag+0x2d8>
    0.00 :   ffff800010463c6c:       b       ffff800010463e70 <blk_mq_get_tag+0x2d8>
         :                      __lse__cmpxchg_case_mb_32():
         :                      __CMPXCHG_CASE(w, h, rel_, 16,  l, "memory")
         :                      __CMPXCHG_CASE(w,  , rel_, 32,  l, "memory")
         :                      __CMPXCHG_CASE(x,  , rel_, 64,  l, "memory")
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, al, "memory")
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, al, "memory")
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, al, "memory")
    0.00 :   ffff800010463c70:       mov     x0, x3
    0.00 :   ffff800010463c74:       mov     w4, w1
    0.00 :   ffff800010463c78:       casal   w4, w2, [x3]
    0.00 :   ffff800010463c7c:       mov     w0, w4
    0.00 :   ffff800010463c80:       ldr     x0, [x20, #32]
         :                      blk_mq_get_tag():
         :                      /*
         :                      * We're out of tags on this hardware queue, kick any
         :                      * pending IO submits before going to sleep waiting for
         :                      * some to complete.
         :                      */
         :                      blk_mq_run_hw_queue(data->hctx, false);
    0.00 :   ffff800010463c84:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010463c88:       bl      ffff80001045ef88 <blk_mq_run_hw_queue>
         :                      blk_mq_get_ctx():
         :                      return __blk_mq_get_ctx(q, raw_smp_processor_id());
    0.00 :   ffff800010463c8c:       adrp    x25, ffff8000114ca000 <bp_hardening_data>
         :                      __blk_mq_get_ctx():
         :                      return per_cpu_ptr(q->queue_ctx, cpu);
    0.00 :   ffff800010463c90:       adrp    x23, ffff800011899000 <page_wait_table+0x1500>
         :                      blk_mq_get_tag():
         :
         :                      /*
         :                      * Retry tag allocation after running the hardware queue,
         :                      * as running the queue may also have found completions.
         :                      */
         :                      tag = __blk_mq_get_tag(data, bt);
    0.00 :   ffff800010463c94:       mov     x1, x19
    0.00 :   ffff800010463c98:       mov     x0, x20
         :                      blk_mq_get_ctx():
         :                      return __blk_mq_get_ctx(q, raw_smp_processor_id());
    0.00 :   ffff800010463c9c:       add     x25, x25, #0x18
         :                      blk_mq_get_tag():
    0.00 :   ffff800010463ca0:       bl      ffff800010463688 <__blk_mq_get_tag>
         :                      __blk_mq_get_ctx():
         :                      return per_cpu_ptr(q->queue_ctx, cpu);
    0.00 :   ffff800010463ca4:       add     x23, x23, #0x8e8
         :                      blk_mq_get_tag():
    0.00 :   ffff800010463ca8:       mov     w26, w0
         :                      if (tag != -1)
    0.00 :   ffff800010463cac:       cmn     w0, #0x1
    0.00 :   ffff800010463cb0:       b.ne    ffff800010463ddc <blk_mq_get_tag+0x244>  // b.any
    0.00 :   ffff800010463cb4:       nop
         :                      break;
         :
         :                      sbitmap_prepare_to_wait(bt, ws, &wait, TASK_UNINTERRUPTIBLE);
    0.00 :   ffff800010463cb8:       add     x2, x29, #0x58
    0.00 :   ffff800010463cbc:       mov     w3, #0x2                        // #2
    0.00 :   ffff800010463cc0:       mov     x1, x21
    0.00 :   ffff800010463cc4:       mov     x0, x19
    0.00 :   ffff800010463cc8:       bl      ffff8000104ab788 <sbitmap_prepare_to_wait>
         :
         :                      tag = __blk_mq_get_tag(data, bt);
    0.00 :   ffff800010463ccc:       mov     x1, x19
    0.00 :   ffff800010463cd0:       mov     x0, x20
    0.00 :   ffff800010463cd4:       bl      ffff800010463688 <__blk_mq_get_tag>
    0.00 :   ffff800010463cd8:       mov     w26, w0
         :                      if (tag != -1)
    0.00 :   ffff800010463cdc:       cmn     w0, #0x1
    0.00 :   ffff800010463ce0:       b.ne    ffff800010463ddc <blk_mq_get_tag+0x244>  // b.any
         :                      break;
         :
         :                      bt_prev = bt;
         :                      io_schedule();
    0.00 :   ffff800010463ce4:       bl      ffff800010cad7f0 <io_schedule>
         :
         :                      sbitmap_finish_wait(bt, ws, &wait);
    0.00 :   ffff800010463ce8:       add     x2, x29, #0x58
    0.00 :   ffff800010463cec:       mov     x1, x21
    0.00 :   ffff800010463cf0:       mov     x0, x19
    0.00 :   ffff800010463cf4:       bl      ffff8000104ab660 <sbitmap_finish_wait>
         :                      blk_mq_get_ctx():
         :                      return __blk_mq_get_ctx(q, raw_smp_processor_id());
    0.00 :   ffff800010463cf8:       mov     x0, x25
         :                      blk_mq_get_tag():
         :
         :                      data->ctx = blk_mq_get_ctx(data->q);
         :                      data->hctx = blk_mq_map_queue(data->q, data->cmd_flags,
    0.00 :   ffff800010463cfc:       ldr     w2, [x20, #16]
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800010463d00:       mrs     x1, tpidr_el1
         :                      __blk_mq_get_ctx():
         :                      return per_cpu_ptr(q->queue_ctx, cpu);
    0.00 :   ffff800010463d04:       ldr     w3, [x0, x1]
         :                      blk_mq_map_queue():
         :                      type = HCTX_TYPE_POLL;
    0.00 :   ffff800010463d08:       mov     w1, #0x2                        // #2
         :                      blk_mq_get_ctx():
         :                      return __blk_mq_get_ctx(q, raw_smp_processor_id());
    0.00 :   ffff800010463d0c:       ldr     x0, [x20]
         :                      __blk_mq_get_ctx():
         :                      return per_cpu_ptr(q->queue_ctx, cpu);
    0.00 :   ffff800010463d10:       ldr     x3, [x23, x3, lsl #3]
    0.00 :   ffff800010463d14:       ldr     x0, [x0, #56]
    0.00 :   ffff800010463d18:       add     x0, x0, x3
         :                      blk_mq_get_tag():
         :                      data->ctx = blk_mq_get_ctx(data->q);
    0.00 :   ffff800010463d1c:       str     x0, [x20, #24]
         :                      blk_mq_map_queue():
         :                      if (flags & REQ_HIPRI)
    0.00 :   ffff800010463d20:       tbnz    w2, #25, ffff800010463d30 <blk_mq_get_tag+0x198>
         :                      else if ((flags & REQ_OP_MASK) == REQ_OP_READ)
    0.00 :   ffff800010463d24:       and     w2, w2, #0xff
    0.00 :   ffff800010463d28:       cmp     w2, #0x0
    0.00 :   ffff800010463d2c:       cset    w1, eq  // eq = none
         :                      return ctx->hctxs[type];
    0.00 :   ffff800010463d30:       ubfiz   x1, x1, #3, #2
         :                      blk_mq_get_tag():
         :                      data->ctx);
         :                      tags = blk_mq_tags_from_data(data);
    0.00 :   ffff800010463d34:       ldr     w2, [x20, #8]
         :                      blk_mq_map_queue():
    0.00 :   ffff800010463d38:       add     x0, x0, x1
         :                      blk_mq_tags_from_data():
         :                      return data->hctx->tags;
    0.00 :   ffff800010463d3c:       tst     x2, #0x4
         :                      blk_mq_map_queue():
         :                      return ctx->hctxs[type];
    0.00 :   ffff800010463d40:       ldr     x0, [x0, #80]
         :                      blk_mq_get_tag():
         :                      data->hctx = blk_mq_map_queue(data->q, data->cmd_flags,
    0.00 :   ffff800010463d44:       str     x0, [x20, #32]
         :                      blk_mq_tags_from_data():
         :                      return data->hctx->sched_tags;
    0.00 :   ffff800010463d48:       ldp     x3, x1, [x0, #336]
         :                      return data->hctx->tags;
    0.00 :   ffff800010463d4c:       csel    x1, x1, x3, ne  // ne = any
         :                      blk_mq_get_tag():
         :                      if (data->flags & BLK_MQ_REQ_RESERVED)
         :                      bt = &tags->breserved_tags;
    0.00 :   ffff800010463d50:       tst     x2, #0x2
    0.00 :   ffff800010463d54:       add     x26, x1, #0x50
    0.00 :   ffff800010463d58:       add     x1, x1, #0x10
    0.00 :   ffff800010463d5c:       csel    x26, x1, x26, eq  // eq = none
         :                      /*
         :                      * If destination hw queue is changed, fake wake up on
         :                      * previous queue for compensating the wake up miss, so
         :                      * other allocations on previous queue won't be starved.
         :                      */
         :                      if (bt != bt_prev)
    0.00 :   ffff800010463d60:       cmp     x19, x26
    0.00 :   ffff800010463d64:       b.eq    ffff800010463d74 <blk_mq_get_tag+0x1dc>  // b.none
         :                      sbitmap_queue_wake_up(bt_prev);
    0.00 :   ffff800010463d68:       mov     x0, x19
    0.00 :   ffff800010463d6c:       bl      ffff8000104ab940 <sbitmap_queue_wake_up>
    0.00 :   ffff800010463d70:       ldr     x0, [x20, #32]
    0.00 :   ffff800010463d74:       ldr     x21, [x26, #40]
         :                      bt_wait_ptr():
    0.00 :   ffff800010463d78:       cbz     x0, ffff800010463db8 <blk_mq_get_tag+0x220>
         :                      __read_once_size():
    0.00 :   ffff800010463d7c:       ldr     w4, [x0, #328]
         :                      atomic_cmpxchg():
    0.00 :   ffff800010463d80:       add     x3, x0, #0x148
         :                      __read_once_size():
    0.00 :   ffff800010463d84:       ldr     w1, [x0, #328]
         :                      sbq_wait_ptr():
         :                      ws = &sbq->ws[atomic_read(wait_index)];
    0.00 :   ffff800010463d88:       sbfiz   x4, x4, #6, #32
         :                      sbq_index_inc():
         :                      return (index + 1) & (SBQ_WAIT_QUEUES - 1);
    0.00 :   ffff800010463d8c:       add     w2, w1, #0x1
         :                      atomic_cmpxchg():
    0.00 :   ffff800010463d90:       sxtw    x0, w1
         :                      sbq_wait_ptr():
         :                      ws = &sbq->ws[atomic_read(wait_index)];
    0.00 :   ffff800010463d94:       add     x21, x21, x4
         :                      sbq_index_inc():
         :                      return (index + 1) & (SBQ_WAIT_QUEUES - 1);
    0.00 :   ffff800010463d98:       and     w2, w2, #0x7
         :                      arch_static_branch_jump():
    0.00 :   ffff800010463d9c:       b       ffff800010463e50 <blk_mq_get_tag+0x2b8>
    0.00 :   ffff800010463da0:       b       ffff800010463e50 <blk_mq_get_tag+0x2b8>
         :                      __lse__cmpxchg_case_mb_32():
    0.00 :   ffff800010463da4:       mov     x0, x3
    0.00 :   ffff800010463da8:       mov     w4, w1
    0.00 :   ffff800010463dac:       casal   w4, w2, [x3]
    0.00 :   ffff800010463db0:       mov     w0, w4
    0.00 :   ffff800010463db4:       ldr     x0, [x20, #32]
         :                      blk_mq_get_tag():
         :                      tag_offset = 0;
    0.00 :   ffff800010463db8:       mov     x19, x26
         :                      blk_mq_run_hw_queue(data->hctx, false);
    0.00 :   ffff800010463dbc:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010463dc0:       bl      ffff80001045ef88 <blk_mq_run_hw_queue>
         :                      tag = __blk_mq_get_tag(data, bt);
    0.00 :   ffff800010463dc4:       mov     x1, x19
    0.00 :   ffff800010463dc8:       mov     x0, x20
    0.00 :   ffff800010463dcc:       bl      ffff800010463688 <__blk_mq_get_tag>
    0.00 :   ffff800010463dd0:       mov     w26, w0
         :                      if (tag != -1)
    0.00 :   ffff800010463dd4:       cmn     w0, #0x1
    0.00 :   ffff800010463dd8:       b.eq    ffff800010463cb8 <blk_mq_get_tag+0x120>  // b.none
         :
         :                      ws = bt_wait_ptr(bt, data->hctx);
         :                      } while (1);
         :
         :                      sbitmap_finish_wait(bt, ws, &wait);
    0.00 :   ffff800010463ddc:       mov     x1, x21
    0.00 :   ffff800010463de0:       add     x2, x29, #0x58
    0.00 :   ffff800010463de4:       mov     x0, x19
    0.00 :   ffff800010463de8:       bl      ffff8000104ab660 <sbitmap_finish_wait>
    0.00 :   ffff800010463dec:       ldr     x21, [x29, #32]
    0.00 :   ffff800010463df0:       ldr     x23, [x29, #48]
    0.00 :   ffff800010463df4:       ldr     x25, [x29, #64]
    0.00 :   ffff800010463df8:       b       ffff800010463e20 <blk_mq_get_tag+0x288>
    0.00 :   ffff800010463dfc:       str     x26, [x29, #72]
         :                      bt = &tags->bitmap_tags;
    0.84 :   ffff800010463e00:       add     x19, x19, #0x10
         :                      tag = __blk_mq_get_tag(data, bt);
    0.00 :   ffff800010463e04:       mov     x0, x20
         :                      tag_offset = tags->nr_reserved_tags;
    6.68 :   ffff800010463e08:       ldur    w24, [x19, #-12]
         :                      tag = __blk_mq_get_tag(data, bt);
    0.00 :   ffff800010463e0c:       mov     x1, x19
    0.00 :   ffff800010463e10:       bl      ffff800010463688 <__blk_mq_get_tag>
    1.94 :   ffff800010463e14:       mov     w26, w0
         :                      if (tag != -1)
    0.00 :   ffff800010463e18:       cmn     w0, #0x1
    0.00 :   ffff800010463e1c:       b.eq    ffff800010463c28 <blk_mq_get_tag+0x90>  // b.none
         :
         :                      found_tag:
         :                      return tag + tag_offset;
    2.78 :   ffff800010463e20:       add     w0, w26, w24
    0.00 :   ffff800010463e24:       ldr     x26, [x29, #72]
         :                      }
    0.00 :   ffff800010463e28:       add     x22, x22, #0x8c8
    1.95 :   ffff800010463e2c:       ldr     x2, [x29, #136]
    1.67 :   ffff800010463e30:       ldr     x1, [x22]
    0.00 :   ffff800010463e34:       eor     x1, x2, x1
    0.00 :   ffff800010463e38:       cbnz    x1, ffff800010463e88 <blk_mq_get_tag+0x2f0>
    2.80 :   ffff800010463e3c:       ldp     x19, x20, [sp, #16]
    2.50 :   ffff800010463e40:       ldr     x22, [sp, #40]
    0.28 :   ffff800010463e44:       ldr     x24, [sp, #56]
    0.00 :   ffff800010463e48:       ldp     x29, x30, [sp], #144
    0.00 :   ffff800010463e4c:       ret
         :                      __ll_sc__cmpxchg_case_mb_32():
         :                      __CMPXCHG_CASE(w, h, rel_, 16,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w,  , rel_, 32,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  , rel_, 64,        ,  , l, "memory", L)
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, dmb ish,  , l, "memory", K)
    0.00 :   ffff800010463e50:       and     x0, x0, #0xffffffff
    0.00 :   ffff800010463e54:       b       ffff800010464510 <blk_mq_tag_update_depth+0x190>
    0.00 :   ffff800010463e58:       ldr     x0, [x20, #32]
         :                      blk_mq_get_tag():
         :                      tag_offset = 0;
    0.00 :   ffff800010463e5c:       mov     x19, x26
    0.00 :   ffff800010463e60:       b       ffff800010463dbc <blk_mq_get_tag+0x224>
         :                      WARN_ON_ONCE(1);
    0.00 :   ffff800010463e64:       brk     #0x800
         :                      return BLK_MQ_TAG_FAIL;
    0.00 :   ffff800010463e68:       mov     w0, #0xffffffff                 // #-1
    0.00 :   ffff800010463e6c:       b       ffff800010463e28 <blk_mq_get_tag+0x290>
         :                      __ll_sc__cmpxchg_case_mb_32():
    0.00 :   ffff800010463e70:       and     x0, x0, #0xffffffff
    0.00 :   ffff800010463e74:       b       ffff800010464530 <blk_mq_tag_update_depth+0x1b0>
    0.00 :   ffff800010463e78:       ldr     x0, [x20, #32]
    0.00 :   ffff800010463e7c:       b       ffff800010463c84 <blk_mq_get_tag+0xec>
    0.00 :   ffff800010463e80:       ldr     x26, [x29, #72]
    0.00 :   ffff800010463e84:       b       ffff800010463e28 <blk_mq_get_tag+0x290>
    0.00 :   ffff800010463e88:       str     x21, [x29, #32]
    0.00 :   ffff800010463e8c:       str     x23, [x29, #48]
    0.00 :   ffff800010463e90:       stp     x25, x26, [x29, #64]
         :                      blk_mq_get_tag():
         :                      }
    0.00 :   ffff800010463e94:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (321 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000106fe670 <iommu_dma_alloc_iova.isra.28>:
         :                      iommu_dma_alloc_iova():
         :                      }
         :
         :                      static dma_addr_t iommu_dma_alloc_iova(struct iommu_domain *domain,
         :                      size_t size, u64 dma_limit, struct device *dev)
         :                      {
         :                      struct iommu_dma_cookie *cookie = domain->iova_cookie;
    0.00 :   ffff8000106fe670:       ldr     x5, [x0, #64]
         :                      struct iova_domain *iovad = &cookie->iovad;
         :                      unsigned long shift, iova_len, iova = 0;
         :
         :                      if (cookie->type == IOMMU_DMA_MSI_COOKIE) {
    3.73 :   ffff8000106fe674:       ldr     w6, [x5]
    0.00 :   ffff8000106fe678:       cmp     w6, #0x1
    0.00 :   ffff8000106fe67c:       b.eq    ffff8000106fe740 <iommu_dma_alloc_iova.isra.28+0xd0>  // b.none
         :                      static dma_addr_t iommu_dma_alloc_iova(struct iommu_domain *domain,
    6.29 :   ffff8000106fe680:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff8000106fe684:       mov     x29, sp
    2.16 :   ffff8000106fe688:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000106fe68c:       stp     x21, x22, [sp, #32]
         :                      __ffs():
         :                      *
         :                      * Undefined if no bit exists, so code should check against 0 first.
         :                      */
         :                      static __always_inline unsigned long __ffs(unsigned long word)
         :                      {
         :                      return __builtin_ctzl(word);
    2.20 :   ffff8000106fe690:       ldr     x21, [x5, #40]
    0.00 :   ffff8000106fe694:       rbit    x21, x21
    0.00 :   ffff8000106fe698:       clz     x21, x21
         :                      iommu_dma_alloc_iova():
         :                      cookie->msi_iova += size;
         :                      return cookie->msi_iova - size;
         :                      }
         :
         :                      shift = iova_shift(iovad);
         :                      iova_len = size >> shift;
    0.00 :   ffff8000106fe69c:       lsr     x19, x1, x21
         :                      * Freeing non-power-of-two-sized allocations back into the IOVA caches
         :                      * will come back to bite us badly, so we have to waste a bit of space
         :                      * rounding up anything cacheable to make sure that can't happen. The
         :                      * order of the unadjusted size will still match upon freeing.
         :                      */
         :                      if (iova_len < (1 << (IOVA_RANGE_CACHE_MAX_SIZE - 1)))
    1.87 :   ffff8000106fe6a0:       cmp     x19, #0x1f
    0.00 :   ffff8000106fe6a4:       b.hi    ffff8000106fe6cc <iommu_dma_alloc_iova.isra.28+0x5c>  // b.pmore
         :                      __roundup_pow_of_two():
         :                      * @n: value to round up
         :                      */
         :                      static inline __attribute__((const))
         :                      unsigned long __roundup_pow_of_two(unsigned long n)
         :                      {
         :                      return 1UL << fls_long(n - 1);
    1.26 :   ffff8000106fe6a8:       sub     x6, x19, #0x1
    0.00 :   ffff8000106fe6ac:       mov     x1, #0x3f                       // #63
    0.00 :   ffff8000106fe6b0:       clz     x7, x6
         :                      fls64():
         :                      return fls(x);
         :                      }
         :                      #elif BITS_PER_LONG == 64
         :                      static __always_inline int fls64(__u64 x)
         :                      {
         :                      if (x == 0)
    0.00 :   ffff8000106fe6b4:       mov     x19, #0x1                       // #1
   10.01 :   ffff8000106fe6b8:       sub     x1, x1, x7
    0.00 :   ffff8000106fe6bc:       cmp     x6, #0x0
    0.31 :   ffff8000106fe6c0:       add     w1, w1, #0x1
    0.00 :   ffff8000106fe6c4:       lsl     x1, x19, x1
    0.64 :   ffff8000106fe6c8:       csel    x19, x1, x19, ne  // ne = any
         :                      iommu_dma_alloc_iova():
         :                      iova_len = roundup_pow_of_two(iova_len);
         :
         :                      dma_limit = min_not_zero(dma_limit, dev->bus_dma_limit);
    0.30 :   ffff8000106fe6cc:       ldr     x22, [x4]
    0.00 :   ffff8000106fe6d0:       cbz     x2, ffff8000106fe6e0 <iommu_dma_alloc_iova.isra.28+0x70>
    1.25 :   ffff8000106fe6d4:       cbz     x22, ffff8000106fe750 <iommu_dma_alloc_iova.isra.28+0xe0>
    0.00 :   ffff8000106fe6d8:       cmp     x22, x2
    0.00 :   ffff8000106fe6dc:       csel    x22, x22, x2, ls  // ls = plast
         :
         :                      if (domain->geometry.force_aperture)
    0.00 :   ffff8000106fe6e0:       ldrb    w1, [x0, #56]
    0.00 :   ffff8000106fe6e4:       cbz     w1, ffff8000106fe6f4 <iommu_dma_alloc_iova.isra.28+0x84>
         :                      dma_limit = min(dma_limit, (u64)domain->geometry.aperture_end);
    0.94 :   ffff8000106fe6e8:       ldr     x0, [x0, #48]
    0.00 :   ffff8000106fe6ec:       cmp     x22, x0
    0.00 :   ffff8000106fe6f0:       csel    x22, x22, x0, ls  // ls = plast
         :
         :                      /* Try to get PCI devices a SAC address */
         :                      if (dma_limit > DMA_BIT_MASK(32) && dev_is_pci(dev))
    0.00 :   ffff8000106fe6f4:       mov     x2, #0xffffffff                 // #4294967295
         :                      struct iova_domain *iovad = &cookie->iovad;
    0.31 :   ffff8000106fe6f8:       add     x20, x5, #0x8
         :                      if (dma_limit > DMA_BIT_MASK(32) && dev_is_pci(dev))
    0.00 :   ffff8000106fe6fc:       cmp     x22, x2
    0.00 :   ffff8000106fe700:       b.ls    ffff8000106fe718 <iommu_dma_alloc_iova.isra.28+0xa8>  // b.plast
    4.06 :   ffff8000106fe704:       ldr     x1, [x3]
    0.32 :   ffff8000106fe708:       adrp    x0, ffff800011929000 <pwm_attrs+0x18>
    0.00 :   ffff8000106fe70c:       add     x0, x0, #0x5f8
    0.00 :   ffff8000106fe710:       cmp     x1, x0
   53.50 :   ffff8000106fe714:       b.eq    ffff8000106fe760 <iommu_dma_alloc_iova.isra.28+0xf0>  // b.none
         :                      iova = alloc_iova_fast(iovad, iova_len,
         :                      DMA_BIT_MASK(32) >> shift, false);
         :
         :                      if (!iova)
         :                      iova = alloc_iova_fast(iovad, iova_len, dma_limit >> shift,
    0.00 :   ffff8000106fe718:       mov     w3, #0x1                        // #1
    0.00 :   ffff8000106fe71c:       lsr     x2, x22, x21
    0.00 :   ffff8000106fe720:       mov     x1, x19
    0.00 :   ffff8000106fe724:       mov     x0, x20
    0.00 :   ffff8000106fe728:       bl      ffff800010702878 <alloc_iova_fast>
         :                      true);
         :
         :                      return (dma_addr_t)iova << shift;
    5.91 :   ffff8000106fe72c:       lsl     x0, x0, x21
         :                      }
    0.00 :   ffff8000106fe730:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000106fe734:       ldp     x21, x22, [sp, #32]
    0.31 :   ffff8000106fe738:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000106fe73c:       ret
         :                      cookie->msi_iova += size;
    0.00 :   ffff8000106fe740:       ldr     x0, [x5, #8]
    0.00 :   ffff8000106fe744:       add     x1, x0, x1
    0.00 :   ffff8000106fe748:       str     x1, [x5, #8]
         :                      }
    0.00 :   ffff8000106fe74c:       ret
         :                      if (domain->geometry.force_aperture)
    1.24 :   ffff8000106fe750:       ldrb    w1, [x0, #56]
         :                      dma_limit = min_not_zero(dma_limit, dev->bus_dma_limit);
    0.00 :   ffff8000106fe754:       mov     x22, x2
         :                      if (domain->geometry.force_aperture)
    0.00 :   ffff8000106fe758:       cbz     w1, ffff8000106fe6f4 <iommu_dma_alloc_iova.isra.28+0x84>
    1.85 :   ffff8000106fe75c:       b       ffff8000106fe6e8 <iommu_dma_alloc_iova.isra.28+0x78>
         :                      iova = alloc_iova_fast(iovad, iova_len,
    0.00 :   ffff8000106fe760:       mov     w3, #0x0                        // #0
    0.00 :   ffff8000106fe764:       lsr     x2, x2, x21
    0.00 :   ffff8000106fe768:       mov     x1, x19
    0.00 :   ffff8000106fe76c:       mov     x0, x20
    1.23 :   ffff8000106fe770:       bl      ffff800010702878 <alloc_iova_fast>
         :                      if (!iova)
    0.31 :   ffff8000106fe774:       cbz     x0, ffff8000106fe718 <iommu_dma_alloc_iova.isra.28+0xa8>
    0.00 :   ffff8000106fe778:       b       ffff8000106fe72c <iommu_dma_alloc_iova.isra.28+0xbc>
 Percent |	Source code & Disassembly of vmlinux for cycles (383 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010112e88 <finish_task_switch>:
         :                      finish_task_switch():
         :                      * past. prev == current is still correct but we need to recalculate this_rq
         :                      * because prev may have moved to another CPU.
         :                      */
         :                      static struct rq *finish_task_switch(struct task_struct *prev)
         :                      __releases(rq->lock)
         :                      {
    0.00 :   ffff800010112e88:       stp     x29, x30, [sp, #-64]!
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800010112e8c:       mrs     x1, sp_el0
         :                      finish_task_switch():
    0.00 :   ffff800010112e90:       mov     x29, sp
    0.00 :   ffff800010112e94:       stp     x19, x20, [sp, #16]
         :                      struct rq *rq = this_rq();
    0.00 :   ffff800010112e98:       adrp    x19, ffff8000114d5000 <tegra_to+0x180>
         :                      {
    0.00 :   ffff800010112e9c:       stp     x21, x22, [sp, #32]
         :                      struct rq *rq = this_rq();
    0.00 :   ffff800010112ea0:       add     x19, x19, #0xd80
         :                      {
    0.00 :   ffff800010112ea4:       stp     x23, x24, [sp, #48]
    0.00 :   ffff800010112ea8:       mov     x22, x0
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800010112eac:       mrs     x3, tpidr_el1
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010112eb0:       ldr     w2, [x1, #16]
    0.00 :   ffff800010112eb4:       mov     x24, x30
         :                      finish_task_switch():
         :                      struct rq *rq = this_rq();
    0.00 :   ffff800010112eb8:       add     x19, x19, x3
         :                      *         __schedule()
         :                      *           raw_spin_lock_irq(&rq->lock)        // 2
         :                      *
         :                      * Also, see FORK_PREEMPT_COUNT.
         :                      */
         :                      if (WARN_ONCE(preempt_count() != 2*PREEMPT_DISABLE_OFFSET,
    0.00 :   ffff800010112ebc:       cmp     w2, #0x2
         :                      struct mm_struct *mm = rq->prev_mm;
    0.00 :   ffff800010112ec0:       ldr     x20, [x19, #2384]
         :                      if (WARN_ONCE(preempt_count() != 2*PREEMPT_DISABLE_OFFSET,
    0.00 :   ffff800010112ec4:       b.ne    ffff800010113040 <finish_task_switch+0x1b8>  // b.any
         :                      "corrupted preempt_count: %s/%d/0x%x\n",
         :                      current->comm, current->pid, preempt_count()))
         :                      preempt_count_set(FORK_PREEMPT_COUNT);
         :
         :                      rq->prev_mm = NULL;
    0.00 :   ffff800010112ec8:       str     xzr, [x19, #2384]
         :                      get_current():
    0.00 :   ffff800010112ecc:       mrs     x21, sp_el0
         :                      finish_task_switch():
         :                      * We must observe prev->state before clearing prev->on_cpu (in
         :                      * finish_task), otherwise a concurrent wakeup can get prev
         :                      * running on another CPU and we could rave with its RUNNING -> DEAD
         :                      * transition, resulting in a double drop.
         :                      */
         :                      prev_state = prev->state;
    0.00 :   ffff800010112ed0:       ldr     x23, [x22, #24]
         :                      arch_static_branch():
         :                      #define JUMP_LABEL_NOP_SIZE             AARCH64_INSN_SIZE
         :
         :                      static __always_inline bool arch_static_branch(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010112ed4:       nop
    0.00 :   ffff800010112ed8:       nop
         :                      finish_task():
         :                      smp_store_release(&prev->on_cpu, 0);
    0.00 :   ffff800010112edc:       mov     w0, #0x0                        // #0
    0.00 :   ffff800010112ee0:       add     x1, x22, #0x40
    0.00 :   ffff800010112ee4:       stlr    w0, [x1]
         :                      finish_lock_switch():
         :                      raw_spin_unlock_irq(&rq->lock);
    0.00 :   ffff800010112ee8:       mov     x0, x19
    0.00 :   ffff800010112eec:       bl      ffff800010cb2bf0 <_raw_spin_unlock_irq>
         :                      get_current():
   17.70 :   ffff800010112ef0:       mrs     x0, sp_el0
         :                      arch_static_branch():
    1.54 :   ffff800010112ef4:       nop
         :                      finish_task_switch():
         :                      *
         :                      * - a full memory barrier for {PRIVATE,GLOBAL}_EXPEDITED, implicitly
         :                      *   provided by mmdrop(),
         :                      * - a sync_core for SYNC_CORE.
         :                      */
         :                      if (mm) {
    0.00 :   ffff800010112ef8:       cbz     x20, ffff800010112f2c <finish_task_switch+0xa4>
         :                      get_current():
    0.00 :   ffff800010112efc:       mrs     x0, sp_el0
         :                      membarrier_mm_sync_core_before_usermode():
         :                      #include <asm/membarrier.h>
         :                      #endif
         :
         :                      static inline void membarrier_mm_sync_core_before_usermode(struct mm_struct *mm)
         :                      {
         :                      if (current->mm != mm)
    1.31 :   ffff800010112f00:       ldr     x0, [x0, #952]
    0.00 :   ffff800010112f04:       cmp     x20, x0
    0.00 :   ffff800010112f08:       b.eq    ffff800010112ff4 <finish_task_switch+0x16c>  // b.none
         :                      mmdrop():
         :                      if (unlikely(atomic_dec_and_test(&mm->mm_count)))
    0.00 :   ffff800010112f0c:       add     x1, x20, #0x50
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010112f10:       b       ffff800010112f4c <finish_task_switch+0xc4>
    0.00 :   ffff800010112f14:       b       ffff800010112f4c <finish_task_switch+0xc4>
         :                      __lse_atomic_sub_return():
         :                      }
         :
         :                      ATOMIC_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC_OP_SUB_RETURN(        , al, "memory")
    1.04 :   ffff800010112f18:       mov     w0, #0x1                        // #1
    0.00 :   ffff800010112f1c:       neg     w0, w0
    0.53 :   ffff800010112f20:       ldaddal w0, w2, [x1]
   63.30 :   ffff800010112f24:       add     w0, w0, w2
         :                      mmdrop():
    0.00 :   ffff800010112f28:       cbz     w0, ffff80001011305c <finish_task_switch+0x1d4>
         :                      finish_task_switch():
         :                      membarrier_mm_sync_core_before_usermode(mm);
         :                      mmdrop(mm);
         :                      }
         :                      if (unlikely(prev_state == TASK_DEAD)) {
    0.00 :   ffff800010112f2c:       cmp     x23, #0x80
    0.00 :   ffff800010112f30:       b.eq    ffff800010113004 <finish_task_switch+0x17c>  // b.none
         :                      put_task_struct_rcu_user(prev);
         :                      }
         :
         :                      tick_nohz_task_switch();
         :                      return rq;
         :                      }
    4.65 :   ffff800010112f34:       mov     x0, x19
    0.49 :   ffff800010112f38:       ldp     x19, x20, [sp, #16]
    0.53 :   ffff800010112f3c:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010112f40:       ldp     x23, x24, [sp, #48]
    2.60 :   ffff800010112f44:       ldp     x29, x30, [sp], #64
    0.00 :   ffff800010112f48:       ret
         :                      __ll_sc_atomic_sub_return():
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff800010112f4c:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010112f50:       add     x3, x20, #0x50
    0.00 :   ffff800010112f54:       b       ffff800010119678 <dump_cpu_task+0x98>
    0.00 :   ffff800010112f58:       b       ffff800010112f28 <finish_task_switch+0xa0>
         :                      perf_event_task_sched_in():
         :                      struct task_struct *task)
         :                      {
         :                      if (static_branch_unlikely(&perf_sched_events))
         :                      __perf_event_task_sched_in(prev, task);
         :
         :                      if (perf_sw_migrate_enabled() && task->sched_migrated) {
    0.00 :   ffff800010112f5c:       ldrb    w0, [x21, #1060]
    0.00 :   ffff800010112f60:       tbz     w0, #2, ffff800010112edc <finish_task_switch+0x54>
         :                      __my_cpu_offset():
         :                      "mrs %0, tpidr_el2",
         :                      ARM64_HAS_VIRT_HOST_EXTN)
         :                      : "=r" (off) :
         :                      "Q" (*(const unsigned long *)current_stack_pointer));
    0.00 :   ffff800010112f64:       mov     x5, sp
         :                      perf_event_task_sched_in():
         :                      struct pt_regs *regs = this_cpu_ptr(&__perf_regs[0]);
    0.00 :   ffff800010112f68:       adrp    x4, ffff8000114d3000 <pmu_sb_events>
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800010112f6c:       mrs     x0, tpidr_el1
         :                      perf_event_task_sched_in():
    0.00 :   ffff800010112f70:       add     x4, x4, #0x50
         :                      perf_fetch_caller_regs():
         :                      perf_arch_fetch_caller_regs(regs, CALLER_ADDR0);
    0.00 :   ffff800010112f74:       mov     x6, #0x5                        // #5
         :                      perf_event_task_sched_in():
         :                      struct pt_regs *regs = this_cpu_ptr(&__perf_regs[0]);
    0.00 :   ffff800010112f78:       add     x4, x4, x0
         :
         :                      perf_fetch_caller_regs(regs);
         :                      ___perf_sw_event(PERF_COUNT_SW_CPU_MIGRATIONS, 1, regs, 0);
    0.00 :   ffff800010112f7c:       mov     x3, #0x0                        // #0
    0.00 :   ffff800010112f80:       mov     w0, #0x4                        // #4
    0.00 :   ffff800010112f84:       mov     x2, x4
    0.00 :   ffff800010112f88:       mov     x1, #0x1                        // #1
         :                      perf_fetch_caller_regs():
         :                      perf_arch_fetch_caller_regs(regs, CALLER_ADDR0);
    0.00 :   ffff800010112f8c:       str     x29, [x4, #232]
    0.00 :   ffff800010112f90:       stp     x5, x24, [x4, #248]
    0.00 :   ffff800010112f94:       str     x6, [x4, #264]
         :                      perf_event_task_sched_in():
         :                      ___perf_sw_event(PERF_COUNT_SW_CPU_MIGRATIONS, 1, regs, 0);
    0.00 :   ffff800010112f98:       bl      ffff8000101c84a8 <___perf_sw_event>
         :                      task->sched_migrated = 0;
    0.00 :   ffff800010112f9c:       ldrb    w0, [x21, #1060]
    0.00 :   ffff800010112fa0:       and     w0, w0, #0xfffffffb
    0.00 :   ffff800010112fa4:       strb    w0, [x21, #1060]
    0.00 :   ffff800010112fa8:       b       ffff800010112edc <finish_task_switch+0x54>
         :                      __fire_sched_in_preempt_notifiers():
         :                      hlist_for_each_entry(notifier, &curr->preempt_notifiers, link)
    0.00 :   ffff800010112fac:       ldr     x21, [x0, #720]
    0.00 :   ffff800010112fb0:       cbz     x21, ffff800010112ef8 <finish_task_switch+0x70>
    0.00 :   ffff800010112fb4:       adrp    x24, ffff8000114ca000 <bp_hardening_data>
         :                      notifier->ops->sched_in(notifier, raw_smp_processor_id());
    0.00 :   ffff800010112fb8:       add     x24, x24, #0x18
    0.00 :   ffff800010112fbc:       ldr     x2, [x21, #16]
    0.00 :   ffff800010112fc0:       mov     x0, x24
         :                      __my_cpu_offset():
    0.00 :   ffff800010112fc4:       mrs     x1, tpidr_el1
         :                      __fire_sched_in_preempt_notifiers():
    0.00 :   ffff800010112fc8:       ldr     x2, [x2]
    0.00 :   ffff800010112fcc:       ldr     w1, [x0, x1]
    0.00 :   ffff800010112fd0:       mov     x0, x21
    0.00 :   ffff800010112fd4:       blr     x2
         :                      hlist_for_each_entry(notifier, &curr->preempt_notifiers, link)
    0.00 :   ffff800010112fd8:       ldr     x21, [x21]
    0.00 :   ffff800010112fdc:       cbnz    x21, ffff800010112fbc <finish_task_switch+0x134>
    0.00 :   ffff800010112fe0:       b       ffff800010112ef8 <finish_task_switch+0x70>
         :                      perf_event_task_sched_in():
         :                      __perf_event_task_sched_in(prev, task);
    0.00 :   ffff800010112fe4:       mov     x1, x21
    0.00 :   ffff800010112fe8:       mov     x0, x22
    0.00 :   ffff800010112fec:       bl      ffff8000101c24d0 <__perf_event_task_sched_in>
    0.00 :   ffff800010112ff0:       b       ffff800010112ed8 <finish_task_switch+0x50>
         :                      __read_once_size():
    4.48 :   ffff800010112ff4:       ldr     w0, [x20, #72]
         :                      mmdrop():
    1.05 :   ffff800010112ff8:       add     x1, x20, #0x50
         :                      arch_static_branch_jump():
    0.00 :   ffff800010112ffc:       b       ffff800010112f4c <finish_task_switch+0xc4>
    0.79 :   ffff800010113000:       b       ffff800010112f14 <finish_task_switch+0x8c>
         :                      finish_task_switch():
         :                      if (prev->sched_class->task_dead)
    0.00 :   ffff800010113004:       ldr     x0, [x22, #128]
    0.00 :   ffff800010113008:       ldr     x1, [x0, #144]
    0.00 :   ffff80001011300c:       cbz     x1, ffff800010113018 <finish_task_switch+0x190>
         :                      prev->sched_class->task_dead(prev);
    0.00 :   ffff800010113010:       mov     x0, x22
    0.00 :   ffff800010113014:       blr     x1
         :                      put_task_stack(prev);
    0.00 :   ffff800010113018:       mov     x0, x22
    0.00 :   ffff80001011301c:       bl      ffff8000100e25c0 <put_task_stack>
         :                      put_task_struct_rcu_user(prev);
    0.00 :   ffff800010113020:       mov     x0, x22
    0.00 :   ffff800010113024:       bl      ffff8000100e8728 <put_task_struct_rcu_user>
         :                      }
    0.00 :   ffff800010113028:       mov     x0, x19
    0.00 :   ffff80001011302c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010113030:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010113034:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff800010113038:       ldp     x29, x30, [sp], #64
    0.00 :   ffff80001011303c:       ret
         :                      if (WARN_ONCE(preempt_count() != 2*PREEMPT_DISABLE_OFFSET,
    0.00 :   ffff800010113040:       adrp    x4, ffff800011a5c000 <pci_serial_quirks+0x138>
    0.00 :   ffff800010113044:       ldrb    w0, [x4, #4087]
    0.00 :   ffff800010113048:       cbz     w0, ffff800010113068 <finish_task_switch+0x1e0>
         :                      get_current():
    0.00 :   ffff80001011304c:       mrs     x0, sp_el0
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010113050:       mov     w1, #0x2                        // #2
    0.00 :   ffff800010113054:       str     w1, [x0, #16]
    0.00 :   ffff800010113058:       b       ffff800010112ec8 <finish_task_switch+0x40>
         :                      mmdrop():
         :                      __mmdrop(mm);
    0.00 :   ffff80001011305c:       mov     x0, x20
    0.00 :   ffff800010113060:       bl      ffff8000100e1448 <__mmdrop>
    0.00 :   ffff800010113064:       b       ffff800010112f2c <finish_task_switch+0xa4>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010113068:       ldr     w3, [x1, #16]
         :                      finish_task_switch():
    0.00 :   ffff80001011306c:       mov     w5, #0x1                        // #1
    0.00 :   ffff800010113070:       ldr     w2, [x1, #1128]
    0.00 :   ffff800010113074:       adrp    x0, ffff800011186000 <kallsyms_token_index+0xc330>
    0.00 :   ffff800010113078:       strb    w5, [x4, #4087]
    0.00 :   ffff80001011307c:       add     x0, x0, #0xe0
    0.00 :   ffff800010113080:       add     x1, x1, #0x620
    0.00 :   ffff800010113084:       bl      ffff8000100e5288 <__warn_printk>
    0.00 :   ffff800010113088:       brk     #0x800
    0.00 :   ffff80001011308c:       b       ffff80001011304c <finish_task_switch+0x1c4>
 Percent |	Source code & Disassembly of vmlinux for cycles (329 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010caccd8 <__schedule>:
         :                      __schedule():
         :                      *          - return from interrupt-handler to user-space
         :                      *
         :                      * WARNING: must be called with preemption disabled!
         :                      */
         :                      static void __sched notrace __schedule(bool preempt)
         :                      {
    6.42 :   ffff800010caccd8:       stp     x29, x30, [sp, #-96]!
    0.00 :   ffff800010caccdc:       mov     x29, sp
   10.47 :   ffff800010cacce0:       stp     x19, x20, [sp, #16]
         :                      struct rq_flags rf;
         :                      struct rq *rq;
         :                      int cpu;
         :
         :                      cpu = smp_processor_id();
         :                      rq = cpu_rq(cpu);
    0.00 :   ffff800010cacce4:       adrp    x19, ffff8000114d5000 <tegra_to+0x180>
         :                      {
    1.85 :   ffff800010cacce8:       stp     x21, x22, [sp, #32]
    0.00 :   ffff800010caccec:       adrp    x22, ffff800011899000 <page_wait_table+0x1500>
    1.51 :   ffff800010caccf0:       stp     x23, x24, [sp, #48]
         :                      cpu = smp_processor_id();
    0.00 :   ffff800010caccf4:       adrp    x23, ffff8000114ca000 <bp_hardening_data>
         :                      {
    3.98 :   ffff800010caccf8:       stp     x25, x26, [sp, #64]
         :                      cpu = smp_processor_id();
    0.00 :   ffff800010caccfc:       add     x1, x23, #0x18
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    1.23 :   ffff800010cacd00:       mrs     x2, tpidr_el1
         :                      __schedule():
         :                      rq = cpu_rq(cpu);
    0.00 :   ffff800010cacd04:       ldrsw   x2, [x1, x2]
         :                      {
    0.00 :   ffff800010cacd08:       add     x3, x22, #0x8c8
         :                      rq = cpu_rq(cpu);
    0.00 :   ffff800010cacd0c:       adrp    x1, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010cacd10:       add     x1, x1, #0x8e8
         :                      {
   10.88 :   ffff800010cacd14:       ldr     x4, [x3]
    3.03 :   ffff800010cacd18:       str     x4, [x29, #88]
    0.00 :   ffff800010cacd1c:       mov     x4, #0x0                        // #0
         :                      rq = cpu_rq(cpu);
    0.00 :   ffff800010cacd20:       add     x19, x19, #0xd80
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800010cacd24:       mrs     x21, sp_el0
         :                      __schedule():
    0.00 :   ffff800010cacd28:       ldr     x2, [x1, x2, lsl #3]
    0.00 :   ffff800010cacd2c:       mov     x25, x30
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
   11.92 :   ffff800010cacd30:       ldr     w1, [x21, #16]
         :                      __schedule():
         :                      {
    0.00 :   ffff800010cacd34:       and     w20, w0, #0xff
         :                      rq = cpu_rq(cpu);
    0.00 :   ffff800010cacd38:       add     x19, x19, x2
         :                      schedule_debug():
         :                      if (unlikely(in_atomic_preempt_off())) {
    0.00 :   ffff800010cacd3c:       cmp     w1, #0x1
         :                      __schedule():
         :                      prev = rq->curr;
    0.00 :   ffff800010cacd40:       ldr     x26, [x19, #2352]
         :                      schedule_debug():
         :                      if (unlikely(in_atomic_preempt_off())) {
    0.00 :   ffff800010cacd44:       b.ne    ffff800010cad204 <__schedule+0x52c>  // b.any
         :                      profile_hit():
         :                      static inline void profile_hit(int type, void *ip)
         :                      {
         :                      /*
         :                      * Speedup for the common (no profiling enabled) case:
         :                      */
         :                      if (unlikely(prof_on == type))
   26.41 :   ffff800010cacd48:       adrp    x0, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff800010cacd4c:       ldr     w0, [x0, #632]
    0.00 :   ffff800010cacd50:       cmp     w0, #0x2
    0.00 :   ffff800010cacd54:       b.eq    ffff800010cad228 <__schedule+0x550>  // b.none
         :                      arch_local_irq_disable():
         :                      u32 pmr = read_sysreg_s(SYS_ICC_PMR_EL1);
         :
         :                      WARN_ON_ONCE(pmr != GIC_PRIO_IRQON && pmr != GIC_PRIO_IRQOFF);
         :                      }
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010cacd58:       mov     x0, #0x60                       // #96
    0.00 :   ffff800010cacd5c:       msr     daifset, #0x2
         :                      __schedule():
         :
         :                      if (sched_feat(HRTICK))
         :                      hrtick_clear(rq);
         :
         :                      local_irq_disable();
         :                      rcu_note_context_switch(preempt);
    0.00 :   ffff800010cacd60:       mov     w0, w20
    0.00 :   ffff800010cacd64:       bl      ffff80001015ea90 <rcu_note_context_switch>
         :                      rq_lock():
         :
         :                      static inline void
         :                      rq_lock(struct rq *rq, struct rq_flags *rf)
         :                      __acquires(rq->lock)
         :                      {
         :                      raw_spin_lock(&rq->lock);
    0.00 :   ffff800010cacd68:       mov     x0, x19
    0.00 :   ffff800010cacd6c:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      __schedule():
         :                      *
         :                      * The membarrier system call requires a full memory barrier
         :                      * after coming from user-space, before storing to rq->curr.
         :                      */
         :                      rq_lock(rq, &rf);
         :                      smp_mb__after_spinlock();
    0.00 :   ffff800010cacd70:       dmb     ish
         :
         :                      /* Promote REQ to ACT */
         :                      rq->clock_update_flags <<= 1;
    0.00 :   ffff800010cacd74:       ldr     w0, [x19, #2392]
    0.00 :   ffff800010cacd78:       lsl     w0, w0, #1
    0.00 :   ffff800010cacd7c:       str     w0, [x19, #2392]
         :                      update_rq_clock():
         :                      if (rq->clock_update_flags & RQCF_ACT_SKIP)
    0.00 :   ffff800010cacd80:       tbz     w0, #1, ffff800010cad088 <__schedule+0x3b0>
         :                      __schedule():
         :                      update_rq_clock(rq);
         :
         :                      switch_count = &prev->nivcsw;
    0.00 :   ffff800010cacd84:       add     x24, x26, #0x588
         :                      if (!preempt && prev->state) {
    0.00 :   ffff800010cacd88:       cbnz    w20, ffff800010cacd94 <__schedule+0xbc>
    0.00 :   ffff800010cacd8c:       ldr     x0, [x26, #24]
    0.00 :   ffff800010cacd90:       cbnz    x0, ffff800010cad020 <__schedule+0x348>
         :                      pick_next_task():
         :                      if (likely((prev->sched_class == &idle_sched_class ||
    0.00 :   ffff800010cacd94:       ldr     x20, [x26, #128]
    0.00 :   ffff800010cacd98:       adrp    x0, ffff800010ce6000 <sched_prio_to_weight+0x10>
    0.00 :   ffff800010cacd9c:       add     x0, x0, #0x220
    0.00 :   ffff800010cacda0:       adrp    x21, ffff800010ce6000 <sched_prio_to_weight+0x10>
    0.00 :   ffff800010cacda4:       cmp     x20, x0
    0.00 :   ffff800010cacda8:       add     x0, x21, #0x158
    0.00 :   ffff800010cacdac:       ccmp    x20, x0, #0x4, ne  // ne = any
    0.00 :   ffff800010cacdb0:       b.ne    ffff800010cad170 <__schedule+0x498>  // b.any
    0.00 :   ffff800010cacdb4:       ldr     w1, [x19, #4]
    0.00 :   ffff800010cacdb8:       ldr     w0, [x19, #156]
    0.00 :   ffff800010cacdbc:       cmp     w1, w0
    0.00 :   ffff800010cacdc0:       b.ne    ffff800010cad170 <__schedule+0x498>  // b.any
         :                      p = pick_next_task_fair(rq, prev, rf);
    0.00 :   ffff800010cacdc4:       add     x2, x29, #0x50
    0.00 :   ffff800010cacdc8:       mov     x1, x26
    0.00 :   ffff800010cacdcc:       mov     x0, x19
    0.00 :   ffff800010cacdd0:       bl      ffff800010126488 <pick_next_task_fair>
    0.00 :   ffff800010cacdd4:       mov     x20, x0
         :                      if (unlikely(p == RETRY_TASK))
    0.00 :   ffff800010cacdd8:       cmn     x0, #0x1
    0.00 :   ffff800010cacddc:       b.eq    ffff800010cad16c <__schedule+0x494>  // b.none
         :                      if (!p) {
    0.00 :   ffff800010cacde0:       cbz     x0, ffff800010cad0ec <__schedule+0x414>
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010cacde4:       b       ffff800010cace34 <__schedule+0x15c>
    0.00 :   ffff800010cacde8:       b       ffff800010cace34 <__schedule+0x15c>
         :                      __lse_atomic64_andnot():
         :                      "       " #asm_op "     %[i], %[v]\n"                                   \
         :                      : [i] "+r" (i), [v] "+Q" (v->counter)                           \
         :                      : "r" (v));                                                     \
         :                      }
         :
         :                      ATOMIC64_OP(andnot, stclr)
    0.00 :   ffff800010cacdec:       mov     x0, #0x2                        // #2
    0.00 :   ffff800010cacdf0:       stclr   x0, [x26]
         :                      get_current():
    0.00 :   ffff800010cacdf4:       mrs     x0, sp_el0
         :                      clear_preempt_need_resched():
         :                      current_thread_info()->preempt.need_resched = 0;
         :                      }
         :
         :                      static inline void clear_preempt_need_resched(void)
         :                      {
         :                      current_thread_info()->preempt.need_resched = 1;
    0.00 :   ffff800010cacdf8:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010cacdfc:       str     w1, [x0, #20]
         :                      __schedule():
         :
         :                      next = pick_next_task(rq, prev, &rf);
         :                      clear_tsk_need_resched(prev);
         :                      clear_preempt_need_resched();
         :
         :                      if (likely(prev != next)) {
    0.00 :   ffff800010cace00:       cmp     x26, x20
    0.00 :   ffff800010cace04:       b.ne    ffff800010cace50 <__schedule+0x178>  // b.any
         :                      trace_sched_switch(preempt, prev, next);
         :
         :                      /* Also unlocks the rq: */
         :                      rq = context_switch(rq, prev, next, &rf);
         :                      } else {
         :                      rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
    0.00 :   ffff800010cace08:       ldr     w1, [x19, #2392]
         :                      rq_unlock_irq():
         :                      static inline void
         :                      rq_unlock_irq(struct rq *rq, struct rq_flags *rf)
         :                      __releases(rq->lock)
         :                      {
         :                      rq_unpin_lock(rq, rf);
         :                      raw_spin_unlock_irq(&rq->lock);
    0.00 :   ffff800010cace0c:       mov     x0, x19
         :                      __schedule():
    0.00 :   ffff800010cace10:       and     w1, w1, #0xfffffffc
    0.00 :   ffff800010cace14:       str     w1, [x19, #2392]
         :                      rq_unlock_irq():
    0.00 :   ffff800010cace18:       bl      ffff800010cb2bf0 <_raw_spin_unlock_irq>
         :                      balance_callback():
         :                      if (unlikely(rq->balance_callback))
    0.00 :   ffff800010cace1c:       ldr     x0, [x19, #2496]
    0.00 :   ffff800010cace20:       cbz     x0, ffff800010cacf38 <__schedule+0x260>
    0.00 :   ffff800010cace24:       nop
         :                      __balance_callback(rq);
    0.00 :   ffff800010cace28:       mov     x0, x19
    0.00 :   ffff800010cace2c:       bl      ffff800010112368 <__balance_callback>
         :                      __schedule():
         :                      rq_unlock_irq(rq, &rf);
         :                      }
         :
         :                      balance_callback(rq);
         :                      }
    0.00 :   ffff800010cace30:       b       ffff800010cacf38 <__schedule+0x260>
         :                      __ll_sc_atomic64_andnot():
         :                      /*
         :                      * GAS converts the mysterious and undocumented BIC (immediate) alias to
         :                      * an AND (immediate) instruction with the immediate inverted. We don't
         :                      * have a constraint for this, so fall back to register.
         :                      */
         :                      ATOMIC64_OPS(andnot, bic, )
    0.00 :   ffff800010cace34:       mov     x0, #0x2                        // #2
    0.00 :   ffff800010cace38:       b       ffff800010cad824 <io_schedule+0x34>
         :                      get_current():
    0.00 :   ffff800010cace3c:       mrs     x0, sp_el0
         :                      clear_preempt_need_resched():
    0.00 :   ffff800010cace40:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010cace44:       str     w1, [x0, #20]
         :                      __schedule():
         :                      if (likely(prev != next)) {
    0.00 :   ffff800010cace48:       cmp     x26, x20
    0.00 :   ffff800010cace4c:       b.eq    ffff800010cace08 <__schedule+0x130>  // b.none
         :                      rq->nr_switches++;
    0.00 :   ffff800010cace50:       ldr     x0, [x19, #64]
         :                      sched_info_on():
         :                      {
         :                      #ifdef CONFIG_SCHEDSTATS
         :                      return 1;
         :                      #elif defined(CONFIG_TASK_DELAY_ACCT)
         :                      extern int delayacct_on;
         :                      return delayacct_on;
    0.00 :   ffff800010cace54:       adrp    x1, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff800010cace58:       str     x20, [x19, #2352]
         :                      __schedule():
    0.00 :   ffff800010cace5c:       add     x0, x0, #0x1
    0.00 :   ffff800010cace60:       str     x0, [x19, #64]
         :                      ++*switch_count;
    0.00 :   ffff800010cace64:       ldr     x0, [x24]
    0.00 :   ffff800010cace68:       add     x0, x0, #0x1
    0.00 :   ffff800010cace6c:       str     x0, [x24]
         :                      sched_info_switch():
         :                      }
         :
         :                      static inline void
         :                      sched_info_switch(struct rq *rq, struct task_struct *prev, struct task_struct *next)
         :                      {
         :                      if (sched_info_on())
    0.00 :   ffff800010cace70:       ldr     w0, [x1, #728]
    0.00 :   ffff800010cace74:       cbnz    w0, ffff800010cacfb4 <__schedule+0x2dc>
         :                      arch_static_branch():
         :                      asm_volatile_goto(
    0.00 :   ffff800010cace78:       nop
    0.00 :   ffff800010cace7c:       nop
         :                      rseq_set_notify_resume():
         :                      RSEQ_EVENT_MIGRATE      = (1U << RSEQ_EVENT_MIGRATE_BIT),
         :                      };
         :
         :                      static inline void rseq_set_notify_resume(struct task_struct *t)
         :                      {
         :                      if (t->rseq)
    0.00 :   ffff800010cace80:       ldr     x1, [x26, #2304]
         :                      __set_bit():
         :                      static inline void __set_bit(int nr, volatile unsigned long *addr)
         :                      {
         :                      unsigned long mask = BIT_MASK(nr);
         :                      unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
         :
         :                      *p  |= mask;
    0.00 :   ffff800010cace84:       ldr     x0, [x26, #2320]
    0.00 :   ffff800010cace88:       orr     x0, x0, #0x1
    0.00 :   ffff800010cace8c:       str     x0, [x26, #2320]
         :                      rseq_set_notify_resume():
    0.00 :   ffff800010cace90:       cbz     x1, ffff800010cacea4 <__schedule+0x1cc>
         :                      arch_static_branch_jump():
         :                      asm_volatile_goto(
    0.00 :   ffff800010cace94:       b       ffff800010cad058 <__schedule+0x380>
    0.00 :   ffff800010cace98:       b       ffff800010cad058 <__schedule+0x380>
         :                      __lse_atomic64_or():
         :                      ATOMIC64_OP(or, stset)
    0.00 :   ffff800010cace9c:       mov     x0, #0x4                        // #4
    0.00 :   ffff800010cacea0:       stset   x0, [x26]
         :                      arch_static_branch():
         :                      asm_volatile_goto(
    0.00 :   ffff800010cacea4:       nop
         :                      context_switch():
         :                      if (!next->mm) {                                // to kernel
    0.00 :   ffff800010cacea8:       ldr     x0, [x20, #952]
         :                      prepare_task():
         :                      next->on_cpu = 1;
    0.00 :   ffff800010caceac:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010caceb0:       str     w1, [x20, #64]
         :                      context_switch():
         :                      if (!next->mm) {                                // to kernel
    0.00 :   ffff800010caceb4:       cbz     x0, ffff800010cad094 <__schedule+0x3bc>
         :                      membarrier_switch_mm():
         :                      struct mm_struct *prev_mm,
         :                      struct mm_struct *next_mm)
         :                      {
         :                      int membarrier_state;
         :
         :                      if (prev_mm == next_mm)
    0.00 :   ffff800010caceb8:       ldr     x1, [x26, #960]
    0.00 :   ffff800010cacebc:       cmp     x0, x1
    0.00 :   ffff800010cacec0:       b.eq    ffff800010cacf08 <__schedule+0x230>  // b.none
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010cacec4:       ldr     w1, [x0, #72]
    0.00 :   ffff800010cacec8:       ldr     w2, [x19, #2460]
         :                      membarrier_switch_mm():
         :                      return;
         :
         :                      membarrier_state = atomic_read(&next_mm->membarrier_state);
         :                      if (READ_ONCE(rq->membarrier_state) == membarrier_state)
    0.00 :   ffff800010cacecc:       cmp     w1, w2
    0.00 :   ffff800010caced0:       b.eq    ffff800010cacee8 <__schedule+0x210>  // b.none
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010caced4:       str     w1, [x19, #2460]
         :                      context_switch():
         :                      switch_mm_irqs_off(prev->active_mm, next->mm, next);
    0.00 :   ffff800010caced8:       ldr     x0, [x20, #952]
         :                      switch_mm():
         :
         :                      static inline void
         :                      switch_mm(struct mm_struct *prev, struct mm_struct *next,
         :                      struct task_struct *tsk)
         :                      {
         :                      if (prev != next)
    0.00 :   ffff800010cacedc:       ldr     x1, [x26, #960]
    0.00 :   ffff800010cacee0:       cmp     x1, x0
    0.00 :   ffff800010cacee4:       b.eq    ffff800010cacf08 <__schedule+0x230>  // b.none
         :                      __switch_mm():
         :                      if (next == &init_mm) {
    0.00 :   ffff800010cacee8:       adrp    x1, ffff8000118bc000 <slab_caches>
    0.00 :   ffff800010caceec:       add     x1, x1, #0x258
         :                      unsigned int cpu = smp_processor_id();
    0.00 :   ffff800010cacef0:       add     x23, x23, #0x18
         :                      if (next == &init_mm) {
    0.00 :   ffff800010cacef4:       cmp     x0, x1
         :                      __my_cpu_offset():
    0.00 :   ffff800010cacef8:       mrs     x2, tpidr_el1
         :                      __switch_mm():
    0.00 :   ffff800010cacefc:       b.eq    ffff800010cad0cc <__schedule+0x3f4>  // b.none
         :                      check_and_switch_context(next, cpu);
    0.00 :   ffff800010cacf00:       ldr     w1, [x23, x2]
    0.00 :   ffff800010cacf04:       bl      ffff8000100a3e60 <check_and_switch_context>
         :                      context_switch():
         :                      if (!prev->mm) {                        // from kernel
    0.00 :   ffff800010cacf08:       ldr     x0, [x26, #952]
    0.00 :   ffff800010cacf0c:       cbz     x0, ffff800010cad0bc <__schedule+0x3e4>
         :                      rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
    0.00 :   ffff800010cacf10:       ldr     w2, [x19, #2392]
         :                      switch_to(prev, next, prev);
    0.00 :   ffff800010cacf14:       mov     x1, x20
    0.00 :   ffff800010cacf18:       mov     x0, x26
         :                      rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
    0.00 :   ffff800010cacf1c:       and     w2, w2, #0xfffffffc
    0.00 :   ffff800010cacf20:       str     w2, [x19, #2392]
         :                      switch_to(prev, next, prev);
    0.00 :   ffff800010cacf24:       bl      ffff800010087680 <__switch_to>
         :                      return finish_task_switch(prev);
    0.00 :   ffff800010cacf28:       bl      ffff800010112e88 <finish_task_switch>
    0.00 :   ffff800010cacf2c:       mov     x19, x0
         :                      balance_callback():
         :                      if (unlikely(rq->balance_callback))
    0.57 :   ffff800010cacf30:       ldr     x0, [x19, #2496]
    0.00 :   ffff800010cacf34:       cbnz    x0, ffff800010cace28 <__schedule+0x150>
         :                      __schedule():
         :                      }
    0.00 :   ffff800010cacf38:       add     x22, x22, #0x8c8
    4.19 :   ffff800010cacf3c:       ldr     x1, [x29, #88]
    1.84 :   ffff800010cacf40:       ldr     x0, [x22]
    0.00 :   ffff800010cacf44:       eor     x0, x1, x0
    0.00 :   ffff800010cacf48:       cbnz    x0, ffff800010cad268 <__schedule+0x590>
    9.67 :   ffff800010cacf4c:       ldp     x19, x20, [sp, #16]
    3.64 :   ffff800010cacf50:       ldp     x21, x22, [sp, #32]
    2.12 :   ffff800010cacf54:       ldp     x23, x24, [sp, #48]
    0.26 :   ffff800010cacf58:       ldp     x25, x26, [sp, #64]
    0.00 :   ffff800010cacf5c:       ldp     x29, x30, [sp], #96
    0.00 :   ffff800010cacf60:       ret
         :                      perf_event_task_sched_out():
         :                      struct task_struct *next)
         :                      {
         :                      perf_sw_event_sched(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, 0);
         :
         :                      if (static_branch_unlikely(&perf_sched_events))
         :                      __perf_event_task_sched_out(prev, next);
    0.00 :   ffff800010cacf64:       mov     x1, x20
    0.00 :   ffff800010cacf68:       mov     x0, x26
    0.00 :   ffff800010cacf6c:       bl      ffff8000101c2da0 <__perf_event_task_sched_out>
    0.00 :   ffff800010cacf70:       b       ffff800010cace80 <__schedule+0x1a8>
         :                      __my_cpu_offset():
         :                      "mrs %0, tpidr_el2",
         :                      ARM64_HAS_VIRT_HOST_EXTN)
         :                      : "=r" (off) :
         :                      "Q" (*(const unsigned long *)current_stack_pointer));
    0.00 :   ffff800010cacf74:       mov     x5, sp
         :                      perf_sw_event_sched():
         :                      struct pt_regs *regs = this_cpu_ptr(&__perf_regs[0]);
    0.00 :   ffff800010cacf78:       adrp    x4, ffff8000114d3000 <pmu_sb_events>
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800010cacf7c:       mrs     x0, tpidr_el1
         :                      perf_sw_event_sched():
    0.00 :   ffff800010cacf80:       add     x4, x4, #0x50
         :                      perf_fetch_caller_regs():
         :                      perf_arch_fetch_caller_regs(regs, CALLER_ADDR0);
    0.00 :   ffff800010cacf84:       mov     x6, #0x5                        // #5
         :                      perf_sw_event_sched():
         :                      struct pt_regs *regs = this_cpu_ptr(&__perf_regs[0]);
    0.00 :   ffff800010cacf88:       add     x4, x4, x0
         :                      ___perf_sw_event(event_id, nr, regs, addr);
    0.00 :   ffff800010cacf8c:       mov     x3, #0x0                        // #0
    0.00 :   ffff800010cacf90:       mov     x2, x4
    0.00 :   ffff800010cacf94:       mov     x1, #0x1                        // #1
    0.00 :   ffff800010cacf98:       mov     w0, #0x3                        // #3
         :                      perf_fetch_caller_regs():
         :                      perf_arch_fetch_caller_regs(regs, CALLER_ADDR0);
    0.00 :   ffff800010cacf9c:       str     x29, [x4, #232]
    0.00 :   ffff800010cacfa0:       stp     x5, x25, [x4, #248]
    0.00 :   ffff800010cacfa4:       str     x6, [x4, #264]
         :                      perf_sw_event_sched():
         :                      ___perf_sw_event(event_id, nr, regs, addr);
    0.00 :   ffff800010cacfa8:       bl      ffff8000101c84a8 <___perf_sw_event>
         :                      arch_static_branch():
    0.00 :   ffff800010cacfac:       nop
    0.00 :   ffff800010cacfb0:       b       ffff800010cace80 <__schedule+0x1a8>
         :                      __sched_info_switch():
         :                      if (prev != rq->idle)
    0.00 :   ffff800010cacfb4:       ldr     x0, [x19, #2360]
    0.00 :   ffff800010cacfb8:       cmp     x26, x0
    0.00 :   ffff800010cacfbc:       b.eq    ffff800010cacfe8 <__schedule+0x310>  // b.none
         :                      sched_info_depart():
         :                      if (t->state == TASK_RUNNING)
    0.00 :   ffff800010cacfc0:       ldr     x1, [x26, #24]
    0.00 :   ffff800010cacfc4:       cbnz    x1, ffff800010cacfe0 <__schedule+0x308>
         :                      sched_info_queued():
         :                      if (!t->sched_info.last_queued)
    0.00 :   ffff800010cacfc8:       ldr     x1, [x26, #864]
    0.00 :   ffff800010cacfcc:       cbnz    x1, ffff800010cacfe0 <__schedule+0x308>
         :                      t->sched_info.last_queued = rq_clock(rq);
    0.00 :   ffff800010cacfd0:       ldr     x0, [x19, #2400]
    0.00 :   ffff800010cacfd4:       str     x0, [x26, #864]
    0.00 :   ffff800010cacfd8:       ldr     x0, [x19, #2360]
    0.00 :   ffff800010cacfdc:       nop
         :                      __sched_info_switch():
         :                      if (next != rq->idle)
    0.00 :   ffff800010cacfe0:       cmp     x0, x20
    0.00 :   ffff800010cacfe4:       b.eq    ffff800010cace78 <__schedule+0x1a0>  // b.none
         :                      sched_info_arrive():
         :                      if (t->sched_info.last_queued)
    0.00 :   ffff800010cacfe8:       ldr     x0, [x20, #864]
         :                      unsigned long long now = rq_clock(rq), delta = 0;
    0.00 :   ffff800010cacfec:       ldr     x3, [x19, #2400]
         :                      t->sched_info.run_delay += delta;
    0.00 :   ffff800010cacff0:       ldr     x1, [x20, #848]
         :                      delta = now - t->sched_info.last_queued;
    0.00 :   ffff800010cacff4:       cmp     x0, #0x0
    0.00 :   ffff800010cacff8:       sub     x4, x3, x0
         :                      t->sched_info.pcount++;
    0.00 :   ffff800010cacffc:       ldr     x2, [x20, #840]
         :                      delta = now - t->sched_info.last_queued;
    0.00 :   ffff800010cad000:       csel    x0, x4, x0, ne  // ne = any
         :                      t->sched_info.run_delay += delta;
    0.00 :   ffff800010cad004:       add     x0, x1, x0
         :                      sched_info_reset_dequeued():
         :                      t->sched_info.last_queued = 0;
    0.00 :   ffff800010cad008:       add     x1, x20, #0x200
         :                      sched_info_arrive():
         :                      t->sched_info.pcount++;
    0.00 :   ffff800010cad00c:       add     x2, x2, #0x1
         :                      sched_info_reset_dequeued():
         :                      t->sched_info.last_queued = 0;
    0.00 :   ffff800010cad010:       stp     x2, x0, [x1, #328]
    0.00 :   ffff800010cad014:       stp     x3, xzr, [x1, #344]
         :                      arch_static_branch():
    0.00 :   ffff800010cad018:       nop
    0.00 :   ffff800010cad01c:       b       ffff800010cace7c <__schedule+0x1a4>
         :                      __schedule():
         :                      if (signal_pending_state(prev->state, prev)) {
    0.00 :   ffff800010cad020:       ldr     x0, [x26, #24]
         :                      signal_pending_state():
         :                      return signal_pending(p) && __fatal_signal_pending(p);
         :                      }
         :
         :                      static inline int signal_pending_state(long state, struct task_struct *p)
         :                      {
         :                      if (!(state & (TASK_INTERRUPTIBLE | TASK_WAKEKILL)))
    0.00 :   ffff800010cad024:       mov     x1, #0x101                      // #257
    0.00 :   ffff800010cad028:       tst     x0, x1
    0.00 :   ffff800010cad02c:       b.eq    ffff800010cad038 <__schedule+0x360>  // b.none
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff800010cad030:       ldr     x1, [x26]
         :                      signal_pending_state():
         :                      return 0;
         :                      if (!signal_pending(p))
    0.00 :   ffff800010cad034:       tbnz    w1, #0, ffff800010cad238 <__schedule+0x560>
         :                      __schedule():
         :                      deactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK);
    0.00 :   ffff800010cad038:       mov     x0, x19
    0.00 :   ffff800010cad03c:       mov     w2, #0x9                        // #9
    0.00 :   ffff800010cad040:       mov     x1, x26
    0.00 :   ffff800010cad044:       bl      ffff800010113d30 <deactivate_task>
         :                      if (prev->in_iowait) {
    0.00 :   ffff800010cad048:       ldrb    w0, [x26, #1064]
    0.00 :   ffff800010cad04c:       tbnz    w0, #1, ffff800010cad120 <__schedule+0x448>
         :                      switch_count = &prev->nvcsw;
    0.00 :   ffff800010cad050:       add     x24, x26, #0x580
    0.00 :   ffff800010cad054:       b       ffff800010cacd94 <__schedule+0xbc>
         :                      __ll_sc_atomic64_or():
         :                      ATOMIC64_OPS(or, orr, L)
    0.00 :   ffff800010cad058:       b       ffff800010cad83c <io_schedule+0x4c>
    0.00 :   ffff800010cad05c:       b       ffff800010cacea4 <__schedule+0x1cc>
         :                      __fire_sched_out_preempt_notifiers():
         :                      hlist_for_each_entry(notifier, &curr->preempt_notifiers, link)
    0.00 :   ffff800010cad060:       ldr     x21, [x26, #720]
    0.00 :   ffff800010cad064:       cbz     x21, ffff800010cacea8 <__schedule+0x1d0>
         :                      notifier->ops->sched_out(notifier, next);
    0.00 :   ffff800010cad068:       ldr     x2, [x21, #16]
    0.00 :   ffff800010cad06c:       mov     x0, x21
    0.00 :   ffff800010cad070:       mov     x1, x20
    0.00 :   ffff800010cad074:       ldr     x2, [x2, #8]
    0.00 :   ffff800010cad078:       blr     x2
         :                      hlist_for_each_entry(notifier, &curr->preempt_notifiers, link)
    0.00 :   ffff800010cad07c:       ldr     x21, [x21]
    0.00 :   ffff800010cad080:       cbnz    x21, ffff800010cad068 <__schedule+0x390>
    0.00 :   ffff800010cad084:       b       ffff800010cacea8 <__schedule+0x1d0>
         :                      update_rq_clock():
    0.00 :   ffff800010cad088:       mov     x0, x19
    0.00 :   ffff800010cad08c:       bl      ffff800010112988 <update_rq_clock.part.89>
    0.00 :   ffff800010cad090:       b       ffff800010cacd84 <__schedule+0xac>
         :                      context_switch():
         :                      enter_lazy_tlb(prev->active_mm, next);
    0.00 :   ffff800010cad094:       ldr     x0, [x26, #960]
         :                      next->active_mm = prev->active_mm;
    0.00 :   ffff800010cad098:       str     x0, [x20, #960]
         :                      if (prev->mm)                           // from user
    0.00 :   ffff800010cad09c:       ldr     x0, [x26, #952]
    0.00 :   ffff800010cad0a0:       cbz     x0, ffff800010cad0c4 <__schedule+0x3ec>
         :                      mmgrab():
         :                      * See also <Documentation/vm/active_mm.rst> for an in-depth explanation
         :                      * of &mm_struct.mm_count vs &mm_struct.mm_users.
         :                      */
         :                      static inline void mmgrab(struct mm_struct *mm)
         :                      {
         :                      atomic_inc(&mm->mm_count);
    0.00 :   ffff800010cad0a4:       ldr     x0, [x26, #960]
    0.00 :   ffff800010cad0a8:       add     x2, x0, #0x50
         :                      arch_static_branch_jump():
         :                      asm_volatile_goto(
    0.00 :   ffff800010cad0ac:       b       ffff800010cad1f8 <__schedule+0x520>
    0.00 :   ffff800010cad0b0:       b       ffff800010cad1f8 <__schedule+0x520>
         :                      __lse_atomic_add():
         :                      ATOMIC_OP(add, stadd)
    0.00 :   ffff800010cad0b4:       stadd   w1, [x2]
    0.00 :   ffff800010cad0b8:       b       ffff800010cacf10 <__schedule+0x238>
         :                      context_switch():
         :                      rq->prev_mm = prev->active_mm;
    0.00 :   ffff800010cad0bc:       ldr     x0, [x26, #960]
    0.00 :   ffff800010cad0c0:       str     x0, [x19, #2384]
         :                      prev->active_mm = NULL;
    0.00 :   ffff800010cad0c4:       str     xzr, [x26, #960]
    0.00 :   ffff800010cad0c8:       b       ffff800010cacf10 <__schedule+0x238>
         :                      cpu_set_reserved_ttbr0():
         :                      unsigned long ttbr = phys_to_ttbr(__pa_symbol(empty_zero_page));
    0.00 :   ffff800010cad0cc:       adrp    x1, ffff8000112ae000 <cpu_ops+0x248>
    0.00 :   ffff800010cad0d0:       adrp    x0, ffff800011a75000 <empty_zero_page>
    0.00 :   ffff800010cad0d4:       add     x0, x0, #0x0
    0.00 :   ffff800010cad0d8:       ldr     x1, [x1, #1904]
    0.00 :   ffff800010cad0dc:       sub     x0, x0, x1
         :                      write_sysreg(ttbr, ttbr0_el1);
    0.00 :   ffff800010cad0e0:       msr     ttbr0_el1, x0
         :                      isb();
    0.00 :   ffff800010cad0e4:       isb
    0.00 :   ffff800010cad0e8:       b       ffff800010cacf08 <__schedule+0x230>
         :                      put_prev_task():
         :                      WARN_ON_ONCE(rq->curr != prev);
    0.00 :   ffff800010cad0ec:       ldr     x0, [x19, #2352]
    0.00 :   ffff800010cad0f0:       cmp     x26, x0
    0.00 :   ffff800010cad0f4:       b.ne    ffff800010cad25c <__schedule+0x584>  // b.any
         :                      prev->sched_class->put_prev_task(rq, prev);
    0.00 :   ffff800010cad0f8:       ldr     x2, [x26, #128]
    0.00 :   ffff800010cad0fc:       mov     x1, x26
    0.00 :   ffff800010cad100:       mov     x0, x19
    0.00 :   ffff800010cad104:       ldr     x2, [x2, #56]
    0.00 :   ffff800010cad108:       blr     x2
         :                      pick_next_task():
         :                      p = pick_next_task_idle(rq);
    0.00 :   ffff800010cad10c:       mov     x0, x19
    0.00 :   ffff800010cad110:       bl      ffff80001011aa20 <pick_next_task_idle>
    0.00 :   ffff800010cad114:       mov     x20, x0
         :                      arch_static_branch_jump():
    0.00 :   ffff800010cad118:       b       ffff800010cace34 <__schedule+0x15c>
    0.00 :   ffff800010cad11c:       b       ffff800010cacde8 <__schedule+0x110>
         :                      __schedule():
         :                      atomic_inc(&rq->nr_iowait);
    0.00 :   ffff800010cad120:       add     x0, x19, #0x998
         :                      arch_static_branch_jump():
    0.00 :   ffff800010cad124:       b       ffff800010cad138 <__schedule+0x460>
    0.00 :   ffff800010cad128:       b       ffff800010cad138 <__schedule+0x460>
         :                      __lse_atomic_add():
    0.00 :   ffff800010cad12c:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010cad130:       stadd   w1, [x0]
    0.00 :   ffff800010cad134:       b       ffff800010cad13c <__schedule+0x464>
         :                      __ll_sc_atomic_add():
         :                      ATOMIC_OPS(add, add, I)
    0.00 :   ffff800010cad138:       b       ffff800010cad854 <io_schedule+0x64>
         :                      get_current():
    0.00 :   ffff800010cad13c:       mrs     x0, sp_el0
         :                      delayacct_set_flag():
         :                      return 0;
         :                      }
         :
         :                      static inline void delayacct_set_flag(int flag)
         :                      {
         :                      if (current->delays)
    0.00 :   ffff800010cad140:       ldr     x0, [x0, #2368]
    0.00 :   ffff800010cad144:       cbz     x0, ffff800010cad154 <__schedule+0x47c>
         :                      current->delays->flags |= flag;
    0.00 :   ffff800010cad148:       ldr     w1, [x0, #4]
    0.00 :   ffff800010cad14c:       orr     w1, w1, #0x2
    0.00 :   ffff800010cad150:       str     w1, [x0, #4]
         :                      get_current():
    0.00 :   ffff800010cad154:       mrs     x0, sp_el0
         :                      delayacct_blkio_start():
         :                      }
         :
         :                      static inline void delayacct_blkio_start(void)
         :                      {
         :                      delayacct_set_flag(DELAYACCT_PF_BLKIO);
         :                      if (current->delays)
    0.00 :   ffff800010cad158:       ldr     x0, [x0, #2368]
    0.00 :   ffff800010cad15c:       cbz     x0, ffff800010cad050 <__schedule+0x378>
         :                      __delayacct_blkio_start();
    0.00 :   ffff800010cad160:       bl      ffff8000101b16d8 <__delayacct_blkio_start>
         :                      __schedule():
         :                      switch_count = &prev->nvcsw;
    0.00 :   ffff800010cad164:       add     x24, x26, #0x580
    0.00 :   ffff800010cad168:       b       ffff800010cacd94 <__schedule+0xbc>
    0.00 :   ffff800010cad16c:       ldr     x20, [x26, #128]
         :                      pick_next_task():
         :                      for_class_range(class, prev->sched_class, &idle_sched_class) {
    0.00 :   ffff800010cad170:       add     x21, x21, #0x158
    0.00 :   ffff800010cad174:       cmp     x20, x21
    0.00 :   ffff800010cad178:       b.ne    ffff800010cad18c <__schedule+0x4b4>  // b.any
    0.00 :   ffff800010cad17c:       b       ffff800010cad1a4 <__schedule+0x4cc>
    0.00 :   ffff800010cad180:       ldr     x20, [x20]
    0.00 :   ffff800010cad184:       cmp     x20, x21
    0.00 :   ffff800010cad188:       b.eq    ffff800010cad1a4 <__schedule+0x4cc>  // b.none
         :                      if (class->balance(rq, prev, rf))
    0.00 :   ffff800010cad18c:       ldr     x3, [x20, #72]
    0.00 :   ffff800010cad190:       add     x2, x29, #0x50
    0.00 :   ffff800010cad194:       mov     x1, x26
    0.00 :   ffff800010cad198:       mov     x0, x19
    0.00 :   ffff800010cad19c:       blr     x3
    0.00 :   ffff800010cad1a0:       cbz     w0, ffff800010cad180 <__schedule+0x4a8>
         :                      put_prev_task():
         :                      WARN_ON_ONCE(rq->curr != prev);
    0.00 :   ffff800010cad1a4:       ldr     x0, [x19, #2352]
    0.00 :   ffff800010cad1a8:       cmp     x26, x0
    0.00 :   ffff800010cad1ac:       b.ne    ffff800010cad254 <__schedule+0x57c>  // b.any
         :                      prev->sched_class->put_prev_task(rq, prev);
    0.00 :   ffff800010cad1b0:       ldr     x2, [x26, #128]
         :                      pick_next_task():
         :                      for_each_class(class) {
    0.00 :   ffff800010cad1b4:       adrp    x21, ffff800010ce6000 <sched_prio_to_weight+0x10>
         :                      put_prev_task():
    0.00 :   ffff800010cad1b8:       mov     x1, x26
         :                      pick_next_task():
    0.00 :   ffff800010cad1bc:       add     x21, x21, #0x480
         :                      put_prev_task():
    0.00 :   ffff800010cad1c0:       mov     x0, x19
    0.00 :   ffff800010cad1c4:       ldr     x2, [x2, #56]
    0.00 :   ffff800010cad1c8:       blr     x2
         :                      pick_next_task():
    0.00 :   ffff800010cad1cc:       cbnz    x21, ffff800010cad1dc <__schedule+0x504>
    0.00 :   ffff800010cad1d0:       b       ffff800010cad264 <__schedule+0x58c>
    0.00 :   ffff800010cad1d4:       ldr     x21, [x21]
    0.00 :   ffff800010cad1d8:       cbz     x21, ffff800010cad264 <__schedule+0x58c>
         :                      p = class->pick_next_task(rq);
    0.00 :   ffff800010cad1dc:       ldr     x1, [x21, #48]
    0.00 :   ffff800010cad1e0:       mov     x0, x19
    0.00 :   ffff800010cad1e4:       blr     x1
    0.00 :   ffff800010cad1e8:       mov     x20, x0
         :                      if (p)
    0.00 :   ffff800010cad1ec:       cbz     x0, ffff800010cad1d4 <__schedule+0x4fc>
         :                      arch_static_branch_jump():
    0.00 :   ffff800010cad1f0:       b       ffff800010cace34 <__schedule+0x15c>
    0.00 :   ffff800010cad1f4:       b       ffff800010cacde8 <__schedule+0x110>
         :                      __ll_sc_atomic_add():
    0.00 :   ffff800010cad1f8:       add     x0, x0, #0x50
    0.00 :   ffff800010cad1fc:       b       ffff800010cad86c <io_schedule+0x7c>
    0.00 :   ffff800010cad200:       b       ffff800010cacf10 <__schedule+0x238>
         :                      schedule_debug():
         :                      __schedule_bug(prev);
    0.00 :   ffff800010cad204:       mov     x0, x26
    0.00 :   ffff800010cad208:       bl      ffff800010112678 <__schedule_bug>
         :                      __write_once_size():
    0.00 :   ffff800010cad20c:       mov     w0, #0x1                        // #1
    0.00 :   ffff800010cad210:       str     w0, [x21, #16]
         :                      profile_hit():
    0.00 :   ffff800010cad214:       adrp    x0, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff800010cad218:       ldr     w0, [x0, #632]
    0.00 :   ffff800010cad21c:       cmp     w0, #0x2
    0.00 :   ffff800010cad220:       b.ne    ffff800010cacd58 <__schedule+0x80>  // b.any
    0.00 :   ffff800010cad224:       nop
         :                      profile_hits(type, ip, 1);
    0.00 :   ffff800010cad228:       mov     w2, #0x1                        // #1
    0.00 :   ffff800010cad22c:       mov     x1, x25
    0.00 :   ffff800010cad230:       bl      ffff8000101650d8 <profile_hits>
    0.00 :   ffff800010cad234:       b       ffff800010cacd58 <__schedule+0x80>
         :                      signal_pending_state():
         :                      return 0;
         :
         :                      return (state & TASK_INTERRUPTIBLE) || __fatal_signal_pending(p);
    0.00 :   ffff800010cad238:       tbnz    w0, #0, ffff800010cad248 <__schedule+0x570>
         :                      sigismember():
         :
         :                      static inline int sigismember(sigset_t *set, int _sig)
         :                      {
         :                      unsigned long sig = _sig - 1;
         :                      if (_NSIG_WORDS == 1)
         :                      return 1 & (set->sig[0] >> sig);
    0.00 :   ffff800010cad23c:       ldr     x0, [x26, #1696]
         :                      signal_pending_state():
    0.00 :   ffff800010cad240:       tst     w0, #0x100
    0.00 :   ffff800010cad244:       b.eq    ffff800010cad038 <__schedule+0x360>  // b.none
         :                      __schedule():
         :                      prev->state = TASK_RUNNING;
    0.00 :   ffff800010cad248:       str     xzr, [x26, #24]
         :                      switch_count = &prev->nvcsw;
    0.00 :   ffff800010cad24c:       add     x24, x26, #0x580
    0.00 :   ffff800010cad250:       b       ffff800010cacd94 <__schedule+0xbc>
         :                      put_prev_task():
         :                      WARN_ON_ONCE(rq->curr != prev);
    0.00 :   ffff800010cad254:       brk     #0x800
    0.00 :   ffff800010cad258:       b       ffff800010cad1b0 <__schedule+0x4d8>
    0.00 :   ffff800010cad25c:       brk     #0x800
    0.00 :   ffff800010cad260:       b       ffff800010cad0f8 <__schedule+0x420>
         :                      pick_next_task():
         :                      BUG();
    0.00 :   ffff800010cad264:       brk     #0x800
         :                      __schedule():
         :                      }
    0.00 :   ffff800010cad268:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (332 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001045dd88 <blk_add_rq_to_plug>:
         :                      list_add_tail():
         :                      * Insert a new entry before the specified head.
         :                      * This is useful for implementing queues.
         :                      */
         :                      static inline void list_add_tail(struct list_head *new, struct list_head *head)
         :                      {
         :                      __list_add(new, head->prev, head);
    0.00 :   ffff80001045dd88:       ldr     x2, [x0, #8]
         :                      blk_add_rq_to_plug():
         :                      hctx->queue->mq_ops->commit_rqs(hctx);
         :                      }
         :
         :                      static void blk_add_rq_to_plug(struct blk_plug *plug, struct request *rq)
         :                      {
         :                      list_add_tail(&rq->queuelist, &plug->mq_list);
    0.00 :   ffff80001045dd8c:       add     x3, x1, #0x48
         :                      __list_add():
         :                      next->prev = new;
    0.90 :   ffff80001045dd90:       str     x3, [x0, #8]
         :                      new->prev = prev;
    0.00 :   ffff80001045dd94:       stp     x0, x2, [x1, #72]
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    3.30 :   ffff80001045dd98:       str     x3, [x2]
         :                      blk_add_rq_to_plug():
         :                      plug->rq_count++;
    0.59 :   ffff80001045dd9c:       ldrh    w2, [x0, #32]
         :                      if (!plug->multiple_queues && !list_is_singular(&plug->mq_list)) {
    0.00 :   ffff80001045dda0:       ldrb    w3, [x0, #34]
         :                      plug->rq_count++;
    0.00 :   ffff80001045dda4:       add     w2, w2, #0x1
    0.00 :   ffff80001045dda8:       strh    w2, [x0, #32]
         :                      if (!plug->multiple_queues && !list_is_singular(&plug->mq_list)) {
    0.00 :   ffff80001045ddac:       cbnz    w3, ffff80001045dde4 <blk_add_rq_to_plug+0x5c>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
   41.87 :   ffff80001045ddb0:       ldr     x3, [x0]
   31.92 :   ffff80001045ddb4:       ldr     x2, [x0]
         :                      list_is_singular():
         :                      * list_is_singular - tests whether a list has just one entry.
         :                      * @head: the list to test.
         :                      */
         :                      static inline int list_is_singular(const struct list_head *head)
         :                      {
         :                      return !list_empty(head) && (head->next == head->prev);
    0.00 :   ffff80001045ddb8:       cmp     x0, x3
    0.00 :   ffff80001045ddbc:       b.eq    ffff80001045ddcc <blk_add_rq_to_plug+0x44>  // b.none
    5.71 :   ffff80001045ddc0:       ldr     x3, [x0, #8]
    0.00 :   ffff80001045ddc4:       cmp     x2, x3
    0.00 :   ffff80001045ddc8:       b.eq    ffff80001045dde4 <blk_add_rq_to_plug+0x5c>  // b.none
         :                      blk_add_rq_to_plug():
         :                      struct request *tmp;
         :
         :                      tmp = list_first_entry(&plug->mq_list, struct request,
         :                      queuelist);
         :                      if (tmp->q != rq->q)
    0.00 :   ffff80001045ddcc:       ldr     x1, [x1]
    0.00 :   ffff80001045ddd0:       ldur    x2, [x2, #-72]
    0.00 :   ffff80001045ddd4:       cmp     x2, x1
    0.00 :   ffff80001045ddd8:       b.eq    ffff80001045dde4 <blk_add_rq_to_plug+0x5c>  // b.none
         :                      plug->multiple_queues = true;
    0.00 :   ffff80001045dddc:       mov     w1, #0x1                        // #1
    0.00 :   ffff80001045dde0:       strb    w1, [x0, #34]
         :                      }
         :                      }
   15.71 :   ffff80001045dde4:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (253 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010c92000 <__arch_copy_to_user>:
         :                      __arch_copy_to_user():
         :                      uao_stp 9998f, \ptr, \regB, \regC, \val
         :                      .endm
         :
         :                      end     .req    x5
         :                      ENTRY(__arch_copy_to_user)
         :                      add     end, x0, x2
    0.40 :   ffff800010c92000:       add     x5, x0, x2
         :                      C_l     .req    x11
         :                      C_h     .req    x12
         :                      D_l     .req    x13
         :                      D_h     .req    x14
         :
         :                      mov     dst, dstin
    0.00 :   ffff800010c92004:       mov     x6, x0
         :                      cmp     count, #16
    0.00 :   ffff800010c92008:       cmp     x2, #0x10
         :                      /*When memory length is less than 16, the accessed are not aligned.*/
         :                      b.lo    .Ltiny15
    0.00 :   ffff800010c9200c:       b.cc    ffff800010c920ac <__arch_copy_to_user+0xac>  // b.lo, b.ul, b.last
         :
         :                      neg     tmp2, src
    0.00 :   ffff800010c92010:       neg     x4, x1
         :                      ands    tmp2, tmp2, #15/* Bytes to reach alignment. */
    0.00 :   ffff800010c92014:       ands    x4, x4, #0xf
         :                      b.eq    .LSrcAligned
    0.00 :   ffff800010c92018:       b.eq    ffff800010c92060 <__arch_copy_to_user+0x60>  // b.none
         :                      sub     count, count, tmp2
    0.00 :   ffff800010c9201c:       sub     x2, x2, x4
         :                      * Copy the leading memory data from src to dst in an increasing
         :                      * address order.By this way,the risk of overwriting the source
         :                      * memory data is eliminated when the distance between src and
         :                      * dst is less than 16. The memory accesses here are alignment.
         :                      */
         :                      tbz     tmp2, #0, 1f
    0.00 :   ffff800010c92020:       tbz     w4, #0, ffff800010c92030 <__arch_copy_to_user+0x30>
         :                      ldrb1   tmp1w, src, #1
    0.00 :   ffff800010c92024:       ldrb    w3, [x1], #1
         :                      strb1   tmp1w, dst, #1
    0.00 :   ffff800010c92028:       strb    w3, [x6], #1
    0.00 :   ffff800010c9202c:       nop
         :                      1:
         :                      tbz     tmp2, #1, 2f
    0.00 :   ffff800010c92030:       tbz     w4, #1, ffff800010c92040 <__arch_copy_to_user+0x40>
         :                      ldrh1   tmp1w, src, #2
    0.00 :   ffff800010c92034:       ldrh    w3, [x1], #2
         :                      strh1   tmp1w, dst, #2
    0.00 :   ffff800010c92038:       strh    w3, [x6], #2
    0.00 :   ffff800010c9203c:       nop
         :                      2:
         :                      tbz     tmp2, #2, 3f
    0.00 :   ffff800010c92040:       tbz     w4, #2, ffff800010c92050 <__arch_copy_to_user+0x50>
         :                      ldr1    tmp1w, src, #4
    0.00 :   ffff800010c92044:       ldr     w3, [x1], #4
         :                      str1    tmp1w, dst, #4
    0.00 :   ffff800010c92048:       str     w3, [x6], #4
    0.00 :   ffff800010c9204c:       nop
         :                      3:
         :                      tbz     tmp2, #3, .LSrcAligned
    0.00 :   ffff800010c92050:       tbz     w4, #3, ffff800010c92060 <__arch_copy_to_user+0x60>
         :                      ldr1    tmp1, src, #8
    0.00 :   ffff800010c92054:       ldr     x3, [x1], #8
         :                      str1    tmp1, dst, #8
    0.00 :   ffff800010c92058:       str     x3, [x6], #8
    0.00 :   ffff800010c9205c:       nop
         :
         :                      .LSrcAligned:
         :                      cmp     count, #64
    2.36 :   ffff800010c92060:       cmp     x2, #0x40
         :                      b.ge    .Lcpy_over64
    0.00 :   ffff800010c92064:       b.ge    ffff800010c920f0 <__arch_copy_to_user+0xf0>  // b.tcont
         :                      .Ltail63:
         :                      /*
         :                      * Copy up to 48 bytes of data. At this point we only need the
         :                      * bottom 6 bits of count to be accurate.
         :                      */
         :                      ands    tmp1, count, #0x30
   17.86 :   ffff800010c92068:       ands    x3, x2, #0x30
         :                      b.eq    .Ltiny15
    0.00 :   ffff800010c9206c:       b.eq    ffff800010c920ac <__arch_copy_to_user+0xac>  // b.none
         :                      cmp     tmp1w, #0x20
    1.18 :   ffff800010c92070:       cmp     w3, #0x20
         :                      b.eq    1f
    0.00 :   ffff800010c92074:       b.eq    ffff800010c9208c <__arch_copy_to_user+0x8c>  // b.none
         :                      b.lt    2f
    0.00 :   ffff800010c92078:       b.lt    ffff800010c9209c <__arch_copy_to_user+0x9c>  // b.tstop
         :                      ldp1    A_l, A_h, src, #16
    0.00 :   ffff800010c9207c:       ldp     x7, x8, [x1], #16
         :                      stp1    A_l, A_h, dst, #16
    0.00 :   ffff800010c92080:       stp     x7, x8, [x6], #16
    0.00 :   ffff800010c92084:       nop
    0.00 :   ffff800010c92088:       nop
         :                      1:
         :                      ldp1    A_l, A_h, src, #16
    0.00 :   ffff800010c9208c:       ldp     x7, x8, [x1], #16
         :                      stp1    A_l, A_h, dst, #16
   50.54 :   ffff800010c92090:       stp     x7, x8, [x6], #16
    0.79 :   ffff800010c92094:       nop
    0.00 :   ffff800010c92098:       nop
         :                      2:
         :                      ldp1    A_l, A_h, src, #16
    0.00 :   ffff800010c9209c:       ldp     x7, x8, [x1], #16
         :                      stp1    A_l, A_h, dst, #16
   19.36 :   ffff800010c920a0:       stp     x7, x8, [x6], #16
    0.80 :   ffff800010c920a4:       nop
    0.00 :   ffff800010c920a8:       nop
         :                      * precondition that src address is at least 16 bytes bigger than dst
         :                      * address,otherwise some source data will be overwritten when memove
         :                      * call memcpy directly. To make memmove simpler and decouple the
         :                      * memcpy's dependency on memmove, withdrew the original process.
         :                      */
         :                      tbz     count, #3, 1f
    0.00 :   ffff800010c920ac:       tbz     w2, #3, ffff800010c920bc <__arch_copy_to_user+0xbc>
         :                      ldr1    tmp1, src, #8
    0.00 :   ffff800010c920b0:       ldr     x3, [x1], #8
         :                      str1    tmp1, dst, #8
    0.00 :   ffff800010c920b4:       str     x3, [x6], #8
    0.00 :   ffff800010c920b8:       nop
         :                      1:
         :                      tbz     count, #2, 2f
    0.00 :   ffff800010c920bc:       tbz     w2, #2, ffff800010c920cc <__arch_copy_to_user+0xcc>
         :                      ldr1    tmp1w, src, #4
    0.00 :   ffff800010c920c0:       ldr     w3, [x1], #4
         :                      str1    tmp1w, dst, #4
    0.00 :   ffff800010c920c4:       str     w3, [x6], #4
    0.00 :   ffff800010c920c8:       nop
         :                      2:
         :                      tbz     count, #1, 3f
    0.39 :   ffff800010c920cc:       tbz     w2, #1, ffff800010c920dc <__arch_copy_to_user+0xdc>
         :                      ldrh1   tmp1w, src, #2
    0.00 :   ffff800010c920d0:       ldrh    w3, [x1], #2
         :                      strh1   tmp1w, dst, #2
    0.00 :   ffff800010c920d4:       strh    w3, [x6], #2
    0.00 :   ffff800010c920d8:       nop
         :                      3:
         :                      tbz     count, #0, .Lexitfunc
    3.55 :   ffff800010c920dc:       tbz     w2, #0, ffff800010c92210 <__arch_copy_to_user+0x210>
         :                      ldrb1   tmp1w, src, #1
    0.00 :   ffff800010c920e0:       ldrb    w3, [x1], #1
         :                      strb1   tmp1w, dst, #1
    0.00 :   ffff800010c920e4:       strb    w3, [x6], #1
    0.00 :   ffff800010c920e8:       nop
         :
         :                      b       .Lexitfunc
    0.00 :   ffff800010c920ec:       b       ffff800010c92210 <__arch_copy_to_user+0x210>
         :
         :                      .Lcpy_over64:
         :                      subs    count, count, #128
    0.00 :   ffff800010c920f0:       subs    x2, x2, #0x80
         :                      b.ge    .Lcpy_body_large
    0.00 :   ffff800010c920f4:       b.ge    ffff800010c92180 <__arch_copy_to_user+0x180>  // b.tcont
         :                      /*
         :                      * Less than 128 bytes to copy, so handle 64 here and then jump
         :                      * to the tail.
         :                      */
         :                      ldp1    A_l, A_h, src, #16
    0.00 :   ffff800010c920f8:       ldp     x7, x8, [x1], #16
         :                      stp1    A_l, A_h, dst, #16
    0.00 :   ffff800010c920fc:       stp     x7, x8, [x6], #16
    0.00 :   ffff800010c92100:       nop
    0.00 :   ffff800010c92104:       nop
         :                      ldp1    B_l, B_h, src, #16
    0.00 :   ffff800010c92108:       ldp     x9, x10, [x1], #16
         :                      ldp1    C_l, C_h, src, #16
    0.00 :   ffff800010c9210c:       ldp     x11, x12, [x1], #16
         :                      stp1    B_l, B_h, dst, #16
    0.00 :   ffff800010c92110:       stp     x9, x10, [x6], #16
    0.00 :   ffff800010c92114:       nop
    0.00 :   ffff800010c92118:       nop
         :                      stp1    C_l, C_h, dst, #16
    0.00 :   ffff800010c9211c:       stp     x11, x12, [x6], #16
    0.00 :   ffff800010c92120:       nop
    0.00 :   ffff800010c92124:       nop
         :                      ldp1    D_l, D_h, src, #16
    0.00 :   ffff800010c92128:       ldp     x13, x14, [x1], #16
         :                      stp1    D_l, D_h, dst, #16
    0.00 :   ffff800010c9212c:       stp     x13, x14, [x6], #16
    0.00 :   ffff800010c92130:       nop
    0.00 :   ffff800010c92134:       nop
         :
         :                      tst     count, #0x3f
    0.00 :   ffff800010c92138:       tst     x2, #0x3f
         :                      b.ne    .Ltail63
    0.00 :   ffff800010c9213c:       b.ne    ffff800010c92068 <__arch_copy_to_user+0x68>  // b.any
         :                      b       .Lexitfunc
    0.00 :   ffff800010c92140:       b       ffff800010c92210 <__arch_copy_to_user+0x210>
    0.00 :   ffff800010c92144:       nop
    0.00 :   ffff800010c92148:       nop
    0.00 :   ffff800010c9214c:       nop
    0.00 :   ffff800010c92150:       nop
    0.00 :   ffff800010c92154:       nop
    0.00 :   ffff800010c92158:       nop
    0.00 :   ffff800010c9215c:       nop
    0.00 :   ffff800010c92160:       nop
    0.00 :   ffff800010c92164:       nop
    0.00 :   ffff800010c92168:       nop
    0.00 :   ffff800010c9216c:       nop
    0.00 :   ffff800010c92170:       nop
    0.00 :   ffff800010c92174:       nop
    0.00 :   ffff800010c92178:       nop
    0.00 :   ffff800010c9217c:       nop
         :                      * 64 bytes per line this ensures the entire loop is in one line.
         :                      */
         :                      .p2align        L1_CACHE_SHIFT
         :                      .Lcpy_body_large:
         :                      /* pre-get 64 bytes data. */
         :                      ldp1    A_l, A_h, src, #16
    0.00 :   ffff800010c92180:       ldp     x7, x8, [x1], #16
         :                      ldp1    B_l, B_h, src, #16
    0.00 :   ffff800010c92184:       ldp     x9, x10, [x1], #16
         :                      ldp1    C_l, C_h, src, #16
    0.00 :   ffff800010c92188:       ldp     x11, x12, [x1], #16
         :                      ldp1    D_l, D_h, src, #16
    0.00 :   ffff800010c9218c:       ldp     x13, x14, [x1], #16
         :                      1:
         :                      /*
         :                      * interlace the load of next 64 bytes data block with store of the last
         :                      * loaded 64 bytes data.
         :                      */
         :                      stp1    A_l, A_h, dst, #16
    0.00 :   ffff800010c92190:       stp     x7, x8, [x6], #16
    0.00 :   ffff800010c92194:       nop
    0.00 :   ffff800010c92198:       nop
         :                      ldp1    A_l, A_h, src, #16
    0.00 :   ffff800010c9219c:       ldp     x7, x8, [x1], #16
         :                      stp1    B_l, B_h, dst, #16
    0.00 :   ffff800010c921a0:       stp     x9, x10, [x6], #16
    0.00 :   ffff800010c921a4:       nop
    0.00 :   ffff800010c921a8:       nop
         :                      ldp1    B_l, B_h, src, #16
    0.00 :   ffff800010c921ac:       ldp     x9, x10, [x1], #16
         :                      stp1    C_l, C_h, dst, #16
    0.00 :   ffff800010c921b0:       stp     x11, x12, [x6], #16
    0.00 :   ffff800010c921b4:       nop
    0.00 :   ffff800010c921b8:       nop
         :                      ldp1    C_l, C_h, src, #16
    0.00 :   ffff800010c921bc:       ldp     x11, x12, [x1], #16
         :                      stp1    D_l, D_h, dst, #16
    0.00 :   ffff800010c921c0:       stp     x13, x14, [x6], #16
    0.00 :   ffff800010c921c4:       nop
    0.00 :   ffff800010c921c8:       nop
         :                      ldp1    D_l, D_h, src, #16
    0.00 :   ffff800010c921cc:       ldp     x13, x14, [x1], #16
         :                      subs    count, count, #64
    0.00 :   ffff800010c921d0:       subs    x2, x2, #0x40
         :                      b.ge    1b
    0.00 :   ffff800010c921d4:       b.ge    ffff800010c92190 <__arch_copy_to_user+0x190>  // b.tcont
         :                      stp1    A_l, A_h, dst, #16
    0.00 :   ffff800010c921d8:       stp     x7, x8, [x6], #16
    0.00 :   ffff800010c921dc:       nop
    0.00 :   ffff800010c921e0:       nop
         :                      stp1    B_l, B_h, dst, #16
    0.00 :   ffff800010c921e4:       stp     x9, x10, [x6], #16
    0.00 :   ffff800010c921e8:       nop
    0.00 :   ffff800010c921ec:       nop
         :                      stp1    C_l, C_h, dst, #16
    0.00 :   ffff800010c921f0:       stp     x11, x12, [x6], #16
    0.00 :   ffff800010c921f4:       nop
    0.00 :   ffff800010c921f8:       nop
         :                      stp1    D_l, D_h, dst, #16
    0.00 :   ffff800010c921fc:       stp     x13, x14, [x6], #16
    0.00 :   ffff800010c92200:       nop
    0.00 :   ffff800010c92204:       nop
         :
         :                      tst     count, #0x3f
    0.00 :   ffff800010c92208:       tst     x2, #0x3f
         :                      b.ne    .Ltail63
    0.00 :   ffff800010c9220c:       b.ne    ffff800010c92068 <__arch_copy_to_user+0x68>  // b.any
         :                      #include "copy_template.S"
         :                      mov     x0, #0
    2.77 :   ffff800010c92210:       mov     x0, #0x0                        // #0
         :                      ret
    0.00 :   ffff800010c92214:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (306 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010482e28 <iov_iter_get_pages>:
         :                      iov_iter_get_pages():
         :                      }
         :
         :                      ssize_t iov_iter_get_pages(struct iov_iter *i,
         :                      struct page **pages, size_t maxsize, unsigned maxpages,
         :                      size_t *start)
         :                      {
    1.64 :   ffff800010482e28:       stp     x29, x30, [sp, #-112]!
    0.00 :   ffff800010482e2c:       mov     x29, sp
    1.97 :   ffff800010482e30:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010482e34:       adrp    x20, ffff800011899000 <page_wait_table+0x1500>
    3.92 :   ffff800010482e38:       stp     x21, x22, [sp, #32]
    0.00 :   ffff800010482e3c:       add     x5, x20, #0x8c8
    1.63 :   ffff800010482e40:       stp     x23, x24, [sp, #48]
    0.00 :   ffff800010482e44:       mov     x21, x1
    0.00 :   ffff800010482e48:       mov     x23, x0
    0.00 :   ffff800010482e4c:       mov     x22, x4
    3.59 :   ffff800010482e50:       ldr     x6, [x5]
    0.64 :   ffff800010482e54:       str     x6, [x29, #104]
    0.00 :   ffff800010482e58:       mov     x6, #0x0                        // #0
    3.92 :   ffff800010482e5c:       ldr     x1, [x0, #16]
         :                      iov_iter_is_pipe():
         :                      return iov_iter_type(i) == ITER_BVEC;
         :                      }
         :
         :                      static inline bool iov_iter_is_pipe(const struct iov_iter *i)
         :                      {
         :                      return iov_iter_type(i) == ITER_PIPE;
    0.66 :   ffff800010482e60:       ldr     w5, [x0]
    0.00 :   ffff800010482e64:       cmp     x1, x2
         :                      iov_iter_type():
         :                      return i->type & ~(READ | WRITE);
    0.00 :   ffff800010482e68:       and     w6, w5, #0xfffffffe
    0.34 :   ffff800010482e6c:       csel    x24, x1, x2, ls  // ls = plast
         :                      iov_iter_get_pages():
         :                      if (maxsize > i->count)
         :                      maxsize = i->count;
         :
         :                      if (unlikely(iov_iter_is_pipe(i)))
    0.98 :   ffff800010482e70:       cmp     w6, #0x20
    0.00 :   ffff800010482e74:       b.eq    ffff800010482f78 <iov_iter_get_pages+0x150>  // b.none
         :                      return pipe_get_pages(i, pages, maxsize, maxpages, start);
         :                      if (unlikely(iov_iter_is_discard(i)))
    0.00 :   ffff800010482e78:       cmp     w6, #0x40
    0.00 :   ffff800010482e7c:       b.eq    ffff800010482f68 <iov_iter_get_pages+0x140>  // b.none
         :                      return -EFAULT;
         :
         :                      iterate_all_kinds(i, maxsize, v, ({
   10.49 :   ffff800010482e80:       cbz     x24, ffff800010482f70 <iov_iter_get_pages+0x148>
    2.28 :   ffff800010482e84:       ldr     x0, [x0, #8]
    0.00 :   ffff800010482e88:       tbnz    w5, #4, ffff8000104830b8 <iov_iter_get_pages+0x290>
    0.66 :   ffff800010482e8c:       tbnz    w5, #3, ffff800010483138 <iov_iter_get_pages+0x310>
    0.33 :   ffff800010482e90:       tbnz    w5, #6, ffff800010482f70 <iov_iter_get_pages+0x148>
    4.57 :   ffff800010482e94:       ldr     x4, [x23, #24]
    0.00 :   ffff800010482e98:       ubfiz   x6, x3, #12, #32
    1.62 :   ffff800010482e9c:       ldr     x19, [x4, #8]
    0.00 :   ffff800010482ea0:       sub     x19, x19, x0
    0.00 :   ffff800010482ea4:       cmp     x19, x24
    0.00 :   ffff800010482ea8:       csel    x5, x19, x24, ls  // ls = plast
    5.86 :   ffff800010482eac:       cbz     x5, ffff800010482f34 <iov_iter_get_pages+0x10c>
    0.00 :   ffff800010482eb0:       ldr     x1, [x4]
    0.00 :   ffff800010482eb4:       mov     x3, x21
    0.00 :   ffff800010482eb8:       add     x0, x1, x0
    0.00 :   ffff800010482ebc:       and     x19, x0, #0xfff
   19.58 :   ffff800010482ec0:       and     x0, x0, #0xfffffffffffff000
    0.00 :   ffff800010482ec4:       str     x19, [x22]
    0.00 :   ffff800010482ec8:       add     x19, x19, x5
    0.00 :   ffff800010482ecc:       cmp     x19, x6
         :                      iov_iter_rw():
         :                      return iov_iter_type(i) == ITER_DISCARD;
         :                      }
         :
         :                      static inline unsigned char iov_iter_rw(const struct iov_iter *i)
         :                      {
         :                      return i->type & (READ | WRITE);
    0.98 :   ffff800010482ed0:       ldr     w2, [x23]
    0.00 :   ffff800010482ed4:       csel    x19, x19, x6, ls  // ls = plast
         :                      iov_iter_get_pages():
    0.00 :   ffff800010482ed8:       add     x21, x19, #0xfff
    0.00 :   ffff800010482edc:       mvn     w2, w2
    0.96 :   ffff800010482ee0:       and     w2, w2, #0x1
    0.00 :   ffff800010482ee4:       lsr     x21, x21, #12
    0.00 :   ffff800010482ee8:       mov     w1, w21
    0.00 :   ffff800010482eec:       bl      ffff800010205b38 <get_user_pages_fast>
    0.33 :   ffff800010482ef0:       tbnz    w0, #31, ffff800010483174 <iov_iter_get_pages+0x34c>
    0.32 :   ffff800010482ef4:       sbfiz   x1, x0, #12, #32
    0.00 :   ffff800010482ef8:       cmp     w0, w21
    3.25 :   ffff800010482efc:       ldr     x0, [x22]
    0.64 :   ffff800010482f00:       csel    x19, x1, x19, ne  // ne = any
    0.00 :   ffff800010482f04:       sub     x19, x19, x0
         :                      }),({
         :                      return -EFAULT;
         :                      })
         :                      )
         :                      return 0;
         :                      }
    0.00 :   ffff800010482f08:       add     x1, x20, #0x8c8
    7.54 :   ffff800010482f0c:       mov     x0, x19
    0.00 :   ffff800010482f10:       ldr     x2, [x29, #104]
    1.30 :   ffff800010482f14:       ldr     x1, [x1]
    0.00 :   ffff800010482f18:       eor     x1, x2, x1
    0.00 :   ffff800010482f1c:       cbnz    x1, ffff8000104831ec <iov_iter_get_pages+0x3c4>
   11.15 :   ffff800010482f20:       ldp     x19, x20, [sp, #16]
    8.84 :   ffff800010482f24:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010482f28:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff800010482f2c:       ldp     x29, x30, [sp], #112
    0.00 :   ffff800010482f30:       ret
         :                      iterate_all_kinds(i, maxsize, v, ({
    0.00 :   ffff800010482f34:       add     x4, x4, #0x10
    0.00 :   ffff800010482f38:       ldr     x0, [x4, #8]
    0.00 :   ffff800010482f3c:       cmp     x0, x24
    0.00 :   ffff800010482f40:       csel    x0, x0, x24, ls  // ls = plast
    0.00 :   ffff800010482f44:       cbz     x0, ffff800010482f34 <iov_iter_get_pages+0x10c>
    0.00 :   ffff800010482f48:       ldr     x1, [x4]
    0.00 :   ffff800010482f4c:       mov     x3, x21
    0.00 :   ffff800010482f50:       and     x19, x1, #0xfff
    0.00 :   ffff800010482f54:       str     x19, [x22]
    0.00 :   ffff800010482f58:       add     x19, x19, x0
    0.00 :   ffff800010482f5c:       and     x0, x1, #0xfffffffffffff000
    0.00 :   ffff800010482f60:       b       ffff800010482ecc <iov_iter_get_pages+0xa4>
    0.00 :   ffff800010482f64:       ldp     x25, x26, [x29, #64]
         :                      pipe_get_pages():
         :                      return -EFAULT;
    0.00 :   ffff800010482f68:       mov     x19, #0xfffffffffffffff2        // #-14
    0.00 :   ffff800010482f6c:       b       ffff800010482f08 <iov_iter_get_pages+0xe0>
         :                      return 0;
    0.00 :   ffff800010482f70:       mov     x19, #0x0                       // #0
    0.00 :   ffff800010482f74:       b       ffff800010482f08 <iov_iter_get_pages+0xe0>
    0.00 :   ffff800010482f78:       mov     x19, #0x0                       // #0
         :                      if (!maxsize)
    0.00 :   ffff800010482f7c:       cbz     x24, ffff800010482f08 <iov_iter_get_pages+0xe0>
    0.00 :   ffff800010482f80:       str     w3, [x29, #92]
         :                      if (!sanity(i))
    0.00 :   ffff800010482f84:       bl      ffff8000104822e0 <sanity>
    0.00 :   ffff800010482f88:       ldr     w3, [x29, #92]
    0.00 :   ffff800010482f8c:       tst     w0, #0xff
    0.00 :   ffff800010482f90:       b.eq    ffff800010482f68 <iov_iter_get_pages+0x140>  // b.none
    0.00 :   ffff800010482f94:       stp     x25, x26, [x29, #64]
         :                      data_start():
         :                      size_t off = i->iov_offset;
    0.00 :   ffff800010482f98:       ldr     x4, [x23, #8]
         :                      unsigned int iter_head = i->head;
    0.00 :   ffff800010482f9c:       ldr     w5, [x23, #32]
         :                      if (off && (!allocated(&i->pipe->bufs[iter_head & p_mask]) ||
    0.00 :   ffff800010482fa0:       cbz     x4, ffff800010482fe4 <iov_iter_get_pages+0x1bc>
         :                      unsigned int p_mask = i->pipe->ring_size - 1;
    0.00 :   ffff800010482fa4:       ldr     x2, [x23, #24]
         :                      if (off && (!allocated(&i->pipe->bufs[iter_head & p_mask]) ||
    0.00 :   ffff800010482fa8:       mov     w7, #0x28                       // #40
    0.00 :   ffff800010482fac:       adrp    x1, ffff800010cf9000 <empty_dir_inode_operations+0x40>
    0.00 :   ffff800010482fb0:       add     x1, x1, #0x280
    0.00 :   ffff800010482fb4:       mov     x6, #0x1000                     // #4096
         :                      unsigned int p_mask = i->pipe->ring_size - 1;
    0.00 :   ffff800010482fb8:       ldr     w0, [x2, #68]
         :                      if (off && (!allocated(&i->pipe->bufs[iter_head & p_mask]) ||
    0.00 :   ffff800010482fbc:       ldr     x2, [x2, #120]
         :                      unsigned int p_mask = i->pipe->ring_size - 1;
    0.00 :   ffff800010482fc0:       sub     w0, w0, #0x1
         :                      if (off && (!allocated(&i->pipe->bufs[iter_head & p_mask]) ||
    0.00 :   ffff800010482fc4:       and     w0, w0, w5
    0.00 :   ffff800010482fc8:       umaddl  x0, w0, w7, x2
    0.00 :   ffff800010482fcc:       ldr     x0, [x0, #16]
    0.00 :   ffff800010482fd0:       cmp     x0, x1
    0.00 :   ffff800010482fd4:       ccmp    x4, x6, #0x4, eq  // eq = none
    0.00 :   ffff800010482fd8:       b.ne    ffff800010482fe4 <iov_iter_get_pages+0x1bc>  // b.any
         :                      iter_head++;
    0.00 :   ffff800010482fdc:       add     w5, w5, #0x1
         :                      off = 0;
    0.00 :   ffff800010482fe0:       mov     x4, #0x0                        // #0
         :                      *offp = off;
    0.00 :   ffff800010482fe4:       str     x4, [x22]
         :                      pipe_get_pages():
         :                      npages = pipe_space_for_user(iter_head, i->pipe->tail, i->pipe);
    0.00 :   ffff800010482fe8:       ldr     x25, [x23, #24]
    0.00 :   ffff800010482fec:       ldp     w0, w1, [x25, #60]
    0.00 :   ffff800010482ff0:       ldr     w26, [x25, #68]
         :                      pipe_occupancy():
         :                      * @head: The pipe ring head pointer
         :                      * @tail: The pipe ring tail pointer
         :                      */
         :                      static inline unsigned int pipe_occupancy(unsigned int head, unsigned int tail)
         :                      {
         :                      return head - tail;
    0.00 :   ffff800010482ff4:       sub     w0, w5, w0
         :                      pipe_space_for_user():
         :                      struct pipe_inode_info *pipe)
         :                      {
         :                      unsigned int p_occupancy, p_space;
         :
         :                      p_occupancy = pipe_occupancy(head, tail);
         :                      if (p_occupancy >= pipe->max_usage)
    0.00 :   ffff800010482ff8:       cmp     w1, w0
    0.00 :   ffff800010482ffc:       b.hi    ffff80001048317c <iov_iter_get_pages+0x354>  // b.pmore
    0.00 :   ffff800010483000:       mov     x1, #0x0                        // #0
    0.00 :   ffff800010483004:       add     x2, x29, #0x70
         :                      pipe_get_pages():
         :                      capacity = min(npages, maxpages) * PAGE_SIZE - *start;
    0.00 :   ffff800010483008:       sub     x1, x1, x4
         :                      return __pipe_get_pages(i, min(maxsize, capacity), pages, iter_head, start);
    0.00 :   ffff80001048300c:       cmp     x1, x24
         :                      __pipe_get_pages():
         :                      ssize_t n = push_pipe(i, maxsize, &iter_head, start);
    0.00 :   ffff800010483010:       mov     x3, x22
    0.00 :   ffff800010483014:       csel    x1, x1, x24, ls  // ls = plast
    0.00 :   ffff800010483018:       mov     x0, x23
    0.00 :   ffff80001048301c:       str     w5, [x2, #-12]!
         :                      unsigned int p_mask = pipe->ring_size - 1;
    0.00 :   ffff800010483020:       sub     w26, w26, #0x1
         :                      ssize_t n = push_pipe(i, maxsize, &iter_head, start);
    0.00 :   ffff800010483024:       bl      ffff8000104823f0 <push_pipe>
    0.00 :   ffff800010483028:       mov     x19, x0
         :                      if (!n)
    0.00 :   ffff80001048302c:       cbz     x0, ffff800010482f64 <iov_iter_get_pages+0x13c>
         :                      n += *start;
    0.00 :   ffff800010483030:       ldr     x0, [x22]
    0.00 :   ffff800010483034:       add     x0, x19, x0
         :                      while (n > 0) {
    0.00 :   ffff800010483038:       cmp     x0, #0x0
    0.00 :   ffff80001048303c:       b.le    ffff8000104830b0 <iov_iter_get_pages+0x288>
    0.00 :   ffff800010483040:       sub     x2, x0, #0x1
    0.00 :   ffff800010483044:       add     x1, x21, #0x8
    0.00 :   ffff800010483048:       ldr     w0, [x29, #100]
         :                      get_page(*pages++ = pipe->bufs[iter_head & p_mask].page);
    0.00 :   ffff80001048304c:       mov     w4, #0x28                       // #40
    0.00 :   ffff800010483050:       lsr     x2, x2, #12
         :                      __lse_atomic_add():
         :                      }
         :
         :                      ATOMIC_OP(andnot, stclr)
         :                      ATOMIC_OP(or, stset)
         :                      ATOMIC_OP(xor, steor)
         :                      ATOMIC_OP(add, stadd)
    0.00 :   ffff800010483054:       mov     w5, #0x1                        // #1
    0.00 :   ffff800010483058:       add     x2, x1, x2, lsl #3
    0.00 :   ffff80001048305c:       nop
         :                      __pipe_get_pages():
    0.00 :   ffff800010483060:       and     w0, w0, w26
    0.00 :   ffff800010483064:       ldr     x1, [x25, #120]
    0.00 :   ffff800010483068:       umull   x0, w0, w4
    0.00 :   ffff80001048306c:       ldr     x0, [x1, x0]
    0.00 :   ffff800010483070:       str     x0, [x21]
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010483074:       ldr     x1, [x0, #8]
         :                      compound_head():
         :                      static inline struct page *compound_head(struct page *page)
         :                      {
         :                      unsigned long head = READ_ONCE(page->compound_head);
         :
         :                      if (unlikely(head & 1))
         :                      return (struct page *) (head - 1);
    0.00 :   ffff800010483078:       sub     x3, x1, #0x1
    0.00 :   ffff80001048307c:       tst     x1, #0x1
    0.00 :   ffff800010483080:       csel    x0, x3, x0, ne  // ne = any
         :                      page_ref_inc():
         :                      __page_ref_mod(page, -nr);
         :                      }
         :
         :                      static inline void page_ref_inc(struct page *page)
         :                      {
         :                      atomic_inc(&page->_refcount);
    0.00 :   ffff800010483084:       add     x0, x0, #0x34
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010483088:       b       ffff80001048316c <iov_iter_get_pages+0x344>
    0.00 :   ffff80001048308c:       b       ffff80001048316c <iov_iter_get_pages+0x344>
         :                      __lse_atomic_add():
    0.00 :   ffff800010483090:       mov     w1, w5
    0.00 :   ffff800010483094:       stadd   w1, [x0]
         :                      __pipe_get_pages():
         :                      iter_head++;
    0.00 :   ffff800010483098:       ldr     w0, [x29, #100]
    0.00 :   ffff80001048309c:       add     x21, x21, #0x8
         :                      while (n > 0) {
    0.00 :   ffff8000104830a0:       cmp     x2, x21
         :                      iter_head++;
    0.00 :   ffff8000104830a4:       add     w0, w0, #0x1
    0.00 :   ffff8000104830a8:       str     w0, [x29, #100]
         :                      while (n > 0) {
    0.00 :   ffff8000104830ac:       b.ne    ffff800010483060 <iov_iter_get_pages+0x238>  // b.any
    0.00 :   ffff8000104830b0:       ldp     x25, x26, [x29, #64]
    0.00 :   ffff8000104830b4:       b       ffff800010482f08 <iov_iter_get_pages+0xe0>
         :                      iov_iter_get_pages():
         :                      iterate_all_kinds(i, maxsize, v, ({
    0.00 :   ffff8000104830b8:       mov     w8, w24
    0.00 :   ffff8000104830bc:       cbz     w24, ffff800010482f70 <iov_iter_get_pages+0x148>
    0.00 :   ffff8000104830c0:       ldr     x4, [x23, #24]
    0.00 :   ffff8000104830c4:       mov     w5, #0x0                        // #0
    0.00 :   ffff8000104830c8:       mov     w9, #0x1000                     // #4096
    0.00 :   ffff8000104830cc:       ldp     w2, w1, [x4, #8]
    0.00 :   ffff8000104830d0:       ldr     x10, [x4]
    0.00 :   ffff8000104830d4:       add     w1, w0, w1
    0.00 :   ffff8000104830d8:       sub     w7, w2, w0
    0.00 :   ffff8000104830dc:       cmp     w7, w8
    0.00 :   ffff8000104830e0:       and     w3, w1, #0xfff
    0.00 :   ffff8000104830e4:       csel    x19, x7, x8, ls  // ls = plast
    0.00 :   ffff8000104830e8:       sub     w6, w9, w3
    0.00 :   ffff8000104830ec:       cmp     w19, w6
    0.00 :   ffff8000104830f0:       csel    x19, x19, x6, ls  // ls = plast
    0.00 :   ffff8000104830f4:       cbz     w19, ffff800010483198 <iov_iter_get_pages+0x370>
    0.00 :   ffff8000104830f8:       lsr     w1, w1, #12
    0.00 :   ffff8000104830fc:       str     x3, [x22]
    0.00 :   ffff800010483100:       add     x1, x10, x1, lsl #6
    0.00 :   ffff800010483104:       str     x1, [x21]
         :                      __read_once_size():
    0.00 :   ffff800010483108:       ldr     x0, [x1, #8]
         :                      compound_head():
    0.00 :   ffff80001048310c:       sub     x2, x0, #0x1
    0.00 :   ffff800010483110:       tst     x0, #0x1
    0.00 :   ffff800010483114:       csel    x1, x2, x1, ne  // ne = any
         :                      page_ref_inc():
    0.00 :   ffff800010483118:       add     x1, x1, #0x34
         :                      arch_static_branch_jump():
    0.00 :   ffff80001048311c:       b       ffff800010483130 <iov_iter_get_pages+0x308>
    0.00 :   ffff800010483120:       b       ffff800010483130 <iov_iter_get_pages+0x308>
         :                      __lse_atomic_add():
    0.00 :   ffff800010483124:       mov     w0, #0x1                        // #1
    0.00 :   ffff800010483128:       stadd   w0, [x1]
    0.00 :   ffff80001048312c:       b       ffff800010482f08 <iov_iter_get_pages+0xe0>
         :                      __ll_sc_atomic_add():
         :                      ATOMIC_FETCH_OP (        , dmb ish,  , l, "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
    0.00 :   ffff800010483130:       b       ffff800010487b70 <iov_iter_get_pages_alloc+0x548>
    0.00 :   ffff800010483134:       b       ffff800010482f08 <iov_iter_get_pages+0xe0>
         :                      iov_iter_get_pages():
    0.00 :   ffff800010483138:       ldr     x2, [x23, #24]
    0.00 :   ffff80001048313c:       ldr     x1, [x2, #8]
    0.00 :   ffff800010483140:       sub     x0, x1, x0
    0.00 :   ffff800010483144:       cmp     x0, x24
    0.00 :   ffff800010483148:       csel    x0, x0, x24, ls  // ls = plast
    0.00 :   ffff80001048314c:       cbnz    x0, ffff800010482f68 <iov_iter_get_pages+0x140>
    0.00 :   ffff800010483150:       add     x2, x2, #0x10
    0.00 :   ffff800010483154:       ldr     x0, [x2, #8]
    0.00 :   ffff800010483158:       cmp     x0, x24
    0.00 :   ffff80001048315c:       csel    x0, x0, x24, ls  // ls = plast
    0.00 :   ffff800010483160:       cbz     x0, ffff800010483150 <iov_iter_get_pages+0x328>
         :                      pipe_get_pages():
         :                      return -EFAULT;
    0.00 :   ffff800010483164:       mov     x19, #0xfffffffffffffff2        // #-14
    0.00 :   ffff800010483168:       b       ffff800010482f08 <iov_iter_get_pages+0xe0>
         :                      __ll_sc_atomic_add():
    0.00 :   ffff80001048316c:       b       ffff800010487b88 <iov_iter_get_pages_alloc+0x560>
    0.00 :   ffff800010483170:       b       ffff800010483098 <iov_iter_get_pages+0x270>
         :                      iov_iter_get_pages():
         :                      iterate_all_kinds(i, maxsize, v, ({
    0.00 :   ffff800010483174:       sxtw    x19, w0
    0.00 :   ffff800010483178:       b       ffff800010482f08 <iov_iter_get_pages+0xe0>
    0.00 :   ffff80001048317c:       cmp     w3, w1
         :                      pipe_space_for_user():
         :                      return 0;
         :                      p_space = pipe->ring_size - p_occupancy;
    0.00 :   ffff800010483180:       sub     w0, w26, w0
    0.00 :   ffff800010483184:       csel    w1, w3, w1, ls  // ls = plast
    0.00 :   ffff800010483188:       cmp     w0, w1
    0.00 :   ffff80001048318c:       csel    w0, w0, w1, ls  // ls = plast
    0.00 :   ffff800010483190:       lsl     x1, x0, #12
    0.00 :   ffff800010483194:       b       ffff800010483004 <iov_iter_get_pages+0x1dc>
         :                      bvec_iter_advance():
         :                      }
         :
         :                      iter->bi_size -= bytes;
         :                      bytes += iter->bi_bvec_done;
         :
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff800010483198:       cbz     w0, ffff8000104830dc <iov_iter_get_pages+0x2b4>
    0.00 :   ffff80001048319c:       cmp     w0, w2
    0.00 :   ffff8000104831a0:       b.cs    ffff8000104831b0 <iov_iter_get_pages+0x388>  // b.hs, b.nlast
    0.00 :   ffff8000104831a4:       b       ffff8000104830dc <iov_iter_get_pages+0x2b4>
    0.00 :   ffff8000104831a8:       cmp     w0, w2
    0.00 :   ffff8000104831ac:       b.cc    ffff8000104831d8 <iov_iter_get_pages+0x3b0>  // b.lo, b.ul, b.last
         :                      bytes -= bv[idx].bv_len;
         :                      idx++;
    0.00 :   ffff8000104831b0:       add     w5, w5, #0x1
         :                      bytes -= bv[idx].bv_len;
    0.00 :   ffff8000104831b4:       sub     w0, w0, w2
    0.00 :   ffff8000104831b8:       ubfiz   x3, x5, #4, #32
    0.00 :   ffff8000104831bc:       add     x3, x4, x3
    0.00 :   ffff8000104831c0:       ldr     w2, [x3, #8]
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff8000104831c4:       cbnz    w0, ffff8000104831a8 <iov_iter_get_pages+0x380>
    0.00 :   ffff8000104831c8:       mov     w7, w2
    0.00 :   ffff8000104831cc:       ldr     w1, [x3, #12]
    0.00 :   ffff8000104831d0:       ldr     x10, [x3]
    0.00 :   ffff8000104831d4:       b       ffff8000104830dc <iov_iter_get_pages+0x2b4>
    0.00 :   ffff8000104831d8:       ldr     w1, [x3, #12]
    0.00 :   ffff8000104831dc:       sub     w7, w2, w0
    0.00 :   ffff8000104831e0:       ldr     x10, [x3]
    0.00 :   ffff8000104831e4:       add     w1, w0, w1
    0.00 :   ffff8000104831e8:       b       ffff8000104830dc <iov_iter_get_pages+0x2b4>
    0.00 :   ffff8000104831ec:       stp     x25, x26, [x29, #64]
         :                      iov_iter_get_pages():
         :                      }
    0.00 :   ffff8000104831f0:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (317 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102ca820 <__fsnotify_parent>:
         :                      __fsnotify_parent():
         :                      spin_unlock(&inode->i_lock);
         :                      }
         :
         :                      /* Notify this dentry's parent about a child's events. */
         :                      int __fsnotify_parent(const struct path *path, struct dentry *dentry, __u32 mask)
         :                      {
    0.00 :   ffff8000102ca820:       stp     x29, x30, [sp, #-144]!
    0.00 :   ffff8000102ca824:       mov     x29, sp
    4.40 :   ffff8000102ca828:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000102ca82c:       adrp    x19, ffff800011899000 <page_wait_table+0x1500>
    1.88 :   ffff8000102ca830:       stp     x21, x22, [sp, #32]
    0.00 :   ffff8000102ca834:       add     x3, x19, #0x8c8
    3.83 :   ffff8000102ca838:       str     x23, [sp, #48]
    0.00 :   ffff8000102ca83c:       mov     w22, w2
    0.33 :   ffff8000102ca840:       mov     x23, x0
    0.00 :   ffff8000102ca844:       mov     x20, x1
    0.00 :   ffff8000102ca848:       ldr     x4, [x3]
   14.54 :   ffff8000102ca84c:       str     x4, [x29, #136]
    0.00 :   ffff8000102ca850:       mov     x4, #0x0                        // #0
         :                      struct dentry *parent;
         :                      struct inode *p_inode;
         :                      int ret = 0;
         :
         :                      if (!dentry)
    0.00 :   ffff8000102ca854:       cbz     x1, ffff8000102ca8dc <__fsnotify_parent+0xbc>
         :                      dentry = path->dentry;
         :
         :                      if (!(dentry->d_flags & DCACHE_FSNOTIFY_PARENT_WATCHED))
    0.32 :   ffff8000102ca858:       ldr     w0, [x20]
         :                      return 0;
    0.00 :   ffff8000102ca85c:       mov     w21, #0x0                       // #0
         :                      if (!(dentry->d_flags & DCACHE_FSNOTIFY_PARENT_WATCHED))
    0.00 :   ffff8000102ca860:       tbnz    w0, #14, ffff8000102ca890 <__fsnotify_parent+0x70>
         :                      }
         :
         :                      dput(parent);
         :
         :                      return ret;
         :                      }
   40.08 :   ffff8000102ca864:       add     x19, x19, #0x8c8
    0.00 :   ffff8000102ca868:       mov     w0, w21
    5.33 :   ffff8000102ca86c:       ldr     x2, [x29, #136]
    0.31 :   ffff8000102ca870:       ldr     x1, [x19]
    0.00 :   ffff8000102ca874:       eor     x1, x2, x1
    0.00 :   ffff8000102ca878:       cbnz    x1, ffff8000102ca968 <__fsnotify_parent+0x148>
    0.00 :   ffff8000102ca87c:       ldp     x19, x20, [sp, #16]
    6.60 :   ffff8000102ca880:       ldp     x21, x22, [sp, #32]
   20.79 :   ffff8000102ca884:       ldr     x23, [sp, #48]
    1.58 :   ffff8000102ca888:       ldp     x29, x30, [sp], #144
    0.00 :   ffff8000102ca88c:       ret
    0.00 :   ffff8000102ca890:       stp     x24, x25, [x29, #56]
         :                      parent = dget_parent(dentry);
    0.00 :   ffff8000102ca894:       mov     x0, x20
    0.00 :   ffff8000102ca898:       bl      ffff8000102985f0 <dget_parent>
         :                      p_inode = parent->d_inode;
    0.00 :   ffff8000102ca89c:       ldr     x25, [x0, #48]
         :                      parent = dget_parent(dentry);
    0.00 :   ffff8000102ca8a0:       mov     x24, x0
         :                      if (unlikely(!fsnotify_inode_watches_children(p_inode))) {
    0.00 :   ffff8000102ca8a4:       ldr     w1, [x25, #556]
         :                      fsnotify_inode_watches_children():
         :                      extern u32 fsnotify_get_cookie(void);
         :
         :                      static inline int fsnotify_inode_watches_children(struct inode *inode)
         :                      {
         :                      /* FS_EVENT_ON_CHILD is set if the inode may care */
         :                      if (!(inode->i_fsnotify_mask & FS_EVENT_ON_CHILD))
    0.00 :   ffff8000102ca8a8:       tbz     w1, #27, ffff8000102ca924 <__fsnotify_parent+0x104>
         :                      return 0;
         :                      /* this inode might care about child events, does it care about the
         :                      * specific set of events that can happen on a child? */
         :                      return inode->i_fsnotify_mask & FS_EVENTS_POSS_ON_CHILD;
    0.00 :   ffff8000102ca8ac:       mov     w0, #0x103f                     // #4159
    0.00 :   ffff8000102ca8b0:       movk    w0, #0x7, lsl #16
         :                      __fsnotify_parent():
    0.00 :   ffff8000102ca8b4:       tst     w1, w0
    0.00 :   ffff8000102ca8b8:       b.eq    ffff8000102ca924 <__fsnotify_parent+0x104>  // b.none
         :                      } else if (p_inode->i_fsnotify_mask & mask & ALL_FSNOTIFY_EVENTS) {
    0.00 :   ffff8000102ca8bc:       mov     w0, #0x1007ffff                 // #268959743
    0.00 :   ffff8000102ca8c0:       and     w0, w22, w0
    0.00 :   ffff8000102ca8c4:       tst     w0, w1
    0.00 :   ffff8000102ca8c8:       b.ne    ffff8000102ca8e4 <__fsnotify_parent+0xc4>  // b.any
         :                      dput(parent);
    0.00 :   ffff8000102ca8cc:       mov     x0, x24
    0.00 :   ffff8000102ca8d0:       bl      ffff800010298308 <dput>
    0.00 :   ffff8000102ca8d4:       ldp     x24, x25, [x29, #56]
         :                      return ret;
    0.00 :   ffff8000102ca8d8:       b       ffff8000102ca864 <__fsnotify_parent+0x44>
         :                      dentry = path->dentry;
    0.00 :   ffff8000102ca8dc:       ldr     x20, [x0, #8]
    0.00 :   ffff8000102ca8e0:       b       ffff8000102ca858 <__fsnotify_parent+0x38>
         :                      mask |= FS_EVENT_ON_CHILD;
    0.00 :   ffff8000102ca8e4:       orr     w22, w22, #0x8000000
         :                      take_dentry_name_snapshot(&name, dentry);
    0.00 :   ffff8000102ca8e8:       mov     x1, x20
    0.00 :   ffff8000102ca8ec:       add     x0, x29, #0x58
    0.00 :   ffff8000102ca8f0:       bl      ffff8000102960d0 <take_dentry_name_snapshot>
         :                      if (path)
    0.00 :   ffff8000102ca8f4:       cbz     x23, ffff8000102ca944 <__fsnotify_parent+0x124>
         :                      ret = fsnotify(p_inode, mask, path, FSNOTIFY_EVENT_PATH,
    0.00 :   ffff8000102ca8f8:       mov     w5, #0x0                        // #0
    0.00 :   ffff8000102ca8fc:       add     x4, x29, #0x58
    0.00 :   ffff8000102ca900:       mov     w3, #0x1                        // #1
    0.00 :   ffff8000102ca904:       mov     x2, x23
    0.00 :   ffff8000102ca908:       mov     w1, w22
    0.00 :   ffff8000102ca90c:       mov     x0, x25
    0.00 :   ffff8000102ca910:       bl      ffff8000102ca4e0 <fsnotify>
    0.00 :   ffff8000102ca914:       mov     w21, w0
         :                      release_dentry_name_snapshot(&name);
    0.00 :   ffff8000102ca918:       add     x0, x29, #0x58
    0.00 :   ffff8000102ca91c:       bl      ffff800010296160 <release_dentry_name_snapshot>
    0.00 :   ffff8000102ca920:       b       ffff8000102ca8cc <__fsnotify_parent+0xac>
         :                      __fsnotify_update_child_dentry_flags():
         :                      if (!S_ISDIR(inode->i_mode))
    0.00 :   ffff8000102ca924:       ldrh    w0, [x25]
         :                      __fsnotify_parent():
         :                      int ret = 0;
    0.00 :   ffff8000102ca928:       mov     w21, #0x0                       // #0
         :                      __fsnotify_update_child_dentry_flags():
         :                      if (!S_ISDIR(inode->i_mode))
    0.00 :   ffff8000102ca92c:       and     w0, w0, #0xf000
    0.00 :   ffff8000102ca930:       cmp     w0, #0x4, lsl #12
    0.00 :   ffff8000102ca934:       b.ne    ffff8000102ca8cc <__fsnotify_parent+0xac>  // b.any
    0.00 :   ffff8000102ca938:       mov     x0, x25
    0.00 :   ffff8000102ca93c:       bl      ffff8000102ca3e0 <__fsnotify_update_child_dentry_flags.part.5>
    0.00 :   ffff8000102ca940:       b       ffff8000102ca8cc <__fsnotify_parent+0xac>
         :                      __fsnotify_parent():
         :                      ret = fsnotify(p_inode, mask, dentry->d_inode, FSNOTIFY_EVENT_INODE,
    0.00 :   ffff8000102ca944:       ldr     x2, [x20, #48]
    0.00 :   ffff8000102ca948:       mov     w5, #0x0                        // #0
    0.00 :   ffff8000102ca94c:       add     x4, x29, #0x58
    0.00 :   ffff8000102ca950:       mov     w3, #0x2                        // #2
    0.00 :   ffff8000102ca954:       mov     w1, w22
    0.00 :   ffff8000102ca958:       mov     x0, x25
    0.00 :   ffff8000102ca95c:       bl      ffff8000102ca4e0 <fsnotify>
    0.00 :   ffff8000102ca960:       mov     w21, w0
    0.00 :   ffff8000102ca964:       b       ffff8000102ca918 <__fsnotify_parent+0xf8>
    0.00 :   ffff8000102ca968:       stp     x24, x25, [x29, #56]
         :                      }
    0.00 :   ffff8000102ca96c:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (169 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010165638 <nsecs_to_jiffies64>:
         :                      div_u64_rem():
         :                      * divide.
         :                      */
         :                      static inline u64 div_u64_rem(u64 dividend, u32 divisor, u32 *remainder)
         :                      {
         :                      *remainder = dividend % divisor;
         :                      return dividend / divisor;
   20.72 :   ffff800010165638:       mov     x1, #0x34db                     // #13531
    5.89 :   ffff80001016563c:       movk    x1, #0xd7b6, lsl #16
    2.40 :   ffff800010165640:       movk    x1, #0xde82, lsl #32
    0.00 :   ffff800010165644:       movk    x1, #0x431b, lsl #48
   56.23 :   ffff800010165648:       umulh   x0, x0, x1
         :                      nsecs_to_jiffies64():
         :                      * Generic case - optimized for cases where HZ is a multiple of 3.
         :                      * overflow after 64.99 years, exact for HZ = 60, 72, 90, 120 etc.
         :                      */
         :                      return div_u64(n * 9, (9ull * NSEC_PER_SEC + HZ / 2) / HZ);
         :                      #endif
         :                      }
   14.77 :   ffff80001016564c:       lsr     x0, x0, #20
    0.00 :   ffff800010165650:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (307 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001047a738 <bio_integrity_prep>:
         :                      bio_integrity_prep():
         :                      * to calling.  In the WRITE case, integrity metadata will be generated using
         :                      * the block device's integrity function.  In the READ case, the buffer
         :                      * will be prepared for DMA and a suitable end_io handler set up.
         :                      */
         :                      bool bio_integrity_prep(struct bio *bio)
         :                      {
   14.99 :   ffff80001047a738:       stp     x29, x30, [sp, #-96]!
    0.00 :   ffff80001047a73c:       mov     x29, sp
    4.88 :   ffff80001047a740:       str     x26, [sp, #72]
         :                      struct bio_integrity_payload *bip;
         :                      struct blk_integrity *bi = blk_get_integrity(bio->bi_disk);
   10.10 :   ffff80001047a744:       ldr     x1, [x0, #8]
   11.41 :   ffff80001047a748:       ldr     x26, [x1, #1040]
         :                      blk_get_integrity():
         :
         :                      static inline struct blk_integrity *blk_get_integrity(struct gendisk *disk)
         :                      {
         :                      struct blk_integrity *bi = &disk->queue->integrity;
         :
         :                      if (!bi->profile)
    0.00 :   ffff80001047a74c:       mov     x1, x26
   18.26 :   ffff80001047a750:       ldr     x2, [x1, #200]!
         :                      bio_integrity_prep():
         :                      unsigned int len, nr_pages;
         :                      unsigned int bytes, offset, i;
         :                      unsigned int intervals;
         :                      blk_status_t status;
         :
         :                      if (!bi)
    0.00 :   ffff80001047a754:       cmp     x1, #0x0
         :                      return true;
    0.00 :   ffff80001047a758:       mov     w1, #0x1                        // #1
         :                      if (!bi)
    0.00 :   ffff80001047a75c:       ccmp    x2, #0x0, #0x4, ne  // ne = any
   34.51 :   ffff80001047a760:       b.eq    ffff80001047a778 <bio_integrity_prep+0x40>  // b.none
         :
         :                      if (bio_op(bio) != REQ_OP_READ && bio_op(bio) != REQ_OP_WRITE)
    0.00 :   ffff80001047a764:       ldr     w3, [x0, #16]
         :                      return true;
    0.00 :   ffff80001047a768:       mov     w1, #0x1                        // #1
         :                      if (bio_op(bio) != REQ_OP_READ && bio_op(bio) != REQ_OP_WRITE)
    0.00 :   ffff80001047a76c:       and     w4, w3, #0xff
    0.00 :   ffff80001047a770:       cmp     w4, w1
    0.00 :   ffff80001047a774:       b.ls    ffff80001047a788 <bio_integrity_prep+0x50>  // b.plast
         :                      err_end_io:
         :                      bio->bi_status = status;
         :                      bio_endio(bio);
         :                      return false;
         :
         :                      }
    0.00 :   ffff80001047a778:       mov     w0, w1
    5.53 :   ffff80001047a77c:       ldr     x26, [sp, #72]
    0.33 :   ffff80001047a780:       ldp     x29, x30, [sp], #96
    0.00 :   ffff80001047a784:       ret
         :                      if (!bio_sectors(bio))
    0.00 :   ffff80001047a788:       ldr     w4, [x0, #40]
    0.00 :   ffff80001047a78c:       lsr     w4, w4, #9
    0.00 :   ffff80001047a790:       cbz     w4, ffff80001047a778 <bio_integrity_prep+0x40>
         :                      bio_integrity():
         :
         :                      #if defined(CONFIG_BLK_DEV_INTEGRITY)
         :
         :                      static inline struct bio_integrity_payload *bio_integrity(struct bio *bio)
         :                      {
         :                      if (bio->bi_opf & REQ_INTEGRITY)
    0.00 :   ffff80001047a794:       tbz     w3, #16, ffff80001047a7a0 <bio_integrity_prep+0x68>
         :                      bio_integrity_prep():
         :                      if (bio_integrity(bio))
    0.00 :   ffff80001047a798:       ldr     x5, [x0, #88]
    0.00 :   ffff80001047a79c:       cbnz    x5, ffff80001047a778 <bio_integrity_prep+0x40>
         :                      if (bio_data_dir(bio) == READ) {
    0.00 :   ffff80001047a7a0:       tbnz    w3, #0, ffff80001047a7bc <bio_integrity_prep+0x84>
         :                      if (!bi->profile->verify_fn ||
    0.00 :   ffff80001047a7a4:       ldr     x2, [x2, #8]
         :                      return true;
    0.00 :   ffff80001047a7a8:       mov     w1, #0x1                        // #1
         :                      if (!bi->profile->verify_fn ||
    0.00 :   ffff80001047a7ac:       cbz     x2, ffff80001047a778 <bio_integrity_prep+0x40>
    0.00 :   ffff80001047a7b0:       ldrb    w2, [x26, #208]
    0.00 :   ffff80001047a7b4:       tbnz    w2, #0, ffff80001047a7d0 <bio_integrity_prep+0x98>
    0.00 :   ffff80001047a7b8:       b       ffff80001047a778 <bio_integrity_prep+0x40>
         :                      if (!bi->profile->generate_fn ||
    0.00 :   ffff80001047a7bc:       ldr     x2, [x2]
         :                      return true;
    0.00 :   ffff80001047a7c0:       mov     w1, #0x1                        // #1
         :                      if (!bi->profile->generate_fn ||
    0.00 :   ffff80001047a7c4:       cbz     x2, ffff80001047a778 <bio_integrity_prep+0x40>
    0.00 :   ffff80001047a7c8:       ldrb    w2, [x26, #208]
    0.00 :   ffff80001047a7cc:       tbz     w2, #1, ffff80001047a778 <bio_integrity_prep+0x40>
    0.00 :   ffff80001047a7d0:       stp     x19, x20, [x29, #16]
    0.00 :   ffff80001047a7d4:       mov     x19, x0
    0.00 :   ffff80001047a7d8:       str     x21, [x29, #32]
    0.00 :   ffff80001047a7dc:       str     x27, [x29, #80]
         :                      bio_integrity_intervals():
         :                      * to the appropriate number of integrity intervals.
         :                      */
         :                      static inline unsigned int bio_integrity_intervals(struct blk_integrity *bi,
         :                      unsigned int sectors)
         :                      {
         :                      return sectors >> (bi->interval_exp - 9);
    0.00 :   ffff80001047a7e0:       ldrb    w20, [x26, #210]
         :                      bio_integrity_prep():
         :                      len = intervals * bi->tuple_size;
    0.00 :   ffff80001047a7e4:       ldrb    w0, [x26, #209]
         :                      bio_integrity_intervals():
    0.00 :   ffff80001047a7e8:       sub     w20, w20, #0x9
         :                      bio_integrity_prep():
         :                      buf = kmalloc(len, GFP_NOIO | q->bounce_gfp);
    0.00 :   ffff80001047a7ec:       ldr     w1, [x26, #120]
         :                      bio_integrity_intervals():
    0.00 :   ffff80001047a7f0:       lsr     w4, w4, w20
         :                      kmalloc():
         :                      return kmem_cache_alloc_trace(
         :                      kmalloc_caches[kmalloc_type(flags)][index],
         :                      flags, size);
         :                      #endif
         :                      }
         :                      return __kmalloc(size, flags);
    0.00 :   ffff80001047a7f4:       orr     w1, w1, #0xc00
         :                      bio_integrity_prep():
         :                      len = intervals * bi->tuple_size;
    0.00 :   ffff80001047a7f8:       mul     w21, w4, w0
         :                      kmalloc():
    0.00 :   ffff80001047a7fc:       mov     x0, x21
         :                      bio_integrity_prep():
    0.00 :   ffff80001047a800:       mov     x20, x21
         :                      kmalloc():
    0.00 :   ffff80001047a804:       bl      ffff800010251988 <__kmalloc>
    0.00 :   ffff80001047a808:       mov     x27, x0
         :                      bio_integrity_prep():
         :                      if (unlikely(buf == NULL)) {
    0.00 :   ffff80001047a80c:       cbz     x0, ffff80001047a950 <bio_integrity_prep+0x218>
         :                      end = (((unsigned long) buf) + len + PAGE_SIZE - 1) >> PAGE_SHIFT;
    0.00 :   ffff80001047a810:       add     x2, x21, #0xfff
    0.00 :   ffff80001047a814:       stp     x22, x23, [x29, #40]
    0.00 :   ffff80001047a818:       add     x2, x2, x0
         :                      start = ((unsigned long) buf) >> PAGE_SHIFT;
    0.00 :   ffff80001047a81c:       lsr     x22, x0, #12
         :                      bip = bio_integrity_alloc(bio, GFP_NOIO, nr_pages);
    0.00 :   ffff80001047a820:       mov     w1, #0xc00                      // #3072
    0.00 :   ffff80001047a824:       mov     x0, x19
         :                      end = (((unsigned long) buf) + len + PAGE_SIZE - 1) >> PAGE_SHIFT;
    0.00 :   ffff80001047a828:       lsr     x2, x2, #12
         :                      nr_pages = end - start;
    0.00 :   ffff80001047a82c:       sub     w22, w2, w22
         :                      bip = bio_integrity_alloc(bio, GFP_NOIO, nr_pages);
    0.00 :   ffff80001047a830:       mov     w2, w22
    0.00 :   ffff80001047a834:       bl      ffff80001047a5c0 <bio_integrity_alloc>
    0.00 :   ffff80001047a838:       mov     x23, x0
         :                      if (IS_ERR(bip)) {
    0.00 :   ffff80001047a83c:       cmn     x0, #0x1, lsl #12
    0.00 :   ffff80001047a840:       b.hi    ffff80001047a980 <bio_integrity_prep+0x248>  // b.pmore
         :                      bip->bip_flags |= BIP_BLOCK_INTEGRITY;
    0.00 :   ffff80001047a844:       ldrh    w0, [x0, #38]
         :                      bip->bip_iter.bi_size = len;
    0.00 :   ffff80001047a848:       str     w21, [x23, #16]
         :                      bip->bip_flags |= BIP_BLOCK_INTEGRITY;
    0.00 :   ffff80001047a84c:       orr     w1, w0, #0x1
    0.00 :   ffff80001047a850:       strh    w1, [x23, #38]
         :                      bip_set_seed(bip, bio->bi_iter.bi_sector);
    0.00 :   ffff80001047a854:       ldr     x1, [x19, #32]
         :                      bip_set_seed():
         :                      }
         :
         :                      static inline void bip_set_seed(struct bio_integrity_payload *bip,
         :                      sector_t seed)
         :                      {
         :                      bip->bip_iter.bi_sector = seed;
    0.00 :   ffff80001047a858:       str     x1, [x23, #8]
         :                      bio_integrity_prep():
         :                      if (bi->flags & BLK_INTEGRITY_IP_CHECKSUM)
    0.00 :   ffff80001047a85c:       ldrb    w1, [x26, #208]
    0.00 :   ffff80001047a860:       tbz     w1, #3, ffff80001047a870 <bio_integrity_prep+0x138>
         :                      bip->bip_flags |= BIP_IP_CHECKSUM;
    0.00 :   ffff80001047a864:       mov     w1, #0x11                       // #17
    0.00 :   ffff80001047a868:       orr     w0, w0, w1
    0.00 :   ffff80001047a86c:       strh    w0, [x23, #38]
         :                      offset = offset_in_page(buf);
    0.00 :   ffff80001047a870:       and     w3, w27, #0xfff
         :                      for (i = 0 ; i < nr_pages ; i++) {
    0.00 :   ffff80001047a874:       cbz     w22, ffff80001047a8f4 <bio_integrity_prep+0x1bc>
         :                      bytes = PAGE_SIZE - offset;
    0.00 :   ffff80001047a878:       mov     w21, #0x1000                    // #4096
    0.00 :   ffff80001047a87c:       sub     w21, w21, w3
         :                      if (len <= 0)
    0.00 :   ffff80001047a880:       cbz     w20, ffff80001047a8f4 <bio_integrity_prep+0x1bc>
    0.00 :   ffff80001047a884:       stp     x24, x25, [x29, #56]
         :                      ret = bio_integrity_add_page(bio, virt_to_page(buf),
    0.00 :   ffff80001047a888:       mov     x24, #0xffffffffffe00000        // #-2097152
    0.00 :   ffff80001047a88c:       str     x28, [x29, #88]
    0.00 :   ffff80001047a890:       mov     x25, #0x1000000000000           // #281474976710656
    0.00 :   ffff80001047a894:       movk    x24, #0xfdff, lsl #32
         :                      for (i = 0 ; i < nr_pages ; i++) {
    0.00 :   ffff80001047a898:       mov     w28, #0x0                       // #0
    0.00 :   ffff80001047a89c:       b       ffff80001047a8bc <bio_integrity_prep+0x184>
         :                      len -= bytes;
    0.00 :   ffff80001047a8a0:       sub     w20, w20, w21
         :                      buf += bytes;
    0.00 :   ffff80001047a8a4:       add     x27, x27, w21, uxtw
         :                      for (i = 0 ; i < nr_pages ; i++) {
    0.00 :   ffff80001047a8a8:       cmp     w22, w28
    0.00 :   ffff80001047a8ac:       b.eq    ffff80001047a8ec <bio_integrity_prep+0x1b4>  // b.none
         :                      offset = 0;
    0.00 :   ffff80001047a8b0:       mov     w3, #0x0                        // #0
         :                      bytes = PAGE_SIZE - offset;
    0.00 :   ffff80001047a8b4:       mov     w21, #0x1000                    // #4096
         :                      if (len <= 0)
    0.00 :   ffff80001047a8b8:       cbz     w20, ffff80001047a8ec <bio_integrity_prep+0x1b4>
         :                      ret = bio_integrity_add_page(bio, virt_to_page(buf),
    0.00 :   ffff80001047a8bc:       add     x1, x27, x25
    0.00 :   ffff80001047a8c0:       cmp     w20, w21
    0.00 :   ffff80001047a8c4:       csel    w21, w20, w21, ls  // ls = plast
         :                      for (i = 0 ; i < nr_pages ; i++) {
    0.00 :   ffff80001047a8c8:       add     w28, w28, #0x1
         :                      ret = bio_integrity_add_page(bio, virt_to_page(buf),
    0.00 :   ffff80001047a8cc:       lsr     x1, x1, #12
    0.00 :   ffff80001047a8d0:       mov     w2, w21
    0.00 :   ffff80001047a8d4:       mov     x0, x19
    0.00 :   ffff80001047a8d8:       add     x1, x24, x1, lsl #6
    0.00 :   ffff80001047a8dc:       bl      ffff80001047a1e8 <bio_integrity_add_page>
         :                      if (ret == 0) {
    0.00 :   ffff80001047a8e0:       cbz     w0, ffff80001047a99c <bio_integrity_prep+0x264>
         :                      if (ret < bytes)
    0.00 :   ffff80001047a8e4:       cmp     w0, w21
    0.00 :   ffff80001047a8e8:       b.cs    ffff80001047a8a0 <bio_integrity_prep+0x168>  // b.hs, b.nlast
    0.00 :   ffff80001047a8ec:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff80001047a8f0:       ldr     x28, [x29, #88]
         :                      op_is_write():
         :                      bio->bi_opf = op | op_flags;
         :                      }
         :
         :                      static inline bool op_is_write(unsigned int op)
         :                      {
         :                      return (op & 1);
    0.00 :   ffff80001047a8f4:       ldr     w0, [x19, #16]
         :                      bio_integrity_prep():
         :                      if (bio_data_dir(bio) == WRITE) {
    0.00 :   ffff80001047a8f8:       tbnz    w0, #0, ffff80001047a924 <bio_integrity_prep+0x1ec>
         :                      bip->bio_iter = bio->bi_iter;
    0.00 :   ffff80001047a8fc:       ldp     x2, x3, [x19, #32]
    0.00 :   ffff80001047a900:       stp     x2, x3, [x23, #40]
         :                      return true;
    0.00 :   ffff80001047a904:       mov     w1, #0x1                        // #1
         :                      bip->bio_iter = bio->bi_iter;
    0.00 :   ffff80001047a908:       ldr     x0, [x19, #48]
    0.00 :   ffff80001047a90c:       str     x0, [x23, #56]
    0.00 :   ffff80001047a910:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff80001047a914:       ldp     x21, x22, [x29, #32]
    0.00 :   ffff80001047a918:       ldr     x23, [x29, #48]
    0.00 :   ffff80001047a91c:       ldr     x27, [x29, #80]
    0.00 :   ffff80001047a920:       b       ffff80001047a778 <bio_integrity_prep+0x40>
         :                      bio_integrity_process(bio, &bio->bi_iter,
    0.00 :   ffff80001047a924:       ldr     x2, [x26, #200]
    0.00 :   ffff80001047a928:       add     x1, x19, #0x20
    0.00 :   ffff80001047a92c:       mov     x0, x19
    0.00 :   ffff80001047a930:       ldr     x2, [x2]
    0.00 :   ffff80001047a934:       bl      ffff80001047a328 <bio_integrity_process>
    0.00 :   ffff80001047a938:       ldr     x23, [x29, #48]
         :                      return true;
    0.00 :   ffff80001047a93c:       mov     w1, #0x1                        // #1
    0.00 :   ffff80001047a940:       ldr     x27, [x29, #80]
    0.00 :   ffff80001047a944:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff80001047a948:       ldp     x21, x22, [x29, #32]
    0.00 :   ffff80001047a94c:       b       ffff80001047a778 <bio_integrity_prep+0x40>
         :                      printk(KERN_ERR "could not allocate integrity buffer\n");
    0.00 :   ffff80001047a950:       adrp    x0, ffff8000111ac000 <kallsyms_token_index+0x32330>
    0.00 :   ffff80001047a954:       add     x0, x0, #0xa60
    0.00 :   ffff80001047a958:       bl      ffff800010148e94 <printk>
         :                      bio->bi_status = status;
    0.00 :   ffff80001047a95c:       mov     w0, #0x9                        // #9
    0.00 :   ffff80001047a960:       strb    w0, [x19, #26]
         :                      bio_endio(bio);
    0.00 :   ffff80001047a964:       mov     x0, x19
    0.00 :   ffff80001047a968:       bl      ffff800010450080 <bio_endio>
         :                      return false;
    0.00 :   ffff80001047a96c:       ldr     x21, [x29, #32]
    0.00 :   ffff80001047a970:       mov     w1, #0x0                        // #0
    0.00 :   ffff80001047a974:       ldr     x27, [x29, #80]
    0.00 :   ffff80001047a978:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff80001047a97c:       b       ffff80001047a778 <bio_integrity_prep+0x40>
         :                      printk(KERN_ERR "could not allocate data integrity bioset\n");
    0.00 :   ffff80001047a980:       adrp    x0, ffff8000111ac000 <kallsyms_token_index+0x32330>
    0.00 :   ffff80001047a984:       add     x0, x0, #0xa88
    0.00 :   ffff80001047a988:       bl      ffff800010148e94 <printk>
         :                      kfree(buf);
    0.00 :   ffff80001047a98c:       mov     x0, x27
    0.00 :   ffff80001047a990:       bl      ffff80001024fe88 <kfree>
         :                      goto err_end_io;
    0.00 :   ffff80001047a994:       ldp     x22, x23, [x29, #40]
    0.00 :   ffff80001047a998:       b       ffff80001047a95c <bio_integrity_prep+0x224>
         :                      printk(KERN_ERR "could not attach integrity payload\n");
    0.00 :   ffff80001047a99c:       adrp    x0, ffff8000111ac000 <kallsyms_token_index+0x32330>
    0.00 :   ffff80001047a9a0:       add     x0, x0, #0xab8
    0.00 :   ffff80001047a9a4:       bl      ffff800010148e94 <printk>
         :                      kfree(buf);
    0.00 :   ffff80001047a9a8:       mov     x0, x27
    0.00 :   ffff80001047a9ac:       bl      ffff80001024fe88 <kfree>
         :                      goto err_end_io;
    0.00 :   ffff80001047a9b0:       ldp     x22, x23, [x29, #40]
    0.00 :   ffff80001047a9b4:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff80001047a9b8:       ldr     x28, [x29, #88]
    0.00 :   ffff80001047a9bc:       b       ffff80001047a95c <bio_integrity_prep+0x224>
 Percent |	Source code & Disassembly of vmlinux for cycles (187 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000106fcb18 <iommu_get_dma_domain>:
         :                      iommu_get_dma_domain():
         :                      * For IOMMU_DOMAIN_DMA implementations which already provide their own
         :                      * guarantees that the group and its default domain are valid and correct.
         :                      */
         :                      struct iommu_domain *iommu_get_dma_domain(struct device *dev)
         :                      {
         :                      return dev->iommu_group->default_domain;
   17.55 :   ffff8000106fcb18:       ldr     x0, [x0, #736]
         :                      }
   64.69 :   ffff8000106fcb1c:       ldr     x0, [x0, #200]
   17.76 :   ffff8000106fcb20:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (285 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010461540 <blk_mq_request_issue_directly>:
         :                      blk_mq_request_issue_directly():
         :
         :                      hctx_unlock(hctx, srcu_idx);
         :                      }
         :
         :                      blk_status_t blk_mq_request_issue_directly(struct request *rq, bool last)
         :                      {
   54.02 :   ffff800010461540:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff800010461544:       mov     x29, sp
   10.52 :   ffff800010461548:       stp     x21, x22, [sp, #32]
    0.00 :   ffff80001046154c:       mov     x21, x0
    3.51 :   ffff800010461550:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010461554:       adrp    x19, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010461558:       add     x19, x19, #0x8c8
    0.00 :   ffff80001046155c:       and     w22, w1, #0xff
         :                      blk_status_t ret;
         :                      int srcu_idx;
         :                      blk_qc_t unused_cookie;
         :                      struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
    3.88 :   ffff800010461560:       ldr     x20, [x21, #16]
         :                      {
    4.92 :   ffff800010461564:       ldr     x0, [x19]
    0.71 :   ffff800010461568:       str     x0, [x29, #56]
    0.00 :   ffff80001046156c:       mov     x0, #0x0                        // #0
         :
         :                      hctx_lock(hctx, &srcu_idx);
    0.00 :   ffff800010461570:       add     x1, x29, #0x30
    0.00 :   ffff800010461574:       mov     x0, x20
    2.80 :   ffff800010461578:       bl      ffff80001045db78 <hctx_lock>
         :                      ret = __blk_mq_try_issue_directly(hctx, rq, &unused_cookie, true, last);
    2.79 :   ffff80001046157c:       add     x2, x29, #0x34
    0.00 :   ffff800010461580:       mov     x1, x21
    0.00 :   ffff800010461584:       mov     w4, w22
    0.00 :   ffff800010461588:       mov     w3, #0x1                        // #1
    2.83 :   ffff80001046158c:       mov     x0, x20
    0.35 :   ffff800010461590:       bl      ffff800010460aa0 <__blk_mq_try_issue_directly>
         :                      hctx_unlock(hctx, srcu_idx);
    1.74 :   ffff800010461594:       ldr     w1, [x29, #48]
         :                      ret = __blk_mq_try_issue_directly(hctx, rq, &unused_cookie, true, last);
    0.00 :   ffff800010461598:       mov     w21, w0
         :                      hctx_unlock(hctx, srcu_idx);
    0.00 :   ffff80001046159c:       mov     x0, x20
    0.34 :   ffff8000104615a0:       bl      ffff80001045dbc0 <hctx_unlock>
         :
         :                      return ret;
         :                      }
    0.00 :   ffff8000104615a4:       ldr     x2, [x29, #56]
    5.61 :   ffff8000104615a8:       ldr     x1, [x19]
    0.00 :   ffff8000104615ac:       eor     x1, x2, x1
    0.00 :   ffff8000104615b0:       cbnz    x1, ffff8000104615c8 <blk_mq_request_issue_directly+0x88>
    1.04 :   ffff8000104615b4:       mov     w0, w21
    2.12 :   ffff8000104615b8:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000104615bc:       ldp     x21, x22, [sp, #32]
    2.81 :   ffff8000104615c0:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000104615c4:       ret
    0.00 :   ffff8000104615c8:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (290 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010463688 <__blk_mq_get_tag>:
         :                      __blk_mq_get_tag():
         :                      return atomic_read(&hctx->nr_active) < depth;
         :                      }
         :
         :                      static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
         :                      struct sbitmap_queue *bt)
         :                      {
   93.10 :   ffff800010463688:       mov     x2, x0
         :                      if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
    0.00 :   ffff80001046368c:       ldr     w0, [x0, #8]
    0.00 :   ffff800010463690:       tbz     w0, #2, ffff8000104636c0 <__blk_mq_get_tag+0x38>
         :                      {
    5.17 :   ffff800010463694:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff800010463698:       mov     x0, x1
    0.00 :   ffff80001046369c:       mov     x29, sp
         :                      !hctx_may_queue(data->hctx, bt))
         :                      return -1;
         :                      if (data->shallow_depth)
    0.35 :   ffff8000104636a0:       ldr     w1, [x2, #12]
    0.00 :   ffff8000104636a4:       cbnz    w1, ffff8000104636b4 <__blk_mq_get_tag+0x2c>
         :                      return __sbitmap_queue_get_shallow(bt, data->shallow_depth);
         :                      else
         :                      return __sbitmap_queue_get(bt);
    0.00 :   ffff8000104636a8:       bl      ffff8000104abbb0 <__sbitmap_queue_get>
         :                      }
    0.00 :   ffff8000104636ac:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000104636b0:       ret
         :                      return __sbitmap_queue_get_shallow(bt, data->shallow_depth);
    0.00 :   ffff8000104636b4:       bl      ffff8000104aba28 <__sbitmap_queue_get_shallow>
         :                      }
    0.00 :   ffff8000104636b8:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000104636bc:       ret
         :                      !hctx_may_queue(data->hctx, bt))
    0.00 :   ffff8000104636c0:       ldr     x0, [x2, #32]
         :                      hctx_may_queue():
         :                      if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_SHARED))
    0.00 :   ffff8000104636c4:       cbz     x0, ffff800010463694 <__blk_mq_get_tag+0xc>
    1.37 :   ffff8000104636c8:       ldr     x3, [x0, #192]
    0.00 :   ffff8000104636cc:       tbz     w3, #1, ffff800010463694 <__blk_mq_get_tag+0xc>
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff8000104636d0:       ldr     x3, [x0, #24]
         :                      hctx_may_queue():
         :                      if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
    0.00 :   ffff8000104636d4:       tst     w3, #0x2
    0.00 :   ffff8000104636d8:       b.eq    ffff800010463694 <__blk_mq_get_tag+0xc>  // b.none
         :                      if (bt->sb.depth == 1)
    0.00 :   ffff8000104636dc:       ldr     w3, [x1]
    0.00 :   ffff8000104636e0:       cmp     w3, #0x1
    0.00 :   ffff8000104636e4:       b.eq    ffff800010463694 <__blk_mq_get_tag+0xc>  // b.none
         :                      users = atomic_read(&hctx->tags->active_queues);
    0.00 :   ffff8000104636e8:       ldr     x4, [x0, #336]
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000104636ec:       ldr     w4, [x4, #8]
         :                      hctx_may_queue():
         :                      if (!users)
    0.00 :   ffff8000104636f0:       cbz     w4, ffff800010463694 <__blk_mq_get_tag+0xc>
         :                      depth = max((bt->sb.depth + users - 1) / users, 4U);
    0.00 :   ffff8000104636f4:       sub     w3, w3, #0x1
         :                      __read_once_size():
    0.00 :   ffff8000104636f8:       ldr     w5, [x0, #432]
         :                      hctx_may_queue():
    0.00 :   ffff8000104636fc:       add     w3, w3, w4
    0.00 :   ffff800010463700:       mov     w6, #0x4                        // #4
         :                      __blk_mq_get_tag():
         :                      return -1;
    0.00 :   ffff800010463704:       mov     w0, #0xffffffff                 // #-1
         :                      hctx_may_queue():
         :                      depth = max((bt->sb.depth + users - 1) / users, 4U);
    0.00 :   ffff800010463708:       udiv    w3, w3, w4
    0.00 :   ffff80001046370c:       cmp     w3, w6
    0.00 :   ffff800010463710:       csel    w3, w3, w6, cs  // cs = hs, nlast
         :                      __blk_mq_get_tag():
         :                      if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
    0.00 :   ffff800010463714:       cmp     w3, w5
    0.00 :   ffff800010463718:       b.hi    ffff800010463694 <__blk_mq_get_tag+0xc>  // b.pmore
         :                      }
    0.00 :   ffff80001046371c:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (176 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010cb2670 <_raw_spin_trylock>:
         :                      _raw_spin_trylock():
         :
         :                      #endif
         :
         :                      #ifndef CONFIG_INLINE_SPIN_TRYLOCK
         :                      int __lockfunc _raw_spin_trylock(raw_spinlock_t *lock)
         :                      {
    0.00 :   ffff800010cb2670:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff800010cb2674:       mov     x3, x0
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800010cb2678:       mrs     x2, sp_el0
         :                      _raw_spin_trylock():
    0.00 :   ffff800010cb267c:       mov     x29, sp
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010cb2680:       ldr     w1, [x2, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff800010cb2684:       add     w1, w1, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    1.16 :   ffff800010cb2688:       str     w1, [x2, #16]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010cb268c:       ldr     w0, [x0]
         :                      queued_spin_trylock():
         :                      */
         :                      static __always_inline int queued_spin_trylock(struct qspinlock *lock)
         :                      {
         :                      u32 val = atomic_read(&lock->val);
         :
         :                      if (unlikely(val))
    1.17 :   ffff800010cb2690:       cbnz    w0, ffff800010cb26d8 <_raw_spin_trylock+0x68>
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
   20.61 :   ffff800010cb2694:       b       ffff800010cb26bc <_raw_spin_trylock+0x4c>
    0.00 :   ffff800010cb2698:       b       ffff800010cb26bc <_raw_spin_trylock+0x4c>
         :                      __lse__cmpxchg_case_acq_32():
         :                      __CMPXCHG_CASE(w, h,     , 16,   )
         :                      __CMPXCHG_CASE(w,  ,     , 32,   )
         :                      __CMPXCHG_CASE(x,  ,     , 64,   )
         :                      __CMPXCHG_CASE(w, b, acq_,  8,  a, "memory")
         :                      __CMPXCHG_CASE(w, h, acq_, 16,  a, "memory")
         :                      __CMPXCHG_CASE(w,  , acq_, 32,  a, "memory")
    0.00 :   ffff800010cb269c:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010cb26a0:       mov     x0, x3
    0.00 :   ffff800010cb26a4:       mov     w2, #0x1                        // #1
    0.00 :   ffff800010cb26a8:       mov     w4, w1
    0.00 :   ffff800010cb26ac:       casa    w4, w2, [x3]
   76.49 :   ffff800010cb26b0:       mov     w0, w4
    0.00 :   ffff800010cb26b4:       mov     w1, w0
    0.00 :   ffff800010cb26b8:       b       ffff800010cb26c8 <_raw_spin_trylock+0x58>
         :                      __ll_sc__cmpxchg_case_acq_32():
         :                      __CMPXCHG_CASE(w, h,     , 16,        ,  ,  ,         , K)
         :                      __CMPXCHG_CASE(w,  ,     , 32,        ,  ,  ,         , K)
         :                      __CMPXCHG_CASE( ,  ,     , 64,        ,  ,  ,         , L)
         :                      __CMPXCHG_CASE(w, b, acq_,  8,        , a,  , "memory", K)
         :                      __CMPXCHG_CASE(w, h, acq_, 16,        , a,  , "memory", K)
         :                      __CMPXCHG_CASE(w,  , acq_, 32,        , a,  , "memory", K)
    0.00 :   ffff800010cb26bc:       mov     x0, #0x0                        // #0
    0.00 :   ffff800010cb26c0:       mov     w2, #0x1                        // #1
    0.00 :   ffff800010cb26c4:       b       ffff800010cb2f40 <_raw_write_lock_irqsave+0xf0>
         :                      __raw_spin_trylock():
         :                      static inline int __raw_spin_trylock(raw_spinlock_t *lock)
         :                      {
         :                      preempt_disable();
         :                      if (do_raw_spin_trylock(lock)) {
         :                      spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
         :                      return 1;
    0.00 :   ffff800010cb26c8:       mov     w0, #0x1                        // #1
         :                      if (do_raw_spin_trylock(lock)) {
    0.00 :   ffff800010cb26cc:       cbnz    w1, ffff800010cb26d8 <_raw_spin_trylock+0x68>
         :                      _raw_spin_trylock():
         :                      return __raw_spin_trylock(lock);
         :                      }
    0.00 :   ffff800010cb26d0:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010cb26d4:       ret
         :                      get_current():
    0.00 :   ffff800010cb26d8:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.00 :   ffff800010cb26dc:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.57 :   ffff800010cb26e0:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010cb26e4:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800010cb26e8:       cbz     x0, ffff800010cb26f4 <_raw_spin_trylock+0x84>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010cb26ec:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff800010cb26f0:       cbnz    x0, ffff800010cb26f8 <_raw_spin_trylock+0x88>
         :                      __raw_spin_trylock():
         :                      }
         :                      preempt_enable();
    0.00 :   ffff800010cb26f4:       bl      ffff800010cad640 <preempt_schedule>
         :                      return 0;
    0.00 :   ffff800010cb26f8:       mov     w0, #0x0                        // #0
         :                      _raw_spin_trylock():
    0.00 :   ffff800010cb26fc:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010cb2700:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (278 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000107076c8 <arm_smmu_map>:
         :                      arm_smmu_map():
         :                      }
         :
         :                      static int arm_smmu_map(struct iommu_domain *domain, unsigned long iova,
         :                      phys_addr_t paddr, size_t size, int prot, gfp_t gfp)
         :                      {
         :                      struct io_pgtable_ops *ops = to_smmu_domain(domain)->pgtbl_ops;
    1.07 :   ffff8000107076c8:       ldur    x0, [x0, #-72]
         :
         :                      if (!ops)
    0.00 :   ffff8000107076cc:       cbz     x0, ffff8000107076e8 <arm_smmu_map+0x20>
         :                      {
    8.61 :   ffff8000107076d0:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff8000107076d4:       mov     x29, sp
         :                      return -ENODEV;
         :
         :                      return ops->map(ops, iova, paddr, size, prot);
    6.48 :   ffff8000107076d8:       ldr     x5, [x0]
    0.00 :   ffff8000107076dc:       blr     x5
         :                      }
   83.84 :   ffff8000107076e0:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000107076e4:       ret
         :                      return -ENODEV;
    0.00 :   ffff8000107076e8:       mov     w0, #0xffffffed                 // #-19
         :                      }
    0.00 :   ffff8000107076ec:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (174 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010124ce8 <load_balance>:
         :                      load_balance():
         :                      * tasks if there is an imbalance.
         :                      */
         :                      static int load_balance(int this_cpu, struct rq *this_rq,
         :                      struct sched_domain *sd, enum cpu_idle_type idle,
         :                      int *continue_balancing)
         :                      {
    0.00 :   ffff800010124ce8:       stp     x29, x30, [sp, #-256]!
    0.00 :   ffff800010124cec:       mov     x7, x2
    0.00 :   ffff800010124cf0:       adrp    x5, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010124cf4:       add     x5, x5, #0x8c8
    0.49 :   ffff800010124cf8:       mov     x29, sp
    0.00 :   ffff800010124cfc:       str     x22, [sp, #40]
         :                      int ld_moved, cur_ld_moved, active_balance = 0;
         :                      struct sched_domain *sd_parent = sd->parent;
         :                      struct sched_group *group;
         :                      struct rq *busiest;
         :                      struct rq_flags rf;
         :                      struct cpumask *cpus = this_cpu_cpumask_var_ptr(load_balance_mask);
    0.00 :   ffff800010124d00:       adrp    x22, ffff8000114cc000 <kernel_cpustat+0x48>
         :                      {
    0.00 :   ffff800010124d04:       str     x26, [sp, #72]
         :                      struct cpumask *cpus = this_cpu_cpumask_var_ptr(load_balance_mask);
    0.00 :   ffff800010124d08:       add     x22, x22, #0x50
         :                      {
    0.00 :   ffff800010124d0c:       str     x2, [x29, #128]
         :
         :                      struct lb_env env = {
    0.00 :   ffff800010124d10:       str     x2, [x29, #144]
    0.00 :   ffff800010124d14:       str     x1, [x29, #168]
         :                      {
    0.00 :   ffff800010124d18:       ldr     x1, [x5]
    0.47 :   ffff800010124d1c:       str     x1, [x29, #248]
    0.00 :   ffff800010124d20:       mov     x1, #0x0                        // #0
         :                      sched_group_span():
         :                      unsigned long           cpumask[0];
         :                      };
         :
         :                      static inline struct cpumask *sched_group_span(struct sched_group *sg)
         :                      {
         :                      return to_cpumask(sg->cpumask);
    0.00 :   ffff800010124d24:       ldr     x2, [x2, #16]
         :                      load_balance():
         :                      struct sched_domain *sd_parent = sd->parent;
    0.00 :   ffff800010124d28:       ldr     x1, [x7]
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800010124d2c:       mrs     x6, tpidr_el1
         :                      load_balance():
         :                      struct cpumask *cpus = this_cpu_cpumask_var_ptr(load_balance_mask);
    0.00 :   ffff800010124d30:       add     x6, x22, x6
         :                      struct lb_env env = {
    0.00 :   ffff800010124d34:       stp     xzr, xzr, [x29, #152]
         :                      sched_group_span():
    0.00 :   ffff800010124d38:       add     x2, x2, #0x20
         :                      load_balance():
         :                      {
    0.00 :   ffff800010124d3c:       stp     w0, w3, [x29, #104]
         :                      struct sched_domain *sd_parent = sd->parent;
    0.70 :   ffff800010124d40:       str     x1, [x29, #120]
         :                      struct lb_env env = {
    0.00 :   ffff800010124d44:       mov     w1, #0x2                        // #2
    0.00 :   ffff800010124d48:       str     w0, [x29, #164]
    0.00 :   ffff800010124d4c:       add     x0, x29, #0xe8
    0.00 :   ffff800010124d50:       str     x2, [x29, #176]
    0.00 :   ffff800010124d54:       mov     w2, #0x20                       // #32
    0.60 :   ffff800010124d58:       stp     xzr, xzr, [x29, #184]
    0.00 :   ffff800010124d5c:       stp     xzr, xzr, [x29, #200]
    0.00 :   ffff800010124d60:       stp     xzr, xzr, [x29, #216]
         :                      struct cpumask *cpus = this_cpu_cpumask_var_ptr(load_balance_mask);
    1.97 :   ffff800010124d64:       str     x6, [x29, #96]
         :                      {
    4.55 :   ffff800010124d68:       str     x4, [x29, #112]
         :                      struct lb_env env = {
    0.00 :   ffff800010124d6c:       str     w3, [x29, #188]
         :                      bitmap_and():
         :                      static inline int bitmap_and(unsigned long *dst, const unsigned long *src1,
         :                      const unsigned long *src2, unsigned int nbits)
         :                      {
         :                      if (small_const_nbits(nbits))
         :                      return (*dst = *src1 & *src2 & BITMAP_LAST_WORD_MASK(nbits)) != 0;
         :                      return __bitmap_and(dst, src1, src2, nbits);
    0.00 :   ffff800010124d70:       mov     w3, #0x100                      // #256
         :                      load_balance():
    0.00 :   ffff800010124d74:       str     x6, [x29, #200]
    0.68 :   ffff800010124d78:       str     w2, [x29, #216]
         :                      bitmap_and():
    0.00 :   ffff800010124d7c:       adrp    x2, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      load_balance():
    0.70 :   ffff800010124d80:       str     w1, [x29, #224]
         :                      bitmap_and():
    0.00 :   ffff800010124d84:       add     x2, x2, #0x188
         :                      load_balance():
    0.49 :   ffff800010124d88:       stp     x0, x0, [x29, #232]
         :                      bitmap_and():
    0.00 :   ffff800010124d8c:       add     x1, x7, #0x88
    0.00 :   ffff800010124d90:       mov     x0, x6
    0.00 :   ffff800010124d94:       bl      ffff80001047f4e0 <__bitmap_and>
         :                      should_we_balance():
         :                      if (!cpumask_test_cpu(env->dst_cpu, env->cpus))
    0.00 :   ffff800010124d98:       ldr     w1, [x29, #164]
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff800010124d9c:       ldr     x2, [x29, #200]
    0.00 :   ffff800010124da0:       add     w0, w1, #0x3f
    0.00 :   ffff800010124da4:       cmp     w1, #0x0
    0.00 :   ffff800010124da8:       csel    w0, w0, w1, lt  // lt = tstop
    0.00 :   ffff800010124dac:       asr     w0, w0, #6
    0.00 :   ffff800010124db0:       sxtw    x0, w0
    0.00 :   ffff800010124db4:       ldr     x0, [x2, x0, lsl #3]
    0.00 :   ffff800010124db8:       lsr     x1, x0, x1
         :                      should_we_balance():
    0.00 :   ffff800010124dbc:       tbz     w1, #0, ffff800010125428 <load_balance+0x740>
         :                      struct sched_group *sg = env->sd->groups;
    3.29 :   ffff800010124dc0:       ldr     x0, [x29, #144]
    0.67 :   ffff800010124dc4:       stp     x19, x20, [x29, #16]
         :                      find_busiest_queue():
         :                      rq = cpu_rq(i);
    0.00 :   ffff800010124dc8:       adrp    x20, ffff8000114d5000 <tegra_to+0x180>
    0.00 :   ffff800010124dcc:       str     x21, [x29, #32]
    0.00 :   ffff800010124dd0:       add     x20, x20, #0xd80
    0.00 :   ffff800010124dd4:       stp     x23, x24, [x29, #48]
    0.00 :   ffff800010124dd8:       str     x25, [x29, #64]
    0.56 :   ffff800010124ddc:       stp     x27, x28, [x29, #80]
         :                      should_we_balance():
         :                      struct sched_group *sg = env->sd->groups;
    0.69 :   ffff800010124de0:       ldr     x19, [x0, #16]
         :                      if (env->idle == CPU_NEWLY_IDLE)
    0.00 :   ffff800010124de4:       ldr     w0, [x29, #188]
    0.00 :   ffff800010124de8:       cmp     w0, #0x2
    0.00 :   ffff800010124dec:       b.eq    ffff800010124e38 <load_balance+0x150>  // b.none
    0.00 :   ffff800010124df0:       adrp    x5, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      for_each_cpu_and(cpu, group_balance_mask(sg), env->cpus) {
    0.00 :   ffff800010124df4:       add     x21, x5, #0x2b4
    0.00 :   ffff800010124df8:       mov     w23, #0xffffffff                // #-1
         :                      group_balance_mask():
         :                      /*
         :                      * See build_balance_mask().
         :                      */
         :                      static inline struct cpumask *group_balance_mask(struct sched_group *sg)
         :                      {
         :                      return to_cpumask(sg->sgc->cpumask);
    0.65 :   ffff800010124dfc:       ldr     x1, [x19, #16]
         :                      should_we_balance():
    0.00 :   ffff800010124e00:       mov     w0, w23
    0.00 :   ffff800010124e04:       add     x1, x1, #0x30
    0.52 :   ffff800010124e08:       bl      ffff800010c93b70 <cpumask_next_and>
    0.59 :   ffff800010124e0c:       ldr     w1, [x21]
    0.00 :   ffff800010124e10:       mov     w23, w0
    0.00 :   ffff800010124e14:       cmp     w0, w1
    0.00 :   ffff800010124e18:       b.cs    ffff8000101256e0 <load_balance+0x9f8>  // b.hs, b.nlast
         :                      if (!idle_cpu(cpu))
    0.00 :   ffff800010124e1c:       bl      ffff800010117488 <idle_cpu>
    0.00 :   ffff800010124e20:       cbz     w0, ffff800010125644 <load_balance+0x95c>
         :                      if (balance_cpu == -1)
    0.00 :   ffff800010124e24:       cmn     w23, #0x1
    0.00 :   ffff800010124e28:       b.eq    ffff8000101256e0 <load_balance+0x9f8>  // b.none
         :                      load_balance():
         :                      cpumask_and(cpus, sched_domain_span(sd), cpu_active_mask);
         :
         :                      schedstat_inc(sd->lb_count[idle]);
         :
         :                      redo:
         :                      if (!should_we_balance(&env)) {
    0.46 :   ffff800010124e2c:       ldr     w0, [x29, #164]
    0.00 :   ffff800010124e30:       cmp     w0, w23
    0.00 :   ffff800010124e34:       b.ne    ffff800010125410 <load_balance+0x728>  // b.any
         :                      *continue_balancing = 0;
         :                      goto out_balanced;
         :                      }
         :
         :                      group = find_busiest_group(&env);
    0.00 :   ffff800010124e38:       add     x0, x29, #0x90
    0.00 :   ffff800010124e3c:       bl      ffff800010124310 <find_busiest_group>
         :                      if (!group) {
    0.00 :   ffff800010124e40:       cbz     x0, ffff80001012585c <load_balance+0xb74>
    0.00 :   ffff800010124e44:       adrp    x5, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      find_busiest_queue():
         :                      rq = cpu_rq(i);
    0.00 :   ffff800010124e48:       adrp    x21, ffff800011899000 <page_wait_table+0x1500>
         :                      unsigned long busiest_util = 0, busiest_load = 0, busiest_capacity = 1;
    0.00 :   ffff800010124e4c:       mov     x1, #0x1                        // #1
         :                      for_each_cpu_and(i, sched_group_span(group), env->cpus) {
    0.00 :   ffff800010124e50:       add     x28, x5, #0x2b4
         :                      unsigned long busiest_util = 0, busiest_load = 0, busiest_capacity = 1;
    0.00 :   ffff800010124e54:       mov     x22, x1
         :                      rq = cpu_rq(i);
    0.00 :   ffff800010124e58:       add     x21, x21, #0x8e8
    0.00 :   ffff800010124e5c:       add     x23, x0, #0x20
         :                      unsigned int busiest_nr = 0;
    0.00 :   ffff800010124e60:       mov     w27, #0x0                       // #0
         :                      for_each_cpu_and(i, sched_group_span(group), env->cpus) {
    0.00 :   ffff800010124e64:       mov     w0, #0xffffffff                 // #-1
         :                      unsigned long busiest_util = 0, busiest_load = 0, busiest_capacity = 1;
    0.00 :   ffff800010124e68:       mov     x25, #0x0                       // #0
    0.00 :   ffff800010124e6c:       mov     x26, #0x0                       // #0
         :                      struct rq *busiest = NULL, *rq;
    0.00 :   ffff800010124e70:       mov     x19, #0x0                       // #0
    0.00 :   ffff800010124e74:       nop
         :                      for_each_cpu_and(i, sched_group_span(group), env->cpus) {
    0.00 :   ffff800010124e78:       ldr     x2, [x29, #200]
    0.00 :   ffff800010124e7c:       mov     x1, x23
    0.00 :   ffff800010124e80:       bl      ffff800010c93b70 <cpumask_next_and>
    0.00 :   ffff800010124e84:       ldr     w1, [x28]
    0.00 :   ffff800010124e88:       cmp     w0, w1
    0.00 :   ffff800010124e8c:       b.cs    ffff800010124f50 <load_balance+0x268>  // b.hs, b.nlast
         :                      rq = cpu_rq(i);
    0.00 :   ffff800010124e90:       ldr     x2, [x21, w0, sxtw #3]
    0.00 :   ffff800010124e94:       mov     x1, x20
    0.00 :   ffff800010124e98:       add     x1, x1, x2
         :                      fbq_classify_rq():
         :                      if (rq->nr_running > rq->nr_numa_running)
    0.00 :   ffff800010124e9c:       ldp     w3, w4, [x1, #4]
    0.00 :   ffff800010124ea0:       cmp     w3, w4
    0.00 :   ffff800010124ea4:       b.hi    ffff800010124ec4 <load_balance+0x1dc>  // b.pmore
         :                      if (rq->nr_running > rq->nr_preferred_running)
    1.31 :   ffff800010124ea8:       ldr     w6, [x1, #12]
         :                      find_busiest_queue():
         :                      if (rt > env->fbq_type)
    0.00 :   ffff800010124eac:       ldr     w4, [x29, #224]
         :                      fbq_classify_rq():
         :                      return remote;
    0.00 :   ffff800010124eb0:       cmp     w3, w6
    0.00 :   ffff800010124eb4:       cset    w3, ls  // ls = plast
    0.00 :   ffff800010124eb8:       add     w3, w3, #0x1
         :                      find_busiest_queue():
         :                      if (rt > env->fbq_type)
    0.00 :   ffff800010124ebc:       cmp     w4, w3
    0.00 :   ffff800010124ec0:       b.cc    ffff800010124e78 <load_balance+0x190>  // b.lo, b.ul, b.last
         :                      if (env->sd->flags & SD_ASYM_CPUCAPACITY &&
    0.00 :   ffff800010124ec4:       ldr     x6, [x29, #144]
         :                      capacity_of():
         :                      return cpu_rq(cpu)->cpu_capacity;
    0.00 :   ffff800010124ec8:       mov     x3, x20
    0.00 :   ffff800010124ecc:       add     x2, x2, x3
         :                      find_busiest_queue():
         :                      nr_running = rq->cfs.h_nr_running;
    0.00 :   ffff800010124ed0:       ldr     w8, [x1, #156]
         :                      if (env->sd->flags & SD_ASYM_CPUCAPACITY &&
    0.00 :   ffff800010124ed4:       ldr     w7, [x6, #56]
         :                      capacity_of():
         :                      return cpu_rq(cpu)->cpu_capacity;
    0.00 :   ffff800010124ed8:       ldr     x4, [x2, #2480]
         :                      find_busiest_queue():
         :                      if (env->sd->flags & SD_ASYM_CPUCAPACITY &&
    0.00 :   ffff800010124edc:       tbz     w7, #6, ffff8000101251a4 <load_balance+0x4bc>
         :                      capacity_of():
         :                      return cpu_rq(cpu)->cpu_capacity;
    0.00 :   ffff800010124ee0:       ldrsw   x2, [x29, #164]
    0.00 :   ffff800010124ee4:       ldr     x2, [x21, x2, lsl #3]
    0.00 :   ffff800010124ee8:       add     x3, x2, x3
         :                      find_busiest_queue():
         :                      if (env->sd->flags & SD_ASYM_CPUCAPACITY &&
    0.00 :   ffff800010124eec:       ldr     x2, [x3, #2480]
    0.00 :   ffff800010124ef0:       cmp     x4, x2
    0.00 :   ffff800010124ef4:       b.ls    ffff8000101251a4 <load_balance+0x4bc>  // b.plast
         :                      capacity_of(env->dst_cpu) < capacity &&
    0.00 :   ffff800010124ef8:       cmp     w8, #0x1
    0.00 :   ffff800010124efc:       b.eq    ffff800010124e78 <load_balance+0x190>  // b.none
         :                      switch (env->migration_type) {
    0.00 :   ffff800010124f00:       ldr     w2, [x29, #228]
    0.00 :   ffff800010124f04:       cmp     w2, #0x1
    0.00 :   ffff800010124f08:       b.eq    ffff8000101253bc <load_balance+0x6d4>  // b.none
    0.00 :   ffff800010124f0c:       cbz     w2, ffff80001012551c <load_balance+0x834>
    0.00 :   ffff800010124f10:       cmp     w2, #0x2
    0.00 :   ffff800010124f14:       b.eq    ffff8000101253fc <load_balance+0x714>  // b.none
    0.00 :   ffff800010124f18:       cmp     w2, #0x3
    0.00 :   ffff800010124f1c:       b.ne    ffff800010124e78 <load_balance+0x190>  // b.any
         :                      if (rq->misfit_task_load > busiest_load) {
    0.00 :   ffff800010124f20:       ldr     x2, [x1, #2512]
         :                      busiest = rq;
    0.00 :   ffff800010124f24:       cmp     x25, x2
    0.00 :   ffff800010124f28:       csel    x3, x2, x25, cc  // cc = lo, ul, last
         :                      for_each_cpu_and(i, sched_group_span(group), env->cpus) {
    0.00 :   ffff800010124f2c:       ldr     x2, [x29, #200]
         :                      busiest = rq;
    0.00 :   ffff800010124f30:       csel    x19, x1, x19, cc  // cc = lo, ul, last
    0.00 :   ffff800010124f34:       mov     x25, x3
         :                      for_each_cpu_and(i, sched_group_span(group), env->cpus) {
    0.00 :   ffff800010124f38:       mov     x1, x23
    0.00 :   ffff800010124f3c:       bl      ffff800010c93b70 <cpumask_next_and>
    0.00 :   ffff800010124f40:       ldr     w1, [x28]
    0.00 :   ffff800010124f44:       cmp     w0, w1
    0.00 :   ffff800010124f48:       b.cc    ffff800010124e90 <load_balance+0x1a8>  // b.lo, b.ul, b.last
    0.00 :   ffff800010124f4c:       nop
         :                      load_balance():
         :                      schedstat_inc(sd->lb_nobusyg[idle]);
         :                      goto out_balanced;
         :                      }
         :
         :                      busiest = find_busiest_queue(&env, group);
         :                      if (!busiest) {
    0.00 :   ffff800010124f50:       cbz     x19, ffff80001012585c <load_balance+0xb74>
         :                      schedstat_inc(sd->lb_nobusyq[idle]);
         :                      goto out_balanced;
         :                      }
         :
         :                      BUG_ON(busiest == env.dst_rq);
    0.00 :   ffff800010124f54:       ldr     x0, [x29, #168]
    0.00 :   ffff800010124f58:       cmp     x0, x19
    0.00 :   ffff800010124f5c:       b.eq    ffff80001012564c <load_balance+0x964>  // b.none
         :
         :                      schedstat_add(sd->lb_imbalance[idle], env.imbalance);
         :
         :                      env.src_cpu = busiest->cpu;
    0.00 :   ffff800010124f60:       ldr     w0, [x19, #2568]
         :                      env.src_rq = busiest;
    1.10 :   ffff800010124f64:       str     x19, [x29, #152]
         :                      env.src_cpu = busiest->cpu;
    0.00 :   ffff800010124f68:       str     w0, [x29, #160]
         :
         :                      ld_moved = 0;
         :                      if (busiest->nr_running > 1) {
    0.00 :   ffff800010124f6c:       ldr     w0, [x19, #4]
    0.00 :   ffff800010124f70:       cmp     w0, #0x1
    0.00 :   ffff800010124f74:       b.ls    ffff800010125480 <load_balance+0x798>  // b.plast
         :                      * Attempt to move tasks. If find_busiest_group has found
         :                      * an imbalance but busiest->nr_running <= 1, the group is
         :                      * still unbalanced. ld_moved simply stays zero, so it is
         :                      * correctly treated as an imbalance.
         :                      */
         :                      env.flags |= LBF_ALL_PINNED;
    0.00 :   ffff800010124f78:       ldr     w0, [x29, #208]
         :                      env.loop_max  = min(sysctl_sched_nr_migrate, busiest->nr_running);
    0.00 :   ffff800010124f7c:       adrp    x1, ffff800010ce6000 <sched_prio_to_weight+0x10>
         :                      capacity_of():
         :                      return cpu_rq(cpu)->cpu_capacity;
    0.00 :   ffff800010124f80:       adrp    x27, ffff800011899000 <page_wait_table+0x1500>
         :                      load_balance():
         :                      ld_moved = 0;
    0.00 :   ffff800010124f84:       mov     w26, #0x0                       // #0
         :                      env.flags |= LBF_ALL_PINNED;
    0.00 :   ffff800010124f88:       orr     w0, w0, #0x1
    0.00 :   ffff800010124f8c:       str     w0, [x29, #208]
         :                      env.loop_max  = min(sysctl_sched_nr_migrate, busiest->nr_running);
    0.00 :   ffff800010124f90:       ldr     w0, [x1, #340]
         :                      capacity_of():
         :                      return cpu_rq(cpu)->cpu_capacity;
    0.00 :   ffff800010124f94:       add     x2, x27, #0x8e8
         :                      load_balance():
         :                      env.loop_max  = min(sysctl_sched_nr_migrate, busiest->nr_running);
    0.00 :   ffff800010124f98:       ldr     w1, [x19, #4]
         :                      capacity_of():
         :                      return cpu_rq(cpu)->cpu_capacity;
    0.00 :   ffff800010124f9c:       str     x2, [x29, #136]
         :                      load_balance():
         :                      env.loop_max  = min(sysctl_sched_nr_migrate, busiest->nr_running);
    0.00 :   ffff800010124fa0:       cmp     w0, w1
    0.00 :   ffff800010124fa4:       csel    w0, w0, w1, ls  // ls = plast
    0.00 :   ffff800010124fa8:       str     w0, [x29, #220]
    0.00 :   ffff800010124fac:       nop
         :                      rq_lock_irqsave():
         :                      raw_spin_lock_irqsave(&rq->lock, rf->flags);
    0.00 :   ffff800010124fb0:       mov     x0, x19
    0.00 :   ffff800010124fb4:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
    0.00 :   ffff800010124fb8:       mov     x25, x0
         :                      load_balance():
         :
         :                      more_balance:
         :                      rq_lock_irqsave(busiest, &rf);
         :                      update_rq_clock(busiest);
    0.00 :   ffff800010124fbc:       mov     x0, x19
    0.00 :   ffff800010124fc0:       bl      ffff8000101133a8 <update_rq_clock>
         :                      detach_tasks():
         :                      if (env->imbalance <= 0)
    0.00 :   ffff800010124fc4:       ldr     x0, [x29, #192]
    0.00 :   ffff800010124fc8:       cmp     x0, #0x0
    0.00 :   ffff800010124fcc:       b.le    ffff800010125198 <load_balance+0x4b0>
         :                      struct list_head *tasks = &env->src_rq->cfs_tasks;
    0.00 :   ffff800010124fd0:       ldr     x28, [x29, #152]
         :                      int detached = 0;
    0.00 :   ffff800010124fd4:       mov     w23, #0x0                       // #0
         :                      struct list_head *tasks = &env->src_rq->cfs_tasks;
    0.00 :   ffff800010124fd8:       add     x21, x28, #0xa10
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010124fdc:       ldr     x0, [x28, #2576]
         :                      detach_tasks():
         :                      while (!list_empty(tasks)) {
    0.00 :   ffff800010124fe0:       cmp     x21, x0
    0.00 :   ffff800010124fe4:       b.eq    ffff8000101250fc <load_balance+0x414>  // b.none
         :                      if (env->idle != CPU_NOT_IDLE && env->src_rq->nr_running <= 1)
    0.00 :   ffff800010124fe8:       ldr     w0, [x29, #188]
    0.00 :   ffff800010124fec:       cmp     w0, #0x1
    0.00 :   ffff800010124ff0:       b.eq    ffff800010125004 <load_balance+0x31c>  // b.none
    0.00 :   ffff800010124ff4:       ldr     x0, [x29, #152]
    0.00 :   ffff800010124ff8:       ldr     w0, [x0, #4]
    0.00 :   ffff800010124ffc:       cmp     w0, #0x1
    0.00 :   ffff800010125000:       b.ls    ffff8000101250fc <load_balance+0x414>  // b.plast
         :                      p = list_last_entry(tasks, struct task_struct, se.group_node);
    0.00 :   ffff800010125004:       add     x27, x28, #0xa00
         :                      env->loop++;
    0.00 :   ffff800010125008:       ldr     w0, [x29, #212]
         :                      if (env->loop > env->loop_max)
    0.00 :   ffff80001012500c:       ldr     w1, [x29, #220]
         :                      env->loop++;
    0.00 :   ffff800010125010:       add     w0, w0, #0x1
         :                      p = list_last_entry(tasks, struct task_struct, se.group_node);
    0.00 :   ffff800010125014:       ldr     x22, [x27, #24]
         :                      env->loop++;
    0.00 :   ffff800010125018:       str     w0, [x29, #212]
         :                      if (env->loop > env->loop_max)
    0.00 :   ffff80001012501c:       cmp     w0, w1
         :                      p = list_last_entry(tasks, struct task_struct, se.group_node);
    0.00 :   ffff800010125020:       sub     x24, x22, #0xf0
         :                      if (env->loop > env->loop_max)
    0.00 :   ffff800010125024:       b.hi    ffff8000101250fc <load_balance+0x414>  // b.pmore
         :                      if (env->loop > env->loop_break) {
    0.00 :   ffff800010125028:       ldr     w1, [x29, #216]
    0.00 :   ffff80001012502c:       cmp     w0, w1
    0.00 :   ffff800010125030:       b.hi    ffff8000101253a4 <load_balance+0x6bc>  // b.pmore
         :                      if (!can_migrate_task(p, env))
    0.00 :   ffff800010125034:       add     x1, x29, #0x90
    0.00 :   ffff800010125038:       mov     x0, x24
    0.00 :   ffff80001012503c:       bl      ffff8000101219d8 <can_migrate_task>
    0.00 :   ffff800010125040:       cbz     w0, ffff8000101251e0 <load_balance+0x4f8>
         :                      switch (env->migration_type) {
    0.00 :   ffff800010125044:       ldr     w0, [x29, #228]
    0.00 :   ffff800010125048:       cmp     w0, #0x1
    0.00 :   ffff80001012504c:       b.eq    ffff800010125284 <load_balance+0x59c>  // b.none
    0.00 :   ffff800010125050:       cbz     w0, ffff8000101252cc <load_balance+0x5e4>
    0.00 :   ffff800010125054:       cmp     w0, #0x2
    0.00 :   ffff800010125058:       b.eq    ffff8000101252bc <load_balance+0x5d4>  // b.none
    0.00 :   ffff80001012505c:       cmp     w0, #0x3
    0.00 :   ffff800010125060:       b.ne    ffff8000101250b0 <load_balance+0x3c8>  // b.any
         :                      capacity_of():
         :                      return cpu_rq(cpu)->cpu_capacity;
    0.00 :   ffff800010125064:       ldrsw   x1, [x29, #160]
    0.00 :   ffff800010125068:       mov     x6, x20
         :                      __read_once_size():
    0.00 :   ffff80001012506c:       ldr     x8, [x22, #192]
         :                      capacity_of():
    0.00 :   ffff800010125070:       ldr     x2, [x29, #136]
         :                      __read_once_size():
    0.00 :   ffff800010125074:       ldr     x7, [x22, #200]
         :                      capacity_of():
    0.00 :   ffff800010125078:       ldr     x1, [x2, x1, lsl #3]
         :                      _task_util_est():
         :                      return (max(ue.ewma, ue.enqueued) | UTIL_AVG_UNCHANGED);
    0.00 :   ffff80001012507c:       lsr     x0, x7, #32
    0.00 :   ffff800010125080:       cmp     w0, w7
         :                      capacity_of():
         :                      return cpu_rq(cpu)->cpu_capacity;
    0.00 :   ffff800010125084:       add     x1, x1, x6
         :                      _task_util_est():
         :                      return (max(ue.ewma, ue.enqueued) | UTIL_AVG_UNCHANGED);
    0.00 :   ffff800010125088:       csel    w0, w0, w7, hi  // hi = pmore
    0.00 :   ffff80001012508c:       orr     w0, w0, #0x1
         :                      task_util_est():
         :                      return max(task_util(p), _task_util_est(p));
    0.00 :   ffff800010125090:       cmp     x0, x8
         :                      task_fits_capacity():
         :                      return fits_capacity(task_util_est(p), capacity);
    0.00 :   ffff800010125094:       ldr     x1, [x1, #2480]
         :                      task_util_est():
         :                      return max(task_util(p), _task_util_est(p));
    0.00 :   ffff800010125098:       csel    x0, x0, x8, cs  // cs = hs, nlast
         :                      task_fits_capacity():
         :                      return fits_capacity(task_util_est(p), capacity);
    0.00 :   ffff80001012509c:       add     x0, x0, x0, lsl #2
    0.00 :   ffff8000101250a0:       lsl     x1, x1, #10
         :                      detach_tasks():
         :                      if (task_fits_capacity(p, capacity_of(env->src_cpu)))
    0.00 :   ffff8000101250a4:       cmp     x1, x0, lsl #8
    0.00 :   ffff8000101250a8:       b.hi    ffff8000101251e0 <load_balance+0x4f8>  // b.pmore
         :                      env->imbalance = 0;
    0.00 :   ffff8000101250ac:       str     xzr, [x29, #192]
         :                      detach_task():
         :                      deactivate_task(env->src_rq, p, DEQUEUE_NOCLOCK);
    0.00 :   ffff8000101250b0:       ldr     x0, [x29, #152]
    0.00 :   ffff8000101250b4:       mov     w2, #0x8                        // #8
    0.00 :   ffff8000101250b8:       mov     x1, x24
         :                      detach_tasks():
         :                      detached++;
    0.00 :   ffff8000101250bc:       add     w23, w23, #0x1
         :                      detach_task():
         :                      deactivate_task(env->src_rq, p, DEQUEUE_NOCLOCK);
    0.00 :   ffff8000101250c0:       bl      ffff800010113d30 <deactivate_task>
         :                      set_task_cpu(p, env->dst_cpu);
    0.00 :   ffff8000101250c4:       ldr     w1, [x29, #164]
    0.00 :   ffff8000101250c8:       mov     x0, x24
    0.00 :   ffff8000101250cc:       bl      ffff800010115018 <set_task_cpu>
         :                      list_add():
         :                      * Insert a new entry after the specified head.
         :                      * This is good for implementing stacks.
         :                      */
         :                      static inline void list_add(struct list_head *new, struct list_head *head)
         :                      {
         :                      __list_add(new, head, head->next);
    0.00 :   ffff8000101250d0:       ldr     x1, [x29, #232]
         :                      __list_add():
         :                      new->prev = prev;
    0.00 :   ffff8000101250d4:       add     x0, x29, #0xe8
         :                      next->prev = new;
    0.00 :   ffff8000101250d8:       str     x22, [x1, #8]
         :                      new->prev = prev;
    0.00 :   ffff8000101250dc:       stp     x1, x0, [x22]
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff8000101250e0:       str     x22, [x29, #232]
         :                      detach_tasks():
         :                      if (env->idle == CPU_NEWLY_IDLE)
    0.00 :   ffff8000101250e4:       ldr     w0, [x29, #188]
    0.00 :   ffff8000101250e8:       cmp     w0, #0x2
    0.00 :   ffff8000101250ec:       b.eq    ffff8000101250fc <load_balance+0x414>  // b.none
         :                      if (env->imbalance <= 0)
    0.00 :   ffff8000101250f0:       ldr     x0, [x29, #192]
    0.00 :   ffff8000101250f4:       cmp     x0, #0x0
    0.00 :   ffff8000101250f8:       b.gt    ffff800010124fdc <load_balance+0x2f4>
         :                      rq_unlock():
         :                      raw_spin_unlock(&rq->lock);
    0.00 :   ffff8000101250fc:       mov     x0, x19
    0.00 :   ffff800010125100:       bl      ffff800010cb2408 <_raw_spin_unlock>
         :                      load_balance():
         :                      * See task_rq_lock() family for the details.
         :                      */
         :
         :                      rq_unlock(busiest, &rf);
         :
         :                      if (cur_ld_moved) {
    0.00 :   ffff800010125104:       cbz     w23, ffff800010125164 <load_balance+0x47c>
         :                      rq_lock():
         :                      raw_spin_lock(&rq->lock);
    0.00 :   ffff800010125108:       ldr     x0, [x29, #168]
         :                      attach_tasks():
         :                      while (!list_empty(tasks)) {
    0.00 :   ffff80001012510c:       add     x21, x29, #0xe8
         :                      rq_lock():
    0.00 :   ffff800010125110:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      attach_tasks():
         :                      update_rq_clock(env->dst_rq);
    0.00 :   ffff800010125114:       ldr     x0, [x29, #168]
    0.00 :   ffff800010125118:       bl      ffff8000101133a8 <update_rq_clock>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001012511c:       ldr     x0, [x29, #232]
         :                      attach_tasks():
         :                      while (!list_empty(tasks)) {
    0.00 :   ffff800010125120:       cmp     x21, x0
    0.00 :   ffff800010125124:       b.eq    ffff800010125158 <load_balance+0x470>  // b.none
         :                      p = list_first_entry(tasks, struct task_struct, se.group_node);
    0.00 :   ffff800010125128:       ldr     x0, [x29, #232]
         :                      attach_task(env->dst_rq, p);
    0.00 :   ffff80001012512c:       sub     x1, x0, #0xf0
         :                      __list_del_entry():
         :                      static inline void __list_del_entry(struct list_head *entry)
         :                      {
         :                      if (!__list_del_entry_valid(entry))
         :                      return;
         :
         :                      __list_del(entry->prev, entry->next);
    0.00 :   ffff800010125130:       ldp     x3, x2, [x0]
         :                      __list_del():
         :                      next->prev = prev;
    0.00 :   ffff800010125134:       str     x2, [x3, #8]
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff800010125138:       str     x3, [x2]
    0.00 :   ffff80001012513c:       str     x0, [x0]
         :                      INIT_LIST_HEAD():
         :                      list->prev = list;
    0.00 :   ffff800010125140:       str     x0, [x0, #8]
         :                      attach_tasks():
    0.00 :   ffff800010125144:       ldr     x0, [x29, #168]
    0.00 :   ffff800010125148:       bl      ffff80001011c8e0 <attach_task>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001012514c:       ldr     x0, [x29, #232]
         :                      attach_tasks():
         :                      while (!list_empty(tasks)) {
    0.00 :   ffff800010125150:       cmp     x21, x0
    0.00 :   ffff800010125154:       b.ne    ffff800010125128 <load_balance+0x440>  // b.any
         :                      rq_unlock():
         :                      raw_spin_unlock(&rq->lock);
    0.00 :   ffff800010125158:       ldr     x0, [x29, #168]
         :                      load_balance():
         :                      attach_tasks(&env);
         :                      ld_moved += cur_ld_moved;
    0.00 :   ffff80001012515c:       add     w26, w26, w23
         :                      rq_unlock():
    0.00 :   ffff800010125160:       bl      ffff800010cb2408 <_raw_spin_unlock>
         :                      arch_local_irq_restore():
         :                      /*
         :                      * restore saved IRQ state
         :                      */
         :                      static inline void arch_local_irq_restore(unsigned long flags)
         :                      {
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010125164:       msr     daif, x25
         :                      load_balance():
         :                      }
         :
         :                      local_irq_restore(rf.flags);
         :
         :                      if (env.flags & LBF_NEED_BREAK) {
    1.72 :   ffff800010125168:       ldr     w0, [x29, #208]
    0.00 :   ffff80001012516c:       tbz     w0, #1, ffff800010125200 <load_balance+0x518>
         :                      env.flags &= ~LBF_NEED_BREAK;
    0.00 :   ffff800010125170:       and     w0, w0, #0xfffffffd
    0.00 :   ffff800010125174:       str     w0, [x29, #208]
         :                      rq_lock_irqsave():
         :                      raw_spin_lock_irqsave(&rq->lock, rf->flags);
    0.00 :   ffff800010125178:       mov     x0, x19
    0.00 :   ffff80001012517c:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
    0.00 :   ffff800010125180:       mov     x25, x0
         :                      load_balance():
         :                      update_rq_clock(busiest);
    0.00 :   ffff800010125184:       mov     x0, x19
    0.00 :   ffff800010125188:       bl      ffff8000101133a8 <update_rq_clock>
         :                      detach_tasks():
         :                      if (env->imbalance <= 0)
    0.00 :   ffff80001012518c:       ldr     x0, [x29, #192]
    0.00 :   ffff800010125190:       cmp     x0, #0x0
    0.00 :   ffff800010125194:       b.gt    ffff800010124fd0 <load_balance+0x2e8>
         :                      rq_unlock():
         :                      raw_spin_unlock(&rq->lock);
    0.00 :   ffff800010125198:       mov     x0, x19
    0.00 :   ffff80001012519c:       bl      ffff800010cb2408 <_raw_spin_unlock>
    0.00 :   ffff8000101251a0:       b       ffff800010125164 <load_balance+0x47c>
         :                      find_busiest_queue():
         :                      switch (env->migration_type) {
    0.00 :   ffff8000101251a4:       ldr     w2, [x29, #228]
    0.00 :   ffff8000101251a8:       cmp     w2, #0x1
    0.00 :   ffff8000101251ac:       b.eq    ffff8000101253bc <load_balance+0x6d4>  // b.none
    0.00 :   ffff8000101251b0:       cbnz    w2, ffff800010124f10 <load_balance+0x228>
         :                      if (nr_running == 1 && load > env->imbalance &&
    0.00 :   ffff8000101251b4:       cmp     w8, #0x1
         :                      cpu_load():
         :                      return cfs_rq_load_avg(&rq->cfs);
    0.00 :   ffff8000101251b8:       ldr     x2, [x1, #288]
         :                      find_busiest_queue():
         :                      if (nr_running == 1 && load > env->imbalance &&
    0.00 :   ffff8000101251bc:       b.eq    ffff8000101254ec <load_balance+0x804>  // b.none
         :                      if (load * busiest_capacity > busiest_load * capacity) {
    0.00 :   ffff8000101251c0:       mul     x3, x4, x25
    0.00 :   ffff8000101251c4:       mul     x6, x22, x2
         :                      busiest = rq;
    0.00 :   ffff8000101251c8:       cmp     x6, x3
    0.00 :   ffff8000101251cc:       csel    x4, x4, x22, hi  // hi = pmore
    0.00 :   ffff8000101251d0:       csel    x25, x2, x25, hi  // hi = pmore
    0.00 :   ffff8000101251d4:       mov     x22, x4
    0.00 :   ffff8000101251d8:       csel    x19, x1, x19, hi  // hi = pmore
    0.00 :   ffff8000101251dc:       b       ffff800010124e78 <load_balance+0x190>
         :                      __list_del_entry():
         :                      __list_del(entry->prev, entry->next);
    0.00 :   ffff8000101251e0:       ldp     x1, x0, [x22]
         :                      __list_del():
         :                      next->prev = prev;
    0.00 :   ffff8000101251e4:       str     x0, [x1, #8]
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff8000101251e8:       str     x1, [x0]
         :                      list_add():
         :                      __list_add(new, head, head->next);
    0.00 :   ffff8000101251ec:       ldr     x0, [x27, #16]
         :                      __list_add():
         :                      next->prev = new;
    0.00 :   ffff8000101251f0:       str     x22, [x0, #8]
         :                      new->prev = prev;
    0.00 :   ffff8000101251f4:       stp     x0, x21, [x22]
         :                      __write_once_size():
    0.00 :   ffff8000101251f8:       str     x22, [x28, #2576]
    0.00 :   ffff8000101251fc:       b       ffff800010124fdc <load_balance+0x2f4>
         :                      load_balance():
         :                      * given_cpu) causing exceess load to be moved to given_cpu.
         :                      * This however should not happen so much in practice and
         :                      * moreover subsequent load balance cycles should correct the
         :                      * excess load moved.
         :                      */
         :                      if ((env.flags & LBF_DST_PINNED) && env.imbalance > 0) {
    0.57 :   ffff800010125200:       tbz     w0, #2, ffff80001012531c <load_balance+0x634>
    0.00 :   ffff800010125204:       ldr     x1, [x29, #192]
    0.00 :   ffff800010125208:       cmp     x1, #0x0
    0.00 :   ffff80001012520c:       b.le    ffff80001012531c <load_balance+0x634>
         :
         :                      /* Prevent to re-select dst_cpu via env's CPUs */
         :                      __cpumask_clear_cpu(env.dst_cpu, env.cpus);
    0.00 :   ffff800010125210:       ldr     w1, [x29, #164]
         :                      __clear_bit():
         :                      unsigned long mask = BIT_MASK(nr);
    0.00 :   ffff800010125214:       mov     x2, #0x1                        // #1
         :                      unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
    0.00 :   ffff800010125218:       ldr     x5, [x29, #200]
         :                      load_balance():
         :
         :                      env.dst_rq       = cpu_rq(env.new_dst_cpu);
    0.00 :   ffff80001012521c:       mov     x4, x20
         :                      __clear_bit():
    0.00 :   ffff800010125220:       cmp     w1, #0x0
    0.00 :   ffff800010125224:       add     w0, w1, #0x3f
    0.00 :   ffff800010125228:       csel    w0, w0, w1, lt  // lt = tstop
         :                      unsigned long mask = BIT_MASK(nr);
    0.00 :   ffff80001012522c:       negs    w3, w1
    0.00 :   ffff800010125230:       and     w3, w3, #0x3f
    0.00 :   ffff800010125234:       and     w1, w1, #0x3f
         :                      unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
    0.00 :   ffff800010125238:       asr     w0, w0, #6
         :                      unsigned long mask = BIT_MASK(nr);
    0.00 :   ffff80001012523c:       csneg   w1, w1, w3, mi  // mi = first
         :                      unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
    0.00 :   ffff800010125240:       sxtw    x0, w0
         :                      unsigned long mask = BIT_MASK(nr);
    0.00 :   ffff800010125244:       lsl     x1, x2, x1
         :                      load_balance():
         :                      env.dst_cpu      = env.new_dst_cpu;
         :                      env.flags       &= ~LBF_DST_PINNED;
         :                      env.loop         = 0;
    0.00 :   ffff800010125248:       mov     x2, #0x2000000000               // #137438953472
         :                      __clear_bit():
         :                      *p &= ~mask;
    0.00 :   ffff80001012524c:       ldr     x3, [x5, x0, lsl #3]
    0.00 :   ffff800010125250:       bic     x1, x3, x1
    0.00 :   ffff800010125254:       str     x1, [x5, x0, lsl #3]
         :                      load_balance():
    0.00 :   ffff800010125258:       stur    x2, [x29, #212]
         :                      env.flags       &= ~LBF_DST_PINNED;
    0.00 :   ffff80001012525c:       ldr     w0, [x29, #208]
         :                      env.dst_rq       = cpu_rq(env.new_dst_cpu);
    0.00 :   ffff800010125260:       ldr     w1, [x29, #184]
         :                      env.flags       &= ~LBF_DST_PINNED;
    0.00 :   ffff800010125264:       and     w0, w0, #0xfffffffb
    0.00 :   ffff800010125268:       str     w0, [x29, #208]
         :                      env.dst_rq       = cpu_rq(env.new_dst_cpu);
    0.00 :   ffff80001012526c:       ldr     x0, [x29, #136]
         :                      env.dst_cpu      = env.new_dst_cpu;
    0.00 :   ffff800010125270:       str     w1, [x29, #164]
         :                      env.dst_rq       = cpu_rq(env.new_dst_cpu);
    0.00 :   ffff800010125274:       ldr     x0, [x0, w1, sxtw #3]
    0.00 :   ffff800010125278:       add     x0, x0, x4
    0.00 :   ffff80001012527c:       str     x0, [x29, #168]
         :
         :                      /*
         :                      * Go back to "more_balance" rather than "redo" since we
         :                      * need to continue with same src_cpu.
         :                      */
         :                      goto more_balance;
    0.00 :   ffff800010125280:       b       ffff800010124fb0 <load_balance+0x2c8>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010125284:       ldr     x7, [x22, #192]
    0.00 :   ffff800010125288:       ldr     x1, [x22, #200]
         :                      detach_tasks():
         :                      if (util > env->imbalance)
    0.00 :   ffff80001012528c:       ldr     x6, [x29, #192]
         :                      _task_util_est():
         :                      return (max(ue.ewma, ue.enqueued) | UTIL_AVG_UNCHANGED);
    0.00 :   ffff800010125290:       lsr     x0, x1, #32
    0.00 :   ffff800010125294:       cmp     w0, w1
    0.00 :   ffff800010125298:       csel    w0, w0, w1, hi  // hi = pmore
    0.00 :   ffff80001012529c:       orr     w0, w0, #0x1
         :                      task_util_est():
         :                      return max(task_util(p), _task_util_est(p));
    0.00 :   ffff8000101252a0:       cmp     x0, x7
    0.00 :   ffff8000101252a4:       csel    x0, x0, x7, cs  // cs = hs, nlast
         :                      detach_tasks():
         :                      if (util > env->imbalance)
    0.00 :   ffff8000101252a8:       cmp     x0, x6
    0.00 :   ffff8000101252ac:       b.hi    ffff8000101251e0 <load_balance+0x4f8>  // b.pmore
         :                      env->imbalance -= util;
    0.00 :   ffff8000101252b0:       sub     x0, x6, x0
    0.00 :   ffff8000101252b4:       str     x0, [x29, #192]
    0.00 :   ffff8000101252b8:       b       ffff8000101250b0 <load_balance+0x3c8>
         :                      env->imbalance--;
    0.00 :   ffff8000101252bc:       ldr     x0, [x29, #192]
    0.00 :   ffff8000101252c0:       sub     x0, x0, #0x1
    0.00 :   ffff8000101252c4:       str     x0, [x29, #192]
    0.00 :   ffff8000101252c8:       b       ffff8000101250b0 <load_balance+0x3c8>
         :                      task_cfs_rq():
         :                      return p->se.cfs_rq;
    0.00 :   ffff8000101252cc:       ldr     x6, [x24, #320]
         :                      task_h_load():
         :                      update_cfs_rq_h_load(cfs_rq);
    0.00 :   ffff8000101252d0:       mov     x0, x6
    0.00 :   ffff8000101252d4:       bl      ffff80001011b4a0 <update_cfs_rq_h_load>
         :                      return div64_ul(p->se.avg.load_avg * cfs_rq->h_load,
    0.00 :   ffff8000101252d8:       ldr     x7, [x6, #280]
    0.00 :   ffff8000101252dc:       ldr     x0, [x24, #416]
    0.00 :   ffff8000101252e0:       ldr     x6, [x6, #160]
         :                      detach_tasks():
         :                      if (load/2 > env->imbalance &&
    0.00 :   ffff8000101252e4:       ldr     x1, [x29, #192]
         :                      task_h_load():
         :                      return div64_ul(p->se.avg.load_avg * cfs_rq->h_load,
    0.00 :   ffff8000101252e8:       add     x6, x6, #0x1
    0.00 :   ffff8000101252ec:       mul     x0, x0, x7
         :                      div64_u64():
         :                      *
         :                      * Return: dividend / divisor
         :                      */
         :                      static inline u64 div64_u64(u64 dividend, u64 divisor)
         :                      {
         :                      return dividend / divisor;
    0.00 :   ffff8000101252f0:       udiv    x0, x0, x6
         :                      detach_tasks():
         :                      if (load/2 > env->imbalance &&
    0.00 :   ffff8000101252f4:       cmp     x1, x0, lsr #1
    0.00 :   ffff8000101252f8:       b.cs    ffff800010125310 <load_balance+0x628>  // b.hs, b.nlast
         :                      env->sd->nr_balance_failed <= env->sd->cache_nice_tries)
    0.00 :   ffff8000101252fc:       ldr     x6, [x29, #144]
         :                      if (load/2 > env->imbalance &&
    0.00 :   ffff800010125300:       ldr     w7, [x6, #48]
    0.00 :   ffff800010125304:       ldr     w6, [x6, #76]
    0.00 :   ffff800010125308:       cmp     w6, w7
    0.00 :   ffff80001012530c:       b.ls    ffff8000101251e0 <load_balance+0x4f8>  // b.plast
         :                      env->imbalance -= load;
    0.00 :   ffff800010125310:       sub     x0, x1, x0
    0.00 :   ffff800010125314:       str     x0, [x29, #192]
    0.00 :   ffff800010125318:       b       ffff8000101250b0 <load_balance+0x3c8>
         :                      load_balance():
         :                      }
         :
         :                      /*
         :                      * We failed to reach balance because of affinity.
         :                      */
         :                      if (sd_parent) {
    0.00 :   ffff80001012531c:       ldr     x2, [x29, #120]
    0.00 :   ffff800010125320:       cbz     x2, ffff800010125348 <load_balance+0x660>
         :                      int *group_imbalance = &sd_parent->groups->sgc->imbalance;
         :
         :                      if ((env.flags & LBF_SOME_PINNED) && env.imbalance > 0)
    0.00 :   ffff800010125324:       tbz     w0, #3, ffff800010125348 <load_balance+0x660>
    0.00 :   ffff800010125328:       ldr     x1, [x29, #192]
    0.00 :   ffff80001012532c:       cmp     x1, #0x0
    0.00 :   ffff800010125330:       b.le    ffff800010125348 <load_balance+0x660>
         :                      int *group_imbalance = &sd_parent->groups->sgc->imbalance;
    0.00 :   ffff800010125334:       ldr     x0, [x2, #16]
         :                      *group_imbalance = 1;
    0.00 :   ffff800010125338:       mov     w1, #0x1                        // #1
         :                      int *group_imbalance = &sd_parent->groups->sgc->imbalance;
    0.00 :   ffff80001012533c:       ldr     x0, [x0, #16]
         :                      *group_imbalance = 1;
    0.00 :   ffff800010125340:       str     w1, [x0, #40]
    0.00 :   ffff800010125344:       ldr     w0, [x29, #208]
         :                      }
         :
         :                      /* All tasks on this runqueue were pinned by CPU affinity */
         :                      if (unlikely(env.flags & LBF_ALL_PINNED)) {
    0.00 :   ffff800010125348:       tbnz    w0, #0, ffff800010125650 <load_balance+0x968>
         :                      }
         :                      goto out_all_pinned;
         :                      }
         :                      }
         :
         :                      if (!ld_moved) {
    0.00 :   ffff80001012534c:       cbz     w26, ffff800010125480 <load_balance+0x798>
         :
         :                      /* We've kicked active balancing, force task migration. */
         :                      sd->nr_balance_failed = sd->cache_nice_tries+1;
         :                      }
         :                      } else
         :                      sd->nr_balance_failed = 0;
    0.00 :   ffff800010125350:       ldr     x0, [x29, #128]
    0.00 :   ffff800010125354:       str     wzr, [x0, #76]
         :
         :                      if (likely(!active_balance) || voluntary_active_balance(&env)) {
         :                      /* We were unbalanced, so reset the balancing interval */
         :                      sd->balance_interval = sd->min_interval;
    0.00 :   ffff800010125358:       ldr     x1, [x29, #128]
    0.00 :   ffff80001012535c:       ldr     x0, [x1, #24]
         :                      * case may not be covered by the all_pinned logic if there
         :                      * is only 1 task on the busy runqueue (because we don't call
         :                      * detach_tasks).
         :                      */
         :                      if (sd->balance_interval < sd->max_interval)
         :                      sd->balance_interval *= 2;
    0.00 :   ffff800010125360:       str     w0, [x1, #72]
    0.00 :   ffff800010125364:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff800010125368:       ldr     x21, [x29, #32]
    0.00 :   ffff80001012536c:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff800010125370:       ldr     x25, [x29, #64]
    0.00 :   ffff800010125374:       ldp     x27, x28, [x29, #80]
         :                      sd->balance_interval < MAX_PINNED_INTERVAL) ||
         :                      sd->balance_interval < sd->max_interval)
         :                      sd->balance_interval *= 2;
         :                      out:
         :                      return ld_moved;
         :                      }
    0.59 :   ffff800010125378:       adrp    x0, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff80001012537c:       add     x1, x0, #0x8c8
    0.00 :   ffff800010125380:       mov     w0, w26
    0.00 :   ffff800010125384:       ldr     x2, [x29, #248]
    0.00 :   ffff800010125388:       ldr     x1, [x1]
    0.00 :   ffff80001012538c:       eor     x1, x2, x1
    0.00 :   ffff800010125390:       cbnz    x1, ffff8000101258a8 <load_balance+0xbc0>
    0.00 :   ffff800010125394:       ldr     x22, [sp, #40]
    0.00 :   ffff800010125398:       ldr     x26, [sp, #72]
    0.00 :   ffff80001012539c:       ldp     x29, x30, [sp], #256
    0.00 :   ffff8000101253a0:       ret
         :                      detach_tasks():
         :                      env->flags |= LBF_NEED_BREAK;
    0.00 :   ffff8000101253a4:       ldr     w0, [x29, #208]
         :                      env->loop_break += sched_nr_migrate_break;
    0.00 :   ffff8000101253a8:       add     w1, w1, #0x20
    0.00 :   ffff8000101253ac:       str     w1, [x29, #216]
         :                      env->flags |= LBF_NEED_BREAK;
    0.00 :   ffff8000101253b0:       orr     w0, w0, #0x2
    0.00 :   ffff8000101253b4:       str     w0, [x29, #208]
    0.00 :   ffff8000101253b8:       b       ffff8000101250fc <load_balance+0x414>
         :                      cpu_util():
         :                      cfs_rq = &cpu_rq(cpu)->cfs;
    0.00 :   ffff8000101253bc:       ldrsw   x3, [x1, #2568]
    0.00 :   ffff8000101253c0:       mov     x2, x20
    0.00 :   ffff8000101253c4:       ldr     x3, [x21, x3, lsl #3]
    0.00 :   ffff8000101253c8:       add     x2, x2, x3
         :                      __read_once_size():
    0.00 :   ffff8000101253cc:       ldr     x3, [x2, #304]
    0.00 :   ffff8000101253d0:       ldr     w4, [x2, #312]
         :                      cpu_util():
         :                      return min_t(unsigned long, util, capacity_orig_of(cpu));
    0.00 :   ffff8000101253d4:       ldr     x2, [x2, #2488]
         :                      util = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));
    0.00 :   ffff8000101253d8:       cmp     w4, w3
         :                      return min_t(unsigned long, util, capacity_orig_of(cpu));
    0.00 :   ffff8000101253dc:       csel    w3, w3, w4, ls  // ls = plast
    0.00 :   ffff8000101253e0:       cmp     x2, x3
    0.00 :   ffff8000101253e4:       csel    x2, x2, x3, ls  // ls = plast
         :                      find_busiest_queue():
         :                      busiest = rq;
    0.00 :   ffff8000101253e8:       cmp     x2, x26
    0.00 :   ffff8000101253ec:       csel    x3, x2, x26, hi  // hi = pmore
    0.00 :   ffff8000101253f0:       csel    x19, x1, x19, hi  // hi = pmore
    0.00 :   ffff8000101253f4:       mov     x26, x3
    0.00 :   ffff8000101253f8:       b       ffff800010124e78 <load_balance+0x190>
         :                      busiest = rq;
    0.00 :   ffff8000101253fc:       cmp     w8, w27
    0.00 :   ffff800010125400:       csel    w2, w8, w27, hi  // hi = pmore
    0.00 :   ffff800010125404:       csel    x19, x1, x19, hi  // hi = pmore
    0.00 :   ffff800010125408:       mov     w27, w2
    0.00 :   ffff80001012540c:       b       ffff800010124e78 <load_balance+0x190>
    0.00 :   ffff800010125410:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff800010125414:       ldr     x21, [x29, #32]
    0.00 :   ffff800010125418:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff80001012541c:       ldr     x25, [x29, #64]
    0.00 :   ffff800010125420:       ldp     x27, x28, [x29, #80]
    0.00 :   ffff800010125424:       nop
         :                      load_balance():
         :                      *continue_balancing = 0;
    0.00 :   ffff800010125428:       ldr     x0, [x29, #112]
    0.65 :   ffff80001012542c:       str     wzr, [x0]
         :                      if (sd_parent && !(env.flags & LBF_ALL_PINNED)) {
    0.59 :   ffff800010125430:       ldr     x1, [x29, #120]
    0.00 :   ffff800010125434:       cbz     x1, ffff800010125440 <load_balance+0x758>
    0.59 :   ffff800010125438:       ldr     w0, [x29, #208]
    0.00 :   ffff80001012543c:       tbz     w0, #0, ffff800010125818 <load_balance+0xb30>
         :                      sd->nr_balance_failed = 0;
   14.17 :   ffff800010125440:       ldr     x0, [x29, #128]
    0.46 :   ffff800010125444:       str     wzr, [x0, #76]
         :                      if (env.idle == CPU_NEWLY_IDLE)
    0.00 :   ffff800010125448:       ldr     w0, [x29, #188]
    0.00 :   ffff80001012544c:       cmp     w0, #0x2
    0.00 :   ffff800010125450:       b.eq    ffff80001012563c <load_balance+0x954>  // b.none
         :                      if ((env.flags & LBF_ALL_PINNED &&
    0.00 :   ffff800010125454:       ldr     w1, [x29, #208]
    0.00 :   ffff800010125458:       ldr     x0, [x29, #128]
    0.00 :   ffff80001012545c:       ldr     w0, [x0, #72]
    0.00 :   ffff800010125460:       tbz     w1, #0, ffff80001012562c <load_balance+0x944>
    0.00 :   ffff800010125464:       cmp     w0, #0x1ff
    0.00 :   ffff800010125468:       b.hi    ffff80001012562c <load_balance+0x944>  // b.pmore
         :                      sd->balance_interval *= 2;
    0.00 :   ffff80001012546c:       ldr     x1, [x29, #128]
    0.00 :   ffff800010125470:       lsl     w0, w0, #1
         :                      ld_moved = 0;
    0.00 :   ffff800010125474:       mov     w26, #0x0                       // #0
         :                      sd->balance_interval *= 2;
    0.00 :   ffff800010125478:       str     w0, [x1, #72]
         :                      return ld_moved;
    0.00 :   ffff80001012547c:       b       ffff800010125378 <load_balance+0x690>
         :                      if (idle != CPU_NEWLY_IDLE)
    0.00 :   ffff800010125480:       ldr     w0, [x29, #108]
    0.00 :   ffff800010125484:       cmp     w0, #0x2
    0.00 :   ffff800010125488:       b.eq    ffff80001012549c <load_balance+0x7b4>  // b.none
         :                      sd->nr_balance_failed++;
    0.00 :   ffff80001012548c:       ldr     x1, [x29, #128]
    0.00 :   ffff800010125490:       ldr     w0, [x1, #76]
    0.00 :   ffff800010125494:       add     w0, w0, #0x1
    0.00 :   ffff800010125498:       str     w0, [x1, #76]
         :                      asym_active_balance():
         :                      return env->idle != CPU_NOT_IDLE && (env->sd->flags & SD_ASYM_PACKING) &&
    0.00 :   ffff80001012549c:       ldr     w0, [x29, #188]
         :                      need_active_balance():
         :                      struct sched_domain *sd = env->sd;
    0.00 :   ffff8000101254a0:       ldr     x20, [x29, #144]
         :                      asym_active_balance():
         :                      return env->idle != CPU_NOT_IDLE && (env->sd->flags & SD_ASYM_PACKING) &&
    0.00 :   ffff8000101254a4:       cmp     w0, #0x1
    0.00 :   ffff8000101254a8:       b.eq    ffff8000101254c4 <load_balance+0x7dc>  // b.none
    0.00 :   ffff8000101254ac:       ldr     w0, [x20, #56]
    0.00 :   ffff8000101254b0:       tbnz    w0, #11, ffff800010125540 <load_balance+0x858>
         :                      voluntary_active_balance():
         :                      (env->src_rq->cfs.h_nr_running == 1)) {
    0.00 :   ffff8000101254b4:       ldr     x0, [x29, #152]
         :                      if ((env->idle != CPU_NOT_IDLE) &&
    0.00 :   ffff8000101254b8:       ldr     w1, [x0, #156]
    0.00 :   ffff8000101254bc:       cmp     w1, #0x1
    0.00 :   ffff8000101254c0:       b.eq    ffff80001012556c <load_balance+0x884>  // b.none
         :                      if (env->migration_type == migrate_misfit)
    0.00 :   ffff8000101254c4:       ldr     w0, [x29, #228]
    0.00 :   ffff8000101254c8:       cmp     w0, #0x3
    0.00 :   ffff8000101254cc:       b.eq    ffff8000101255cc <load_balance+0x8e4>  // b.none
         :                      need_active_balance():
         :                      return unlikely(sd->nr_balance_failed > sd->cache_nice_tries+2);
    0.00 :   ffff8000101254d0:       ldr     w0, [x20, #48]
         :                      load_balance():
         :                      if (need_active_balance(&env)) {
    0.00 :   ffff8000101254d4:       ldr     w1, [x20, #76]
         :                      need_active_balance():
         :                      return unlikely(sd->nr_balance_failed > sd->cache_nice_tries+2);
    0.00 :   ffff8000101254d8:       add     w0, w0, #0x2
         :                      load_balance():
         :                      if (need_active_balance(&env)) {
    0.00 :   ffff8000101254dc:       cmp     w1, w0
    0.00 :   ffff8000101254e0:       b.hi    ffff8000101255cc <load_balance+0x8e4>  // b.pmore
         :                      sd->nr_balance_failed = sd->cache_nice_tries+1;
    0.00 :   ffff8000101254e4:       mov     w26, #0x0                       // #0
    0.00 :   ffff8000101254e8:       b       ffff800010125358 <load_balance+0x670>
         :                      find_busiest_queue():
         :                      if (nr_running == 1 && load > env->imbalance &&
    0.00 :   ffff8000101254ec:       ldr     x3, [x29, #192]
    0.00 :   ffff8000101254f0:       cmp     x2, x3
    0.00 :   ffff8000101254f4:       b.ls    ffff8000101251c0 <load_balance+0x4d8>  // b.plast
         :                      check_cpu_capacity():
         :                      return ((rq->cpu_capacity * sd->imbalance_pct) <
    0.00 :   ffff8000101254f8:       ldr     w3, [x6, #44]
    0.00 :   ffff8000101254fc:       ldr     x6, [x1, #2480]
         :                      (rq->cpu_capacity_orig * 100));
    0.00 :   ffff800010125500:       ldr     x7, [x1, #2488]
         :                      return ((rq->cpu_capacity * sd->imbalance_pct) <
    0.00 :   ffff800010125504:       mul     x3, x3, x6
         :                      (rq->cpu_capacity_orig * 100));
    0.00 :   ffff800010125508:       add     x6, x7, x7, lsl #1
    0.00 :   ffff80001012550c:       add     x6, x7, x6, lsl #3
         :                      find_busiest_queue():
         :                      if (nr_running == 1 && load > env->imbalance &&
    0.00 :   ffff800010125510:       cmp     x3, x6, lsl #2
    0.00 :   ffff800010125514:       b.cs    ffff800010124e78 <load_balance+0x190>  // b.hs, b.nlast
    0.00 :   ffff800010125518:       b       ffff8000101251c0 <load_balance+0x4d8>
         :                      cpu_load():
         :                      return cfs_rq_load_avg(&rq->cfs);
    0.00 :   ffff80001012551c:       ldr     x2, [x1, #288]
         :                      find_busiest_queue():
         :                      if (load * busiest_capacity > busiest_load * capacity) {
    0.00 :   ffff800010125520:       mul     x3, x4, x25
    0.00 :   ffff800010125524:       mul     x6, x22, x2
         :                      busiest = rq;
    0.00 :   ffff800010125528:       cmp     x6, x3
    0.00 :   ffff80001012552c:       csel    x4, x4, x22, hi  // hi = pmore
    0.00 :   ffff800010125530:       csel    x25, x2, x25, hi  // hi = pmore
    0.00 :   ffff800010125534:       mov     x22, x4
    0.00 :   ffff800010125538:       csel    x19, x1, x19, hi  // hi = pmore
    0.00 :   ffff80001012553c:       b       ffff800010124e78 <load_balance+0x190>
         :                      sched_asym_prefer():
         :                      return arch_asym_cpu_priority(a) > arch_asym_cpu_priority(b);
    0.00 :   ffff800010125540:       ldp     w22, w0, [x29, #160]
    0.00 :   ffff800010125544:       bl      ffff800010122780 <arch_asym_cpu_priority>
    0.00 :   ffff800010125548:       mov     w21, w0
    0.00 :   ffff80001012554c:       mov     w0, w22
    0.00 :   ffff800010125550:       bl      ffff800010122780 <arch_asym_cpu_priority>
         :                      asym_active_balance():
         :                      return env->idle != CPU_NOT_IDLE && (env->sd->flags & SD_ASYM_PACKING) &&
    0.00 :   ffff800010125554:       cmp     w21, w0
    0.00 :   ffff800010125558:       b.gt    ffff8000101255cc <load_balance+0x8e4>
         :                      voluntary_active_balance():
         :                      if ((env->idle != CPU_NOT_IDLE) &&
    0.00 :   ffff80001012555c:       ldr     w0, [x29, #188]
    0.00 :   ffff800010125560:       cmp     w0, #0x1
    0.00 :   ffff800010125564:       b.ne    ffff8000101254b4 <load_balance+0x7cc>  // b.any
    0.00 :   ffff800010125568:       b       ffff8000101254c4 <load_balance+0x7dc>
         :                      check_cpu_capacity():
         :                      return ((rq->cpu_capacity * sd->imbalance_pct) <
    0.00 :   ffff80001012556c:       ldr     x2, [x0, #2480]
         :                      (rq->cpu_capacity_orig * 100));
    0.00 :   ffff800010125570:       ldr     x4, [x0, #2488]
         :                      return ((rq->cpu_capacity * sd->imbalance_pct) <
    0.00 :   ffff800010125574:       ldr     w1, [x20, #44]
         :                      (rq->cpu_capacity_orig * 100));
    0.00 :   ffff800010125578:       add     x3, x4, x4, lsl #1
         :                      return ((rq->cpu_capacity * sd->imbalance_pct) <
    0.00 :   ffff80001012557c:       mul     x0, x1, x2
         :                      (rq->cpu_capacity_orig * 100));
    0.00 :   ffff800010125580:       add     x2, x4, x3, lsl #3
         :                      voluntary_active_balance():
         :                      if ((check_cpu_capacity(env->src_rq, sd)) &&
    0.00 :   ffff800010125584:       cmp     x0, x2, lsl #2
    0.00 :   ffff800010125588:       b.cs    ffff8000101254c4 <load_balance+0x7dc>  // b.hs, b.nlast
         :                      capacity_of():
         :                      return cpu_rq(cpu)->cpu_capacity;
    0.00 :   ffff80001012558c:       ldpsw   x3, x4, [x29, #160]
    0.00 :   ffff800010125590:       adrp    x2, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010125594:       add     x2, x2, #0x8e8
    0.00 :   ffff800010125598:       adrp    x0, ffff8000114d5000 <tegra_to+0x180>
    0.00 :   ffff80001012559c:       add     x0, x0, #0xd80
    0.00 :   ffff8000101255a0:       ldr     x3, [x2, x3, lsl #3]
    0.00 :   ffff8000101255a4:       ldr     x2, [x2, x4, lsl #3]
    0.00 :   ffff8000101255a8:       add     x3, x3, x0
    0.00 :   ffff8000101255ac:       add     x0, x2, x0
         :                      voluntary_active_balance():
         :                      (capacity_of(env->src_cpu)*sd->imbalance_pct < capacity_of(env->dst_cpu)*100))
    0.00 :   ffff8000101255b0:       ldr     x3, [x3, #2480]
    0.00 :   ffff8000101255b4:       ldr     x2, [x0, #2480]
    0.00 :   ffff8000101255b8:       add     x0, x2, x2, lsl #1
    0.00 :   ffff8000101255bc:       mul     x1, x1, x3
    0.00 :   ffff8000101255c0:       add     x0, x2, x0, lsl #3
         :                      if ((check_cpu_capacity(env->src_rq, sd)) &&
    0.00 :   ffff8000101255c4:       cmp     x1, x0, lsl #2
    0.00 :   ffff8000101255c8:       b.cs    ffff8000101254c4 <load_balance+0x7dc>  // b.hs, b.nlast
         :                      load_balance():
         :                      raw_spin_lock_irqsave(&busiest->lock, flags);
    0.00 :   ffff8000101255cc:       mov     x0, x19
    0.00 :   ffff8000101255d0:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
         :                      test_bit():
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff8000101255d4:       ldr     w1, [x29, #104]
         :                      load_balance():
         :                      if (!cpumask_test_cpu(this_cpu, busiest->curr->cpus_ptr)) {
    0.00 :   ffff8000101255d8:       ldr     x3, [x19, #2352]
         :                      test_bit():
    0.00 :   ffff8000101255dc:       add     w2, w1, #0x3f
    0.00 :   ffff8000101255e0:       cmp     w1, #0x0
    0.00 :   ffff8000101255e4:       csel    w2, w2, w1, lt  // lt = tstop
    0.00 :   ffff8000101255e8:       ldr     x3, [x3, #736]
    0.00 :   ffff8000101255ec:       asr     w2, w2, #6
    0.00 :   ffff8000101255f0:       sxtw    x2, w2
    0.00 :   ffff8000101255f4:       ldr     x2, [x3, x2, lsl #3]
    0.00 :   ffff8000101255f8:       lsr     x2, x2, x1
         :                      load_balance():
    0.00 :   ffff8000101255fc:       tbz     w2, #0, ffff8000101257e8 <load_balance+0xb00>
         :                      if (!busiest->active_balance) {
    0.00 :   ffff800010125600:       ldr     w26, [x19, #2520]
    0.00 :   ffff800010125604:       cbz     w26, ffff8000101256f0 <load_balance+0xa08>
         :                      raw_spin_unlock_irqrestore(&busiest->lock, flags);
    0.00 :   ffff800010125608:       mov     x1, x0
    0.00 :   ffff80001012560c:       mov     x0, x19
    0.00 :   ffff800010125610:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      sd->nr_balance_failed = sd->cache_nice_tries+1;
    0.00 :   ffff800010125614:       mov     w26, #0x0                       // #0
    0.00 :   ffff800010125618:       ldr     x1, [x29, #128]
    0.00 :   ffff80001012561c:       ldr     w0, [x1, #48]
    0.00 :   ffff800010125620:       add     w0, w0, #0x1
    0.00 :   ffff800010125624:       str     w0, [x1, #76]
    0.00 :   ffff800010125628:       b       ffff800010125358 <load_balance+0x670>
         :                      sd->balance_interval < MAX_PINNED_INTERVAL) ||
    0.00 :   ffff80001012562c:       ldr     x1, [x29, #128]
    0.00 :   ffff800010125630:       ldr     x1, [x1, #32]
    0.00 :   ffff800010125634:       cmp     x1, w0, uxtw
    0.00 :   ffff800010125638:       b.hi    ffff80001012546c <load_balance+0x784>  // b.pmore
         :                      sd->nr_balance_failed = sd->cache_nice_tries+1;
    0.00 :   ffff80001012563c:       mov     w26, #0x0                       // #0
    0.00 :   ffff800010125640:       b       ffff800010125378 <load_balance+0x690>
    0.00 :   ffff800010125644:       ldr     x2, [x29, #200]
    0.00 :   ffff800010125648:       b       ffff800010124dfc <load_balance+0x114>
         :                      BUG_ON(busiest == env.dst_rq);
    0.00 :   ffff80001012564c:       brk     #0x800
         :                      __cpumask_clear_cpu(cpu_of(busiest), cpus);
    0.00 :   ffff800010125650:       ldr     w3, [x19, #2568]
         :                      __clear_bit():
         :                      unsigned long mask = BIT_MASK(nr);
    0.00 :   ffff800010125654:       mov     x4, #0x1                        // #1
         :                      bitmap_subset():
         :                      const unsigned long *src2, unsigned int nbits)
         :                      {
         :                      if (small_const_nbits(nbits))
         :                      return ! ((*src1 & ~(*src2)) & BITMAP_LAST_WORD_MASK(nbits));
         :                      else
         :                      return __bitmap_subset(src1, src2, nbits);
    0.00 :   ffff800010125658:       ldr     x6, [x29, #96]
    0.00 :   ffff80001012565c:       mov     w2, #0x100                      // #256
         :                      __clear_bit():
         :                      unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
    0.00 :   ffff800010125660:       cmp     w3, #0x0
    0.00 :   ffff800010125664:       add     w1, w3, #0x3f
    0.00 :   ffff800010125668:       csel    w1, w1, w3, lt  // lt = tstop
         :                      unsigned long mask = BIT_MASK(nr);
    0.00 :   ffff80001012566c:       negs    w5, w3
    0.00 :   ffff800010125670:       and     w5, w5, #0x3f
    0.00 :   ffff800010125674:       and     w3, w3, #0x3f
         :                      unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
    0.00 :   ffff800010125678:       asr     w1, w1, #6
         :                      unsigned long mask = BIT_MASK(nr);
    0.00 :   ffff80001012567c:       csneg   w3, w3, w5, mi  // mi = first
         :                      bitmap_subset():
    0.00 :   ffff800010125680:       mov     x0, x6
         :                      __clear_bit():
         :                      unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
    0.00 :   ffff800010125684:       sxtw    x1, w1
         :                      unsigned long mask = BIT_MASK(nr);
    0.00 :   ffff800010125688:       lsl     x3, x4, x3
         :                      *p &= ~mask;
    0.00 :   ffff80001012568c:       ldr     x4, [x6, x1, lsl #3]
    0.00 :   ffff800010125690:       bic     x3, x4, x3
    0.00 :   ffff800010125694:       str     x3, [x6, x1, lsl #3]
         :                      bitmap_subset():
    0.00 :   ffff800010125698:       ldr     x1, [x29, #176]
    0.00 :   ffff80001012569c:       bl      ffff80001047f718 <__bitmap_subset>
         :                      load_balance():
         :                      if (!cpumask_subset(cpus, env.dst_grpmask)) {
    0.00 :   ffff8000101256a0:       cbnz    w0, ffff800010125890 <load_balance+0xba8>
         :                      should_we_balance():
         :                      if (!cpumask_test_cpu(env->dst_cpu, env->cpus))
    0.00 :   ffff8000101256a4:       ldr     w1, [x29, #164]
         :                      load_balance():
         :                      env.loop = 0;
    0.00 :   ffff8000101256a8:       mov     x0, #0x2000000000               // #137438953472
    0.00 :   ffff8000101256ac:       stur    x0, [x29, #212]
         :                      test_bit():
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff8000101256b0:       cmp     w1, #0x0
    0.00 :   ffff8000101256b4:       add     w0, w1, #0x3f
    0.00 :   ffff8000101256b8:       csel    w0, w0, w1, lt  // lt = tstop
    0.00 :   ffff8000101256bc:       ldr     x2, [x29, #200]
    0.00 :   ffff8000101256c0:       asr     w0, w0, #6
    0.00 :   ffff8000101256c4:       sxtw    x0, w0
    0.00 :   ffff8000101256c8:       ldr     x0, [x2, x0, lsl #3]
    0.00 :   ffff8000101256cc:       lsr     x1, x0, x1
         :                      should_we_balance():
         :                      if (!cpumask_test_cpu(env->dst_cpu, env->cpus))
    0.00 :   ffff8000101256d0:       tbz     w1, #0, ffff800010125410 <load_balance+0x728>
         :                      struct sched_group *sg = env->sd->groups;
    0.00 :   ffff8000101256d4:       ldr     x0, [x29, #144]
    0.00 :   ffff8000101256d8:       ldr     x19, [x0, #16]
    0.00 :   ffff8000101256dc:       b       ffff800010124de4 <load_balance+0xfc>
         :                      balance_cpu = group_balance_cpu(sg);
    0.00 :   ffff8000101256e0:       mov     x0, x19
    0.00 :   ffff8000101256e4:       bl      ffff8000101319e0 <group_balance_cpu>
    0.00 :   ffff8000101256e8:       mov     w23, w0
    0.00 :   ffff8000101256ec:       b       ffff800010124e2c <load_balance+0x144>
         :                      load_balance():
         :                      busiest->active_balance = 1;
    0.00 :   ffff8000101256f0:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000101256f4:       str     w1, [x19, #2520]
         :                      busiest->push_cpu = this_cpu;
    0.00 :   ffff8000101256f8:       ldr     w1, [x29, #104]
    0.00 :   ffff8000101256fc:       str     w1, [x19, #2524]
         :                      raw_spin_unlock_irqrestore(&busiest->lock, flags);
    0.00 :   ffff800010125700:       mov     x1, x0
    0.00 :   ffff800010125704:       mov     x0, x19
    0.00 :   ffff800010125708:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      stop_one_cpu_nowait(cpu_of(busiest),
    0.00 :   ffff80001012570c:       ldr     w0, [x19, #2568]
    0.00 :   ffff800010125710:       mov     x2, x19
    0.00 :   ffff800010125714:       adrp    x1, ffff800010121000 <task_tick_fair+0x3d8>
    0.00 :   ffff800010125718:       add     x3, x19, #0x9e0
    0.00 :   ffff80001012571c:       add     x1, x1, #0xd58
    0.00 :   ffff800010125720:       bl      ffff8000101a16f8 <stop_one_cpu_nowait>
         :                      sd->nr_balance_failed = sd->cache_nice_tries+1;
    0.00 :   ffff800010125724:       ldr     x2, [x29, #128]
         :                      asym_active_balance():
         :                      return env->idle != CPU_NOT_IDLE && (env->sd->flags & SD_ASYM_PACKING) &&
    0.00 :   ffff800010125728:       ldr     w1, [x29, #188]
         :                      load_balance():
         :                      sd->nr_balance_failed = sd->cache_nice_tries+1;
    0.00 :   ffff80001012572c:       ldr     w0, [x2, #48]
         :                      asym_active_balance():
         :                      return env->idle != CPU_NOT_IDLE && (env->sd->flags & SD_ASYM_PACKING) &&
    0.00 :   ffff800010125730:       cmp     w1, #0x1
         :                      load_balance():
         :                      sd->nr_balance_failed = sd->cache_nice_tries+1;
    0.00 :   ffff800010125734:       add     w0, w0, #0x1
    0.00 :   ffff800010125738:       str     w0, [x2, #76]
         :                      asym_active_balance():
         :                      return env->idle != CPU_NOT_IDLE && (env->sd->flags & SD_ASYM_PACKING) &&
    0.00 :   ffff80001012573c:       b.eq    ffff8000101257bc <load_balance+0xad4>  // b.none
         :                      voluntary_active_balance():
         :                      struct sched_domain *sd = env->sd;
    0.00 :   ffff800010125740:       ldr     x19, [x29, #144]
         :                      asym_active_balance():
         :                      return env->idle != CPU_NOT_IDLE && (env->sd->flags & SD_ASYM_PACKING) &&
    0.00 :   ffff800010125744:       ldr     w0, [x19, #56]
    0.00 :   ffff800010125748:       tbnz    w0, #11, ffff800010125830 <load_balance+0xb48>
         :                      voluntary_active_balance():
         :                      (env->src_rq->cfs.h_nr_running == 1)) {
    0.00 :   ffff80001012574c:       ldr     x2, [x29, #152]
         :                      if ((env->idle != CPU_NOT_IDLE) &&
    0.00 :   ffff800010125750:       ldr     w0, [x2, #156]
    0.00 :   ffff800010125754:       cmp     w0, #0x1
    0.00 :   ffff800010125758:       b.ne    ffff8000101257bc <load_balance+0xad4>  // b.any
         :                      check_cpu_capacity():
         :                      return ((rq->cpu_capacity * sd->imbalance_pct) <
    0.00 :   ffff80001012575c:       ldr     x0, [x2, #2480]
         :                      (rq->cpu_capacity_orig * 100));
    0.00 :   ffff800010125760:       ldr     x3, [x2, #2488]
         :                      return ((rq->cpu_capacity * sd->imbalance_pct) <
    0.00 :   ffff800010125764:       ldr     w1, [x19, #44]
         :                      (rq->cpu_capacity_orig * 100));
    0.00 :   ffff800010125768:       add     x2, x3, x3, lsl #1
         :                      return ((rq->cpu_capacity * sd->imbalance_pct) <
    0.00 :   ffff80001012576c:       mul     x0, x1, x0
         :                      (rq->cpu_capacity_orig * 100));
    0.00 :   ffff800010125770:       add     x2, x3, x2, lsl #3
         :                      voluntary_active_balance():
         :                      if ((check_cpu_capacity(env->src_rq, sd)) &&
    0.00 :   ffff800010125774:       cmp     x0, x2, lsl #2
    0.00 :   ffff800010125778:       b.cs    ffff8000101257bc <load_balance+0xad4>  // b.hs, b.nlast
         :                      capacity_of():
         :                      return cpu_rq(cpu)->cpu_capacity;
    0.00 :   ffff80001012577c:       ldpsw   x3, x4, [x29, #160]
    0.00 :   ffff800010125780:       adrp    x2, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010125784:       add     x2, x2, #0x8e8
    0.00 :   ffff800010125788:       adrp    x0, ffff8000114d5000 <tegra_to+0x180>
    0.00 :   ffff80001012578c:       add     x0, x0, #0xd80
    0.00 :   ffff800010125790:       ldr     x3, [x2, x3, lsl #3]
    0.00 :   ffff800010125794:       ldr     x2, [x2, x4, lsl #3]
    0.00 :   ffff800010125798:       add     x3, x3, x0
    0.00 :   ffff80001012579c:       add     x0, x2, x0
         :                      voluntary_active_balance():
         :                      (capacity_of(env->src_cpu)*sd->imbalance_pct < capacity_of(env->dst_cpu)*100))
    0.00 :   ffff8000101257a0:       ldr     x3, [x3, #2480]
    0.00 :   ffff8000101257a4:       ldr     x2, [x0, #2480]
    0.00 :   ffff8000101257a8:       add     x0, x2, x2, lsl #1
    0.00 :   ffff8000101257ac:       mul     x1, x1, x3
    0.00 :   ffff8000101257b0:       add     x0, x2, x0, lsl #3
         :                      if ((check_cpu_capacity(env->src_rq, sd)) &&
    0.00 :   ffff8000101257b4:       cmp     x1, x0, lsl #2
    0.00 :   ffff8000101257b8:       b.cc    ffff8000101254e4 <load_balance+0x7fc>  // b.lo, b.ul, b.last
         :                      if (env->migration_type == migrate_misfit)
    0.00 :   ffff8000101257bc:       ldr     w0, [x29, #228]
    0.00 :   ffff8000101257c0:       cmp     w0, #0x3
    0.00 :   ffff8000101257c4:       b.eq    ffff8000101254e4 <load_balance+0x7fc>  // b.none
         :                      load_balance():
         :                      if (sd->balance_interval < sd->max_interval)
    0.00 :   ffff8000101257c8:       ldr     x0, [x29, #128]
    0.00 :   ffff8000101257cc:       ldr     x1, [x0, #32]
    0.00 :   ffff8000101257d0:       ldr     w0, [x0, #72]
    0.00 :   ffff8000101257d4:       cmp     x1, w0, uxtw
    0.00 :   ffff8000101257d8:       b.ls    ffff800010125874 <load_balance+0xb8c>  // b.plast
         :                      sd->balance_interval *= 2;
    0.00 :   ffff8000101257dc:       lsl     w0, w0, #1
    0.00 :   ffff8000101257e0:       ldr     x1, [x29, #128]
    0.00 :   ffff8000101257e4:       b       ffff800010125360 <load_balance+0x678>
         :                      raw_spin_unlock_irqrestore(&busiest->lock,
    0.00 :   ffff8000101257e8:       mov     x1, x0
    0.00 :   ffff8000101257ec:       mov     x0, x19
    0.00 :   ffff8000101257f0:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      goto out_one_pinned;
    0.00 :   ffff8000101257f4:       ldr     x21, [x29, #32]
         :                      env.flags |= LBF_ALL_PINNED;
    0.00 :   ffff8000101257f8:       ldr     w0, [x29, #208]
         :                      goto out_one_pinned;
    0.00 :   ffff8000101257fc:       ldp     x19, x20, [x29, #16]
         :                      env.flags |= LBF_ALL_PINNED;
    0.00 :   ffff800010125800:       orr     w0, w0, #0x1
         :                      goto out_one_pinned;
    0.00 :   ffff800010125804:       ldp     x23, x24, [x29, #48]
         :                      env.flags |= LBF_ALL_PINNED;
    0.00 :   ffff800010125808:       str     w0, [x29, #208]
         :                      goto out_one_pinned;
    0.00 :   ffff80001012580c:       ldr     x25, [x29, #64]
    0.00 :   ffff800010125810:       ldp     x27, x28, [x29, #80]
    0.00 :   ffff800010125814:       b       ffff800010125448 <load_balance+0x760>
         :                      int *group_imbalance = &sd_parent->groups->sgc->imbalance;
    0.00 :   ffff800010125818:       ldr     x0, [x1, #16]
   28.78 :   ffff80001012581c:       ldr     x0, [x0, #16]
         :                      if (*group_imbalance)
   30.83 :   ffff800010125820:       ldr     w1, [x0, #40]
    0.00 :   ffff800010125824:       cbz     w1, ffff800010125440 <load_balance+0x758>
         :                      *group_imbalance = 0;
    0.00 :   ffff800010125828:       str     wzr, [x0, #40]
    0.00 :   ffff80001012582c:       b       ffff800010125440 <load_balance+0x758>
         :                      sched_asym_prefer():
    0.00 :   ffff800010125830:       ldp     w21, w0, [x29, #160]
    0.00 :   ffff800010125834:       bl      ffff800010122780 <arch_asym_cpu_priority>
    0.00 :   ffff800010125838:       mov     w20, w0
    0.00 :   ffff80001012583c:       mov     w0, w21
    0.00 :   ffff800010125840:       bl      ffff800010122780 <arch_asym_cpu_priority>
         :                      asym_active_balance():
         :                      return env->idle != CPU_NOT_IDLE && (env->sd->flags & SD_ASYM_PACKING) &&
    0.00 :   ffff800010125844:       cmp     w20, w0
    0.00 :   ffff800010125848:       b.gt    ffff8000101254e4 <load_balance+0x7fc>
         :                      voluntary_active_balance():
         :                      if ((env->idle != CPU_NOT_IDLE) &&
    0.00 :   ffff80001012584c:       ldr     w0, [x29, #188]
    0.00 :   ffff800010125850:       cmp     w0, #0x1
    0.00 :   ffff800010125854:       b.ne    ffff80001012574c <load_balance+0xa64>  // b.any
    0.00 :   ffff800010125858:       b       ffff8000101257bc <load_balance+0xad4>
    0.00 :   ffff80001012585c:       ldp     x19, x20, [x29, #16]
    0.58 :   ffff800010125860:       ldr     x21, [x29, #32]
    0.00 :   ffff800010125864:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff800010125868:       ldr     x25, [x29, #64]
    0.00 :   ffff80001012586c:       ldp     x27, x28, [x29, #80]
    0.00 :   ffff800010125870:       b       ffff800010125430 <load_balance+0x748>
    0.00 :   ffff800010125874:       ldp     x19, x20, [x29, #16]
         :                      load_balance():
         :                      sd->nr_balance_failed = sd->cache_nice_tries+1;
    0.00 :   ffff800010125878:       mov     w26, #0x0                       // #0
    0.00 :   ffff80001012587c:       ldr     x21, [x29, #32]
    0.00 :   ffff800010125880:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff800010125884:       ldr     x25, [x29, #64]
    0.00 :   ffff800010125888:       ldp     x27, x28, [x29, #80]
    0.00 :   ffff80001012588c:       b       ffff800010125378 <load_balance+0x690>
    0.00 :   ffff800010125890:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff800010125894:       ldr     x21, [x29, #32]
    0.00 :   ffff800010125898:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff80001012589c:       ldr     x25, [x29, #64]
    0.00 :   ffff8000101258a0:       ldp     x27, x28, [x29, #80]
    0.00 :   ffff8000101258a4:       b       ffff800010125440 <load_balance+0x758>
    0.00 :   ffff8000101258a8:       stp     x19, x20, [x29, #16]
    0.00 :   ffff8000101258ac:       str     x21, [x29, #32]
    0.00 :   ffff8000101258b0:       stp     x23, x24, [x29, #48]
    0.00 :   ffff8000101258b4:       str     x25, [x29, #64]
    0.00 :   ffff8000101258b8:       stp     x27, x28, [x29, #80]
         :                      }
    0.00 :   ffff8000101258bc:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (269 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010702878 <alloc_iova_fast>:
         :                      alloc_iova_fast():
         :                      * fails too and the flush_rcache flag is set then the rcache will be flushed.
         :                      */
         :                      unsigned long
         :                      alloc_iova_fast(struct iova_domain *iovad, unsigned long size,
         :                      unsigned long limit_pfn, bool flush_rcache)
         :                      {
    2.56 :   ffff800010702878:       stp     x29, x30, [sp, #-96]!
         :                      __order_base_2():
         :                      )
         :
         :                      static inline __attribute_const__
         :                      int __order_base_2(unsigned long n)
         :                      {
         :                      return n > 1 ? ilog2(n - 1) + 1 : 0;
    0.00 :   ffff80001070287c:       cmp     x1, #0x1
         :                      alloc_iova_fast():
    7.44 :   ffff800010702880:       mov     x29, sp
    0.37 :   ffff800010702884:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010702888:       and     w20, w3, #0xff
    3.31 :   ffff80001070288c:       stp     x21, x22, [sp, #32]
    0.00 :   ffff800010702890:       mov     x21, x0
    4.09 :   ffff800010702894:       stp     x23, x24, [sp, #48]
    0.00 :   ffff800010702898:       mov     x24, x1
    4.46 :   ffff80001070289c:       str     x25, [sp, #64]
    0.00 :   ffff8000107028a0:       mov     x25, x2
         :                      __order_base_2():
    0.00 :   ffff8000107028a4:       b.ls    ffff800010702924 <alloc_iova_fast+0xac>  // b.plast
    0.00 :   ffff8000107028a8:       sub     x0, x1, #0x1
         :                      __fls():
         :                      *
         :                      * Undefined if no set bit exists, so code should check against 0 first.
         :                      */
         :                      static __always_inline unsigned long __fls(unsigned long word)
         :                      {
         :                      return (sizeof(word) * 8) - 1 - __builtin_clzl(word);
    0.00 :   ffff8000107028ac:       mov     x19, #0x3f                      // #63
    0.00 :   ffff8000107028b0:       clz     x0, x0
    0.00 :   ffff8000107028b4:       sub     x19, x19, x0
         :                      fls64():
         :                      #elif BITS_PER_LONG == 64
         :                      static __always_inline int fls64(__u64 x)
         :                      {
         :                      if (x == 0)
         :                      return 0;
         :                      return __fls(x) + 1;
    0.00 :   ffff8000107028b8:       add     w19, w19, #0x1
         :                      iova_rcache_get():
         :                      unsigned long size,
         :                      unsigned long limit_pfn)
         :                      {
         :                      unsigned int log_size = order_base_2(size);
         :
         :                      if (log_size >= IOVA_RANGE_CACHE_MAX_SIZE)
    0.00 :   ffff8000107028bc:       cmp     w19, #0x5
    0.00 :   ffff8000107028c0:       b.ls    ffff800010702aec <alloc_iova_fast+0x274>  // b.plast
    0.00 :   ffff8000107028c4:       adrp    x23, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff8000107028c8:       adrp    x22, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      alloc_iova_fast():
         :                      for_each_online_cpu(cpu)
    0.00 :   ffff8000107028cc:       add     x23, x23, #0x120
    0.00 :   ffff8000107028d0:       add     x22, x22, #0x2b4
    0.00 :   ffff8000107028d4:       nop
         :                      new_iova = alloc_iova(iovad, size, limit_pfn, true);
    0.00 :   ffff8000107028d8:       mov     w3, #0x1                        // #1
    0.00 :   ffff8000107028dc:       mov     x2, x25
    0.00 :   ffff8000107028e0:       mov     x1, x24
    0.00 :   ffff8000107028e4:       mov     x0, x21
    0.00 :   ffff8000107028e8:       bl      ffff800010701850 <alloc_iova>
         :                      if (!new_iova) {
    0.00 :   ffff8000107028ec:       cbnz    x0, ffff800010702a5c <alloc_iova_fast+0x1e4>
         :                      if (!flush_rcache)
    0.00 :   ffff8000107028f0:       cbz     w20, ffff800010702a7c <alloc_iova_fast+0x204>
         :                      for_each_online_cpu(cpu)
    0.00 :   ffff8000107028f4:       mov     w19, #0xffffffff                // #-1
    0.00 :   ffff8000107028f8:       mov     x1, x23
    0.00 :   ffff8000107028fc:       mov     w0, w19
    0.00 :   ffff800010702900:       bl      ffff800010c93a58 <cpumask_next>
         :                      flush_rcache = false;
    0.00 :   ffff800010702904:       mov     w20, #0x0                       // #0
         :                      for_each_online_cpu(cpu)
    0.00 :   ffff800010702908:       ldr     w1, [x22]
    0.00 :   ffff80001070290c:       mov     w19, w0
    0.00 :   ffff800010702910:       cmp     w0, w1
    0.00 :   ffff800010702914:       b.cs    ffff8000107028d8 <alloc_iova_fast+0x60>  // b.hs, b.nlast
         :                      free_cpu_cached_iovas(cpu, iovad);
    0.00 :   ffff800010702918:       mov     x1, x21
    0.00 :   ffff80001070291c:       bl      ffff8000107027e8 <free_cpu_cached_iovas>
    0.00 :   ffff800010702920:       b       ffff8000107028f8 <alloc_iova_fast+0x80>
    3.71 :   ffff800010702924:       stp     x26, x27, [x29, #72]
         :                      __order_base_2():
    0.00 :   ffff800010702928:       mov     x23, #0x80                      // #128
         :                      iova_rcache_get():
         :                      unsigned int log_size = order_base_2(size);
    0.00 :   ffff80001070292c:       mov     x19, #0x0                       // #0
         :                      __iova_rcache_get():
         :                      cpu_rcache = raw_cpu_ptr(rcache->cpu_rcaches);
    0.00 :   ffff800010702930:       add     x0, x19, x19, lsl #3
         :                      iova_rcache_get():
         :                      return 0;
         :
         :                      return __iova_rcache_get(&iovad->rcaches[log_size], limit_pfn - size);
   30.13 :   ffff800010702934:       sub     x26, x25, x24
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800010702938:       mrs     x1, tpidr_el1
         :                      __iova_rcache_get():
         :                      cpu_rcache = raw_cpu_ptr(rcache->cpu_rcaches);
    0.74 :   ffff80001070293c:       lsl     x0, x0, #2
         :                      iova_rcache_get():
         :                      return __iova_rcache_get(&iovad->rcaches[log_size], limit_pfn - size);
    0.00 :   ffff800010702940:       add     x26, x26, #0x1
         :                      __iova_rcache_get():
         :                      cpu_rcache = raw_cpu_ptr(rcache->cpu_rcaches);
    0.00 :   ffff800010702944:       sub     x0, x0, x19
    0.00 :   ffff800010702948:       add     x0, x21, x0, lsl #3
    0.74 :   ffff80001070294c:       ldr     x22, [x0, #400]
    0.00 :   ffff800010702950:       add     x22, x22, x1
         :                      spin_lock_irqsave(&cpu_rcache->lock, flags);
    0.00 :   ffff800010702954:       mov     x0, x22
    0.00 :   ffff800010702958:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
    0.00 :   ffff80001070295c:       mov     x27, x0
         :                      if (!iova_magazine_empty(cpu_rcache->loaded)) {
    0.00 :   ffff800010702960:       ldr     x3, [x22, #8]
         :                      iova_magazine_empty():
         :                      return (!mag || mag->size == 0);
    0.00 :   ffff800010702964:       cbz     x3, ffff800010702970 <alloc_iova_fast+0xf8>
    0.00 :   ffff800010702968:       ldr     x5, [x3]
    0.00 :   ffff80001070296c:       cbnz    x5, ffff8000107029e0 <alloc_iova_fast+0x168>
    0.00 :   ffff800010702970:       str     x28, [x29, #88]
         :                      __iova_rcache_get():
         :                      } else if (!iova_magazine_empty(cpu_rcache->prev)) {
    0.00 :   ffff800010702974:       ldr     x0, [x22, #16]
         :                      iova_magazine_empty():
         :                      return (!mag || mag->size == 0);
    0.00 :   ffff800010702978:       cbz     x0, ffff800010702984 <alloc_iova_fast+0x10c>
    0.00 :   ffff80001070297c:       ldr     x1, [x0]
    0.00 :   ffff800010702980:       cbnz    x1, ffff8000107029c8 <alloc_iova_fast+0x150>
         :                      iova_rcache_get():
         :                      return __iova_rcache_get(&iovad->rcaches[log_size], limit_pfn - size);
    0.00 :   ffff800010702984:       add     x23, x21, x23
         :                      spin_lock():
         :                      raw_spin_lock_init(&(_lock)->rlock);            \
         :                      } while (0)
         :
         :                      static __always_inline void spin_lock(spinlock_t *lock)
         :                      {
         :                      raw_spin_lock(&lock->rlock);
    0.00 :   ffff800010702988:       mov     x0, x23
    0.00 :   ffff80001070298c:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      __iova_rcache_get():
         :                      if (rcache->depot_size > 0) {
    0.00 :   ffff800010702990:       add     x0, x19, x19, lsl #3
    0.00 :   ffff800010702994:       lsl     x0, x0, #2
    0.00 :   ffff800010702998:       sub     x19, x0, x19
    0.00 :   ffff80001070299c:       add     x28, x21, x19, lsl #3
    0.00 :   ffff8000107029a0:       ldr     x0, [x28, #136]
    0.00 :   ffff8000107029a4:       cbnz    x0, ffff800010702ab0 <alloc_iova_fast+0x238>
         :                      spin_unlock():
         :                      raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
         :                      } while (0)
         :
         :                      static __always_inline void spin_unlock(spinlock_t *lock)
         :                      {
         :                      raw_spin_unlock(&lock->rlock);
    0.00 :   ffff8000107029a8:       mov     x0, x23
    0.00 :   ffff8000107029ac:       bl      ffff800010cb2408 <_raw_spin_unlock>
         :                      spin_unlock_irqrestore():
         :                      raw_spin_unlock_irq(&lock->rlock);
         :                      }
         :
         :                      static __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
         :                      {
         :                      raw_spin_unlock_irqrestore(&lock->rlock, flags);
    0.00 :   ffff8000107029b0:       mov     x1, x27
    0.00 :   ffff8000107029b4:       mov     x0, x22
    0.00 :   ffff8000107029b8:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
    0.00 :   ffff8000107029bc:       ldr     x28, [x29, #88]
    0.00 :   ffff8000107029c0:       ldp     x26, x27, [x29, #72]
    0.00 :   ffff8000107029c4:       b       ffff8000107028c4 <alloc_iova_fast+0x4c>
         :                      __iova_rcache_get():
         :                      swap(cpu_rcache->prev, cpu_rcache->loaded);
    0.00 :   ffff8000107029c8:       stp     x0, x3, [x22, #8]
    0.00 :   ffff8000107029cc:       ldr     x5, [x0]
         :                      iova_magazine_empty():
         :                      return (!mag || mag->size == 0);
    0.00 :   ffff8000107029d0:       cbnz    x5, ffff8000107029d8 <alloc_iova_fast+0x160>
         :                      iova_magazine_pop():
         :                      BUG_ON(iova_magazine_empty(mag));
    0.00 :   ffff8000107029d4:       brk     #0x800
    0.00 :   ffff8000107029d8:       ldr     x28, [x29, #88]
         :                      iova_magazine_empty():
         :                      return (!mag || mag->size == 0);
    0.00 :   ffff8000107029dc:       mov     x3, x0
         :                      iova_magazine_pop():
         :                      for (i = mag->size - 1; mag->pfns[i] > limit_pfn; i--)
    0.00 :   ffff8000107029e0:       sub     w0, w5, #0x1
    0.00 :   ffff8000107029e4:       sxtw    x2, w0
    0.00 :   ffff8000107029e8:       add     x1, x3, x2, lsl #3
    0.00 :   ffff8000107029ec:       ldr     x19, [x1, #8]
    0.00 :   ffff8000107029f0:       cmp     x26, x19
    0.00 :   ffff8000107029f4:       b.cs    ffff800010702a14 <alloc_iova_fast+0x19c>  // b.hs, b.nlast
         :                      if (i == 0)
    0.00 :   ffff8000107029f8:       cbz     w0, ffff800010702a9c <alloc_iova_fast+0x224>
         :                      for (i = mag->size - 1; mag->pfns[i] > limit_pfn; i--)
    0.00 :   ffff8000107029fc:       sub     w0, w0, #0x1
    0.00 :   ffff800010702a00:       sxtw    x2, w0
    0.00 :   ffff800010702a04:       add     x4, x3, x2, lsl #3
    0.00 :   ffff800010702a08:       ldr     x19, [x4, #8]
    0.00 :   ffff800010702a0c:       cmp     x26, x19
    0.00 :   ffff800010702a10:       b.cc    ffff8000107029f8 <alloc_iova_fast+0x180>  // b.lo, b.ul, b.last
         :                      mag->pfns[i] = mag->pfns[--mag->size];
    0.00 :   ffff800010702a14:       sub     x5, x5, #0x1
    0.00 :   ffff800010702a18:       str     x5, [x3]
    0.00 :   ffff800010702a1c:       add     x2, x3, x2, lsl #3
         :                      spin_unlock_irqrestore():
    0.00 :   ffff800010702a20:       mov     x1, x27
         :                      iova_magazine_pop():
    0.00 :   ffff800010702a24:       add     x5, x3, x5, lsl #3
         :                      spin_unlock_irqrestore():
    0.00 :   ffff800010702a28:       mov     x0, x22
         :                      iova_magazine_pop():
    0.00 :   ffff800010702a2c:       ldr     x3, [x5, #8]
    0.00 :   ffff800010702a30:       str     x3, [x2, #8]
         :                      spin_unlock_irqrestore():
    0.00 :   ffff800010702a34:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
    4.09 :   ffff800010702a38:       ldp     x26, x27, [x29, #72]
         :                      alloc_iova_fast():
         :                      if (iova_pfn)
    0.00 :   ffff800010702a3c:       cbz     x19, ffff8000107028c4 <alloc_iova_fast+0x4c>
         :                      }
   33.17 :   ffff800010702a40:       mov     x0, x19
    0.00 :   ffff800010702a44:       ldr     x25, [sp, #64]
    0.00 :   ffff800010702a48:       ldp     x19, x20, [sp, #16]
    0.37 :   ffff800010702a4c:       ldp     x21, x22, [sp, #32]
    4.81 :   ffff800010702a50:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff800010702a54:       ldp     x29, x30, [sp], #96
    0.00 :   ffff800010702a58:       ret
         :                      return new_iova->pfn_lo;
    0.00 :   ffff800010702a5c:       ldr     x19, [x0, #32]
         :                      }
    0.00 :   ffff800010702a60:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010702a64:       mov     x0, x19
    0.00 :   ffff800010702a68:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010702a6c:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff800010702a70:       ldr     x25, [sp, #64]
    0.00 :   ffff800010702a74:       ldp     x29, x30, [sp], #96
    0.00 :   ffff800010702a78:       ret
         :                      return 0;
    0.00 :   ffff800010702a7c:       mov     x19, #0x0                       // #0
         :                      }
    0.00 :   ffff800010702a80:       ldr     x25, [sp, #64]
    0.00 :   ffff800010702a84:       mov     x0, x19
    0.00 :   ffff800010702a88:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010702a8c:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010702a90:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff800010702a94:       ldp     x29, x30, [sp], #96
    0.00 :   ffff800010702a98:       ret
         :                      spin_unlock_irqrestore():
    0.00 :   ffff800010702a9c:       mov     x1, x27
    0.00 :   ffff800010702aa0:       mov     x0, x22
    0.00 :   ffff800010702aa4:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
    0.00 :   ffff800010702aa8:       ldp     x26, x27, [x29, #72]
    0.00 :   ffff800010702aac:       b       ffff8000107028c4 <alloc_iova_fast+0x4c>
         :                      iova_magazine_free():
         :                      kfree(mag);
    0.00 :   ffff800010702ab0:       ldr     x0, [x22, #8]
    0.00 :   ffff800010702ab4:       bl      ffff80001024fe88 <kfree>
         :                      __iova_rcache_get():
         :                      cpu_rcache->loaded = rcache->depot[--rcache->depot_size];
    0.00 :   ffff800010702ab8:       ldr     x2, [x28, #136]
         :                      spin_unlock():
         :                      raw_spin_unlock(&lock->rlock);
    0.00 :   ffff800010702abc:       mov     x0, x23
         :                      __iova_rcache_get():
    0.00 :   ffff800010702ac0:       sub     x2, x2, #0x1
    0.00 :   ffff800010702ac4:       str     x2, [x28, #136]
    0.00 :   ffff800010702ac8:       add     x19, x19, x2
    0.00 :   ffff800010702acc:       add     x2, x19, #0x12
    0.00 :   ffff800010702ad0:       ldr     x1, [x21, x2, lsl #3]
    0.00 :   ffff800010702ad4:       str     x1, [x22, #8]
         :                      spin_unlock():
    0.00 :   ffff800010702ad8:       bl      ffff800010cb2408 <_raw_spin_unlock>
         :                      __iova_rcache_get():
         :                      iova_pfn = iova_magazine_pop(cpu_rcache->loaded, limit_pfn);
    0.00 :   ffff800010702adc:       ldr     x0, [x22, #8]
         :                      iova_magazine_empty():
         :                      return (!mag || mag->size == 0);
    0.00 :   ffff800010702ae0:       cbz     x0, ffff8000107029d4 <alloc_iova_fast+0x15c>
    0.00 :   ffff800010702ae4:       ldr     x5, [x0]
    0.00 :   ffff800010702ae8:       b       ffff8000107029d0 <alloc_iova_fast+0x158>
    0.00 :   ffff800010702aec:       mov     w23, #0x118                     // #280
    0.00 :   ffff800010702af0:       mov     x0, #0x80                       // #128
    0.00 :   ffff800010702af4:       stp     x26, x27, [x29, #72]
    0.00 :   ffff800010702af8:       umaddl  x23, w19, w23, x0
    0.00 :   ffff800010702afc:       b       ffff800010702930 <alloc_iova_fast+0xb8>
 Percent |	Source code & Disassembly of vmlinux for cycles (141 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001024af98 <__my_cpu_offset>:
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
   97.05 :   ffff80001024af98:       mrs     x0, tpidr_el1
         :                      ARM64_HAS_VIRT_HOST_EXTN)
         :                      : "=r" (off) :
         :                      "Q" (*(const unsigned long *)current_stack_pointer));
         :
         :                      return off;
         :                      }
    2.95 :   ffff80001024af9c:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (156 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010487cf0 <find_next_and_bit>:
         :                      _find_next_bit():
         :                      const unsigned long *addr2, unsigned long nbits,
         :                      unsigned long start, unsigned long invert)
         :                      {
         :                      unsigned long tmp;
         :
         :                      if (unlikely(start >= nbits))
    5.89 :   ffff800010487cf0:       cmp     x2, x3
    0.74 :   ffff800010487cf4:       b.ls    ffff800010487d58 <find_next_and_bit+0x68>  // b.plast
         :                      return nbits;
         :
         :                      tmp = addr1[start / BITS_PER_LONG];
    3.69 :   ffff800010487cf8:       lsr     x4, x3, #6
    5.79 :   ffff800010487cfc:       ldr     x6, [x0, x4, lsl #3]
         :                      if (addr2)
    0.00 :   ffff800010487d00:       cbz     x1, ffff800010487d0c <find_next_and_bit+0x1c>
         :                      tmp &= addr2[start / BITS_PER_LONG];
   17.02 :   ffff800010487d04:       ldr     x4, [x1, x4, lsl #3]
    2.60 :   ffff800010487d08:       and     x6, x6, x4
         :                      tmp ^= invert;
         :
         :                      /* Handle 1st word. */
         :                      tmp &= BITMAP_FIRST_WORD_MASK(start);
    0.72 :   ffff800010487d0c:       mov     x4, #0xffffffffffffffff         // #-1
         :                      start = round_down(start, BITS_PER_LONG);
    1.10 :   ffff800010487d10:       and     x5, x3, #0xffffffffffffffc0
         :                      tmp &= BITMAP_FIRST_WORD_MASK(start);
    9.38 :   ffff800010487d14:       lsl     x3, x4, x3
         :
         :                      while (!tmp) {
    2.52 :   ffff800010487d18:       ands    x3, x3, x6
    0.00 :   ffff800010487d1c:       b.ne    ffff800010487d44 <find_next_and_bit+0x54>  // b.any
         :                      start += BITS_PER_LONG;
    7.15 :   ffff800010487d20:       add     x5, x5, #0x40
         :                      if (start >= nbits)
    1.42 :   ffff800010487d24:       cmp     x2, x5
    0.77 :   ffff800010487d28:       b.ls    ffff800010487d58 <find_next_and_bit+0x68>  // b.plast
         :                      return nbits;
         :
         :                      tmp = addr1[start / BITS_PER_LONG];
   15.06 :   ffff800010487d2c:       lsr     x4, x5, #6
    2.98 :   ffff800010487d30:       ldr     x3, [x0, x4, lsl #3]
         :                      if (addr2)
    1.25 :   ffff800010487d34:       cbz     x1, ffff800010487d40 <find_next_and_bit+0x50>
         :                      tmp &= addr2[start / BITS_PER_LONG];
    9.89 :   ffff800010487d38:       ldr     x4, [x1, x4, lsl #3]
    0.67 :   ffff800010487d3c:       and     x3, x3, x4
         :                      while (!tmp) {
    0.00 :   ffff800010487d40:       cbz     x3, ffff800010487d20 <find_next_and_bit+0x30>
         :                      __ffs():
         :                      *
         :                      * Undefined if no bit exists, so code should check against 0 first.
         :                      */
         :                      static __always_inline unsigned long __ffs(unsigned long word)
         :                      {
         :                      return __builtin_ctzl(word);
    4.37 :   ffff800010487d44:       rbit    x3, x3
    0.00 :   ffff800010487d48:       clz     x3, x3
         :                      _find_next_bit():
         :                      tmp ^= invert;
         :                      }
         :
         :                      return min(start + __ffs(tmp), nbits);
    0.00 :   ffff800010487d4c:       add     x3, x3, x5
    0.00 :   ffff800010487d50:       cmp     x2, x3
    5.67 :   ffff800010487d54:       csel    x2, x2, x3, ls  // ls = plast
         :                      find_next_and_bit():
         :                      unsigned long find_next_and_bit(const unsigned long *addr1,
         :                      const unsigned long *addr2, unsigned long size,
         :                      unsigned long offset)
         :                      {
         :                      return _find_next_bit(addr1, addr2, size, offset, 0UL);
         :                      }
    1.32 :   ffff800010487d58:       mov     x0, x2
    0.00 :   ffff800010487d5c:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (234 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001044fed8 <bio_associate_blkg>:
         :                      bio_associate_blkg():
         :                      * If one is not found, bio_lookup_blkg() creates the blkg.  If a blkg is
         :                      * already associated, the css is reused and association redone as the
         :                      * request_queue may have changed.
         :                      */
         :                      void bio_associate_blkg(struct bio *bio)
         :                      {
    3.86 :   ffff80001044fed8:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff80001044fedc:       mov     x29, sp
    2.99 :   ffff80001044fee0:       str     x19, [sp, #16]
    0.00 :   ffff80001044fee4:       mov     x19, x0
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff80001044fee8:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      bio_associate_blkg():
         :                      struct cgroup_subsys_state *css;
         :
         :                      rcu_read_lock();
         :
         :                      if (bio->bi_blkg)
    2.98 :   ffff80001044feec:       ldr     x0, [x19, #72]
    0.00 :   ffff80001044fef0:       cbz     x0, ffff80001044ff18 <bio_associate_blkg+0x40>
         :                      bio_blkcg():
         :                      */
         :                      static inline struct blkcg *bio_blkcg(struct bio *bio)
         :                      {
         :                      if (bio && bio->bi_blkg)
         :                      return bio->bi_blkg->blkcg;
         :                      return NULL;
    0.00 :   ffff80001044fef4:       mov     x1, #0x0                        // #0
         :                      if (bio && bio->bi_blkg)
    0.00 :   ffff80001044fef8:       cbz     x19, ffff80001044ff00 <bio_associate_blkg+0x28>
         :                      return bio->bi_blkg->blkcg;
    0.00 :   ffff80001044fefc:       ldr     x1, [x0, #40]
         :                      bio_associate_blkg():
         :                      css = &bio_blkcg(bio)->css;
         :                      else
         :                      css = blkcg_css();
         :
         :                      bio_associate_blkg_from_css(bio, css);
   59.45 :   ffff80001044ff00:       mov     x0, x19
    0.00 :   ffff80001044ff04:       bl      ffff80001044fe58 <bio_associate_blkg_from_css>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    2.55 :   ffff80001044ff08:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      bio_associate_blkg():
         :
         :                      rcu_read_unlock();
         :                      }
    0.43 :   ffff80001044ff0c:       ldr     x19, [sp, #16]
    3.00 :   ffff80001044ff10:       ldp     x29, x30, [sp], #32
    0.00 :   ffff80001044ff14:       ret
         :                      blkcg_css():
         :                      css = kthread_blkcg();
    1.26 :   ffff80001044ff18:       bl      ffff80001010a970 <kthread_blkcg>
    6.46 :   ffff80001044ff1c:       mov     x1, x0
         :                      if (css)
    0.00 :   ffff80001044ff20:       cbnz    x0, ffff80001044ff00 <bio_associate_blkg+0x28>
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.85 :   ffff80001044ff24:       mrs     x0, sp_el0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.42 :   ffff80001044ff28:       ldr     x0, [x0, #2000]
         :                      task_css():
         :                      * See task_css_check().
         :                      */
         :                      static inline struct cgroup_subsys_state *task_css(struct task_struct *task,
         :                      int subsys_id)
         :                      {
         :                      return task_css_check(task, subsys_id, false);
   15.75 :   ffff80001044ff2c:       ldr     x1, [x0, #24]
    0.00 :   ffff80001044ff30:       b       ffff80001044ff00 <bio_associate_blkg+0x28>
 Percent |	Source code & Disassembly of vmlinux for cycles (223 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000106fe780 <__iommu_dma_map>:
         :                      __iommu_dma_map():
         :                      iommu_dma_free_iova(cookie, dma_addr, size);
         :                      }
         :
         :                      static dma_addr_t __iommu_dma_map(struct device *dev, phys_addr_t phys,
         :                      size_t size, int prot, u64 dma_mask)
         :                      {
    4.95 :   ffff8000106fe780:       stp     x29, x30, [sp, #-80]!
    0.00 :   ffff8000106fe784:       mov     x29, sp
    0.45 :   ffff8000106fe788:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000106fe78c:       mov     x20, x0
    0.45 :   ffff8000106fe790:       stp     x21, x22, [sp, #32]
    0.00 :   ffff8000106fe794:       mov     x22, x1
    3.14 :   ffff8000106fe798:       stp     x23, x24, [sp, #48]
    0.00 :   ffff8000106fe79c:       mov     w23, w3
    2.26 :   ffff8000106fe7a0:       stp     x25, x26, [sp, #64]
    0.00 :   ffff8000106fe7a4:       sub     x24, x2, #0x1
    0.00 :   ffff8000106fe7a8:       mov     x25, x4
         :                      struct iommu_domain *domain = iommu_get_dma_domain(dev);
    0.00 :   ffff8000106fe7ac:       bl      ffff8000106fcb18 <iommu_get_dma_domain>
    8.11 :   ffff8000106fe7b0:       mov     x21, x0
         :                      struct iommu_dma_cookie *cookie = domain->iova_cookie;
         :                      struct iova_domain *iovad = &cookie->iovad;
         :                      size_t iova_off = iova_offset(iovad, phys);
         :                      dma_addr_t iova;
         :
         :                      if (unlikely(iommu_dma_deferred_attach(dev, domain)))
    0.00 :   ffff8000106fe7b4:       mov     x0, x20
    0.00 :   ffff8000106fe7b8:       mov     x1, x21
         :                      struct iommu_dma_cookie *cookie = domain->iova_cookie;
    0.45 :   ffff8000106fe7bc:       ldr     x26, [x21, #64]
         :                      iova_mask():
         :                      return __ffs(iovad->granule);
         :                      }
         :
         :                      static inline unsigned long iova_mask(struct iova_domain *iovad)
         :                      {
         :                      return iovad->granule - 1;
   15.22 :   ffff8000106fe7c0:       ldr     x19, [x26, #40]
    0.00 :   ffff8000106fe7c4:       sub     x19, x19, #0x1
         :                      iova_offset():
         :                      }
         :
         :                      static inline size_t iova_offset(struct iova_domain *iovad, dma_addr_t iova)
         :                      {
         :                      return iova & iova_mask(iovad);
    0.00 :   ffff8000106fe7c8:       and     x19, x19, x22
         :                      __iommu_dma_map():
         :                      if (unlikely(iommu_dma_deferred_attach(dev, domain)))
    0.00 :   ffff8000106fe7cc:       bl      ffff8000106fdf18 <iommu_dma_deferred_attach>
    0.45 :   ffff8000106fe7d0:       cbnz    w0, ffff8000106fe838 <__iommu_dma_map+0xb8>
         :                      return DMA_MAPPING_ERROR;
         :
         :                      size = iova_align(iovad, size + iova_off);
   11.60 :   ffff8000106fe7d4:       ldr     x0, [x26, #40]
         :
         :                      iova = iommu_dma_alloc_iova(domain, size, dma_mask, dev);
    0.00 :   ffff8000106fe7d8:       add     x4, x20, #0x258
    0.00 :   ffff8000106fe7dc:       add     x3, x20, #0x60
    0.00 :   ffff8000106fe7e0:       mov     x2, x25
         :                      iova_align():
         :                      }
         :
         :                      static inline size_t iova_align(struct iova_domain *iovad, size_t size)
         :                      {
         :                      return ALIGN(size, iovad->granule);
    6.74 :   ffff8000106fe7e4:       add     x20, x0, x19
    0.00 :   ffff8000106fe7e8:       neg     x1, x0
    0.00 :   ffff8000106fe7ec:       add     x20, x20, x24
         :                      __iommu_dma_map():
    0.00 :   ffff8000106fe7f0:       mov     x0, x21
         :                      iova_align():
    0.44 :   ffff8000106fe7f4:       and     x20, x20, x1
         :                      __iommu_dma_map():
    0.00 :   ffff8000106fe7f8:       mov     x1, x20
    0.00 :   ffff8000106fe7fc:       bl      ffff8000106fe670 <iommu_dma_alloc_iova.isra.28>
    6.73 :   ffff8000106fe800:       mov     x24, x0
         :                      if (!iova)
    0.00 :   ffff8000106fe804:       cbz     x0, ffff8000106fe838 <__iommu_dma_map+0xb8>
         :                      return DMA_MAPPING_ERROR;
         :
         :                      if (iommu_map_atomic(domain, iova, phys - iova_off, size, prot)) {
    0.00 :   ffff8000106fe808:       sub     x2, x22, x19
    0.00 :   ffff8000106fe80c:       mov     x1, x0
    0.00 :   ffff8000106fe810:       mov     w4, w23
    0.00 :   ffff8000106fe814:       mov     x3, x20
    0.00 :   ffff8000106fe818:       mov     x0, x21
         :                      iommu_dma_free_iova(cookie, iova, size);
         :                      return DMA_MAPPING_ERROR;
         :                      }
         :                      return iova + iova_off;
    0.00 :   ffff8000106fe81c:       add     x19, x24, x19
         :                      if (iommu_map_atomic(domain, iova, phys - iova_off, size, prot)) {
    0.00 :   ffff8000106fe820:       bl      ffff8000106fcd00 <iommu_map_atomic>
    7.65 :   ffff8000106fe824:       cbz     w0, ffff8000106fe83c <__iommu_dma_map+0xbc>
         :                      iommu_dma_free_iova(cookie, iova, size);
    0.00 :   ffff8000106fe828:       mov     x2, x20
    0.00 :   ffff8000106fe82c:       mov     x1, x24
    0.00 :   ffff8000106fe830:       mov     x0, x26
    0.00 :   ffff8000106fe834:       bl      ffff8000106fdda0 <iommu_dma_free_iova>
         :                      return DMA_MAPPING_ERROR;
    0.00 :   ffff8000106fe838:       mov     x19, #0xffffffffffffffff        // #-1
         :                      }
    4.92 :   ffff8000106fe83c:       mov     x0, x19
    2.65 :   ffff8000106fe840:       ldp     x19, x20, [sp, #16]
   18.40 :   ffff8000106fe844:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000106fe848:       ldp     x23, x24, [sp, #48]
    2.24 :   ffff8000106fe84c:       ldp     x25, x26, [sp, #64]
    3.15 :   ffff8000106fe850:       ldp     x29, x30, [sp], #80
    0.00 :   ffff8000106fe854:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (241 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001010a970 <kthread_blkcg>:
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    9.91 :   ffff80001010a970:       mrs     x0, sp_el0
         :                      kthread_blkcg():
         :                      */
         :                      struct cgroup_subsys_state *kthread_blkcg(void)
         :                      {
         :                      struct kthread *kthread;
         :
         :                      if (current->flags & PF_KTHREAD) {
    6.20 :   ffff80001010a974:       ldr     w1, [x0, #44]
    0.00 :   ffff80001010a978:       tbz     w1, #21, ffff80001010a98c <kthread_blkcg+0x1c>
         :                      to_kthread():
         :                      return (__force void *)k->set_child_tid;
    0.00 :   ffff80001010a97c:       ldr     x0, [x0, #1344]
         :                      kthread_blkcg():
         :                      kthread = to_kthread(current);
         :                      if (kthread)
    0.00 :   ffff80001010a980:       cbz     x0, ffff80001010a98c <kthread_blkcg+0x1c>
         :                      return kthread->blkcg_css;
    0.00 :   ffff80001010a984:       ldr     x0, [x0, #88]
         :                      }
         :                      return NULL;
         :                      }
    0.00 :   ffff80001010a988:       ret
         :                      return NULL;
   83.89 :   ffff80001010a98c:       mov     x0, #0x0                        // #0
         :                      }
    0.00 :   ffff80001010a990:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (230 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010455f58 <blk_attempt_plug_merge>:
         :                      blk_queue_is_zoned():
         :                      return q->limits.zoned;
         :                      }
         :
         :                      static inline bool blk_queue_is_zoned(struct request_queue *q)
         :                      {
         :                      switch (blk_queue_zoned_model(q)) {
    6.56 :   ffff800010455f58:       ldr     w4, [x0, #1140]
    0.00 :   ffff800010455f5c:       sub     w4, w4, #0x1
    0.43 :   ffff800010455f60:       cmp     w4, #0x1
    0.00 :   ffff800010455f64:       b.hi    ffff800010455f70 <blk_attempt_plug_merge+0x18>  // b.pmore
         :                      op_is_write():
         :                      bio->bi_opf = op | op_flags;
         :                      }
         :
         :                      static inline bool op_is_write(unsigned int op)
         :                      {
         :                      return (op & 1);
    0.00 :   ffff800010455f68:       ldr     w4, [x1, #16]
         :                      blk_mq_plug():
         :                      {
         :                      /*
         :                      * For regular block devices or read operations, use the context plug
         :                      * which may be NULL if blk_start_plug() was not executed.
         :                      */
         :                      if (!blk_queue_is_zoned(q) || !op_is_write(bio_op(bio)))
    0.00 :   ffff800010455f6c:       tbnz    w4, #0, ffff800010456074 <blk_attempt_plug_merge+0x11c>
         :                      blk_attempt_plug_merge():
         :                      *
         :                      * Caller must ensure !blk_queue_nomerges(q) beforehand.
         :                      */
         :                      bool blk_attempt_plug_merge(struct request_queue *q, struct bio *bio,
         :                      unsigned int nr_segs, struct request **same_queue_rq)
         :                      {
   12.13 :   ffff800010455f70:       stp     x29, x30, [sp, #-80]!
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    3.50 :   ffff800010455f74:       mrs     x4, sp_el0
         :                      blk_attempt_plug_merge():
    0.00 :   ffff800010455f78:       mov     x29, sp
    4.36 :   ffff800010455f7c:       str     x21, [sp, #32]
         :                      blk_mq_plug():
         :                      return current->plug;
    2.58 :   ffff800010455f80:       ldr     x21, [x4, #1840]
         :                      blk_attempt_plug_merge():
         :                      struct blk_plug *plug;
         :                      struct request *rq;
         :                      struct list_head *plug_list;
         :
         :                      plug = blk_mq_plug(q, bio);
         :                      if (!plug)
    0.00 :   ffff800010455f84:       cbz     x21, ffff800010456080 <blk_attempt_plug_merge+0x128>
   15.21 :   ffff800010455f88:       str     x19, [x29, #16]
         :                      return false;
         :
         :                      plug_list = &plug->mq_list;
         :
         :                      list_for_each_entry_reverse(rq, plug_list, queuelist) {
    4.80 :   ffff800010455f8c:       ldr     x4, [x21, #8]
    0.00 :   ffff800010455f90:       sub     x19, x4, #0x48
    0.00 :   ffff800010455f94:       cmp     x21, x4
    0.00 :   ffff800010455f98:       b.eq    ffff80001045607c <blk_attempt_plug_merge+0x124>  // b.none
    0.00 :   ffff800010455f9c:       cmp     x3, #0x0
    0.00 :   ffff800010455fa0:       str     x20, [x29, #24]
    0.00 :   ffff800010455fa4:       stp     x22, x23, [x29, #40]
    0.00 :   ffff800010455fa8:       mov     x20, x0
    0.00 :   ffff800010455fac:       stp     x24, x25, [x29, #56]
    0.00 :   ffff800010455fb0:       mov     x23, x1
    0.00 :   ffff800010455fb4:       mov     x24, x3
    0.00 :   ffff800010455fb8:       mov     w25, w2
    0.00 :   ffff800010455fbc:       cset    w22, ne  // ne = any
    0.00 :   ffff800010455fc0:       b       ffff800010455fd4 <blk_attempt_plug_merge+0x7c>
    0.00 :   ffff800010455fc4:       ldr     x4, [x19, #80]
    0.00 :   ffff800010455fc8:       sub     x19, x4, #0x48
    0.00 :   ffff800010455fcc:       cmp     x21, x4
    0.00 :   ffff800010455fd0:       b.eq    ffff800010456090 <blk_attempt_plug_merge+0x138>  // b.none
         :                      bool merged = false;
         :
         :                      if (rq->q == q && same_queue_rq) {
    0.00 :   ffff800010455fd4:       ldr     x4, [x19]
    0.00 :   ffff800010455fd8:       cmp     w22, #0x0
    0.00 :   ffff800010455fdc:       ccmp    x4, x20, #0x0, ne  // ne = any
    0.00 :   ffff800010455fe0:       b.eq    ffff80001045604c <blk_attempt_plug_merge+0xf4>  // b.none
         :                      * rq in a queue
         :                      **/
         :                      *same_queue_rq = rq;
         :                      }
         :
         :                      if (rq->q != q || !blk_rq_merge_ok(rq, bio))
    0.00 :   ffff800010455fe4:       cmp     x20, x4
    0.00 :   ffff800010455fe8:       b.ne    ffff800010455fc4 <blk_attempt_plug_merge+0x6c>  // b.any
    0.00 :   ffff800010455fec:       mov     x1, x23
    0.00 :   ffff800010455ff0:       mov     x0, x19
    0.00 :   ffff800010455ff4:       bl      ffff80001045c340 <blk_rq_merge_ok>
    0.00 :   ffff800010455ff8:       tst     w0, #0xff
    0.00 :   ffff800010455ffc:       b.eq    ffff800010455fc4 <blk_attempt_plug_merge+0x6c>  // b.none
         :                      continue;
         :
         :                      switch (blk_try_merge(rq, bio)) {
    0.00 :   ffff800010456000:       mov     x1, x23
    0.00 :   ffff800010456004:       mov     x0, x19
    0.00 :   ffff800010456008:       bl      ffff80001045c498 <blk_try_merge>
    0.00 :   ffff80001045600c:       cmp     w0, #0x2
    0.00 :   ffff800010456010:       b.eq    ffff8000104560a0 <blk_attempt_plug_merge+0x148>  // b.none
    0.00 :   ffff800010456014:       cmp     w0, #0x3
    0.00 :   ffff800010456018:       b.eq    ffff800010456058 <blk_attempt_plug_merge+0x100>  // b.none
    0.00 :   ffff80001045601c:       cmp     w0, #0x1
    0.00 :   ffff800010456020:       b.ne    ffff800010455fc4 <blk_attempt_plug_merge+0x6c>  // b.any
         :                      case ELEVATOR_BACK_MERGE:
         :                      merged = bio_attempt_back_merge(rq, bio, nr_segs);
         :                      break;
         :                      case ELEVATOR_FRONT_MERGE:
         :                      merged = bio_attempt_front_merge(rq, bio, nr_segs);
    0.00 :   ffff800010456024:       mov     w2, w25
    0.00 :   ffff800010456028:       mov     x1, x23
    0.00 :   ffff80001045602c:       mov     x0, x19
    0.00 :   ffff800010456030:       bl      ffff800010455cf0 <bio_attempt_front_merge>
    0.00 :   ffff800010456034:       and     w0, w0, #0xff
         :                      break;
         :                      default:
         :                      break;
         :                      }
         :
         :                      if (merged)
    0.00 :   ffff800010456038:       cbz     w0, ffff800010455fc4 <blk_attempt_plug_merge+0x6c>
    0.00 :   ffff80001045603c:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff800010456040:       ldp     x22, x23, [x29, #40]
    0.00 :   ffff800010456044:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff800010456048:       b       ffff800010456084 <blk_attempt_plug_merge+0x12c>
         :                      *same_queue_rq = rq;
    0.00 :   ffff80001045604c:       str     x19, [x24]
    0.00 :   ffff800010456050:       ldr     x4, [x19]
    0.00 :   ffff800010456054:       b       ffff800010455fe4 <blk_attempt_plug_merge+0x8c>
         :                      merged = bio_attempt_discard_merge(q, rq, bio);
    0.00 :   ffff800010456058:       mov     x2, x23
    0.00 :   ffff80001045605c:       mov     x1, x19
    0.00 :   ffff800010456060:       mov     x0, x20
    0.00 :   ffff800010456064:       bl      ffff800010455da8 <bio_attempt_discard_merge>
    0.00 :   ffff800010456068:       and     w0, w0, #0xff
         :                      if (merged)
    0.00 :   ffff80001045606c:       cbz     w0, ffff800010455fc4 <blk_attempt_plug_merge+0x6c>
    0.00 :   ffff800010456070:       b       ffff80001045603c <blk_attempt_plug_merge+0xe4>
         :                      return false;
    0.00 :   ffff800010456074:       mov     w0, #0x0                        // #0
         :                      return true;
         :                      }
         :
         :                      return false;
         :                      }
    0.00 :   ffff800010456078:       ret
   25.19 :   ffff80001045607c:       ldr     x19, [x29, #16]
         :                      return false;
    3.47 :   ffff800010456080:       mov     w0, #0x0                        // #0
         :                      }
    6.13 :   ffff800010456084:       ldr     x21, [sp, #32]
   15.63 :   ffff800010456088:       ldp     x29, x30, [sp], #80
    0.00 :   ffff80001045608c:       ret
    0.00 :   ffff800010456090:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff800010456094:       ldp     x22, x23, [x29, #40]
    0.00 :   ffff800010456098:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff80001045609c:       b       ffff800010456080 <blk_attempt_plug_merge+0x128>
         :                      merged = bio_attempt_back_merge(rq, bio, nr_segs);
    0.00 :   ffff8000104560a0:       mov     w2, w25
    0.00 :   ffff8000104560a4:       mov     x1, x23
    0.00 :   ffff8000104560a8:       mov     x0, x19
    0.00 :   ffff8000104560ac:       bl      ffff800010455c40 <bio_attempt_back_merge>
    0.00 :   ffff8000104560b0:       and     w0, w0, #0xff
         :                      if (merged)
    0.00 :   ffff8000104560b4:       cbz     w0, ffff800010455fc4 <blk_attempt_plug_merge+0x6c>
    0.00 :   ffff8000104560b8:       b       ffff80001045603c <blk_attempt_plug_merge+0xe4>
 Percent |	Source code & Disassembly of vmlinux for cycles (225 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101d0140 <filemap_check_errors>:
         :                      filemap_check_errors():
         :                      for (i = 0; i < pagevec_count(pvec); i++)
         :                      page_cache_free_page(mapping, pvec->pages[i]);
         :                      }
         :
         :                      int filemap_check_errors(struct address_space *mapping)
         :                      {
   11.09 :   ffff8000101d0140:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff8000101d0144:       adrp    x1, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000101d0148:       add     x2, x1, #0x8c8
    0.00 :   ffff8000101d014c:       mov     x29, sp
   30.20 :   ffff8000101d0150:       ldr     x3, [x2]
    3.11 :   ffff8000101d0154:       str     x3, [x29, #24]
    0.00 :   ffff8000101d0158:       mov     x3, #0x0                        // #0
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    2.21 :   ffff8000101d015c:       ldr     x2, [x0, #120]
         :                      filemap_check_errors():
         :                      int ret = 0;
         :                      /* Check for outstanding write errors */
         :                      if (test_bit(AS_ENOSPC, &mapping->flags) &&
    0.00 :   ffff8000101d0160:       tst     w2, #0x2
    0.00 :   ffff8000101d0164:       b.ne    ffff8000101d0194 <filemap_check_errors+0x54>  // b.any
         :                      int ret = 0;
    3.13 :   ffff8000101d0168:       mov     w2, #0x0                        // #0
         :                      test_bit():
    4.90 :   ffff8000101d016c:       ldr     x3, [x0, #120]
         :                      filemap_check_errors():
         :                      test_and_clear_bit(AS_ENOSPC, &mapping->flags))
         :                      ret = -ENOSPC;
         :                      if (test_bit(AS_EIO, &mapping->flags) &&
    0.00 :   ffff8000101d0170:       tbnz    w3, #0, ffff8000101d01bc <filemap_check_errors+0x7c>
         :                      test_and_clear_bit(AS_EIO, &mapping->flags))
         :                      ret = -EIO;
         :                      return ret;
         :                      }
    0.00 :   ffff8000101d0174:       add     x1, x1, #0x8c8
    0.00 :   ffff8000101d0178:       mov     w0, w2
    6.65 :   ffff8000101d017c:       ldr     x2, [x29, #24]
   23.59 :   ffff8000101d0180:       ldr     x1, [x1]
    0.00 :   ffff8000101d0184:       eor     x1, x2, x1
    0.00 :   ffff8000101d0188:       cbnz    x1, ffff8000101d0210 <filemap_check_errors+0xd0>
   15.14 :   ffff8000101d018c:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000101d0190:       ret
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000101d0194:       ldr     x2, [x0, #120]
         :                      test_and_clear_bit():
         :                      {
         :                      long old;
         :                      unsigned long mask = BIT_MASK(nr);
         :
         :                      p += BIT_WORD(nr);
         :                      if (!(READ_ONCE(*p) & mask))
    0.00 :   ffff8000101d0198:       tbz     w2, #1, ffff8000101d0168 <filemap_check_errors+0x28>
         :                      filemap_check_errors():
         :                      test_and_clear_bit(AS_ENOSPC, &mapping->flags))
    0.00 :   ffff8000101d019c:       add     x3, x0, #0x78
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000101d01a0:       b       ffff8000101d0200 <filemap_check_errors+0xc0>
    0.00 :   ffff8000101d01a4:       b       ffff8000101d0200 <filemap_check_errors+0xc0>
         :                      __lse_atomic64_fetch_andnot():
         :                      ATOMIC64_FETCH_OP(_relaxed,   , op, asm_op)                     \
         :                      ATOMIC64_FETCH_OP(_acquire,  a, op, asm_op, "memory")           \
         :                      ATOMIC64_FETCH_OP(_release,  l, op, asm_op, "memory")           \
         :                      ATOMIC64_FETCH_OP(        , al, op, asm_op, "memory")
         :
         :                      ATOMIC64_FETCH_OPS(andnot, ldclr)
    0.00 :   ffff8000101d01a8:       mov     x2, #0x2                        // #2
    0.00 :   ffff8000101d01ac:       ldclral x2, x2, [x3]
         :                      filemap_check_errors():
         :                      if (test_bit(AS_ENOSPC, &mapping->flags) &&
    0.00 :   ffff8000101d01b0:       tbz     w2, #1, ffff8000101d0168 <filemap_check_errors+0x28>
         :                      ret = -ENOSPC;
    0.00 :   ffff8000101d01b4:       mov     w2, #0xffffffe4                 // #-28
    0.00 :   ffff8000101d01b8:       b       ffff8000101d016c <filemap_check_errors+0x2c>
         :                      __read_once_size():
    0.00 :   ffff8000101d01bc:       ldr     x3, [x0, #120]
         :                      test_and_clear_bit():
    0.00 :   ffff8000101d01c0:       tbz     w3, #0, ffff8000101d0174 <filemap_check_errors+0x34>
         :                      filemap_check_errors():
         :                      test_and_clear_bit(AS_EIO, &mapping->flags))
    0.00 :   ffff8000101d01c4:       add     x4, x0, #0x78
         :                      arch_static_branch_jump():
    0.00 :   ffff8000101d01c8:       b       ffff8000101d01ec <filemap_check_errors+0xac>
    0.00 :   ffff8000101d01cc:       b       ffff8000101d01ec <filemap_check_errors+0xac>
         :                      __lse_atomic64_fetch_andnot():
    0.00 :   ffff8000101d01d0:       mov     x3, #0x1                        // #1
    0.00 :   ffff8000101d01d4:       ldclral x3, x3, [x4]
    0.00 :   ffff8000101d01d8:       mov     x0, x3
         :                      filemap_check_errors():
         :                      ret = -EIO;
    0.00 :   ffff8000101d01dc:       tst     x0, #0x1
    0.00 :   ffff8000101d01e0:       mov     w0, #0xfffffffb                 // #-5
    0.00 :   ffff8000101d01e4:       csel    w2, w2, w0, eq  // eq = none
    0.00 :   ffff8000101d01e8:       b       ffff8000101d0174 <filemap_check_errors+0x34>
         :                      __ll_sc_atomic64_fetch_andnot():
         :                      /*
         :                      * GAS converts the mysterious and undocumented BIC (immediate) alias to
         :                      * an AND (immediate) instruction with the immediate inverted. We don't
         :                      * have a constraint for this, so fall back to register.
         :                      */
         :                      ATOMIC64_OPS(andnot, bic, )
    0.00 :   ffff8000101d01ec:       mov     x4, #0x1                        // #1
    0.00 :   ffff8000101d01f0:       add     x0, x0, #0x78
    0.00 :   ffff8000101d01f4:       b       ffff8000101d5318 <generic_file_write_iter+0x2e0>
    0.00 :   ffff8000101d01f8:       mov     x0, x3
    0.00 :   ffff8000101d01fc:       b       ffff8000101d01dc <filemap_check_errors+0x9c>
    0.00 :   ffff8000101d0200:       mov     x3, #0x2                        // #2
    0.00 :   ffff8000101d0204:       add     x6, x0, #0x78
    0.00 :   ffff8000101d0208:       b       ffff8000101d5334 <generic_file_write_iter+0x2fc>
    0.00 :   ffff8000101d020c:       b       ffff8000101d01b0 <filemap_check_errors+0x70>
         :                      filemap_check_errors():
         :                      }
    0.00 :   ffff8000101d0210:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (224 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010483fe8 <iov_iter_advance>:
         :                      iov_iter_advance():
         :                      /* ... and discard everything past that point */
         :                      pipe_truncate(i);
         :                      }
         :
         :                      void iov_iter_advance(struct iov_iter *i, size_t size)
         :                      {
    0.00 :   ffff800010483fe8:       stp     x29, x30, [sp, #-80]!
    0.00 :   ffff800010483fec:       mov     x29, sp
    5.36 :   ffff800010483ff0:       str     x20, [sp, #24]
    0.00 :   ffff800010483ff4:       mov     x20, x0
         :                      iov_iter_is_pipe():
         :                      return iov_iter_type(i) == ITER_BVEC;
         :                      }
         :
         :                      static inline bool iov_iter_is_pipe(const struct iov_iter *i)
         :                      {
         :                      return iov_iter_type(i) == ITER_PIPE;
    6.29 :   ffff800010483ff8:       ldr     w0, [x0]
    0.00 :   ffff800010483ffc:       ldr     x5, [x20, #16]
         :                      iov_iter_type():
         :                      return i->type & ~(READ | WRITE);
    0.44 :   ffff800010484000:       and     w2, w0, #0xfffffffe
         :                      iov_iter_advance():
         :                      if (unlikely(iov_iter_is_pipe(i))) {
    0.00 :   ffff800010484004:       cmp     w2, #0x20
    0.00 :   ffff800010484008:       b.eq    ffff800010484094 <iov_iter_advance+0xac>  // b.none
         :                      pipe_advance(i, size);
         :                      return;
         :                      }
         :                      if (unlikely(iov_iter_is_discard(i))) {
    0.00 :   ffff80001048400c:       cmp     w2, #0x40
    0.00 :   ffff800010484010:       b.eq    ffff80001048419c <iov_iter_advance+0x1b4>  // b.none
         :                      i->count -= size;
         :                      return;
         :                      }
         :                      iterate_and_advance(i, size, v, 0, 0, 0)
    3.54 :   ffff800010484014:       cbz     x5, ffff800010484088 <iov_iter_advance+0xa0>
    5.81 :   ffff800010484018:       cmp     x1, x5
    0.00 :   ffff80001048401c:       str     x19, [x29, #16]
    0.00 :   ffff800010484020:       csel    x1, x1, x5, ls  // ls = plast
    0.89 :   ffff800010484024:       ldr     x19, [x20, #8]
    0.00 :   ffff800010484028:       tbnz    w0, #4, ffff8000104841ec <iov_iter_advance+0x204>
    0.88 :   ffff80001048402c:       tbnz    w0, #3, ffff8000104842d0 <iov_iter_advance+0x2e8>
    4.48 :   ffff800010484030:       tbnz    w0, #6, ffff80001048433c <iov_iter_advance+0x354>
    0.00 :   ffff800010484034:       ldr     x2, [x20, #24]
    3.62 :   ffff800010484038:       ldr     x4, [x2, #8]
    0.00 :   ffff80001048403c:       sub     x0, x4, x19
    7.15 :   ffff800010484040:       cmp     x0, x1
    0.00 :   ffff800010484044:       csel    x0, x0, x1, ls  // ls = plast
    4.06 :   ffff800010484048:       cbz     x0, ffff800010484344 <iov_iter_advance+0x35c>
    0.46 :   ffff80001048404c:       add     x19, x19, x0
    0.00 :   ffff800010484050:       sub     x0, x1, x0
    0.00 :   ffff800010484054:       mov     x3, x2
    0.00 :   ffff800010484058:       cbnz    x0, ffff8000104841cc <iov_iter_advance+0x1e4>
   25.00 :   ffff80001048405c:       cmp     x19, x4
    0.00 :   ffff800010484060:       b.ne    ffff80001048406c <iov_iter_advance+0x84>  // b.any
    7.90 :   ffff800010484064:       add     x3, x3, #0x10
    0.00 :   ffff800010484068:       mov     x19, #0x0                       // #0
    3.11 :   ffff80001048406c:       ldr     x0, [x20, #32]
    0.00 :   ffff800010484070:       sub     x2, x3, x2
    0.00 :   ffff800010484074:       sub     x2, x0, x2, asr #4
    3.12 :   ffff800010484078:       stp     x3, x2, [x20, #24]
    0.00 :   ffff80001048407c:       sub     x1, x5, x1
    3.13 :   ffff800010484080:       stp     x19, x1, [x20, #8]
   11.21 :   ffff800010484084:       ldr     x19, [x29, #16]
         :                      }
    2.68 :   ffff800010484088:       ldr     x20, [sp, #24]
    0.89 :   ffff80001048408c:       ldp     x29, x30, [sp], #80
    0.00 :   ffff800010484090:       ret
    0.00 :   ffff800010484094:       str     x19, [x29, #16]
    0.00 :   ffff800010484098:       cmp     x1, x5
    0.00 :   ffff80001048409c:       str     x21, [x29, #32]
    0.00 :   ffff8000104840a0:       csel    x1, x1, x5, ls  // ls = plast
         :                      pipe_advance():
         :                      struct pipe_inode_info *pipe = i->pipe;
    0.00 :   ffff8000104840a4:       ldr     x21, [x20, #24]
         :                      if (size) {
    0.00 :   ffff8000104840a8:       cbz     x1, ffff80001048411c <iov_iter_advance+0x134>
         :                      unsigned int p_mask = pipe->ring_size - 1;
    0.00 :   ffff8000104840ac:       ldr     w6, [x21, #68]
    0.00 :   ffff8000104840b0:       mov     w8, #0x28                       // #40
         :                      unsigned int i_head = i->head;
    0.00 :   ffff8000104840b4:       ldr     w3, [x20, #32]
         :                      if (off) /* make it relative to the beginning of buffer */
    0.00 :   ffff8000104840b8:       mov     x2, x1
         :                      unsigned int p_mask = pipe->ring_size - 1;
    0.00 :   ffff8000104840bc:       sub     w6, w6, #0x1
    0.00 :   ffff8000104840c0:       ldr     x7, [x21, #120]
         :                      size_t off = i->iov_offset, left = size;
    0.00 :   ffff8000104840c4:       ldr     x4, [x20, #8]
    0.00 :   ffff8000104840c8:       and     w0, w6, w3
    0.00 :   ffff8000104840cc:       umaddl  x0, w0, w8, x7
         :                      if (off) /* make it relative to the beginning of buffer */
    0.00 :   ffff8000104840d0:       cbz     x4, ffff8000104840e0 <iov_iter_advance+0xf8>
         :                      left += off - pipe->bufs[i_head & p_mask].offset;
    0.00 :   ffff8000104840d4:       ldr     w2, [x0, #8]
    0.00 :   ffff8000104840d8:       add     x4, x4, x1
    0.00 :   ffff8000104840dc:       sub     x2, x4, x2
         :                      if (left <= buf->len)
    0.00 :   ffff8000104840e0:       ldr     w4, [x0, #12]
    0.00 :   ffff8000104840e4:       cmp     x4, x2
    0.00 :   ffff8000104840e8:       b.cs    ffff80001048410c <iov_iter_advance+0x124>  // b.hs, b.nlast
         :                      buf = &pipe->bufs[i_head & p_mask];
    0.00 :   ffff8000104840ec:       mov     w8, #0x28                       // #40
         :                      i_head++;
    0.00 :   ffff8000104840f0:       add     w3, w3, #0x1
         :                      left -= buf->len;
    0.00 :   ffff8000104840f4:       sub     x2, x2, x4
         :                      buf = &pipe->bufs[i_head & p_mask];
    0.00 :   ffff8000104840f8:       and     w0, w6, w3
    0.00 :   ffff8000104840fc:       umaddl  x0, w0, w8, x7
         :                      if (left <= buf->len)
    0.00 :   ffff800010484100:       ldr     w4, [x0, #12]
    0.00 :   ffff800010484104:       cmp     x4, x2
    0.00 :   ffff800010484108:       b.cc    ffff8000104840f0 <iov_iter_advance+0x108>  // b.lo, b.ul, b.last
         :                      i->head = i_head;
    0.00 :   ffff80001048410c:       str     w3, [x20, #32]
         :                      i->iov_offset = buf->offset + left;
    0.00 :   ffff800010484110:       ldr     w0, [x0, #8]
    0.00 :   ffff800010484114:       add     x2, x0, x2
    0.00 :   ffff800010484118:       str     x2, [x20, #8]
         :                      i->count -= size;
    0.00 :   ffff80001048411c:       sub     x1, x5, x1
    0.00 :   ffff800010484120:       str     x1, [x20, #16]
         :                      pipe_truncate():
         :                      if (!pipe_empty(p_head, p_tail)) {
    0.00 :   ffff800010484124:       ldp     w19, w0, [x21, #56]
    0.00 :   ffff800010484128:       cmp     w0, w19
    0.00 :   ffff80001048412c:       b.eq    ffff80001048434c <iov_iter_advance+0x364>  // b.none
    0.00 :   ffff800010484130:       stp     x22, x23, [x29, #40]
         :                      size_t off = i->iov_offset;
    0.00 :   ffff800010484134:       ldr     x0, [x20, #8]
         :                      unsigned int p_mask = pipe->ring_size - 1;
    0.00 :   ffff800010484138:       ldr     w22, [x21, #68]
         :                      unsigned int i_head = i->head;
    0.00 :   ffff80001048413c:       ldr     w20, [x20, #32]
         :                      unsigned int p_mask = pipe->ring_size - 1;
    0.00 :   ffff800010484140:       sub     w22, w22, #0x1
         :                      if (off) {
    0.00 :   ffff800010484144:       cbnz    x0, ffff8000104841a8 <iov_iter_advance+0x1c0>
         :                      pipe_buf_release(pipe, &pipe->bufs[p_head & p_mask]);
    0.00 :   ffff800010484148:       mov     w23, #0x28                      // #40
         :                      while (p_head != i_head) {
    0.00 :   ffff80001048414c:       cmp     w19, w20
    0.00 :   ffff800010484150:       b.eq    ffff800010484188 <iov_iter_advance+0x1a0>  // b.none
    0.00 :   ffff800010484154:       nop
         :                      pipe_buf_release(pipe, &pipe->bufs[p_head & p_mask]);
    0.00 :   ffff800010484158:       ldr     x1, [x21, #120]
         :                      p_head--;
    0.00 :   ffff80001048415c:       sub     w19, w19, #0x1
         :                      pipe_buf_release(pipe, &pipe->bufs[p_head & p_mask]);
    0.00 :   ffff800010484160:       and     w2, w22, w19
         :                      pipe_buf_release():
         :                      struct pipe_buffer *buf)
         :                      {
         :                      const struct pipe_buf_operations *ops = buf->ops;
         :
         :                      buf->ops = NULL;
         :                      ops->release(pipe, buf);
    0.00 :   ffff800010484164:       mov     x0, x21
         :                      pipe_truncate():
    0.00 :   ffff800010484168:       umaddl  x2, w2, w23, x1
         :                      pipe_buf_release():
    0.00 :   ffff80001048416c:       mov     x1, x2
         :                      const struct pipe_buf_operations *ops = buf->ops;
    0.00 :   ffff800010484170:       ldr     x3, [x2, #16]
         :                      buf->ops = NULL;
    0.00 :   ffff800010484174:       str     xzr, [x2, #16]
         :                      ops->release(pipe, buf);
    0.00 :   ffff800010484178:       ldr     x2, [x3, #8]
    0.00 :   ffff80001048417c:       blr     x2
         :                      pipe_truncate():
         :                      while (p_head != i_head) {
    0.00 :   ffff800010484180:       cmp     w19, w20
    0.00 :   ffff800010484184:       b.ne    ffff800010484158 <iov_iter_advance+0x170>  // b.any
         :                      pipe->head = p_head;
    0.00 :   ffff800010484188:       str     w20, [x21, #56]
    0.00 :   ffff80001048418c:       ldr     x19, [x29, #16]
    0.00 :   ffff800010484190:       ldp     x21, x22, [x29, #32]
    0.00 :   ffff800010484194:       ldr     x23, [x29, #48]
    0.00 :   ffff800010484198:       b       ffff800010484088 <iov_iter_advance+0xa0>
         :                      iov_iter_advance():
         :                      i->count -= size;
    0.00 :   ffff80001048419c:       sub     x1, x5, x1
    0.00 :   ffff8000104841a0:       str     x1, [x20, #16]
         :                      return;
    0.00 :   ffff8000104841a4:       b       ffff800010484088 <iov_iter_advance+0xa0>
         :                      pipe_truncate():
         :                      buf = &pipe->bufs[i_head & p_mask];
    0.00 :   ffff8000104841a8:       ldr     x2, [x21, #120]
    0.00 :   ffff8000104841ac:       and     w1, w22, w20
    0.00 :   ffff8000104841b0:       mov     w3, #0x28                       // #40
    0.00 :   ffff8000104841b4:       add     w20, w20, #0x1
    0.00 :   ffff8000104841b8:       umaddl  x1, w1, w3, x2
         :                      buf->len = off - buf->offset;
    0.00 :   ffff8000104841bc:       ldr     w2, [x1, #8]
    0.00 :   ffff8000104841c0:       sub     w0, w0, w2
    0.00 :   ffff8000104841c4:       str     w0, [x1, #12]
    0.00 :   ffff8000104841c8:       b       ffff800010484148 <iov_iter_advance+0x160>
         :                      iov_iter_advance():
         :                      iterate_and_advance(i, size, v, 0, 0, 0)
    0.00 :   ffff8000104841cc:       add     x3, x3, #0x10
    0.00 :   ffff8000104841d0:       ldr     x4, [x3, #8]
    0.00 :   ffff8000104841d4:       cmp     x0, x4
    0.00 :   ffff8000104841d8:       csel    x19, x0, x4, ls  // ls = plast
    0.00 :   ffff8000104841dc:       cbz     x19, ffff8000104841cc <iov_iter_advance+0x1e4>
    0.00 :   ffff8000104841e0:       subs    x0, x0, x19
    0.00 :   ffff8000104841e4:       b.eq    ffff80001048405c <iov_iter_advance+0x74>  // b.none
    0.00 :   ffff8000104841e8:       b       ffff8000104841cc <iov_iter_advance+0x1e4>
    0.00 :   ffff8000104841ec:       stp     x21, x22, [x29, #32]
    0.00 :   ffff8000104841f0:       mov     w6, w1
    0.00 :   ffff8000104841f4:       mov     w2, #0x0                        // #0
    0.00 :   ffff8000104841f8:       mov     x4, #0x0                        // #0
    0.00 :   ffff8000104841fc:       ldr     x21, [x20, #24]
    0.00 :   ffff800010484200:       mov     w8, #0x1000                     // #4096
    0.00 :   ffff800010484204:       nop
    0.00 :   ffff800010484208:       lsl     x22, x4, #4
    0.00 :   ffff80001048420c:       add     x3, x21, x22
    0.00 :   ffff800010484210:       cbz     w6, ffff8000104842b4 <iov_iter_advance+0x2cc>
    0.00 :   ffff800010484214:       ldp     w0, w3, [x3, #8]
    0.00 :   ffff800010484218:       sub     w7, w0, w19
    0.00 :   ffff80001048421c:       add     w3, w19, w3
    0.00 :   ffff800010484220:       cmp     w7, w6
    0.00 :   ffff800010484224:       and     w3, w3, #0xfff
    0.00 :   ffff800010484228:       sub     w3, w8, w3
    0.00 :   ffff80001048422c:       csel    w7, w7, w6, ls  // ls = plast
    0.00 :   ffff800010484230:       cmp     w3, w7
    0.00 :   ffff800010484234:       csel    w3, w3, w7, ls  // ls = plast
         :                      bvec_iter_advance():
         :                      static inline bool bvec_iter_advance(const struct bio_vec *bv,
         :                      struct bvec_iter *iter, unsigned bytes)
         :                      {
         :                      unsigned int idx = iter->bi_idx;
         :
         :                      if (WARN_ONCE(bytes > iter->bi_size,
    0.00 :   ffff800010484238:       cmp     w6, w3
    0.00 :   ffff80001048423c:       b.cc    ffff80001048427c <iov_iter_advance+0x294>  // b.lo, b.ul, b.last
         :                      "Attempted to advance past end of bvec iter\n")) {
         :                      iter->bi_size = 0;
         :                      return false;
         :                      }
         :
         :                      iter->bi_size -= bytes;
    0.00 :   ffff800010484240:       sub     w6, w6, w3
         :                      bytes += iter->bi_bvec_done;
         :
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff800010484244:       adds    w19, w19, w3
    0.00 :   ffff800010484248:       b.eq    ffff800010484208 <iov_iter_advance+0x220>  // b.none
    0.00 :   ffff80001048424c:       cmp     w0, w19
    0.00 :   ffff800010484250:       b.hi    ffff800010484208 <iov_iter_advance+0x220>  // b.pmore
    0.00 :   ffff800010484254:       nop
         :                      bytes -= bv[idx].bv_len;
         :                      idx++;
    0.00 :   ffff800010484258:       add     w4, w2, #0x1
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff80001048425c:       subs    w19, w19, w0
         :                      idx++;
    0.00 :   ffff800010484260:       mov     x2, x4
    0.00 :   ffff800010484264:       add     x0, x21, x4, lsl #4
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff800010484268:       b.eq    ffff800010484208 <iov_iter_advance+0x220>  // b.none
    0.00 :   ffff80001048426c:       ldr     w0, [x0, #8]
    0.00 :   ffff800010484270:       cmp     w0, w19
    0.00 :   ffff800010484274:       b.ls    ffff800010484258 <iov_iter_advance+0x270>  // b.plast
    0.00 :   ffff800010484278:       b       ffff800010484208 <iov_iter_advance+0x220>
         :                      if (WARN_ONCE(bytes > iter->bi_size,
    0.00 :   ffff80001048427c:       adrp    x2, ffff800011a5d000 <__print_once.28906>
    0.00 :   ffff800010484280:       mov     x3, x21
    0.00 :   ffff800010484284:       ldrb    w0, [x2, #82]
    0.00 :   ffff800010484288:       cbnz    w0, ffff8000104842b0 <iov_iter_advance+0x2c8>
    0.00 :   ffff80001048428c:       mov     w3, #0x1                        // #1
    0.00 :   ffff800010484290:       str     x1, [x29, #72]
    0.00 :   ffff800010484294:       strb    w3, [x2, #82]
    0.00 :   ffff800010484298:       adrp    x0, ffff8000111aa000 <kallsyms_token_index+0x30330>
    0.00 :   ffff80001048429c:       add     x0, x0, #0xea8
    0.00 :   ffff8000104842a0:       bl      ffff8000100e5288 <__warn_printk>
    0.00 :   ffff8000104842a4:       brk     #0x800
    0.00 :   ffff8000104842a8:       ldp     x5, x3, [x20, #16]
    0.00 :   ffff8000104842ac:       ldr     x1, [x29, #72]
    0.00 :   ffff8000104842b0:       add     x3, x3, x22
         :                      iov_iter_advance():
    0.00 :   ffff8000104842b4:       ldr     x2, [x20, #32]
    0.00 :   ffff8000104842b8:       sub     x0, x3, x21
    0.00 :   ffff8000104842bc:       mov     w19, w19
    0.00 :   ffff8000104842c0:       sub     x0, x2, x0, asr #4
    0.00 :   ffff8000104842c4:       stp     x3, x0, [x20, #24]
    0.00 :   ffff8000104842c8:       ldp     x21, x22, [x29, #32]
    0.00 :   ffff8000104842cc:       b       ffff80001048407c <iov_iter_advance+0x94>
    0.00 :   ffff8000104842d0:       ldr     x2, [x20, #24]
    0.00 :   ffff8000104842d4:       ldr     x4, [x2, #8]
    0.00 :   ffff8000104842d8:       sub     x0, x4, x19
    0.00 :   ffff8000104842dc:       cmp     x0, x1
    0.00 :   ffff8000104842e0:       csel    x0, x0, x1, ls  // ls = plast
    0.00 :   ffff8000104842e4:       cbz     x0, ffff800010484358 <iov_iter_advance+0x370>
    0.00 :   ffff8000104842e8:       add     x19, x19, x0
    0.00 :   ffff8000104842ec:       sub     x0, x1, x0
    0.00 :   ffff8000104842f0:       mov     x3, x2
    0.00 :   ffff8000104842f4:       cbnz    x0, ffff80001048431c <iov_iter_advance+0x334>
    0.00 :   ffff8000104842f8:       cmp     x19, x4
    0.00 :   ffff8000104842fc:       b.ne    ffff800010484308 <iov_iter_advance+0x320>  // b.any
    0.00 :   ffff800010484300:       add     x3, x3, #0x10
    0.00 :   ffff800010484304:       mov     x19, #0x0                       // #0
    0.00 :   ffff800010484308:       ldr     x4, [x20, #32]
    0.00 :   ffff80001048430c:       sub     x0, x3, x2
    0.00 :   ffff800010484310:       sub     x0, x4, x0, asr #4
    0.00 :   ffff800010484314:       stp     x3, x0, [x20, #24]
    0.00 :   ffff800010484318:       b       ffff80001048407c <iov_iter_advance+0x94>
    0.00 :   ffff80001048431c:       add     x3, x3, #0x10
    0.00 :   ffff800010484320:       ldr     x4, [x3, #8]
    0.00 :   ffff800010484324:       cmp     x0, x4
    0.00 :   ffff800010484328:       csel    x19, x0, x4, ls  // ls = plast
    0.00 :   ffff80001048432c:       cbz     x19, ffff80001048431c <iov_iter_advance+0x334>
    0.00 :   ffff800010484330:       subs    x0, x0, x19
    0.00 :   ffff800010484334:       b.eq    ffff8000104842f8 <iov_iter_advance+0x310>  // b.none
    0.00 :   ffff800010484338:       b       ffff80001048431c <iov_iter_advance+0x334>
    0.00 :   ffff80001048433c:       add     x19, x19, x1
    0.00 :   ffff800010484340:       b       ffff80001048407c <iov_iter_advance+0x94>
    0.00 :   ffff800010484344:       mov     x0, x1
    0.00 :   ffff800010484348:       b       ffff800010484054 <iov_iter_advance+0x6c>
    0.00 :   ffff80001048434c:       ldr     x19, [x29, #16]
    0.00 :   ffff800010484350:       ldr     x21, [x29, #32]
    0.00 :   ffff800010484354:       b       ffff800010484088 <iov_iter_advance+0xa0>
    0.00 :   ffff800010484358:       mov     x0, x1
    0.00 :   ffff80001048435c:       b       ffff8000104842f0 <iov_iter_advance+0x308>
 Percent |	Source code & Disassembly of vmlinux for cycles (200 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001044e420 <__bio_add_page>:
         :                      __bio_add_page():
         :                      * that @bio has space for another bvec.
         :                      */
         :                      void __bio_add_page(struct bio *bio, struct page *page,
         :                      unsigned int len, unsigned int off)
         :                      {
         :                      struct bio_vec *bv = &bio->bi_io_vec[bio->bi_vcnt];
    2.02 :   ffff80001044e420:       ldrh    w4, [x0, #96]
         :                      bio_flagged():
         :                      atomic_set(&bio->__bi_cnt, count);
         :                      }
         :
         :                      static inline bool bio_flagged(struct bio *bio, unsigned int bit)
         :                      {
         :                      return (bio->bi_flags & (1U << bit)) != 0;
    1.98 :   ffff80001044e424:       ldrh    w8, [x0, #20]
         :                      __bio_add_page():
    0.50 :   ffff80001044e428:       ldr     x7, [x0, #104]
    0.00 :   ffff80001044e42c:       ubfiz   x6, x4, #4, #16
    0.00 :   ffff80001044e430:       add     x5, x7, x6
         :
         :                      WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED));
    0.00 :   ffff80001044e434:       tbnz    w8, #1, ffff80001044e4b0 <__bio_add_page+0x90>
         :                      bio_full():
         :                      if (bio->bi_vcnt >= bio->bi_max_vecs)
    3.49 :   ffff80001044e438:       ldrh    w8, [x0, #98]
    0.00 :   ffff80001044e43c:       cmp     w8, w4
    0.00 :   ffff80001044e440:       b.hi    ffff80001044e474 <__bio_add_page+0x54>  // b.pmore
         :                      __bio_add_page():
         :                      WARN_ON_ONCE(bio_full(bio, len));
    0.00 :   ffff80001044e444:       brk     #0x800
         :
         :                      bv->bv_page = page;
   11.49 :   ffff80001044e448:       str     x1, [x7, x6]
         :                      bv->bv_offset = off;
   14.02 :   ffff80001044e44c:       stp     w2, w3, [x5, #8]
         :                      bv->bv_len = len;
         :
         :                      bio->bi_iter.bi_size += len;
         :                      bio->bi_vcnt++;
    5.50 :   ffff80001044e450:       ldrh    w3, [x0, #96]
         :                      bio->bi_iter.bi_size += len;
    0.99 :   ffff80001044e454:       ldr     w4, [x0, #40]
         :
         :                      if (!bio_flagged(bio, BIO_WORKINGSET) && unlikely(PageWorkingset(page)))
    1.50 :   ffff80001044e458:       ldrh    w5, [x0, #20]
         :                      bio->bi_vcnt++;
    0.00 :   ffff80001044e45c:       add     w3, w3, #0x1
         :                      bio->bi_iter.bi_size += len;
    0.00 :   ffff80001044e460:       add     w2, w4, w2
    3.53 :   ffff80001044e464:       str     w2, [x0, #40]
         :                      bio->bi_vcnt++;
    2.50 :   ffff80001044e468:       strh    w3, [x0, #96]
         :                      if (!bio_flagged(bio, BIO_WORKINGSET) && unlikely(PageWorkingset(page)))
    0.00 :   ffff80001044e46c:       tbz     w5, #5, ffff80001044e488 <__bio_add_page+0x68>
         :                      bio_set_flag(bio, BIO_WORKINGSET);
         :                      }
   27.06 :   ffff80001044e470:       ret
         :                      bio_full():
         :                      if (bio->bi_iter.bi_size > UINT_MAX - len)
    9.95 :   ffff80001044e474:       ldr     w8, [x0, #40]
    0.00 :   ffff80001044e478:       mvn     w4, w2
    0.00 :   ffff80001044e47c:       cmp     w8, w4
   11.01 :   ffff80001044e480:       b.ls    ffff80001044e448 <__bio_add_page+0x28>  // b.plast
    0.00 :   ffff80001044e484:       b       ffff80001044e444 <__bio_add_page+0x24>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    1.00 :   ffff80001044e488:       ldr     x2, [x1, #8]
         :                      compound_head():
         :                      static inline struct page *compound_head(struct page *page)
         :                      {
         :                      unsigned long head = READ_ONCE(page->compound_head);
         :
         :                      if (unlikely(head & 1))
         :                      return (struct page *) (head - 1);
    0.00 :   ffff80001044e48c:       sub     x3, x2, #0x1
    0.00 :   ffff80001044e490:       tst     x2, #0x1
    0.00 :   ffff80001044e494:       csel    x1, x3, x1, ne  // ne = any
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    3.46 :   ffff80001044e498:       ldr     x1, [x1]
         :                      __bio_add_page():
         :                      if (!bio_flagged(bio, BIO_WORKINGSET) && unlikely(PageWorkingset(page)))
    0.00 :   ffff80001044e49c:       tst     w1, #0x40
    0.00 :   ffff80001044e4a0:       b.eq    ffff80001044e470 <__bio_add_page+0x50>  // b.none
         :                      bio_set_flag():
         :                      }
         :
         :                      static inline void bio_set_flag(struct bio *bio, unsigned int bit)
         :                      {
         :                      bio->bi_flags |= (1U << bit);
    0.00 :   ffff80001044e4a4:       orr     w5, w5, #0x20
    0.00 :   ffff80001044e4a8:       strh    w5, [x0, #20]
         :                      __bio_add_page():
         :                      }
    0.00 :   ffff80001044e4ac:       ret
         :                      WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED));
    0.00 :   ffff80001044e4b0:       brk     #0x800
    0.00 :   ffff80001044e4b4:       ldrh    w4, [x0, #96]
    0.00 :   ffff80001044e4b8:       b       ffff80001044e438 <__bio_add_page+0x18>
 Percent |	Source code & Disassembly of vmlinux for cycles (162 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010c91a40 <__arch_copy_from_user>:
         :                      __arch_copy_from_user():
         :                      stp \ptr, \regB, [\regC], \val
         :                      .endm
         :
         :                      end     .req    x5
         :                      ENTRY(__arch_copy_from_user)
         :                      add     end, x0, x2
    0.00 :   ffff800010c91a40:       add     x5, x0, x2
         :                      C_l     .req    x11
         :                      C_h     .req    x12
         :                      D_l     .req    x13
         :                      D_h     .req    x14
         :
         :                      mov     dst, dstin
    0.00 :   ffff800010c91a44:       mov     x6, x0
         :                      cmp     count, #16
    0.00 :   ffff800010c91a48:       cmp     x2, #0x10
         :                      /*When memory length is less than 16, the accessed are not aligned.*/
         :                      b.lo    .Ltiny15
    0.00 :   ffff800010c91a4c:       b.cc    ffff800010c91aec <__arch_copy_from_user+0xac>  // b.lo, b.ul, b.last
         :
         :                      neg     tmp2, src
    2.41 :   ffff800010c91a50:       neg     x4, x1
         :                      ands    tmp2, tmp2, #15/* Bytes to reach alignment. */
    0.00 :   ffff800010c91a54:       ands    x4, x4, #0xf
         :                      b.eq    .LSrcAligned
    0.00 :   ffff800010c91a58:       b.eq    ffff800010c91aa0 <__arch_copy_from_user+0x60>  // b.none
         :                      sub     count, count, tmp2
    0.00 :   ffff800010c91a5c:       sub     x2, x2, x4
         :                      * Copy the leading memory data from src to dst in an increasing
         :                      * address order.By this way,the risk of overwriting the source
         :                      * memory data is eliminated when the distance between src and
         :                      * dst is less than 16. The memory accesses here are alignment.
         :                      */
         :                      tbz     tmp2, #0, 1f
    0.00 :   ffff800010c91a60:       tbz     w4, #0, ffff800010c91a70 <__arch_copy_from_user+0x30>
         :                      ldrb1   tmp1w, src, #1
    0.00 :   ffff800010c91a64:       ldrb    w3, [x1], #1
    0.00 :   ffff800010c91a68:       nop
         :                      strb1   tmp1w, dst, #1
    0.00 :   ffff800010c91a6c:       strb    w3, [x6], #1
         :                      1:
         :                      tbz     tmp2, #1, 2f
    0.00 :   ffff800010c91a70:       tbz     w4, #1, ffff800010c91a80 <__arch_copy_from_user+0x40>
         :                      ldrh1   tmp1w, src, #2
    0.00 :   ffff800010c91a74:       ldrh    w3, [x1], #2
    0.00 :   ffff800010c91a78:       nop
         :                      strh1   tmp1w, dst, #2
    0.00 :   ffff800010c91a7c:       strh    w3, [x6], #2
         :                      2:
         :                      tbz     tmp2, #2, 3f
    0.00 :   ffff800010c91a80:       tbz     w4, #2, ffff800010c91a90 <__arch_copy_from_user+0x50>
         :                      ldr1    tmp1w, src, #4
    0.00 :   ffff800010c91a84:       ldr     w3, [x1], #4
    0.00 :   ffff800010c91a88:       nop
         :                      str1    tmp1w, dst, #4
    0.00 :   ffff800010c91a8c:       str     w3, [x6], #4
         :                      3:
         :                      tbz     tmp2, #3, .LSrcAligned
    0.00 :   ffff800010c91a90:       tbz     w4, #3, ffff800010c91aa0 <__arch_copy_from_user+0x60>
         :                      ldr1    tmp1, src, #8
    0.00 :   ffff800010c91a94:       ldr     x3, [x1], #8
    0.00 :   ffff800010c91a98:       nop
         :                      str1    tmp1, dst, #8
    0.00 :   ffff800010c91a9c:       str     x3, [x6], #8
         :
         :                      .LSrcAligned:
         :                      cmp     count, #64
    5.54 :   ffff800010c91aa0:       cmp     x2, #0x40
         :                      b.ge    .Lcpy_over64
    0.00 :   ffff800010c91aa4:       b.ge    ffff800010c91b30 <__arch_copy_from_user+0xf0>  // b.tcont
         :                      .Ltail63:
         :                      /*
         :                      * Copy up to 48 bytes of data. At this point we only need the
         :                      * bottom 6 bits of count to be accurate.
         :                      */
         :                      ands    tmp1, count, #0x30
    0.00 :   ffff800010c91aa8:       ands    x3, x2, #0x30
         :                      b.eq    .Ltiny15
    0.00 :   ffff800010c91aac:       b.eq    ffff800010c91aec <__arch_copy_from_user+0xac>  // b.none
         :                      cmp     tmp1w, #0x20
    0.00 :   ffff800010c91ab0:       cmp     w3, #0x20
         :                      b.eq    1f
    0.00 :   ffff800010c91ab4:       b.eq    ffff800010c91acc <__arch_copy_from_user+0x8c>  // b.none
         :                      b.lt    2f
    0.00 :   ffff800010c91ab8:       b.lt    ffff800010c91adc <__arch_copy_from_user+0x9c>  // b.tstop
         :                      ldp1    A_l, A_h, src, #16
    0.00 :   ffff800010c91abc:       ldp     x7, x8, [x1], #16
    0.00 :   ffff800010c91ac0:       nop
    0.00 :   ffff800010c91ac4:       nop
         :                      stp1    A_l, A_h, dst, #16
    0.00 :   ffff800010c91ac8:       stp     x7, x8, [x6], #16
         :                      1:
         :                      ldp1    A_l, A_h, src, #16
    0.00 :   ffff800010c91acc:       ldp     x7, x8, [x1], #16
    0.00 :   ffff800010c91ad0:       nop
    0.00 :   ffff800010c91ad4:       nop
         :                      stp1    A_l, A_h, dst, #16
    0.00 :   ffff800010c91ad8:       stp     x7, x8, [x6], #16
         :                      2:
         :                      ldp1    A_l, A_h, src, #16
    0.00 :   ffff800010c91adc:       ldp     x7, x8, [x1], #16
    0.00 :   ffff800010c91ae0:       nop
    0.00 :   ffff800010c91ae4:       nop
         :                      stp1    A_l, A_h, dst, #16
    0.00 :   ffff800010c91ae8:       stp     x7, x8, [x6], #16
         :                      * precondition that src address is at least 16 bytes bigger than dst
         :                      * address,otherwise some source data will be overwritten when memove
         :                      * call memcpy directly. To make memmove simpler and decouple the
         :                      * memcpy's dependency on memmove, withdrew the original process.
         :                      */
         :                      tbz     count, #3, 1f
    0.00 :   ffff800010c91aec:       tbz     w2, #3, ffff800010c91afc <__arch_copy_from_user+0xbc>
         :                      ldr1    tmp1, src, #8
    0.00 :   ffff800010c91af0:       ldr     x3, [x1], #8
    0.00 :   ffff800010c91af4:       nop
         :                      str1    tmp1, dst, #8
    0.00 :   ffff800010c91af8:       str     x3, [x6], #8
         :                      1:
         :                      tbz     count, #2, 2f
    0.00 :   ffff800010c91afc:       tbz     w2, #2, ffff800010c91b0c <__arch_copy_from_user+0xcc>
         :                      ldr1    tmp1w, src, #4
    0.00 :   ffff800010c91b00:       ldr     w3, [x1], #4
    0.00 :   ffff800010c91b04:       nop
         :                      str1    tmp1w, dst, #4
    0.00 :   ffff800010c91b08:       str     w3, [x6], #4
         :                      2:
         :                      tbz     count, #1, 3f
    0.00 :   ffff800010c91b0c:       tbz     w2, #1, ffff800010c91b1c <__arch_copy_from_user+0xdc>
         :                      ldrh1   tmp1w, src, #2
    0.00 :   ffff800010c91b10:       ldrh    w3, [x1], #2
    0.00 :   ffff800010c91b14:       nop
         :                      strh1   tmp1w, dst, #2
    0.00 :   ffff800010c91b18:       strh    w3, [x6], #2
         :                      3:
         :                      tbz     count, #0, .Lexitfunc
    0.00 :   ffff800010c91b1c:       tbz     w2, #0, ffff800010c91c50 <__arch_copy_from_user+0x210>
         :                      ldrb1   tmp1w, src, #1
    0.00 :   ffff800010c91b20:       ldrb    w3, [x1], #1
    0.00 :   ffff800010c91b24:       nop
         :                      strb1   tmp1w, dst, #1
    0.00 :   ffff800010c91b28:       strb    w3, [x6], #1
         :
         :                      b       .Lexitfunc
    0.00 :   ffff800010c91b2c:       b       ffff800010c91c50 <__arch_copy_from_user+0x210>
         :
         :                      .Lcpy_over64:
         :                      subs    count, count, #128
    8.57 :   ffff800010c91b30:       subs    x2, x2, #0x80
         :                      b.ge    .Lcpy_body_large
    0.00 :   ffff800010c91b34:       b.ge    ffff800010c91bc0 <__arch_copy_from_user+0x180>  // b.tcont
         :                      /*
         :                      * Less than 128 bytes to copy, so handle 64 here and then jump
         :                      * to the tail.
         :                      */
         :                      ldp1    A_l, A_h, src, #16
    5.59 :   ffff800010c91b38:       ldp     x7, x8, [x1], #16
   25.35 :   ffff800010c91b3c:       nop
    0.00 :   ffff800010c91b40:       nop
         :                      stp1    A_l, A_h, dst, #16
    3.70 :   ffff800010c91b44:       stp     x7, x8, [x6], #16
         :                      ldp1    B_l, B_h, src, #16
    8.64 :   ffff800010c91b48:       ldp     x9, x10, [x1], #16
    9.26 :   ffff800010c91b4c:       nop
    0.00 :   ffff800010c91b50:       nop
         :                      ldp1    C_l, C_h, src, #16
    0.61 :   ffff800010c91b54:       ldp     x11, x12, [x1], #16
    5.52 :   ffff800010c91b58:       nop
    0.00 :   ffff800010c91b5c:       nop
         :                      stp1    B_l, B_h, dst, #16
    0.63 :   ffff800010c91b60:       stp     x9, x10, [x6], #16
         :                      stp1    C_l, C_h, dst, #16
    4.34 :   ffff800010c91b64:       stp     x11, x12, [x6], #16
         :                      ldp1    D_l, D_h, src, #16
    7.40 :   ffff800010c91b68:       ldp     x13, x14, [x1], #16
    5.57 :   ffff800010c91b6c:       nop
    0.00 :   ffff800010c91b70:       nop
         :                      stp1    D_l, D_h, dst, #16
    1.86 :   ffff800010c91b74:       stp     x13, x14, [x6], #16
         :
         :                      tst     count, #0x3f
    0.00 :   ffff800010c91b78:       tst     x2, #0x3f
         :                      b.ne    .Ltail63
    0.00 :   ffff800010c91b7c:       b.ne    ffff800010c91aa8 <__arch_copy_from_user+0x68>  // b.any
         :                      b       .Lexitfunc
    0.00 :   ffff800010c91b80:       b       ffff800010c91c50 <__arch_copy_from_user+0x210>
    0.00 :   ffff800010c91b84:       nop
    0.00 :   ffff800010c91b88:       nop
    0.00 :   ffff800010c91b8c:       nop
    0.00 :   ffff800010c91b90:       nop
    0.00 :   ffff800010c91b94:       nop
    0.00 :   ffff800010c91b98:       nop
    0.00 :   ffff800010c91b9c:       nop
    0.00 :   ffff800010c91ba0:       nop
    0.00 :   ffff800010c91ba4:       nop
    0.00 :   ffff800010c91ba8:       nop
    0.00 :   ffff800010c91bac:       nop
    0.00 :   ffff800010c91bb0:       nop
    0.00 :   ffff800010c91bb4:       nop
    0.00 :   ffff800010c91bb8:       nop
    0.00 :   ffff800010c91bbc:       nop
         :                      * 64 bytes per line this ensures the entire loop is in one line.
         :                      */
         :                      .p2align        L1_CACHE_SHIFT
         :                      .Lcpy_body_large:
         :                      /* pre-get 64 bytes data. */
         :                      ldp1    A_l, A_h, src, #16
    0.00 :   ffff800010c91bc0:       ldp     x7, x8, [x1], #16
    0.00 :   ffff800010c91bc4:       nop
    0.00 :   ffff800010c91bc8:       nop
         :                      ldp1    B_l, B_h, src, #16
    0.00 :   ffff800010c91bcc:       ldp     x9, x10, [x1], #16
    0.00 :   ffff800010c91bd0:       nop
    0.00 :   ffff800010c91bd4:       nop
         :                      ldp1    C_l, C_h, src, #16
    0.00 :   ffff800010c91bd8:       ldp     x11, x12, [x1], #16
    0.00 :   ffff800010c91bdc:       nop
    0.00 :   ffff800010c91be0:       nop
         :                      ldp1    D_l, D_h, src, #16
    0.00 :   ffff800010c91be4:       ldp     x13, x14, [x1], #16
    0.00 :   ffff800010c91be8:       nop
    0.00 :   ffff800010c91bec:       nop
         :                      1:
         :                      /*
         :                      * interlace the load of next 64 bytes data block with store of the last
         :                      * loaded 64 bytes data.
         :                      */
         :                      stp1    A_l, A_h, dst, #16
    0.00 :   ffff800010c91bf0:       stp     x7, x8, [x6], #16
         :                      ldp1    A_l, A_h, src, #16
    0.00 :   ffff800010c91bf4:       ldp     x7, x8, [x1], #16
    0.00 :   ffff800010c91bf8:       nop
    0.00 :   ffff800010c91bfc:       nop
         :                      stp1    B_l, B_h, dst, #16
    0.00 :   ffff800010c91c00:       stp     x9, x10, [x6], #16
         :                      ldp1    B_l, B_h, src, #16
    0.00 :   ffff800010c91c04:       ldp     x9, x10, [x1], #16
    0.00 :   ffff800010c91c08:       nop
    0.00 :   ffff800010c91c0c:       nop
         :                      stp1    C_l, C_h, dst, #16
    0.00 :   ffff800010c91c10:       stp     x11, x12, [x6], #16
         :                      ldp1    C_l, C_h, src, #16
    0.00 :   ffff800010c91c14:       ldp     x11, x12, [x1], #16
    0.00 :   ffff800010c91c18:       nop
    0.00 :   ffff800010c91c1c:       nop
         :                      stp1    D_l, D_h, dst, #16
    0.00 :   ffff800010c91c20:       stp     x13, x14, [x6], #16
         :                      ldp1    D_l, D_h, src, #16
    0.00 :   ffff800010c91c24:       ldp     x13, x14, [x1], #16
    0.00 :   ffff800010c91c28:       nop
    0.00 :   ffff800010c91c2c:       nop
         :                      subs    count, count, #64
    0.00 :   ffff800010c91c30:       subs    x2, x2, #0x40
         :                      b.ge    1b
    0.00 :   ffff800010c91c34:       b.ge    ffff800010c91bf0 <__arch_copy_from_user+0x1b0>  // b.tcont
         :                      stp1    A_l, A_h, dst, #16
    0.00 :   ffff800010c91c38:       stp     x7, x8, [x6], #16
         :                      stp1    B_l, B_h, dst, #16
    0.00 :   ffff800010c91c3c:       stp     x9, x10, [x6], #16
         :                      stp1    C_l, C_h, dst, #16
    0.00 :   ffff800010c91c40:       stp     x11, x12, [x6], #16
         :                      stp1    D_l, D_h, dst, #16
    0.00 :   ffff800010c91c44:       stp     x13, x14, [x6], #16
         :
         :                      tst     count, #0x3f
    0.00 :   ffff800010c91c48:       tst     x2, #0x3f
         :                      b.ne    .Ltail63
    0.00 :   ffff800010c91c4c:       b.ne    ffff800010c91aa8 <__arch_copy_from_user+0x68>  // b.any
         :                      #include "copy_template.S"
         :                      mov     x0, #0                          // Nothing to copy
    5.01 :   ffff800010c91c50:       mov     x0, #0x0                        // #0
         :                      ret
    0.00 :   ffff800010c91c54:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (165 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000106fdf18 <iommu_dma_deferred_attach>:
         :                      is_kdump_kernel():
         :                      * of previous kernel.
         :                      */
         :
         :                      static inline bool is_kdump_kernel(void)
         :                      {
         :                      return elfcorehdr_addr != ELFCORE_ADDR_MAX;
   97.56 :   ffff8000106fdf18:       adrp    x2, ffff8000118bb000 <pmus_lock+0x18>
         :                      iommu_dma_deferred_attach():
         :                      static int iommu_dma_deferred_attach(struct device *dev,
         :                      struct iommu_domain *domain)
         :                      {
         :                      const struct iommu_ops *ops = domain->ops;
         :
         :                      if (!is_kdump_kernel())
    1.23 :   ffff8000106fdf1c:       ldr     x2, [x2, #1864]
    0.00 :   ffff8000106fdf20:       cmn     x2, #0x1
    0.00 :   ffff8000106fdf24:       b.eq    ffff8000106fdf34 <iommu_dma_deferred_attach+0x1c>  // b.none
         :                      return 0;
         :
         :                      if (unlikely(ops->is_attach_deferred &&
    0.00 :   ffff8000106fdf28:       ldr     x2, [x1, #8]
    0.00 :   ffff8000106fdf2c:       ldr     x2, [x2, #176]
    0.00 :   ffff8000106fdf30:       cbnz    x2, ffff8000106fdf3c <iommu_dma_deferred_attach+0x24>
         :                      return 0;
    1.21 :   ffff8000106fdf34:       mov     w0, #0x0                        // #0
         :                      ops->is_attach_deferred(domain, dev)))
         :                      return iommu_attach_device(domain, dev);
         :
         :                      return 0;
         :                      }
    0.00 :   ffff8000106fdf38:       ret
         :                      {
    0.00 :   ffff8000106fdf3c:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff8000106fdf40:       mov     x29, sp
    0.00 :   ffff8000106fdf44:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000106fdf48:       mov     x19, x1
    0.00 :   ffff8000106fdf4c:       mov     x20, x0
         :                      if (unlikely(ops->is_attach_deferred &&
    0.00 :   ffff8000106fdf50:       mov     x1, x0
    0.00 :   ffff8000106fdf54:       mov     x0, x19
    0.00 :   ffff8000106fdf58:       blr     x2
    0.00 :   ffff8000106fdf5c:       tst     w0, #0xff
    0.00 :   ffff8000106fdf60:       b.ne    ffff8000106fdf74 <iommu_dma_deferred_attach+0x5c>  // b.any
         :                      return 0;
    0.00 :   ffff8000106fdf64:       mov     w0, #0x0                        // #0
         :                      }
    0.00 :   ffff8000106fdf68:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000106fdf6c:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000106fdf70:       ret
         :                      return iommu_attach_device(domain, dev);
    0.00 :   ffff8000106fdf74:       mov     x1, x20
    0.00 :   ffff8000106fdf78:       mov     x0, x19
    0.00 :   ffff8000106fdf7c:       bl      ffff8000106fb898 <iommu_attach_device>
    0.00 :   ffff8000106fdf80:       b       ffff8000106fdf68 <iommu_dma_deferred_attach+0x50>
 Percent |	Source code & Disassembly of vmlinux for cycles (187 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104828b8 <import_single_range>:
         :                      import_single_range():
         :                      EXPORT_SYMBOL(compat_import_iovec);
         :                      #endif
         :
         :                      int import_single_range(int rw, void __user *buf, size_t len,
         :                      struct iovec *iov, struct iov_iter *i)
         :                      {
   21.91 :   ffff8000104828b8:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff8000104828bc:       mov     x6, #0x7ffff000                 // #2147479552
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
   21.40 :   ffff8000104828c0:       mrs     x5, sp_el0
         :                      import_single_range():
    0.00 :   ffff8000104828c4:       mov     x29, sp
         :                      __range_ok():
         :                      * Asynchronous I/O running in a kernel thread does not have the
         :                      * TIF_TAGGED_ADDR flag of the process owning the mm, so always untag
         :                      * the user address before checking.
         :                      */
         :                      if (IS_ENABLED(CONFIG_ARM64_TAGGED_ADDR_ABI) &&
         :                      (current->flags & PF_KTHREAD || test_thread_flag(TIF_TAGGED_ADDR)))
    0.00 :   ffff8000104828c8:       ldr     w8, [x5, #44]
    0.00 :   ffff8000104828cc:       cmp     x2, x6
    0.00 :   ffff8000104828d0:       csel    x6, x2, x6, ls  // ls = plast
         :                      unsigned long ret, limit = current_thread_info()->addr_limit;
   31.57 :   ffff8000104828d4:       ldr     x7, [x5, #8]
         :                      if (IS_ENABLED(CONFIG_ARM64_TAGGED_ADDR_ABI) &&
    0.00 :   ffff8000104828d8:       tbnz    w8, #21, ffff8000104828ec <import_single_range+0x34>
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff8000104828dc:       ldr     x2, [x5]
         :                      __range_ok():
    0.00 :   ffff8000104828e0:       mov     x5, x1
    0.00 :   ffff8000104828e4:       tst     w2, #0x4000000
    0.00 :   ffff8000104828e8:       b.eq    ffff8000104828f4 <import_single_range+0x3c>  // b.none
         :                      sign_extend64():
         :                      * @index: 0 based bit index (0<=index<64) to sign bit
         :                      */
         :                      static inline __s64 sign_extend64(__u64 value, int index)
         :                      {
         :                      __u8 shift = 63 - index;
         :                      return (__s64)(value << shift) >> shift;
    0.00 :   ffff8000104828ec:       sbfx    x5, x1, #0, #56
         :                      __range_ok():
         :                      addr = untagged_addr(addr);
    0.00 :   ffff8000104828f0:       and     x5, x1, x5
         :
         :                      __chk_user_ptr(addr);
         :                      asm volatile(
    1.07 :   ffff8000104828f4:       adds    x5, x5, x6
    0.00 :   ffff8000104828f8:       csel    x7, xzr, x7, hi  // hi = pmore
    0.00 :   ffff8000104828fc:       csinv   x5, x5, xzr, cc  // cc = lo, ul, last
    1.07 :   ffff800010482900:       sbcs    xzr, x5, x7
    2.17 :   ffff800010482904:       cset    x5, ls  // ls = plast
         :                      import_single_range():
         :                      if (len > MAX_RW_COUNT)
         :                      len = MAX_RW_COUNT;
         :                      if (unlikely(!access_ok(buf, len)))
    0.00 :   ffff800010482908:       cbz     x5, ffff80001048293c <import_single_range+0x84>
    2.13 :   ffff80001048290c:       mov     x2, x3
    0.00 :   ffff800010482910:       mov     x7, x4
    0.00 :   ffff800010482914:       mov     x5, x1
         :                      return -EFAULT;
         :
         :                      iov->iov_base = buf;
         :                      iov->iov_len = len;
         :                      iov_iter_init(i, rw, iov, 1, len);
    0.00 :   ffff800010482918:       mov     x4, x6
   12.79 :   ffff80001048291c:       mov     w1, w0
    0.00 :   ffff800010482920:       mov     x3, #0x1                        // #1
         :                      iov->iov_len = len;
    0.00 :   ffff800010482924:       stp     x5, x6, [x2]
         :                      iov_iter_init(i, rw, iov, 1, len);
    0.00 :   ffff800010482928:       mov     x0, x7
    0.00 :   ffff80001048292c:       bl      ffff800010482208 <iov_iter_init>
         :                      return 0;
    4.83 :   ffff800010482930:       mov     w0, #0x0                        // #0
         :                      }
    1.06 :   ffff800010482934:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010482938:       ret
         :                      return -EFAULT;
    0.00 :   ffff80001048293c:       mov     w0, #0xfffffff2                 // #-14
         :                      }
    0.00 :   ffff800010482940:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010482944:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (107 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101240e8 <update_group_capacity>:
         :                      update_group_capacity():
         :                      sdg->sgc->min_capacity = capacity;
         :                      sdg->sgc->max_capacity = capacity;
         :                      }
         :
         :                      void update_group_capacity(struct sched_domain *sd, int cpu)
         :                      {
    0.00 :   ffff8000101240e8:       stp     x29, x30, [sp, #-80]!
    0.00 :   ffff8000101240ec:       mov     x29, sp
    0.00 :   ffff8000101240f0:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000101240f4:       mov     x19, x0
    2.80 :   ffff8000101240f8:       str     x21, [sp, #32]
    0.00 :   ffff8000101240fc:       sxtw    x21, w1
    1.08 :   ffff800010124100:       str     x25, [sp, #64]
         :                      msecs_to_jiffies():
         :                      if (__builtin_constant_p(m)) {
         :                      if ((int)m < 0)
         :                      return MAX_JIFFY_OFFSET;
         :                      return _msecs_to_jiffies(m);
         :                      } else {
         :                      return __msecs_to_jiffies(m);
    1.54 :   ffff800010124104:       ldr     w0, [x0, #72]
         :                      update_group_capacity():
         :                      struct sched_domain *child = sd->child;
         :                      struct sched_group *group, *sdg = sd->groups;
    0.00 :   ffff800010124108:       ldp     x20, x25, [x19, #8]
         :                      msecs_to_jiffies():
    0.00 :   ffff80001012410c:       bl      ffff8000101653f0 <__msecs_to_jiffies>
         :                      update_group_capacity():
         :                      unsigned long capacity, min_capacity, max_capacity;
         :                      unsigned long interval;
         :
         :                      interval = msecs_to_jiffies(sd->balance_interval);
         :                      interval = clamp(interval, 1UL, max_load_balance_interval);
    0.00 :   ffff800010124110:       cmp     x0, #0x0
    0.00 :   ffff800010124114:       adrp    x1, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      sdg->sgc->next_update = jiffies + interval;
    0.00 :   ffff800010124118:       adrp    x4, ffff800011897000 <bit_wait_table+0xe80>
         :                      interval = clamp(interval, 1UL, max_load_balance_interval);
    0.00 :   ffff80001012411c:       mov     x3, #0x1                        // #1
         :                      sdg->sgc->next_update = jiffies + interval;
    0.00 :   ffff800010124120:       ldr     x2, [x25, #16]
         :                      interval = clamp(interval, 1UL, max_load_balance_interval);
    0.00 :   ffff800010124124:       ldr     x1, [x1, #520]
    0.00 :   ffff800010124128:       csel    x0, x0, x3, ne  // ne = any
         :                      sdg->sgc->next_update = jiffies + interval;
    0.00 :   ffff80001012412c:       ldr     x4, [x4, #2432]
         :                      interval = clamp(interval, 1UL, max_load_balance_interval);
    0.00 :   ffff800010124130:       cmp     x0, x1
    0.00 :   ffff800010124134:       csel    x0, x0, x1, ls  // ls = plast
         :                      sdg->sgc->next_update = jiffies + interval;
    0.00 :   ffff800010124138:       add     x0, x0, x4
    0.00 :   ffff80001012413c:       str     x0, [x2, #32]
         :
         :                      if (!child) {
    0.00 :   ffff800010124140:       cbz     x20, ffff80001012425c <update_group_capacity+0x174>
         :
         :                      capacity = 0;
         :                      min_capacity = ULONG_MAX;
         :                      max_capacity = 0;
         :
         :                      if (child->flags & SD_OVERLAP) {
    0.00 :   ffff800010124144:       ldr     w0, [x20, #56]
    0.00 :   ffff800010124148:       tbz     w0, #13, ffff800010124208 <update_group_capacity+0x120>
    0.00 :   ffff80001012414c:       stp     x22, x23, [x29, #40]
         :                      min_capacity = ULONG_MAX;
    0.00 :   ffff800010124150:       mov     x20, #0xffffffffffffffff        // #-1
         :                      * span the current group.
         :                      */
         :
         :                      for_each_cpu(cpu, sched_group_span(sdg)) {
         :                      struct sched_group_capacity *sgc;
         :                      struct rq *rq = cpu_rq(cpu);
    0.00 :   ffff800010124154:       adrp    x22, ffff8000114d5000 <tegra_to+0x180>
    0.00 :   ffff800010124158:       adrp    x23, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff80001012415c:       str     x24, [x29, #56]
         :                      max_capacity = 0;
    0.00 :   ffff800010124160:       mov     x21, #0x0                       // #0
    0.00 :   ffff800010124164:       str     x26, [x29, #72]
         :                      capacity = 0;
    0.00 :   ffff800010124168:       mov     x19, #0x0                       // #0
    0.00 :   ffff80001012416c:       adrp    x26, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      for_each_cpu(cpu, sched_group_span(sdg)) {
    0.00 :   ffff800010124170:       mov     w0, w20
         :                      struct rq *rq = cpu_rq(cpu);
    0.00 :   ffff800010124174:       add     x22, x22, #0xd80
    0.00 :   ffff800010124178:       add     x23, x23, #0x8e8
    0.00 :   ffff80001012417c:       add     x24, x25, #0x20
         :                      for_each_cpu(cpu, sched_group_span(sdg)) {
    0.00 :   ffff800010124180:       add     x26, x26, #0x2b4
    0.00 :   ffff800010124184:       b       ffff8000101241b8 <update_group_capacity+0xd0>
         :                      struct rq *rq = cpu_rq(cpu);
    0.00 :   ffff800010124188:       ldr     x2, [x23, w0, sxtw #3]
         :                      * in update_cpu_capacity().
         :                      *
         :                      * This avoids capacity from being 0 and
         :                      * causing divide-by-zero issues on boot.
         :                      */
         :                      if (unlikely(!rq->sd)) {
    0.00 :   ffff80001012418c:       add     x1, x2, x1
    0.00 :   ffff800010124190:       ldr     x2, [x1, #2472]
    0.84 :   ffff800010124194:       cbz     x2, ffff800010124250 <update_group_capacity+0x168>
         :                      capacity += capacity_of(cpu);
         :                      } else {
         :                      sgc = rq->sd->groups->sgc;
    4.50 :   ffff800010124198:       ldr     x1, [x2, #16]
         :                      capacity += sgc->capacity;
   14.29 :   ffff80001012419c:       ldr     x1, [x1, #16]
   15.46 :   ffff8000101241a0:       ldr     x1, [x1, #8]
    0.71 :   ffff8000101241a4:       add     x19, x19, x1
         :                      }
         :
         :                      min_capacity = min(capacity, min_capacity);
    2.12 :   ffff8000101241a8:       cmp     x20, x19
    0.00 :   ffff8000101241ac:       csel    x20, x20, x19, ls  // ls = plast
         :                      max_capacity = max(capacity, max_capacity);
   19.12 :   ffff8000101241b0:       cmp     x21, x19
    0.00 :   ffff8000101241b4:       csel    x21, x21, x19, cs  // cs = hs, nlast
         :                      for_each_cpu(cpu, sched_group_span(sdg)) {
    0.00 :   ffff8000101241b8:       mov     x1, x24
    0.00 :   ffff8000101241bc:       bl      ffff800010c93a58 <cpumask_next>
    0.00 :   ffff8000101241c0:       ldr     w2, [x26]
         :                      struct rq *rq = cpu_rq(cpu);
    0.00 :   ffff8000101241c4:       mov     x1, x22
         :                      for_each_cpu(cpu, sched_group_span(sdg)) {
    0.00 :   ffff8000101241c8:       cmp     w0, w2
    0.00 :   ffff8000101241cc:       b.cc    ffff800010124188 <update_group_capacity+0xa0>  // b.lo, b.ul, b.last
    0.00 :   ffff8000101241d0:       ldp     x22, x23, [x29, #40]
    0.00 :   ffff8000101241d4:       ldr     x24, [x29, #56]
    0.00 :   ffff8000101241d8:       ldr     x26, [x29, #72]
         :                      max_capacity = max(sgc->max_capacity, max_capacity);
         :                      group = group->next;
         :                      } while (group != child->groups);
         :                      }
         :
         :                      sdg->sgc->capacity = capacity;
    0.00 :   ffff8000101241dc:       ldr     x0, [x25, #16]
    0.00 :   ffff8000101241e0:       str     x19, [x0, #8]
         :                      sdg->sgc->min_capacity = min_capacity;
    0.00 :   ffff8000101241e4:       ldr     x0, [x25, #16]
    0.00 :   ffff8000101241e8:       str     x20, [x0, #16]
         :                      sdg->sgc->max_capacity = max_capacity;
    0.00 :   ffff8000101241ec:       ldr     x0, [x25, #16]
    0.00 :   ffff8000101241f0:       str     x21, [x0, #24]
         :                      }
    0.00 :   ffff8000101241f4:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000101241f8:       ldr     x21, [sp, #32]
    0.00 :   ffff8000101241fc:       ldr     x25, [sp, #64]
    0.00 :   ffff800010124200:       ldp     x29, x30, [sp], #80
    0.00 :   ffff800010124204:       ret
         :                      group = child->groups;
    0.00 :   ffff800010124208:       ldr     x4, [x20, #16]
         :                      max_capacity = 0;
    0.00 :   ffff80001012420c:       mov     x21, #0x0                       // #0
         :                      capacity = 0;
    0.00 :   ffff800010124210:       mov     x19, #0x0                       // #0
         :                      min_capacity = ULONG_MAX;
    0.00 :   ffff800010124214:       mov     x20, #0xffffffffffffffff        // #-1
         :                      group = child->groups;
    0.00 :   ffff800010124218:       mov     x0, x4
    0.00 :   ffff80001012421c:       nop
         :                      struct sched_group_capacity *sgc = group->sgc;
    0.00 :   ffff800010124220:       ldr     x1, [x0, #16]
         :                      group = group->next;
    3.36 :   ffff800010124224:       ldr     x0, [x0]
         :                      max_capacity = max(sgc->max_capacity, max_capacity);
    0.00 :   ffff800010124228:       ldp     x3, x2, [x1, #16]
         :                      capacity += sgc->capacity;
    6.76 :   ffff80001012422c:       ldr     x1, [x1, #8]
         :                      min_capacity = min(sgc->min_capacity, min_capacity);
    0.00 :   ffff800010124230:       cmp     x20, x3
    0.00 :   ffff800010124234:       csel    x20, x20, x3, ls  // ls = plast
         :                      max_capacity = max(sgc->max_capacity, max_capacity);
    0.00 :   ffff800010124238:       cmp     x21, x2
    0.00 :   ffff80001012423c:       csel    x21, x21, x2, cs  // cs = hs, nlast
         :                      capacity += sgc->capacity;
    0.00 :   ffff800010124240:       add     x19, x19, x1
         :                      } while (group != child->groups);
    0.00 :   ffff800010124244:       cmp     x4, x0
    0.00 :   ffff800010124248:       b.ne    ffff800010124220 <update_group_capacity+0x138>  // b.any
    0.00 :   ffff80001012424c:       b       ffff8000101241dc <update_group_capacity+0xf4>
         :                      capacity += capacity_of(cpu);
    0.00 :   ffff800010124250:       ldr     x1, [x1, #2480]
    0.00 :   ffff800010124254:       add     x19, x19, x1
    0.00 :   ffff800010124258:       b       ffff8000101241a8 <update_group_capacity+0xc0>
         :                      scale_rt_capacity():
         :                      struct rq *rq = cpu_rq(cpu);
    0.00 :   ffff80001012425c:       adrp    x6, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010124260:       add     x2, x6, #0x8e8
    0.00 :   ffff800010124264:       adrp    x4, ffff8000114d5000 <tegra_to+0x180>
    0.00 :   ffff800010124268:       add     x1, x4, #0xd80
    0.00 :   ffff80001012426c:       ldr     x2, [x2, x21, lsl #3]
         :                      topology_get_cpu_scale():
         :
         :                      struct sched_domain;
         :                      static inline
         :                      unsigned long topology_get_cpu_scale(int cpu)
         :                      {
         :                      return per_cpu(cpu_scale, cpu);
    0.00 :   ffff800010124270:       adrp    x0, ffff8000114d4000 <bh_lrus+0x18>
    0.00 :   ffff800010124274:       add     x0, x0, #0x900
    0.00 :   ffff800010124278:       ldr     x8, [x19, #16]
         :                      scale_rt_capacity():
    0.00 :   ffff80001012427c:       add     x1, x1, x2
         :                      topology_get_cpu_scale():
    0.00 :   ffff800010124280:       ldr     x5, [x2, x0]
         :                      scale_rt_capacity():
         :                      irq = cpu_util_irq(rq);
   24.93 :   ffff800010124284:       ldr     x7, [x1, #2800]
         :                      if (unlikely(irq >= max))
    0.00 :   ffff800010124288:       cmp     x5, x7
    0.00 :   ffff80001012428c:       b.ls    ffff800010124300 <update_group_capacity+0x218>  // b.plast
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010124290:       ldr     x2, [x1, #2672]
    0.00 :   ffff800010124294:       ldr     x0, [x1, #2736]
         :                      scale_rt_capacity():
         :                      used += READ_ONCE(rq->avg_dl.util_avg);
    0.00 :   ffff800010124298:       add     x2, x2, x0
         :                      if (unlikely(used >= max))
    0.00 :   ffff80001012429c:       cmp     x5, x2
    0.00 :   ffff8000101242a0:       b.ls    ffff800010124300 <update_group_capacity+0x218>  // b.plast
         :                      free = max - used;
    0.00 :   ffff8000101242a4:       sub     x0, x5, x2
         :                      scale_irq_capacity():
         :                      }
         :
         :                      static inline
         :                      unsigned long scale_irq_capacity(unsigned long util, unsigned long irq, unsigned long max)
         :                      {
         :                      util *= (max - irq);
    0.00 :   ffff8000101242a8:       sub     x7, x5, x7
         :                      update_cpu_capacity():
         :                      cpu_rq(cpu)->cpu_capacity_orig = arch_scale_cpu_capacity(cpu);
    0.00 :   ffff8000101242ac:       str     x5, [x1, #2488]
         :                      scale_irq_capacity():
    0.00 :   ffff8000101242b0:       mul     x0, x0, x7
         :                      util /= max;
    0.00 :   ffff8000101242b4:       udiv    x0, x0, x5
         :                      update_cpu_capacity():
         :                      capacity = 1;
    0.00 :   ffff8000101242b8:       cmp     x0, #0x0
    0.00 :   ffff8000101242bc:       csel    x0, x0, x3, ne  // ne = any
         :                      cpu_rq(cpu)->cpu_capacity = capacity;
    0.00 :   ffff8000101242c0:       add     x6, x6, #0x8e8
    0.00 :   ffff8000101242c4:       add     x1, x4, #0xd80
    0.00 :   ffff8000101242c8:       ldr     x2, [x6, x21, lsl #3]
    0.00 :   ffff8000101242cc:       add     x1, x2, x1
    0.00 :   ffff8000101242d0:       str     x0, [x1, #2480]
         :                      sdg->sgc->capacity = capacity;
    0.00 :   ffff8000101242d4:       ldr     x1, [x8, #16]
    0.00 :   ffff8000101242d8:       str     x0, [x1, #8]
         :                      sdg->sgc->min_capacity = capacity;
    0.00 :   ffff8000101242dc:       ldr     x1, [x8, #16]
    0.00 :   ffff8000101242e0:       str     x0, [x1, #16]
         :                      sdg->sgc->max_capacity = capacity;
    0.00 :   ffff8000101242e4:       ldr     x1, [x8, #16]
    0.00 :   ffff8000101242e8:       str     x0, [x1, #24]
         :                      update_group_capacity():
         :                      }
    1.83 :   ffff8000101242ec:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000101242f0:       ldr     x21, [sp, #32]
    0.67 :   ffff8000101242f4:       ldr     x25, [sp, #64]
    0.00 :   ffff8000101242f8:       ldp     x29, x30, [sp], #80
    0.00 :   ffff8000101242fc:       ret
         :                      scale_rt_capacity():
         :                      return 1;
    0.00 :   ffff800010124300:       mov     x0, x3
         :                      update_cpu_capacity():
         :                      cpu_rq(cpu)->cpu_capacity_orig = arch_scale_cpu_capacity(cpu);
    0.00 :   ffff800010124304:       str     x5, [x1, #2488]
    0.00 :   ffff800010124308:       b       ffff8000101242c0 <update_group_capacity+0x1d8>
 Percent |	Source code & Disassembly of vmlinux for cycles (153 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102d64c8 <read_events>:
         :                      read_events():
         :                      }
         :
         :                      static long read_events(struct kioctx *ctx, long min_nr, long nr,
         :                      struct io_event __user *event,
         :                      ktime_t until)
         :                      {
    3.95 :   ffff8000102d64c8:       stp     x29, x30, [sp, #-208]!
    0.00 :   ffff8000102d64cc:       mov     x29, sp
   18.24 :   ffff8000102d64d0:       str     x19, [sp, #16]
    0.00 :   ffff8000102d64d4:       adrp    x19, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000102d64d8:       add     x5, x19, #0x8c8
    1.31 :   ffff8000102d64dc:       ldr     x6, [x5]
   19.65 :   ffff8000102d64e0:       str     x6, [x29, #200]
    0.00 :   ffff8000102d64e4:       mov     x6, #0x0                        // #0
         :                      long ret = 0;
    0.66 :   ffff8000102d64e8:       str     xzr, [x29, #80]
         :                      * TASK_RUNNING and return 0 too much - that causes us to spin. That
         :                      * will only happen if the mutex_lock() call blocks, and we then find
         :                      * the ringbuffer empty. So in practice we should be ok, but it's
         :                      * something to be aware of when touching this code.
         :                      */
         :                      if (until == 0)
    0.00 :   ffff8000102d64ec:       cbz     x4, ffff8000102d6550 <read_events+0x88>
    1.30 :   ffff8000102d64f0:       stp     x20, x21, [x29, #24]
    0.00 :   ffff8000102d64f4:       mov     x21, x0
    0.00 :   ffff8000102d64f8:       stp     x22, x23, [x29, #40]
    0.00 :   ffff8000102d64fc:       mov     x20, x4
    5.25 :   ffff8000102d6500:       str     x24, [x29, #56]
    0.00 :   ffff8000102d6504:       mov     x22, x1
    0.00 :   ffff8000102d6508:       mov     x23, x2
         :                      aio_read_events(ctx, min_nr, nr, event, &ret);
         :                      else
         :                      wait_event_interruptible_hrtimeout(ctx->wait,
    0.00 :   ffff8000102d650c:       add     x4, x29, #0x50
   20.27 :   ffff8000102d6510:       mov     x24, x3
    0.00 :   ffff8000102d6514:       bl      ffff8000102d6130 <aio_read_events>
   11.73 :   ffff8000102d6518:       tst     w0, #0xff
    0.00 :   ffff8000102d651c:       b.eq    ffff8000102d655c <read_events+0x94>  // b.none
    0.00 :   ffff8000102d6520:       ldp     x20, x21, [x29, #24]
    0.66 :   ffff8000102d6524:       ldp     x22, x23, [x29, #40]
    0.65 :   ffff8000102d6528:       ldr     x24, [x29, #56]
         :                      aio_read_events(ctx, min_nr, nr, event, &ret),
         :                      until);
         :                      return ret;
         :                      }
    0.00 :   ffff8000102d652c:       add     x19, x19, #0x8c8
    6.53 :   ffff8000102d6530:       ldr     x0, [x29, #80]
    0.00 :   ffff8000102d6534:       ldr     x2, [x29, #200]
    1.31 :   ffff8000102d6538:       ldr     x1, [x19]
    0.00 :   ffff8000102d653c:       eor     x1, x2, x1
    0.00 :   ffff8000102d6540:       cbnz    x1, ffff8000102d6610 <read_events+0x148>
    7.83 :   ffff8000102d6544:       ldr     x19, [sp, #16]
    0.66 :   ffff8000102d6548:       ldp     x29, x30, [sp], #208
    0.00 :   ffff8000102d654c:       ret
         :                      aio_read_events(ctx, min_nr, nr, event, &ret);
    0.00 :   ffff8000102d6550:       add     x4, x29, #0x50
    0.00 :   ffff8000102d6554:       bl      ffff8000102d6130 <aio_read_events>
    0.00 :   ffff8000102d6558:       b       ffff8000102d652c <read_events+0x64>
         :                      hrtimer_init_sleeper_on_stack():
         :
         :                      static inline void hrtimer_init_sleeper_on_stack(struct hrtimer_sleeper *sl,
         :                      clockid_t clock_id,
         :                      enum hrtimer_mode mode)
         :                      {
         :                      hrtimer_init_sleeper(sl, clock_id, mode);
    0.00 :   ffff8000102d655c:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000102d6560:       add     x0, x29, #0x80
    0.00 :   ffff8000102d6564:       mov     w1, w2
    0.00 :   ffff8000102d6568:       str     x25, [x29, #64]
    0.00 :   ffff8000102d656c:       bl      ffff800010169970 <hrtimer_init_sleeper>
         :                      read_events():
         :                      wait_event_interruptible_hrtimeout(ctx->wait,
    0.00 :   ffff8000102d6570:       mov     x0, #0x7fffffffffffffff         // #9223372036854775807
    0.00 :   ffff8000102d6574:       cmp     x20, x0
    0.00 :   ffff8000102d6578:       b.eq    ffff8000102d6594 <read_events+0xcc>  // b.none
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff8000102d657c:       mrs     x0, sp_el0
         :                      read_events():
    0.00 :   ffff8000102d6580:       ldr     x2, [x0, #2392]
    0.00 :   ffff8000102d6584:       mov     w3, #0x1                        // #1
    0.00 :   ffff8000102d6588:       mov     x1, x20
    0.00 :   ffff8000102d658c:       add     x0, x29, #0x80
    0.00 :   ffff8000102d6590:       bl      ffff800010169b40 <hrtimer_start_range_ns>
    0.00 :   ffff8000102d6594:       add     x25, x21, #0x1a0
    0.00 :   ffff8000102d6598:       mov     w1, #0x0                        // #0
    0.00 :   ffff8000102d659c:       add     x0, x29, #0x58
    0.00 :   ffff8000102d65a0:       bl      ffff80001012e648 <init_wait_entry>
    0.00 :   ffff8000102d65a4:       b       ffff8000102d65b8 <read_events+0xf0>
    0.00 :   ffff8000102d65a8:       cbnz    x20, ffff8000102d65f8 <read_events+0x130>
    0.00 :   ffff8000102d65ac:       ldr     x0, [x29, #192]
    0.00 :   ffff8000102d65b0:       cbz     x0, ffff8000102d65ec <read_events+0x124>
    0.00 :   ffff8000102d65b4:       bl      ffff800010cad270 <schedule>
    0.00 :   ffff8000102d65b8:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000102d65bc:       add     x1, x29, #0x58
    0.00 :   ffff8000102d65c0:       mov     x0, x25
    0.00 :   ffff8000102d65c4:       bl      ffff80001012e740 <prepare_to_wait_event>
    0.00 :   ffff8000102d65c8:       add     x4, x29, #0x50
    0.00 :   ffff8000102d65cc:       mov     x20, x0
    0.00 :   ffff8000102d65d0:       mov     x3, x24
    0.00 :   ffff8000102d65d4:       mov     x2, x23
    0.00 :   ffff8000102d65d8:       mov     x1, x22
    0.00 :   ffff8000102d65dc:       mov     x0, x21
    0.00 :   ffff8000102d65e0:       bl      ffff8000102d6130 <aio_read_events>
    0.00 :   ffff8000102d65e4:       tst     w0, #0xff
    0.00 :   ffff8000102d65e8:       b.eq    ffff8000102d65a8 <read_events+0xe0>  // b.none
    0.00 :   ffff8000102d65ec:       add     x1, x29, #0x58
    0.00 :   ffff8000102d65f0:       mov     x0, x25
    0.00 :   ffff8000102d65f4:       bl      ffff80001012e670 <finish_wait>
    0.00 :   ffff8000102d65f8:       add     x0, x29, #0x80
    0.00 :   ffff8000102d65fc:       bl      ffff800010169ec0 <hrtimer_cancel>
    0.00 :   ffff8000102d6600:       ldp     x20, x21, [x29, #24]
    0.00 :   ffff8000102d6604:       ldp     x22, x23, [x29, #40]
    0.00 :   ffff8000102d6608:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff8000102d660c:       b       ffff8000102d652c <read_events+0x64>
    0.00 :   ffff8000102d6610:       stp     x20, x21, [x29, #24]
    0.00 :   ffff8000102d6614:       stp     x22, x23, [x29, #40]
    0.00 :   ffff8000102d6618:       stp     x24, x25, [x29, #56]
         :                      }
    0.00 :   ffff8000102d661c:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (169 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010167980 <round_jiffies_up>:
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    5.89 :   ffff800010167980:       mrs     x2, tpidr_el1
         :                      round_jiffies_up():
         :                      * of firing does not matter too much, as long as they don't fire too
         :                      * early.
         :                      */
         :                      unsigned long round_jiffies_up(unsigned long j)
         :                      {
         :                      return round_jiffies_common(j, raw_smp_processor_id(), true);
    5.93 :   ffff800010167984:       adrp    x1, ffff8000114ca000 <bp_hardening_data>
    0.00 :   ffff800010167988:       add     x1, x1, #0x18
         :                      round_jiffies_common():
         :                      j += cpu * 3;
    4.13 :   ffff80001016798c:       ldr     w2, [x1, x2]
         :                      rem = j % HZ;
    0.00 :   ffff800010167990:       mov     x5, #0xf7cf                     // #63439
         :                      return time_is_after_jiffies(j) ? j : original;
    2.38 :   ffff800010167994:       adrp    x1, ffff800011897000 <bit_wait_table+0xe80>
         :                      rem = j % HZ;
    0.00 :   ffff800010167998:       movk    x5, #0xe353, lsl #16
   62.66 :   ffff80001016799c:       movk    x5, #0x9ba5, lsl #32
         :                      j -= cpu * 3;
    0.00 :   ffff8000101679a0:       add     x3, x0, #0xfa
         :                      j += cpu * 3;
    0.00 :   ffff8000101679a4:       add     w2, w2, w2, lsl #1
         :                      return time_is_after_jiffies(j) ? j : original;
    1.78 :   ffff8000101679a8:       ldr     x4, [x1, #2432]
         :                      rem = j % HZ;
    0.60 :   ffff8000101679ac:       movk    x5, #0x20c4, lsl #48
         :                      j += cpu * 3;
    0.00 :   ffff8000101679b0:       add     x2, x0, w2, sxtw
         :                      rem = j % HZ;
    0.59 :   ffff8000101679b4:       lsr     x1, x2, #1
    2.38 :   ffff8000101679b8:       umulh   x1, x1, x5
    0.00 :   ffff8000101679bc:       lsr     x1, x1, #4
    0.60 :   ffff8000101679c0:       lsl     x5, x1, #5
    0.00 :   ffff8000101679c4:       sub     x5, x5, x1
    1.19 :   ffff8000101679c8:       add     x1, x1, x5, lsl #2
    0.00 :   ffff8000101679cc:       sub     x1, x2, x1, lsl #1
         :                      j -= cpu * 3;
    2.95 :   ffff8000101679d0:       sub     x1, x3, x1
         :                      return time_is_after_jiffies(j) ? j : original;
    0.00 :   ffff8000101679d4:       sub     x2, x4, x1
    8.92 :   ffff8000101679d8:       cmp     x2, #0x0
         :                      round_jiffies_up():
         :                      }
    0.00 :   ffff8000101679dc:       csel    x0, x1, x0, lt  // lt = tstop
    0.00 :   ffff8000101679e0:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (151 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104562e8 <blk_finish_plug>:
         :                      blk_finish_plug():
         :                      * must be paired with an initial call to blk_start_plug().  The intent
         :                      * is to allow the block layer to optimize I/O submission.  See the
         :                      * documentation for blk_start_plug() for more information.
         :                      */
         :                      void blk_finish_plug(struct blk_plug *plug)
         :                      {
   12.54 :   ffff8000104562e8:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff8000104562ec:       mov     x29, sp
   29.83 :   ffff8000104562f0:       str     x19, [sp, #16]
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.66 :   ffff8000104562f4:       mrs     x19, sp_el0
         :                      blk_finish_plug():
         :                      if (plug != current->plug)
    0.00 :   ffff8000104562f8:       ldr     x1, [x19, #1840]
    0.00 :   ffff8000104562fc:       cmp     x1, x0
    0.00 :   ffff800010456300:       b.eq    ffff800010456310 <blk_finish_plug+0x28>  // b.none
         :                      return;
         :                      blk_flush_plug_list(plug, false);
         :
         :                      current->plug = NULL;
         :                      }
    0.00 :   ffff800010456304:       ldr     x19, [sp, #16]
    0.00 :   ffff800010456308:       ldp     x29, x30, [sp], #32
    0.00 :   ffff80001045630c:       ret
         :                      blk_flush_plug_list(plug, false);
   45.70 :   ffff800010456310:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010456314:       bl      ffff8000104561e0 <blk_flush_plug_list>
         :                      current->plug = NULL;
    6.62 :   ffff800010456318:       str     xzr, [x19, #1840]
         :                      }
    4.66 :   ffff80001045631c:       ldr     x19, [sp, #16]
    0.00 :   ffff800010456320:       ldp     x29, x30, [sp], #32
    0.00 :   ffff800010456324:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (145 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104834c8 <iov_iter_alignment>:
         :                      iov_iter_alignment():
         :                      i->iov_offset = 0;
         :                      }
         :                      EXPORT_SYMBOL(iov_iter_discard);
         :
         :                      unsigned long iov_iter_alignment(const struct iov_iter *i)
         :                      {
   31.73 :   ffff8000104834c8:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff8000104834cc:       mov     x29, sp
    4.12 :   ffff8000104834d0:       str     x19, [sp, #16]
         :                      iov_iter_is_pipe():
         :                      return iov_iter_type(i) == ITER_BVEC;
         :                      }
         :
         :                      static inline bool iov_iter_is_pipe(const struct iov_iter *i)
         :                      {
         :                      return iov_iter_type(i) == ITER_PIPE;
    2.77 :   ffff8000104834d4:       ldr     w3, [x0]
         :                      iov_iter_alignment():
         :                      unsigned long res = 0;
         :                      size_t size = i->count;
    1.40 :   ffff8000104834d8:       ldr     x2, [x0, #16]
         :                      iov_iter_type():
         :                      return i->type & ~(READ | WRITE);
    0.00 :   ffff8000104834dc:       and     w1, w3, #0xfffffffe
         :                      iov_iter_alignment():
         :
         :                      if (unlikely(iov_iter_is_pipe(i))) {
    0.00 :   ffff8000104834e0:       cmp     w1, #0x20
    0.00 :   ffff8000104834e4:       b.eq    ffff80001048357c <iov_iter_alignment+0xb4>  // b.none
         :
         :                      if (size && i->iov_offset && allocated(&i->pipe->bufs[i->head & p_mask]))
         :                      return size | i->iov_offset;
         :                      return size;
         :                      }
         :                      iterate_all_kinds(i, size, v,
    8.31 :   ffff8000104834e8:       cbz     x2, ffff800010483568 <iov_iter_alignment+0xa0>
    2.07 :   ffff8000104834ec:       ldr     x1, [x0, #8]
    0.00 :   ffff8000104834f0:       tbnz    w3, #4, ffff8000104835cc <iov_iter_alignment+0x104>
    0.00 :   ffff8000104834f4:       tbnz    w3, #3, ffff80001048365c <iov_iter_alignment+0x194>
    0.00 :   ffff8000104834f8:       tbnz    w3, #6, ffff800010483568 <iov_iter_alignment+0xa0>
    8.99 :   ffff8000104834fc:       ldr     x3, [x0, #24]
    0.69 :   ffff800010483500:       ldr     x0, [x3, #8]
    0.00 :   ffff800010483504:       sub     x0, x0, x1
    0.00 :   ffff800010483508:       cmp     x0, x2
    0.00 :   ffff80001048350c:       csel    x0, x0, x2, ls  // ls = plast
    6.91 :   ffff800010483510:       cbz     x0, ffff800010483538 <iov_iter_alignment+0x70>
    0.00 :   ffff800010483514:       ldr     x19, [x3]
    0.00 :   ffff800010483518:       subs    x2, x2, x0
    0.00 :   ffff80001048351c:       add     x1, x19, x1
    0.00 :   ffff800010483520:       orr     x19, x1, x0
   17.18 :   ffff800010483524:       b.ne    ffff80001048353c <iov_iter_alignment+0x74>  // b.any
         :                      (res |= (unsigned long)v.iov_base | v.iov_len, 0),
         :                      res |= v.bv_offset | v.bv_len,
         :                      res |= (unsigned long)v.iov_base | v.iov_len
         :                      )
         :                      return res;
         :                      }
    0.00 :   ffff800010483528:       mov     x0, x19
   15.84 :   ffff80001048352c:       ldr     x19, [sp, #16]
    0.00 :   ffff800010483530:       ldp     x29, x30, [sp], #32
    0.00 :   ffff800010483534:       ret
         :                      unsigned long res = 0;
    0.00 :   ffff800010483538:       mov     x19, #0x0                       // #0
         :                      iterate_all_kinds(i, size, v,
    0.00 :   ffff80001048353c:       add     x3, x3, #0x10
    0.00 :   ffff800010483540:       ldr     x1, [x3, #8]
    0.00 :   ffff800010483544:       cmp     x1, x2
    0.00 :   ffff800010483548:       csel    x1, x1, x2, ls  // ls = plast
    0.00 :   ffff80001048354c:       cbz     x1, ffff80001048353c <iov_iter_alignment+0x74>
    0.00 :   ffff800010483550:       ldr     x0, [x3]
    0.00 :   ffff800010483554:       subs    x2, x2, x1
    0.00 :   ffff800010483558:       orr     x0, x0, x19
    0.00 :   ffff80001048355c:       orr     x19, x1, x0
    0.00 :   ffff800010483560:       b.eq    ffff800010483528 <iov_iter_alignment+0x60>  // b.none
    0.00 :   ffff800010483564:       b       ffff80001048353c <iov_iter_alignment+0x74>
    0.00 :   ffff800010483568:       mov     x19, #0x0                       // #0
         :                      }
    0.00 :   ffff80001048356c:       mov     x0, x19
    0.00 :   ffff800010483570:       ldr     x19, [sp, #16]
    0.00 :   ffff800010483574:       ldp     x29, x30, [sp], #32
    0.00 :   ffff800010483578:       ret
    0.00 :   ffff80001048357c:       mov     x19, #0x0                       // #0
         :                      if (size && i->iov_offset && allocated(&i->pipe->bufs[i->head & p_mask]))
    0.00 :   ffff800010483580:       cbz     x2, ffff800010483528 <iov_iter_alignment+0x60>
    0.00 :   ffff800010483584:       ldr     x1, [x0, #8]
    0.00 :   ffff800010483588:       mov     x19, x2
    0.00 :   ffff80001048358c:       cbz     x1, ffff800010483528 <iov_iter_alignment+0x60>
         :                      unsigned int p_mask = i->pipe->ring_size - 1;
    0.00 :   ffff800010483590:       ldr     x3, [x0, #24]
         :                      if (size && i->iov_offset && allocated(&i->pipe->bufs[i->head & p_mask]))
    0.00 :   ffff800010483594:       mov     w4, #0x28                       // #40
    0.00 :   ffff800010483598:       ldr     w5, [x0, #32]
    0.00 :   ffff80001048359c:       adrp    x2, ffff800010cf9000 <empty_dir_inode_operations+0x40>
    0.00 :   ffff8000104835a0:       add     x2, x2, #0x280
         :                      return size | i->iov_offset;
    0.00 :   ffff8000104835a4:       orr     x1, x1, x19
         :                      unsigned int p_mask = i->pipe->ring_size - 1;
    0.00 :   ffff8000104835a8:       ldr     w0, [x3, #68]
         :                      if (size && i->iov_offset && allocated(&i->pipe->bufs[i->head & p_mask]))
    0.00 :   ffff8000104835ac:       ldr     x3, [x3, #120]
         :                      unsigned int p_mask = i->pipe->ring_size - 1;
    0.00 :   ffff8000104835b0:       sub     w0, w0, #0x1
         :                      if (size && i->iov_offset && allocated(&i->pipe->bufs[i->head & p_mask]))
    0.00 :   ffff8000104835b4:       and     w0, w0, w5
    0.00 :   ffff8000104835b8:       umaddl  x0, w0, w4, x3
    0.00 :   ffff8000104835bc:       ldr     x0, [x0, #16]
         :                      return size | i->iov_offset;
    0.00 :   ffff8000104835c0:       cmp     x0, x2
    0.00 :   ffff8000104835c4:       csel    x19, x1, x19, eq  // eq = none
    0.00 :   ffff8000104835c8:       b       ffff800010483528 <iov_iter_alignment+0x60>
         :                      iterate_all_kinds(i, size, v,
    0.00 :   ffff8000104835cc:       mov     w3, w2
    0.00 :   ffff8000104835d0:       mov     w4, #0x0                        // #0
         :                      unsigned long res = 0;
    0.00 :   ffff8000104835d4:       mov     x19, #0x0                       // #0
         :                      iterate_all_kinds(i, size, v,
    0.00 :   ffff8000104835d8:       mov     w9, #0x1000                     // #4096
    0.00 :   ffff8000104835dc:       nop
    0.00 :   ffff8000104835e0:       cbz     w3, ffff800010483528 <iov_iter_alignment+0x60>
    0.00 :   ffff8000104835e4:       ldr     x7, [x0, #24]
    0.00 :   ffff8000104835e8:       ubfiz   x5, x4, #4, #32
    0.00 :   ffff8000104835ec:       add     x5, x7, x5
    0.00 :   ffff8000104835f0:       ldp     w2, w6, [x5, #8]
    0.00 :   ffff8000104835f4:       sub     w5, w2, w1
    0.00 :   ffff8000104835f8:       add     w6, w1, w6
    0.00 :   ffff8000104835fc:       cmp     w5, w3
    0.00 :   ffff800010483600:       and     w6, w6, #0xfff
    0.00 :   ffff800010483604:       csel    w5, w5, w3, ls  // ls = plast
    0.00 :   ffff800010483608:       sub     w8, w9, w6
    0.00 :   ffff80001048360c:       cmp     w5, w8
    0.00 :   ffff800010483610:       csel    w5, w5, w8, ls  // ls = plast
    0.00 :   ffff800010483614:       cbz     w5, ffff800010483628 <iov_iter_alignment+0x160>
    0.00 :   ffff800010483618:       orr     w6, w6, w5
         :                      bvec_iter_advance():
         :                      static inline bool bvec_iter_advance(const struct bio_vec *bv,
         :                      struct bvec_iter *iter, unsigned bytes)
         :                      {
         :                      unsigned int idx = iter->bi_idx;
         :
         :                      if (WARN_ONCE(bytes > iter->bi_size,
    0.00 :   ffff80001048361c:       cmp     w3, w5
         :                      iov_iter_alignment():
    0.00 :   ffff800010483620:       orr     x19, x19, x6
         :                      bvec_iter_advance():
    0.00 :   ffff800010483624:       b.cc    ffff8000104836b4 <iov_iter_alignment+0x1ec>  // b.lo, b.ul, b.last
         :                      "Attempted to advance past end of bvec iter\n")) {
         :                      iter->bi_size = 0;
         :                      return false;
         :                      }
         :
         :                      iter->bi_size -= bytes;
    0.00 :   ffff800010483628:       sub     w3, w3, w5
         :                      bytes += iter->bi_bvec_done;
         :
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff80001048362c:       adds    w1, w1, w5
    0.00 :   ffff800010483630:       b.eq    ffff8000104835e0 <iov_iter_alignment+0x118>  // b.none
    0.00 :   ffff800010483634:       nop
    0.00 :   ffff800010483638:       cmp     w2, w1
    0.00 :   ffff80001048363c:       b.hi    ffff8000104835e0 <iov_iter_alignment+0x118>  // b.pmore
    0.00 :   ffff800010483640:       subs    w1, w1, w2
         :                      bytes -= bv[idx].bv_len;
         :                      idx++;
    0.00 :   ffff800010483644:       add     w4, w4, #0x1
         :                      while (bytes && bytes >= bv[idx].bv_len) {
    0.00 :   ffff800010483648:       b.eq    ffff8000104835e0 <iov_iter_alignment+0x118>  // b.none
    0.00 :   ffff80001048364c:       ubfiz   x2, x4, #4, #32
    0.00 :   ffff800010483650:       add     x2, x7, x2
    0.00 :   ffff800010483654:       ldr     w2, [x2, #8]
    0.00 :   ffff800010483658:       b       ffff800010483638 <iov_iter_alignment+0x170>
         :                      iov_iter_alignment():
    0.00 :   ffff80001048365c:       ldr     x3, [x0, #24]
    0.00 :   ffff800010483660:       ldr     x0, [x3, #8]
    0.00 :   ffff800010483664:       sub     x0, x0, x1
    0.00 :   ffff800010483668:       cmp     x0, x2
    0.00 :   ffff80001048366c:       csel    x0, x0, x2, ls  // ls = plast
    0.00 :   ffff800010483670:       cbz     x0, ffff8000104836dc <iov_iter_alignment+0x214>
    0.00 :   ffff800010483674:       ldr     x19, [x3]
    0.00 :   ffff800010483678:       subs    x2, x2, x0
    0.00 :   ffff80001048367c:       add     x1, x19, x1
    0.00 :   ffff800010483680:       orr     x19, x1, x0
    0.00 :   ffff800010483684:       b.eq    ffff800010483528 <iov_iter_alignment+0x60>  // b.none
    0.00 :   ffff800010483688:       add     x3, x3, #0x10
    0.00 :   ffff80001048368c:       ldr     x0, [x3, #8]
    0.00 :   ffff800010483690:       cmp     x0, x2
    0.00 :   ffff800010483694:       csel    x0, x0, x2, ls  // ls = plast
    0.00 :   ffff800010483698:       cbz     x0, ffff800010483688 <iov_iter_alignment+0x1c0>
    0.00 :   ffff80001048369c:       ldr     x1, [x3]
    0.00 :   ffff8000104836a0:       subs    x2, x2, x0
    0.00 :   ffff8000104836a4:       orr     x19, x1, x19
    0.00 :   ffff8000104836a8:       orr     x19, x19, x0
    0.00 :   ffff8000104836ac:       b.eq    ffff800010483528 <iov_iter_alignment+0x60>  // b.none
    0.00 :   ffff8000104836b0:       b       ffff800010483688 <iov_iter_alignment+0x1c0>
         :                      bvec_iter_advance():
         :                      if (WARN_ONCE(bytes > iter->bi_size,
    0.00 :   ffff8000104836b4:       adrp    x1, ffff800011a5d000 <__print_once.28906>
    0.00 :   ffff8000104836b8:       ldrb    w0, [x1, #82]
    0.00 :   ffff8000104836bc:       cbnz    w0, ffff800010483528 <iov_iter_alignment+0x60>
    0.00 :   ffff8000104836c0:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000104836c4:       adrp    x0, ffff8000111aa000 <kallsyms_token_index+0x30330>
    0.00 :   ffff8000104836c8:       strb    w2, [x1, #82]
    0.00 :   ffff8000104836cc:       add     x0, x0, #0xea8
    0.00 :   ffff8000104836d0:       bl      ffff8000100e5288 <__warn_printk>
    0.00 :   ffff8000104836d4:       brk     #0x800
    0.00 :   ffff8000104836d8:       b       ffff800010483528 <iov_iter_alignment+0x60>
         :                      iov_iter_alignment():
         :                      unsigned long res = 0;
    0.00 :   ffff8000104836dc:       mov     x19, #0x0                       // #0
    0.00 :   ffff8000104836e0:       b       ffff800010483688 <iov_iter_alignment+0x1c0>
 Percent |	Source code & Disassembly of vmlinux for cycles (149 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104504e0 <bio_alloc_bioset>:
         :                      bio_alloc_bioset():
         :                      *   RETURNS:
         :                      *   Pointer to new bio on success, NULL on failure.
         :                      */
         :                      struct bio *bio_alloc_bioset(gfp_t gfp_mask, unsigned int nr_iovecs,
         :                      struct bio_set *bs)
         :                      {
    0.00 :   ffff8000104504e0:       stp     x29, x30, [sp, #-112]!
    0.00 :   ffff8000104504e4:       mov     x29, sp
    0.00 :   ffff8000104504e8:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000104504ec:       mov     x20, x2
    6.01 :   ffff8000104504f0:       stp     x21, x22, [sp, #32]
    0.00 :   ffff8000104504f4:       adrp    x21, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000104504f8:       str     x23, [sp, #48]
    0.00 :   ffff8000104504fc:       add     x3, x21, #0x8c8
    0.00 :   ffff800010450500:       mov     w22, w1
    0.00 :   ffff800010450504:       mov     w23, w0
    0.00 :   ffff800010450508:       ldr     x4, [x3]
    0.67 :   ffff80001045050c:       str     x4, [x29, #104]
    0.00 :   ffff800010450510:       mov     x4, #0x0                        // #0
         :                      unsigned inline_vecs;
         :                      struct bio_vec *bvl = NULL;
         :                      struct bio *bio;
         :                      void *p;
         :
         :                      if (!bs) {
    0.00 :   ffff800010450514:       cbz     x2, ffff800010450638 <bio_alloc_bioset+0x158>
         :                      gfp_mask);
         :                      front_pad = 0;
         :                      inline_vecs = nr_iovecs;
         :                      } else {
         :                      /* should not use nobvec bioset for nr_iovecs > 0 */
         :                      if (WARN_ON_ONCE(!mempool_initialized(&bs->bvec_pool) &&
    6.08 :   ffff800010450518:       ldr     x0, [x2, #104]
    0.00 :   ffff80001045051c:       cmp     x0, #0x0
    0.00 :   ffff800010450520:       ccmp    w1, #0x0, #0x4, eq  // eq = none
    0.00 :   ffff800010450524:       b.ne    ffff8000104506dc <bio_alloc_bioset+0x1fc>  // b.any
    0.00 :   ffff800010450528:       stp     x24, x25, [x29, #56]
    0.00 :   ffff80001045052c:       str     x26, [x29, #72]
    0.00 :   ffff800010450530:       add     x26, x2, #0x10
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    2.00 :   ffff800010450534:       mrs     x0, sp_el0
         :                      bio_alloc_bioset():
         :                      * without __GFP_DIRECT_RECLAIM; if that fails, we punt those
         :                      * bios we would be blocking to the rescuer workqueue before
         :                      * we retry with the original gfp_flags.
         :                      */
         :
         :                      if (current->bio_list &&
    7.41 :   ffff800010450538:       ldr     x0, [x0, #1832]
    0.00 :   ffff80001045053c:       cbz     x0, ffff800010450620 <bio_alloc_bioset+0x140>
    0.00 :   ffff800010450540:       ldr     x1, [x0]
    0.00 :   ffff800010450544:       cbz     x1, ffff800010450614 <bio_alloc_bioset+0x134>
         :                      (!bio_list_empty(&current->bio_list[0]) ||
         :                      !bio_list_empty(&current->bio_list[1])) &&
    0.00 :   ffff800010450548:       ldr     x0, [x20, #360]
    0.00 :   ffff80001045054c:       cbz     x0, ffff800010450620 <bio_alloc_bioset+0x140>
         :                      bs->rescue_workqueue)
         :                      gfp_mask &= ~__GFP_DIRECT_RECLAIM;
    0.00 :   ffff800010450550:       and     w25, w23, #0xfffffbff
         :
         :                      p = mempool_alloc(&bs->bio_pool, gfp_mask);
    0.00 :   ffff800010450554:       mov     x0, x26
    0.00 :   ffff800010450558:       mov     w1, w25
    0.00 :   ffff80001045055c:       bl      ffff8000101d5c38 <mempool_alloc>
         :                      if (!p && gfp_mask != saved_gfp) {
    0.00 :   ffff800010450560:       cmp     x0, #0x0
         :                      p = mempool_alloc(&bs->bio_pool, gfp_mask);
    0.00 :   ffff800010450564:       mov     x24, x0
         :                      if (!p && gfp_mask != saved_gfp) {
    0.00 :   ffff800010450568:       ccmp    w23, w25, #0x4, eq  // eq = none
    0.00 :   ffff80001045056c:       b.eq    ffff80001045058c <bio_alloc_bioset+0xac>  // b.none
         :                      punt_bios_to_rescuer(bs);
    0.00 :   ffff800010450570:       mov     x0, x20
    0.00 :   ffff800010450574:       bl      ffff80001044e4f8 <punt_bios_to_rescuer>
         :                      gfp_mask = saved_gfp;
         :                      p = mempool_alloc(&bs->bio_pool, gfp_mask);
    0.00 :   ffff800010450578:       mov     w1, w23
    0.00 :   ffff80001045057c:       mov     x0, x26
    0.00 :   ffff800010450580:       mov     w25, w23
    0.00 :   ffff800010450584:       bl      ffff8000101d5c38 <mempool_alloc>
    0.00 :   ffff800010450588:       mov     x24, x0
         :                      }
         :
         :                      front_pad = bs->front_pad;
   11.39 :   ffff80001045058c:       ldr     w19, [x20, #8]
         :                      inline_vecs = BIO_INLINE_VECS;
         :                      }
         :
         :                      if (unlikely(!p))
    0.00 :   ffff800010450590:       cbz     x24, ffff800010450708 <bio_alloc_bioset+0x228>
         :                      return NULL;
         :
         :                      bio = p + front_pad;
    0.00 :   ffff800010450594:       add     x19, x24, w19, uxtw
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010450598:       mov     w0, #0x1                        // #1
         :                      bio_alloc_bioset():
         :                      bio_init(bio, NULL, 0);
         :
         :                      if (nr_iovecs > inline_vecs) {
    0.00 :   ffff80001045059c:       cmp     w22, #0x4
         :                      bio_init():
         :                      memset(bio, 0, sizeof(*bio));
    0.00 :   ffff8000104505a0:       stp     xzr, xzr, [x19, #16]
         :                      __write_once_size():
    0.00 :   ffff8000104505a4:       str     w0, [x19, #28]
         :                      bio_init():
   14.06 :   ffff8000104505a8:       stp     xzr, xzr, [x19, #96]
    0.00 :   ffff8000104505ac:       stp     xzr, xzr, [x19]
    0.00 :   ffff8000104505b0:       stp     xzr, xzr, [x19, #32]
    0.00 :   ffff8000104505b4:       stp     xzr, xzr, [x19, #48]
   12.76 :   ffff8000104505b8:       stp     xzr, xzr, [x19, #64]
    0.00 :   ffff8000104505bc:       stp     xzr, xzr, [x19, #80]
         :                      __write_once_size():
    0.00 :   ffff8000104505c0:       str     w0, [x19, #100]
         :                      bio_init():
    0.00 :   ffff8000104505c4:       str     xzr, [x19, #112]
         :                      bio_alloc_bioset():
         :                      if (nr_iovecs > inline_vecs) {
    0.00 :   ffff8000104505c8:       b.hi    ffff800010450688 <bio_alloc_bioset+0x1a8>  // b.pmore
    8.10 :   ffff8000104505cc:       ldp     x24, x25, [x29, #56]
    0.68 :   ffff8000104505d0:       ldr     x26, [x29, #72]
         :                      if (unlikely(!bvl))
         :                      goto err_free;
         :
         :                      bio->bi_flags |= idx << BVEC_POOL_OFFSET;
         :                      } else if (nr_iovecs) {
         :                      bvl = bio->bi_inline_vecs;
    0.00 :   ffff8000104505d4:       add     x0, x19, #0x78
    0.00 :   ffff8000104505d8:       cmp     w22, #0x0
    0.00 :   ffff8000104505dc:       csel    x0, x0, xzr, ne  // ne = any
         :                      }
         :
         :                      bio->bi_pool = bs;
         :                      bio->bi_max_vecs = nr_iovecs;
    0.00 :   ffff8000104505e0:       strh    w22, [x19, #98]
         :                      bio->bi_pool = bs;
    0.00 :   ffff8000104505e4:       stp     x0, x20, [x19, #104]
         :                      return bio;
         :
         :                      err_free:
         :                      mempool_free(p, &bs->bio_pool);
         :                      return NULL;
         :                      }
    0.00 :   ffff8000104505e8:       add     x21, x21, #0x8c8
   14.11 :   ffff8000104505ec:       mov     x0, x19
    2.03 :   ffff8000104505f0:       ldr     x2, [x29, #104]
    0.00 :   ffff8000104505f4:       ldr     x1, [x21]
    0.00 :   ffff8000104505f8:       eor     x1, x2, x1
    0.00 :   ffff8000104505fc:       cbnz    x1, ffff800010450738 <bio_alloc_bioset+0x258>
    6.05 :   ffff800010450600:       ldp     x19, x20, [sp, #16]
    1.98 :   ffff800010450604:       ldp     x21, x22, [sp, #32]
    1.33 :   ffff800010450608:       ldr     x23, [sp, #48]
    0.66 :   ffff80001045060c:       ldp     x29, x30, [sp], #112
    0.00 :   ffff800010450610:       ret
         :                      (!bio_list_empty(&current->bio_list[0]) ||
    0.00 :   ffff800010450614:       ldr     x0, [x0, #16]
    0.00 :   ffff800010450618:       cbnz    x0, ffff800010450548 <bio_alloc_bioset+0x68>
    0.00 :   ffff80001045061c:       nop
         :                      p = mempool_alloc(&bs->bio_pool, gfp_mask);
    4.68 :   ffff800010450620:       mov     w1, w23
    0.00 :   ffff800010450624:       mov     x0, x26
    0.00 :   ffff800010450628:       mov     w25, w23
    0.00 :   ffff80001045062c:       bl      ffff8000101d5c38 <mempool_alloc>
    0.00 :   ffff800010450630:       mov     x24, x0
    0.00 :   ffff800010450634:       b       ffff80001045058c <bio_alloc_bioset+0xac>
         :                      if (nr_iovecs > UIO_MAXIOV)
    0.00 :   ffff800010450638:       cmp     w1, #0x400
    0.00 :   ffff80001045063c:       b.hi    ffff8000104506e0 <bio_alloc_bioset+0x200>  // b.pmore
         :                      p = kmalloc(sizeof(struct bio) +
    0.00 :   ffff800010450640:       ubfiz   x0, x22, #4, #32
         :                      kmalloc():
         :                      return kmem_cache_alloc_trace(
         :                      kmalloc_caches[kmalloc_type(flags)][index],
         :                      flags, size);
         :                      #endif
         :                      }
         :                      return __kmalloc(size, flags);
    0.00 :   ffff800010450644:       mov     w1, w23
    0.00 :   ffff800010450648:       add     x0, x0, #0x78
    0.00 :   ffff80001045064c:       bl      ffff800010251988 <__kmalloc>
    0.00 :   ffff800010450650:       mov     x19, x0
         :                      bio_alloc_bioset():
         :                      if (unlikely(!p))
    0.00 :   ffff800010450654:       cbz     x0, ffff8000104506e0 <bio_alloc_bioset+0x200>
         :                      bio_init():
         :                      memset(bio, 0, sizeof(*bio));
    0.00 :   ffff800010450658:       stp     xzr, xzr, [x0, #16]
         :                      __write_once_size():
    0.00 :   ffff80001045065c:       mov     w0, #0x1                        // #1
    0.00 :   ffff800010450660:       str     w0, [x19, #28]
         :                      bio_init():
    0.00 :   ffff800010450664:       stp     xzr, xzr, [x19, #96]
    0.00 :   ffff800010450668:       stp     xzr, xzr, [x19]
    0.00 :   ffff80001045066c:       stp     xzr, xzr, [x19, #32]
    0.00 :   ffff800010450670:       stp     xzr, xzr, [x19, #48]
    0.00 :   ffff800010450674:       stp     xzr, xzr, [x19, #64]
    0.00 :   ffff800010450678:       stp     xzr, xzr, [x19, #80]
         :                      __write_once_size():
    0.00 :   ffff80001045067c:       str     w0, [x19, #100]
         :                      bio_init():
    0.00 :   ffff800010450680:       str     xzr, [x19, #112]
    0.00 :   ffff800010450684:       b       ffff8000104505d4 <bio_alloc_bioset+0xf4>
    0.00 :   ffff800010450688:       stp     x27, x28, [x29, #80]
         :                      bio_alloc_bioset():
         :                      unsigned long idx = 0;
    0.00 :   ffff80001045068c:       add     x28, x29, #0x70
         :                      bvl = bvec_alloc(gfp_mask, nr_iovecs, &idx, &bs->bvec_pool);
    0.00 :   ffff800010450690:       add     x27, x20, #0x58
    0.00 :   ffff800010450694:       mov     w1, w22
    0.00 :   ffff800010450698:       mov     x3, x27
    0.00 :   ffff80001045069c:       mov     w0, w25
         :                      unsigned long idx = 0;
    0.00 :   ffff8000104506a0:       str     xzr, [x28, #-16]!
         :                      bvl = bvec_alloc(gfp_mask, nr_iovecs, &idx, &bs->bvec_pool);
    0.00 :   ffff8000104506a4:       mov     x2, x28
    0.00 :   ffff8000104506a8:       bl      ffff8000104503b0 <bvec_alloc>
         :                      if (!bvl && gfp_mask != saved_gfp) {
    0.00 :   ffff8000104506ac:       cmp     x0, #0x0
    0.00 :   ffff8000104506b0:       ccmp    w25, w23, #0x4, eq  // eq = none
    0.00 :   ffff8000104506b4:       b.ne    ffff8000104506e8 <bio_alloc_bioset+0x208>  // b.any
         :                      if (unlikely(!bvl))
    0.00 :   ffff8000104506b8:       cbz     x0, ffff800010450718 <bio_alloc_bioset+0x238>
         :                      bio->bi_flags |= idx << BVEC_POOL_OFFSET;
    0.00 :   ffff8000104506bc:       ldrh    w2, [x19, #20]
    0.00 :   ffff8000104506c0:       ldr     x1, [x29, #96]
    0.00 :   ffff8000104506c4:       orr     w1, w2, w1, lsl #13
    0.00 :   ffff8000104506c8:       strh    w1, [x19, #20]
    0.00 :   ffff8000104506cc:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff8000104506d0:       ldp     x26, x27, [x29, #72]
    0.00 :   ffff8000104506d4:       ldr     x28, [x29, #88]
    0.00 :   ffff8000104506d8:       b       ffff8000104505e0 <bio_alloc_bioset+0x100>
         :                      if (WARN_ON_ONCE(!mempool_initialized(&bs->bvec_pool) &&
    0.00 :   ffff8000104506dc:       brk     #0x800
         :                      return NULL;
    0.00 :   ffff8000104506e0:       mov     x19, #0x0                       // #0
    0.00 :   ffff8000104506e4:       b       ffff8000104505e8 <bio_alloc_bioset+0x108>
         :                      punt_bios_to_rescuer(bs);
    0.00 :   ffff8000104506e8:       mov     x0, x20
    0.00 :   ffff8000104506ec:       bl      ffff80001044e4f8 <punt_bios_to_rescuer>
         :                      bvl = bvec_alloc(gfp_mask, nr_iovecs, &idx, &bs->bvec_pool);
    0.00 :   ffff8000104506f0:       mov     x3, x27
    0.00 :   ffff8000104506f4:       mov     x2, x28
    0.00 :   ffff8000104506f8:       mov     w1, w22
    0.00 :   ffff8000104506fc:       mov     w0, w23
    0.00 :   ffff800010450700:       bl      ffff8000104503b0 <bvec_alloc>
    0.00 :   ffff800010450704:       b       ffff8000104506b8 <bio_alloc_bioset+0x1d8>
    0.00 :   ffff800010450708:       ldp     x24, x25, [x29, #56]
         :                      return NULL;
    0.00 :   ffff80001045070c:       mov     x19, #0x0                       // #0
    0.00 :   ffff800010450710:       ldr     x26, [x29, #72]
    0.00 :   ffff800010450714:       b       ffff8000104505e8 <bio_alloc_bioset+0x108>
         :                      mempool_free(p, &bs->bio_pool);
    0.00 :   ffff800010450718:       mov     x1, x26
    0.00 :   ffff80001045071c:       mov     x0, x24
         :                      return NULL;
    0.00 :   ffff800010450720:       mov     x19, #0x0                       // #0
         :                      mempool_free(p, &bs->bio_pool);
    0.00 :   ffff800010450724:       bl      ffff8000101d5b40 <mempool_free>
         :                      return NULL;
    0.00 :   ffff800010450728:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff80001045072c:       ldp     x26, x27, [x29, #72]
    0.00 :   ffff800010450730:       ldr     x28, [x29, #88]
    0.00 :   ffff800010450734:       b       ffff8000104505e8 <bio_alloc_bioset+0x108>
    0.00 :   ffff800010450738:       stp     x24, x25, [x29, #56]
    0.00 :   ffff80001045073c:       stp     x26, x27, [x29, #72]
    0.00 :   ffff800010450740:       str     x28, [x29, #88]
         :                      }
    0.00 :   ffff800010450744:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (87 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010cad270 <schedule>:
         :                      schedule():
         :                      io_wq_worker_running(tsk);
         :                      }
         :                      }
         :
         :                      asmlinkage __visible void __sched schedule(void)
         :                      {
    0.00 :   ffff800010cad270:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff800010cad274:       mov     x29, sp
    0.00 :   ffff800010cad278:       stp     x19, x20, [sp, #16]
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
   11.55 :   ffff800010cad27c:       mrs     x20, sp_el0
         :                      sched_submit_work():
         :                      if (!tsk->state)
    0.00 :   ffff800010cad280:       ldr     x0, [x20, #24]
    0.00 :   ffff800010cad284:       cbz     x0, ffff800010cad2cc <schedule+0x5c>
         :                      if (tsk->flags & (PF_WQ_WORKER | PF_IO_WORKER)) {
    0.00 :   ffff800010cad288:       ldr     w0, [x20, #44]
    0.00 :   ffff800010cad28c:       and     w0, w0, #0x3fffffe0
    0.00 :   ffff800010cad290:       and     w0, w0, #0xe000003f
    0.00 :   ffff800010cad294:       cbz     w0, ffff800010cad2c4 <schedule+0x54>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010cad298:       ldr     w0, [x20, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff800010cad29c:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010cad2a0:       str     w0, [x20, #16]
         :                      sched_submit_work():
         :                      if (tsk->flags & PF_WQ_WORKER)
    0.00 :   ffff800010cad2a4:       ldr     w0, [x20, #44]
    0.00 :   ffff800010cad2a8:       tbnz    w0, #5, ffff800010cad350 <schedule+0xe0>
         :                      io_wq_worker_sleeping(tsk);
    0.00 :   ffff800010cad2ac:       mov     x0, x20
    0.00 :   ffff800010cad2b0:       bl      ffff8000102e4d90 <io_wq_worker_sleeping>
         :                      get_current():
    0.00 :   ffff800010cad2b4:       mrs     x1, sp_el0
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010cad2b8:       ldr     w0, [x1, #16]
         :                      __preempt_count_sub():
         :                      }
         :
         :                      static inline void __preempt_count_sub(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc -= val;
    0.00 :   ffff800010cad2bc:       sub     w0, w0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010cad2c0:       str     w0, [x1, #16]
         :                      sched_submit_work():
         :                      if (tsk_is_pi_blocked(tsk))
    0.00 :   ffff800010cad2c4:       ldr     x0, [x20, #1816]
    0.00 :   ffff800010cad2c8:       cbz     x0, ffff800010cad330 <schedule+0xc0>
         :                      get_current():
    0.00 :   ffff800010cad2cc:       mrs     x19, sp_el0
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010cad2d0:       ldr     w0, [x19, #16]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff800010cad2d4:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010cad2d8:       str     w0, [x19, #16]
         :                      schedule():
         :                      struct task_struct *tsk = current;
         :
         :                      sched_submit_work(tsk);
         :                      do {
         :                      preempt_disable();
         :                      __schedule(false);
    0.00 :   ffff800010cad2dc:       mov     w0, #0x0                        // #0
    0.00 :   ffff800010cad2e0:       bl      ffff800010caccd8 <__schedule>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
   17.54 :   ffff800010cad2e4:       ldr     w0, [x19, #16]
         :                      __preempt_count_sub():
         :                      pc -= val;
    0.00 :   ffff800010cad2e8:       sub     w0, w0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
   32.83 :   ffff800010cad2ec:       str     w0, [x19, #16]
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    1.07 :   ffff800010cad2f0:       ldr     x0, [x19]
         :                      schedule():
         :                      sched_preempt_enable_no_resched();
         :                      } while (need_resched());
    0.00 :   ffff800010cad2f4:       tbnz    w0, #1, ffff800010cad2cc <schedule+0x5c>
         :                      sched_update_worker():
         :                      if (tsk->flags & (PF_WQ_WORKER | PF_IO_WORKER)) {
    5.82 :   ffff800010cad2f8:       ldr     w1, [x20, #44]
    0.00 :   ffff800010cad2fc:       and     w0, w1, #0x3fffffe0
    1.18 :   ffff800010cad300:       and     w0, w0, #0xe000003f
    0.00 :   ffff800010cad304:       cbz     w0, ffff800010cad314 <schedule+0xa4>
         :                      wq_worker_running(tsk);
    0.00 :   ffff800010cad308:       mov     x0, x20
         :                      if (tsk->flags & PF_WQ_WORKER)
    0.00 :   ffff800010cad30c:       tbnz    w1, #5, ffff800010cad320 <schedule+0xb0>
         :                      io_wq_worker_running(tsk);
    0.00 :   ffff800010cad310:       bl      ffff8000102e4d30 <io_wq_worker_running>
         :                      schedule():
         :                      sched_update_worker(tsk);
         :                      }
    2.32 :   ffff800010cad314:       ldp     x19, x20, [sp, #16]
   21.87 :   ffff800010cad318:       ldp     x29, x30, [sp], #32
    0.00 :   ffff800010cad31c:       ret
         :                      sched_update_worker():
         :                      wq_worker_running(tsk);
    0.00 :   ffff800010cad320:       bl      ffff800010105518 <wq_worker_running>
         :                      schedule():
         :                      }
    0.00 :   ffff800010cad324:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010cad328:       ldp     x29, x30, [sp], #32
    0.00 :   ffff800010cad32c:       ret
         :                      blk_needs_flush_plug():
         :                      blk_flush_plug_list(plug, true);
         :                      }
         :
         :                      static inline bool blk_needs_flush_plug(struct task_struct *tsk)
         :                      {
         :                      struct blk_plug *plug = tsk->plug;
    5.83 :   ffff800010cad330:       ldr     x0, [x20, #1840]
         :
         :                      return plug &&
    0.00 :   ffff800010cad334:       cbz     x0, ffff800010cad2cc <schedule+0x5c>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010cad338:       ldr     x1, [x0]
         :                      blk_needs_flush_plug():
    0.00 :   ffff800010cad33c:       cmp     x0, x1
    0.00 :   ffff800010cad340:       b.eq    ffff800010cad35c <schedule+0xec>  // b.none
         :                      blk_schedule_flush_plug():
         :                      blk_flush_plug_list(plug, true);
    0.00 :   ffff800010cad344:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010cad348:       bl      ffff8000104561e0 <blk_flush_plug_list>
    0.00 :   ffff800010cad34c:       b       ffff800010cad2cc <schedule+0x5c>
         :                      sched_submit_work():
         :                      wq_worker_sleeping(tsk);
    0.00 :   ffff800010cad350:       mov     x0, x20
    0.00 :   ffff800010cad354:       bl      ffff800010105570 <wq_worker_sleeping>
    0.00 :   ffff800010cad358:       b       ffff800010cad2b4 <schedule+0x44>
         :                      __read_once_size():
    0.00 :   ffff800010cad35c:       mov     x1, x0
    0.00 :   ffff800010cad360:       ldr     x2, [x1, #16]!
         :                      blk_needs_flush_plug():
         :                      (!list_empty(&plug->mq_list) ||
    0.00 :   ffff800010cad364:       cmp     x1, x2
    0.00 :   ffff800010cad368:       b.ne    ffff800010cad344 <schedule+0xd4>  // b.any
    0.00 :   ffff800010cad36c:       b       ffff800010cad2cc <schedule+0x5c>
 Percent |	Source code & Disassembly of vmlinux for cycles (141 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101daf40 <set_page_dirty>:
         :                      set_page_dirty():
         :                      *
         :                      * If the mapping doesn't provide a set_page_dirty a_op, then
         :                      * just fall through and assume that it wants buffer_heads.
         :                      */
         :                      int set_page_dirty(struct page *page)
         :                      {
   14.15 :   ffff8000101daf40:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff8000101daf44:       mov     x29, sp
    0.00 :   ffff8000101daf48:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000101daf4c:       adrp    x20, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000101daf50:       add     x1, x20, #0x8c8
    0.00 :   ffff8000101daf54:       mov     x19, x0
    0.00 :   ffff8000101daf58:       ldr     x2, [x1]
    0.00 :   ffff8000101daf5c:       str     x2, [x29, #40]
    0.00 :   ffff8000101daf60:       mov     x2, #0x0                        // #0
         :                      struct address_space *mapping = page_mapping(page);
    0.00 :   ffff8000101daf64:       bl      ffff8000101f1160 <page_mapping>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000101daf68:       ldr     x1, [x19, #8]
         :                      compound_head():
         :                      static inline struct page *compound_head(struct page *page)
         :                      {
         :                      unsigned long head = READ_ONCE(page->compound_head);
         :
         :                      if (unlikely(head & 1))
         :                      return (struct page *) (head - 1);
    0.00 :   ffff8000101daf6c:       sub     x2, x1, #0x1
    0.00 :   ffff8000101daf70:       tst     x1, #0x1
    0.00 :   ffff8000101daf74:       csel    x19, x2, x19, ne  // ne = any
         :                      __read_once_size():
    0.00 :   ffff8000101daf78:       ldr     x1, [x19, #8]
         :                      set_page_dirty():
         :
         :                      page = compound_head(page);
         :                      if (likely(mapping)) {
    0.00 :   ffff8000101daf7c:       cbz     x0, ffff8000101db004 <set_page_dirty+0xc4>
         :                      compound_head():
    0.00 :   ffff8000101daf80:       tst     x1, #0x1
         :                      set_page_dirty():
         :                      int (*spd)(struct page *) = mapping->a_ops->set_page_dirty;
    0.00 :   ffff8000101daf84:       ldr     x2, [x0, #112]
         :                      compound_head():
    0.00 :   ffff8000101daf88:       sub     x0, x1, #0x1
    0.00 :   ffff8000101daf8c:       csel    x0, x0, x19, ne  // ne = any
         :                      set_page_dirty():
    0.00 :   ffff8000101daf90:       ldr     x1, [x2, #24]
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff8000101daf94:       ldr     x0, [x0]
         :                      set_page_dirty():
         :                      * About lru_deactivate_page, if the page is redirty, the flag
         :                      * will be reset. So no problem. but if the page is used by readahead
         :                      * it will confuse readahead and make it restart the size rampup
         :                      * process. But it's a trivial problem.
         :                      */
         :                      if (PageReclaim(page))
    0.00 :   ffff8000101daf98:       tst     w0, #0x40000
    0.00 :   ffff8000101daf9c:       b.eq    ffff8000101dafc0 <set_page_dirty+0x80>  // b.none
         :                      __read_once_size():
    0.00 :   ffff8000101dafa0:       ldr     x2, [x19, #8]
         :                      compound_head():
    0.00 :   ffff8000101dafa4:       sub     x0, x2, #0x1
    0.00 :   ffff8000101dafa8:       tst     x2, #0x1
    0.00 :   ffff8000101dafac:       csel    x0, x0, x19, ne  // ne = any
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000101dafb0:       b       ffff8000101daff8 <set_page_dirty+0xb8>
    0.00 :   ffff8000101dafb4:       b       ffff8000101daff8 <set_page_dirty+0xb8>
         :                      __lse_atomic64_andnot():
         :                      "       " #asm_op "     %[i], %[v]\n"                                   \
         :                      : [i] "+r" (i), [v] "+Q" (v->counter)                           \
         :                      : "r" (v));                                                     \
         :                      }
         :
         :                      ATOMIC64_OP(andnot, stclr)
    0.00 :   ffff8000101dafb8:       mov     x2, #0x40000                    // #262144
    0.00 :   ffff8000101dafbc:       stclr   x2, [x0]
         :                      set_page_dirty():
         :                      ClearPageReclaim(page);
         :                      #ifdef CONFIG_BLOCK
         :                      if (!spd)
         :                      spd = __set_page_dirty_buffers;
    0.00 :   ffff8000101dafc0:       cmp     x1, #0x0
    0.00 :   ffff8000101dafc4:       adrp    x2, ffff8000102bc000 <mark_buffer_dirty+0xf8>
    0.00 :   ffff8000101dafc8:       add     x2, x2, #0xd68
         :                      #endif
         :                      return (*spd)(page);
    0.00 :   ffff8000101dafcc:       mov     x0, x19
         :                      spd = __set_page_dirty_buffers;
    0.00 :   ffff8000101dafd0:       csel    x1, x2, x1, eq  // eq = none
         :                      return (*spd)(page);
    0.00 :   ffff8000101dafd4:       blr     x1
         :                      if (!PageDirty(page)) {
         :                      if (!TestSetPageDirty(page))
         :                      return 1;
         :                      }
         :                      return 0;
         :                      }
    9.92 :   ffff8000101dafd8:       add     x20, x20, #0x8c8
    2.13 :   ffff8000101dafdc:       ldr     x2, [x29, #40]
    0.00 :   ffff8000101dafe0:       ldr     x1, [x20]
    0.00 :   ffff8000101dafe4:       eor     x1, x2, x1
    0.00 :   ffff8000101dafe8:       cbnz    x1, ffff8000101db064 <set_page_dirty+0x124>
    0.00 :   ffff8000101dafec:       ldp     x19, x20, [sp, #16]
    9.93 :   ffff8000101daff0:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000101daff4:       ret
         :                      __ll_sc_atomic64_andnot():
         :                      /*
         :                      * GAS converts the mysterious and undocumented BIC (immediate) alias to
         :                      * an AND (immediate) instruction with the immediate inverted. We don't
         :                      * have a constraint for this, so fall back to register.
         :                      */
         :                      ATOMIC64_OPS(andnot, bic, )
    0.00 :   ffff8000101daff8:       mov     x2, #0x40000                    // #262144
    0.00 :   ffff8000101daffc:       b       ffff8000101dd0dc <test_clear_page_writeback+0x41c>
    0.00 :   ffff8000101db000:       b       ffff8000101dafc0 <set_page_dirty+0x80>
         :                      compound_head():
   17.75 :   ffff8000101db004:       tst     x1, #0x1
    0.00 :   ffff8000101db008:       sub     x0, x1, #0x1
    0.00 :   ffff8000101db00c:       csel    x0, x0, x19, ne  // ne = any
         :                      test_bit():
    0.00 :   ffff8000101db010:       ldr     x0, [x0]
         :                      set_page_dirty():
         :                      if (!PageDirty(page)) {
    0.00 :   ffff8000101db014:       tst     w0, #0x8
    0.00 :   ffff8000101db018:       b.ne    ffff8000101db034 <set_page_dirty+0xf4>  // b.any
         :                      __read_once_size():
    0.00 :   ffff8000101db01c:       ldr     x0, [x19, #8]
         :                      compound_head():
    0.00 :   ffff8000101db020:       sub     x1, x0, #0x1
    0.00 :   ffff8000101db024:       tst     x0, #0x1
    0.00 :   ffff8000101db028:       csel    x19, x1, x19, ne  // ne = any
         :                      __read_once_size():
    0.00 :   ffff8000101db02c:       ldr     x0, [x19]
         :                      test_and_set_bit():
         :                      {
         :                      long old;
         :                      unsigned long mask = BIT_MASK(nr);
         :
         :                      p += BIT_WORD(nr);
         :                      if (READ_ONCE(*p) & mask)
    0.00 :   ffff8000101db030:       tbz     w0, #3, ffff8000101db03c <set_page_dirty+0xfc>
         :                      set_page_dirty():
         :                      return 0;
   46.12 :   ffff8000101db034:       mov     w0, #0x0                        // #0
    0.00 :   ffff8000101db038:       b       ffff8000101dafd8 <set_page_dirty+0x98>
         :                      arch_static_branch_jump():
    0.00 :   ffff8000101db03c:       b       ffff8000101db05c <set_page_dirty+0x11c>
    0.00 :   ffff8000101db040:       b       ffff8000101db05c <set_page_dirty+0x11c>
         :                      __lse_atomic64_fetch_or():
         :                      ATOMIC64_FETCH_OP(_acquire,  a, op, asm_op, "memory")           \
         :                      ATOMIC64_FETCH_OP(_release,  l, op, asm_op, "memory")           \
         :                      ATOMIC64_FETCH_OP(        , al, op, asm_op, "memory")
         :
         :                      ATOMIC64_FETCH_OPS(andnot, ldclr)
         :                      ATOMIC64_FETCH_OPS(or, ldset)
    0.00 :   ffff8000101db044:       mov     x0, #0x8                        // #8
    0.00 :   ffff8000101db048:       ldsetal x0, x0, [x19]
         :                      set_page_dirty():
         :                      if (!TestSetPageDirty(page))
    0.00 :   ffff8000101db04c:       lsr     x0, x0, #3
    0.00 :   ffff8000101db050:       eor     x0, x0, #0x1
    0.00 :   ffff8000101db054:       and     w0, w0, #0x1
    0.00 :   ffff8000101db058:       b       ffff8000101dafd8 <set_page_dirty+0x98>
         :                      __ll_sc_atomic64_fetch_or():
         :                      ATOMIC64_OPS(or, orr, L)
    0.00 :   ffff8000101db05c:       b       ffff8000101dd0f4 <test_clear_page_writeback+0x434>
    0.00 :   ffff8000101db060:       b       ffff8000101db04c <set_page_dirty+0x10c>
         :                      set_page_dirty():
         :                      }
    0.00 :   ffff8000101db064:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (89 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001017a068 <tick_nohz_idle_enter>:
         :                      tick_nohz_idle_enter():
         :                      * tick_nohz_idle_enter - prepare for entering idle on the current CPU
         :                      *
         :                      * Called when we start the idle loop.
         :                      */
         :                      void tick_nohz_idle_enter(void)
         :                      {
    4.50 :   ffff80001017a068:       stp     x29, x30, [sp, #-32]!
         :                      arch_local_irq_disable():
         :                      u32 pmr = read_sysreg_s(SYS_ICC_PMR_EL1);
         :
         :                      WARN_ON_ONCE(pmr != GIC_PRIO_IRQON && pmr != GIC_PRIO_IRQOFF);
         :                      }
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001017a06c:       mov     x0, #0x60                       // #96
         :                      tick_nohz_idle_enter():
    0.00 :   ffff80001017a070:       mov     x29, sp
    0.00 :   ffff80001017a074:       str     x19, [sp, #16]
         :                      arch_local_irq_disable():
    0.00 :   ffff80001017a078:       msr     daifset, #0x2
         :                      tick_nohz_idle_enter():
         :
         :                      lockdep_assert_irqs_enabled();
         :
         :                      local_irq_disable();
         :
         :                      ts = this_cpu_ptr(&tick_cpu_sched);
    0.00 :   ffff80001017a07c:       adrp    x19, ffff8000114d2000 <timer_bases+0x1c80>
    0.00 :   ffff80001017a080:       add     x19, x19, #0xdd0
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff80001017a084:       mrs     x0, tpidr_el1
         :                      tick_nohz_idle_enter():
    0.00 :   ffff80001017a088:       add     x19, x19, x0
         :
         :                      WARN_ON_ONCE(ts->timer_expires_base);
    0.00 :   ffff80001017a08c:       ldr     x0, [x19, #176]
    0.00 :   ffff80001017a090:       cbnz    x0, ffff80001017a0c8 <tick_nohz_idle_enter+0x60>
         :
         :                      ts->inidle = 1;
    0.00 :   ffff80001017a094:       ldrb    w0, [x19, #76]
    0.00 :   ffff80001017a098:       orr     w0, w0, #0x1
    0.00 :   ffff80001017a09c:       strb    w0, [x19, #76]
         :                      tick_nohz_start_idle():
         :                      ts->idle_entrytime = ktime_get();
    0.00 :   ffff80001017a0a0:       bl      ffff80001016ad10 <ktime_get>
    0.00 :   ffff80001017a0a4:       str     x0, [x19, #120]
         :                      ts->idle_active = 1;
    0.00 :   ffff80001017a0a8:       ldrb    w1, [x19, #76]
         :                      arch_local_irq_enable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001017a0ac:       mov     x0, #0xe0                       // #224
         :                      tick_nohz_start_idle():
    0.00 :   ffff80001017a0b0:       orr     w1, w1, #0x4
    0.00 :   ffff80001017a0b4:       strb    w1, [x19, #76]
         :                      arch_local_irq_enable():
    0.00 :   ffff80001017a0b8:       msr     daifclr, #0x2
         :                      tick_nohz_idle_enter():
         :                      tick_nohz_start_idle(ts);
         :
         :                      local_irq_enable();
         :                      }
   88.60 :   ffff80001017a0bc:       ldr     x19, [sp, #16]
    6.91 :   ffff80001017a0c0:       ldp     x29, x30, [sp], #32
    0.00 :   ffff80001017a0c4:       ret
         :                      WARN_ON_ONCE(ts->timer_expires_base);
    0.00 :   ffff80001017a0c8:       brk     #0x800
    0.00 :   ffff80001017a0cc:       b       ffff80001017a094 <tick_nohz_idle_enter+0x2c>
 Percent |	Source code & Disassembly of vmlinux for cycles (113 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000100a2608 <flush_dcache_page>:
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    4.40 :   ffff8000100a2608:       ldr     x1, [x0]
         :                      flush_dcache_page():
         :                      * it as dirty for later flushing when mapped in user space (if executable,
         :                      * see __sync_icache_dcache).
         :                      */
         :                      void flush_dcache_page(struct page *page)
         :                      {
         :                      if (test_bit(PG_dcache_clean, &page->flags))
    0.00 :   ffff8000100a260c:       tst     w1, #0x800
    0.00 :   ffff8000100a2610:       b.ne    ffff8000100a2618 <flush_dcache_page+0x10>  // b.any
         :                      clear_bit(PG_dcache_clean, &page->flags);
         :                      }
   95.60 :   ffff8000100a2614:       ret
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000100a2618:       b       ffff8000100a262c <flush_dcache_page+0x24>
    0.00 :   ffff8000100a261c:       b       ffff8000100a262c <flush_dcache_page+0x24>
         :                      __lse_atomic64_andnot():
         :                      "       " #asm_op "     %[i], %[v]\n"                                   \
         :                      : [i] "+r" (i), [v] "+Q" (v->counter)                           \
         :                      : "r" (v));                                                     \
         :                      }
         :
         :                      ATOMIC64_OP(andnot, stclr)
    0.00 :   ffff8000100a2620:       mov     x1, #0x800                      // #2048
    0.00 :   ffff8000100a2624:       stclr   x1, [x0]
         :                      flush_dcache_page():
    0.00 :   ffff8000100a2628:       ret
         :                      __ll_sc_atomic64_andnot():
         :                      /*
         :                      * GAS converts the mysterious and undocumented BIC (immediate) alias to
         :                      * an AND (immediate) instruction with the immediate inverted. We don't
         :                      * have a constraint for this, so fall back to register.
         :                      */
         :                      ATOMIC64_OPS(andnot, bic, )
    0.00 :   ffff8000100a262c:       mov     x1, #0x800                      // #2048
    0.00 :   ffff8000100a2630:       b       ffff8000100a2790 <copy_to_user_page+0x48>
         :                      flush_dcache_page():
    0.00 :   ffff8000100a2634:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (136 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101d5b20 <mempool_alloc_slab>:
         :                      mempool_alloc_slab():
         :
         :                      /*
         :                      * A commonly used alloc and free fn.
         :                      */
         :                      void *mempool_alloc_slab(gfp_t gfp_mask, void *pool_data)
         :                      {
   14.69 :   ffff8000101d5b20:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff8000101d5b24:       mov     x2, x1
         :                      struct kmem_cache *mem = pool_data;
         :                      VM_BUG_ON(mem->ctor);
         :                      return kmem_cache_alloc(mem, gfp_mask);
    0.00 :   ffff8000101d5b28:       mov     w1, w0
    0.00 :   ffff8000101d5b2c:       mov     x0, x2
         :                      {
   71.41 :   ffff8000101d5b30:       mov     x29, sp
         :                      return kmem_cache_alloc(mem, gfp_mask);
    0.00 :   ffff8000101d5b34:       bl      ffff800010252178 <kmem_cache_alloc>
         :                      }
   13.90 :   ffff8000101d5b38:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000101d5b3c:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (81 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101258c0 <rebalance_domains>:
         :                      rebalance_domains():
         :                      * and initiates a balancing operation if so.
         :                      *
         :                      * Balancing parameters are set up in init_sched_domains.
         :                      */
         :                      static void rebalance_domains(struct rq *rq, enum cpu_idle_type idle)
         :                      {
    0.00 :   ffff8000101258c0:       stp     x29, x30, [sp, #-160]!
    0.00 :   ffff8000101258c4:       mov     x29, sp
    4.97 :   ffff8000101258c8:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000101258cc:       adrp    x20, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000101258d0:       stp     x21, x22, [sp, #32]
    0.00 :   ffff8000101258d4:       add     x2, x20, #0x8c8
    0.00 :   ffff8000101258d8:       str     x24, [sp, #56]
    0.00 :   ffff8000101258dc:       mov     x21, x0
    0.00 :   ffff8000101258e0:       stp     x26, x27, [sp, #72]
         :                      int continue_balancing = 1;
    0.00 :   ffff8000101258e4:       mov     w0, #0x1                        // #1
         :                      int cpu = rq->cpu;
         :                      unsigned long interval;
         :                      struct sched_domain *sd;
         :                      /* Earliest time when we have to do rebalance again */
         :                      unsigned long next_balance = jiffies + 60*HZ;
    0.00 :   ffff8000101258e8:       adrp    x26, ffff800011897000 <bit_wait_table+0xe80>
         :                      int continue_balancing = 1;
    0.00 :   ffff8000101258ec:       str     w0, [x29, #148]
         :                      {
    0.00 :   ffff8000101258f0:       ldr     x3, [x2]
    0.00 :   ffff8000101258f4:       str     x3, [x29, #152]
    0.00 :   ffff8000101258f8:       mov     x3, #0x0                        // #0
         :                      int cpu = rq->cpu;
    0.00 :   ffff8000101258fc:       ldr     w22, [x21, #2568]
         :                      {
    0.00 :   ffff800010125900:       mov     w24, w1
         :                      int cpu = rq->cpu;
    0.00 :   ffff800010125904:       str     w22, [x29, #132]
         :                      unsigned long next_balance = jiffies + 60*HZ;
    0.00 :   ffff800010125908:       ldr     x19, [x26, #2432]
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff80001012590c:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      rebalance_domains():
         :                      int update_next_balance = 0;
         :                      int need_serialize, need_decay = 0;
         :                      u64 max_cost = 0;
         :
         :                      rcu_read_lock();
         :                      for_each_domain(cpu, sd) {
    0.00 :   ffff800010125910:       adrp    x1, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010125914:       add     x1, x1, #0x8e8
    0.00 :   ffff800010125918:       adrp    x0, ffff8000114d5000 <tegra_to+0x180>
    0.00 :   ffff80001012591c:       add     x0, x0, #0xd80
    0.00 :   ffff800010125920:       ldr     x1, [x1, w22, sxtw #3]
    0.00 :   ffff800010125924:       add     x0, x0, x1
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010125928:       ldr     x27, [x0, #2472]
         :                      rebalance_domains():
    0.00 :   ffff80001012592c:       cbz     x27, ffff800010125ba0 <rebalance_domains+0x2e0>
    6.08 :   ffff800010125930:       mov     x0, #0x3a98                     // #15000
    3.67 :   ffff800010125934:       str     x25, [x29, #64]
    0.00 :   ffff800010125938:       add     x19, x19, x0
         :                      spin_trylock():
         :                      raw_spin_lock_bh(&lock->rlock);
         :                      }
         :
         :                      static __always_inline int spin_trylock(spinlock_t *lock)
         :                      {
         :                      return raw_spin_trylock(&lock->rlock);
    0.00 :   ffff80001012593c:       adrp    x0, ffff800011a7f000 <ucounts_hashtable+0x12f0>
    0.00 :   ffff800010125940:       add     x25, x0, #0xfc0
         :                      get_sd_balance_interval():
         :                      interval = clamp(interval, 1UL, max_load_balance_interval);
    0.00 :   ffff800010125944:       adrp    x22, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      spin_trylock():
    0.00 :   ffff800010125948:       add     x0, x25, #0x38
    0.00 :   ffff80001012594c:       str     x23, [x29, #48]
    0.00 :   ffff800010125950:       str     x28, [x29, #88]
         :                      rebalance_domains():
         :                      int need_serialize, need_decay = 0;
    0.00 :   ffff800010125954:       mov     w25, #0x0                       // #0
         :                      spin_trylock():
    0.00 :   ffff800010125958:       str     x0, [x29, #112]
         :                      rebalance_domains():
         :                      u64 max_cost = 0;
    0.00 :   ffff80001012595c:       mov     x28, #0x0                       // #0
         :                      get_sd_balance_interval():
         :                      interval = clamp(interval, 1UL, max_load_balance_interval);
    0.00 :   ffff800010125960:       add     x0, x22, #0x200
         :                      rebalance_domains():
         :                      int update_next_balance = 0;
    0.00 :   ffff800010125964:       mov     w23, #0x0                       // #0
         :                      get_sd_balance_interval():
         :                      interval = clamp(interval, 1UL, max_load_balance_interval);
    0.00 :   ffff800010125968:       str     x0, [x29, #136]
         :                      rebalance_domains():
         :                      /*
         :                      * Decay the newidle max times here because this is a regular
         :                      * visit to all the domains. Decay ~1% per second.
         :                      */
         :                      if (time_after(jiffies, sd->next_decay_max_lb_cost)) {
    1.48 :   ffff80001012596c:       ldr     x1, [x26, #2432]
    1.43 :   ffff800010125970:       ldp     x6, x0, [x27, #80]
    4.18 :   ffff800010125974:       sub     x0, x0, x1
    0.00 :   ffff800010125978:       tbz     x0, #63, ffff80001012599c <rebalance_domains+0xdc>
         :                      sd->max_newidle_lb_cost =
         :                      (sd->max_newidle_lb_cost * 253) / 256;
    1.28 :   ffff80001012597c:       lsl     x0, x6, #6
         :                      sd->next_decay_max_lb_cost = jiffies + HZ;
    1.19 :   ffff800010125980:       ldr     x1, [x26, #2432]
         :                      (sd->max_newidle_lb_cost * 253) / 256;
    0.00 :   ffff800010125984:       sub     x0, x0, x6
         :                      need_decay = 1;
    0.00 :   ffff800010125988:       mov     w25, #0x1                       // #1
         :                      sd->next_decay_max_lb_cost = jiffies + HZ;
    0.00 :   ffff80001012598c:       add     x1, x1, #0xfa
         :                      (sd->max_newidle_lb_cost * 253) / 256;
    0.00 :   ffff800010125990:       add     x6, x6, x0, lsl #2
    0.00 :   ffff800010125994:       lsr     x6, x6, #8
         :                      sd->next_decay_max_lb_cost = jiffies + HZ;
    0.00 :   ffff800010125998:       stp     x6, x1, [x27, #80]
         :                      }
         :                      max_cost += sd->max_newidle_lb_cost;
         :
         :                      if (!(sd->flags & SD_LOAD_BALANCE))
   33.74 :   ffff80001012599c:       ldr     w0, [x27, #56]
    0.00 :   ffff8000101259a0:       and     w22, w0, #0x1
    0.00 :   ffff8000101259a4:       tbz     w0, #0, ffff800010125b0c <rebalance_domains+0x24c>
         :                      /*
         :                      * Stop the load balance at this level. There is another
         :                      * CPU in our sched group which is doing load balancing more
         :                      * actively.
         :                      */
         :                      if (!continue_balancing) {
   13.93 :   ffff8000101259a8:       ldr     w0, [x29, #148]
    0.00 :   ffff8000101259ac:       cbnz    w0, ffff800010125a10 <rebalance_domains+0x150>
         :                      if (need_decay)
    0.00 :   ffff8000101259b0:       cbnz    w25, ffff800010125b0c <rebalance_domains+0x24c>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.00 :   ffff8000101259b4:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      rebalance_domains():
         :                      /*
         :                      * next_balance will be updated only when there is a need.
         :                      * When the cpu is attached to null domain for ex, it will not be
         :                      * updated.
         :                      */
         :                      if (likely(update_next_balance)) {
    0.00 :   ffff8000101259b8:       cbz     w23, ffff8000101259d8 <rebalance_domains+0x118>
         :                      rq->next_balance = next_balance;
    0.00 :   ffff8000101259bc:       str     x19, [x21, #2376]
         :                      * nohz_idle_balance() and nohz.next_balance has been
         :                      * updated accordingly. This CPU is now running the idle load
         :                      * balance for itself and we need to update the
         :                      * nohz.next_balance accordingly.
         :                      */
         :                      if ((idle == CPU_IDLE) && time_after(nohz.next_balance, rq->next_balance))
    0.00 :   ffff8000101259c0:       cbnz    w24, ffff8000101259d8 <rebalance_domains+0x118>
    1.18 :   ffff8000101259c4:       adrp    x0, ffff800011a7f000 <ucounts_hashtable+0x12f0>
    0.00 :   ffff8000101259c8:       add     x0, x0, #0xfc0
    0.00 :   ffff8000101259cc:       ldr     x1, [x0, #40]
    0.00 :   ffff8000101259d0:       sub     x1, x19, x1
    0.74 :   ffff8000101259d4:       tbnz    x1, #63, ffff800010125b34 <rebalance_domains+0x274>
    5.03 :   ffff8000101259d8:       ldr     x23, [x29, #48]
    1.35 :   ffff8000101259dc:       ldr     x25, [x29, #64]
    0.00 :   ffff8000101259e0:       ldr     x28, [x29, #88]
         :                      nohz.next_balance = rq->next_balance;
         :                      #endif
         :                      }
         :                      }
    0.00 :   ffff8000101259e4:       add     x20, x20, #0x8c8
    0.00 :   ffff8000101259e8:       ldr     x1, [x29, #152]
    0.00 :   ffff8000101259ec:       ldr     x0, [x20]
    0.00 :   ffff8000101259f0:       eor     x0, x1, x0
    0.00 :   ffff8000101259f4:       cbnz    x0, ffff800010125ba8 <rebalance_domains+0x2e8>
    0.89 :   ffff8000101259f8:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000101259fc:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010125a00:       ldr     x24, [sp, #56]
    0.00 :   ffff800010125a04:       ldp     x26, x27, [sp, #72]
    1.19 :   ffff800010125a08:       ldp     x29, x30, [sp], #160
    0.00 :   ffff800010125a0c:       ret
         :                      interval = get_sd_balance_interval(sd, idle != CPU_IDLE);
    1.27 :   ffff800010125a10:       ldr     w2, [x27, #40]
         :                      get_sd_balance_interval():
         :                      interval *= sd->busy_factor;
    0.00 :   ffff800010125a14:       cmp     w24, #0x0
         :                      rebalance_domains():
         :                      interval = get_sd_balance_interval(sd, idle != CPU_IDLE);
    1.42 :   ffff800010125a18:       ldr     w1, [x27, #72]
    1.27 :   ffff800010125a1c:       str     x6, [x29, #120]
         :                      get_sd_balance_interval():
         :                      interval *= sd->busy_factor;
    0.00 :   ffff800010125a20:       mov     w0, w1
    0.00 :   ffff800010125a24:       umull   x1, w1, w2
         :                      msecs_to_jiffies():
         :                      if (__builtin_constant_p(m)) {
         :                      if ((int)m < 0)
         :                      return MAX_JIFFY_OFFSET;
         :                      return _msecs_to_jiffies(m);
         :                      } else {
         :                      return __msecs_to_jiffies(m);
    0.00 :   ffff800010125a28:       csel    x0, x0, x1, eq  // eq = none
    0.00 :   ffff800010125a2c:       bl      ffff8000101653f0 <__msecs_to_jiffies>
         :                      get_sd_balance_interval():
         :                      interval = clamp(interval, 1UL, max_load_balance_interval);
    0.00 :   ffff800010125a30:       cmp     x0, #0x0
    0.00 :   ffff800010125a34:       ldr     x1, [x29, #136]
         :                      rebalance_domains():
         :                      if (need_serialize) {
    1.28 :   ffff800010125a38:       ldr     x6, [x29, #120]
         :                      get_sd_balance_interval():
         :                      interval = clamp(interval, 1UL, max_load_balance_interval);
    0.00 :   ffff800010125a3c:       ldr     x2, [x1, #8]
    0.00 :   ffff800010125a40:       csinc   x1, x0, xzr, ne  // ne = any
         :                      rebalance_domains():
         :                      need_serialize = sd->flags & SD_SERIALIZE;
    0.00 :   ffff800010125a44:       ldr     w0, [x27, #56]
         :                      get_sd_balance_interval():
         :                      interval = clamp(interval, 1UL, max_load_balance_interval);
    0.00 :   ffff800010125a48:       cmp     x1, x2
    0.00 :   ffff800010125a4c:       csel    x1, x1, x2, ls  // ls = plast
         :                      rebalance_domains():
         :                      if (need_serialize) {
    0.00 :   ffff800010125a50:       and     w2, w0, #0x400
    0.00 :   ffff800010125a54:       str     w2, [x29, #104]
    0.00 :   ffff800010125a58:       tbnz    w0, #10, ffff800010125b48 <rebalance_domains+0x288>
         :                      if (time_after_eq(jiffies, sd->last_balance + interval)) {
    0.00 :   ffff800010125a5c:       ldr     x2, [x26, #2432]
    0.00 :   ffff800010125a60:       ldr     x0, [x27, #64]
    0.00 :   ffff800010125a64:       sub     x2, x2, x0
    0.00 :   ffff800010125a68:       sub     x2, x2, x1
    0.00 :   ffff800010125a6c:       tbnz    x2, #63, ffff800010125af8 <rebalance_domains+0x238>
         :                      if (load_balance(cpu, rq, sd, idle, &continue_balancing)) {
    4.16 :   ffff800010125a70:       ldr     w0, [x29, #132]
    0.00 :   ffff800010125a74:       add     x4, x29, #0x94
    0.80 :   ffff800010125a78:       str     x6, [x29, #120]
    0.00 :   ffff800010125a7c:       mov     w3, w24
    0.00 :   ffff800010125a80:       mov     x2, x27
    0.00 :   ffff800010125a84:       mov     x1, x21
    0.00 :   ffff800010125a88:       bl      ffff800010124ce8 <load_balance>
    0.00 :   ffff800010125a8c:       ldr     x6, [x29, #120]
    0.00 :   ffff800010125a90:       cbz     w0, ffff800010125aa8 <rebalance_domains+0x1e8>
         :                      idle = idle_cpu(cpu) ? CPU_IDLE : CPU_NOT_IDLE;
    0.00 :   ffff800010125a94:       ldr     w0, [x29, #132]
    0.00 :   ffff800010125a98:       bl      ffff800010117488 <idle_cpu>
    0.00 :   ffff800010125a9c:       cmp     w0, #0x0
    0.00 :   ffff800010125aa0:       ldr     x6, [x29, #120]
    0.00 :   ffff800010125aa4:       cset    w24, eq  // eq = none
         :                      interval = get_sd_balance_interval(sd, idle != CPU_IDLE);
    0.00 :   ffff800010125aa8:       ldr     w2, [x27, #40]
         :                      get_sd_balance_interval():
         :                      interval *= sd->busy_factor;
    0.00 :   ffff800010125aac:       cmp     w24, #0x0
         :                      rebalance_domains():
         :                      interval = get_sd_balance_interval(sd, idle != CPU_IDLE);
    0.00 :   ffff800010125ab0:       ldr     w1, [x27, #72]
         :                      sd->last_balance = jiffies;
    0.00 :   ffff800010125ab4:       ldr     x0, [x26, #2432]
    0.00 :   ffff800010125ab8:       str     x0, [x27, #64]
         :                      get_sd_balance_interval():
         :                      interval *= sd->busy_factor;
    0.00 :   ffff800010125abc:       mov     w0, w1
    0.00 :   ffff800010125ac0:       str     x6, [x29, #120]
    0.00 :   ffff800010125ac4:       umull   x1, w1, w2
         :                      msecs_to_jiffies():
    0.00 :   ffff800010125ac8:       csel    x0, x0, x1, eq  // eq = none
    0.00 :   ffff800010125acc:       bl      ffff8000101653f0 <__msecs_to_jiffies>
         :                      get_sd_balance_interval():
         :                      interval = clamp(interval, 1UL, max_load_balance_interval);
    0.00 :   ffff800010125ad0:       cmp     x0, #0x0
    0.00 :   ffff800010125ad4:       ldr     x1, [x29, #136]
         :                      rebalance_domains():
         :                      if (need_serialize)
    0.00 :   ffff800010125ad8:       ldr     x6, [x29, #120]
         :                      get_sd_balance_interval():
         :                      interval = clamp(interval, 1UL, max_load_balance_interval);
    0.00 :   ffff800010125adc:       ldr     x2, [x1, #8]
    0.00 :   ffff800010125ae0:       csinc   x1, x0, xzr, ne  // ne = any
         :                      rebalance_domains():
         :                      if (need_serialize)
    0.00 :   ffff800010125ae4:       ldr     w0, [x29, #104]
         :                      get_sd_balance_interval():
         :                      interval = clamp(interval, 1UL, max_load_balance_interval);
    0.00 :   ffff800010125ae8:       cmp     x1, x2
    0.00 :   ffff800010125aec:       csel    x1, x1, x2, ls  // ls = plast
         :                      rebalance_domains():
         :                      if (need_serialize)
    0.00 :   ffff800010125af0:       cbnz    w0, ffff800010125b78 <rebalance_domains+0x2b8>
    0.00 :   ffff800010125af4:       ldr     x0, [x27, #64]
         :                      if (time_after(next_balance, sd->last_balance + interval)) {
    1.53 :   ffff800010125af8:       add     x1, x1, x0
         :                      next_balance = sd->last_balance + interval;
    0.00 :   ffff800010125afc:       subs    x0, x1, x19
    0.00 :   ffff800010125b00:       csel    w23, w22, w23, mi  // mi = first
    0.00 :   ffff800010125b04:       cmp     x0, #0x0
    2.18 :   ffff800010125b08:       csel    x19, x1, x19, lt  // lt = tstop
         :                      for_each_domain(cpu, sd) {
    0.00 :   ffff800010125b0c:       ldr     x27, [x27]
         :                      max_cost += sd->max_newidle_lb_cost;
    0.00 :   ffff800010125b10:       add     x28, x28, x6
         :                      for_each_domain(cpu, sd) {
    0.00 :   ffff800010125b14:       cbnz    x27, ffff80001012596c <rebalance_domains+0xac>
         :                      if (need_decay) {
    0.00 :   ffff800010125b18:       cbz     w25, ffff8000101259b4 <rebalance_domains+0xf4>
         :                      max((u64)sysctl_sched_migration_cost, max_cost);
    0.00 :   ffff800010125b1c:       mov     x0, #0xa120                     // #41248
    0.00 :   ffff800010125b20:       movk    x0, #0x7, lsl #16
    0.00 :   ffff800010125b24:       cmp     x28, x0
    0.00 :   ffff800010125b28:       csel    x7, x28, x0, cs  // cs = hs, nlast
         :                      rq->max_idle_balance_cost =
    0.00 :   ffff800010125b2c:       str     x7, [x21, #2832]
    0.00 :   ffff800010125b30:       b       ffff8000101259b4 <rebalance_domains+0xf4>
         :                      nohz.next_balance = rq->next_balance;
    0.00 :   ffff800010125b34:       str     x19, [x0, #40]
         :                      }
    0.00 :   ffff800010125b38:       ldr     x23, [x29, #48]
    0.00 :   ffff800010125b3c:       ldr     x25, [x29, #64]
    0.00 :   ffff800010125b40:       ldr     x28, [x29, #88]
    0.00 :   ffff800010125b44:       b       ffff8000101259e4 <rebalance_domains+0x124>
         :                      spin_trylock():
    1.26 :   ffff800010125b48:       ldr     x0, [x29, #112]
    0.00 :   ffff800010125b4c:       str     x6, [x29, #96]
    0.00 :   ffff800010125b50:       str     x1, [x29, #120]
    0.00 :   ffff800010125b54:       bl      ffff800010cb2670 <_raw_spin_trylock>
         :                      rebalance_domains():
         :                      if (!spin_trylock(&balancing))
    0.00 :   ffff800010125b58:       ldr     x6, [x29, #96]
    0.00 :   ffff800010125b5c:       ldr     x1, [x29, #120]
    0.00 :   ffff800010125b60:       cbz     w0, ffff800010125af4 <rebalance_domains+0x234>
         :                      if (time_after_eq(jiffies, sd->last_balance + interval)) {
    0.00 :   ffff800010125b64:       ldr     x0, [x26, #2432]
    1.37 :   ffff800010125b68:       ldr     x2, [x27, #64]
    0.00 :   ffff800010125b6c:       sub     x0, x0, x2
    0.00 :   ffff800010125b70:       sub     x0, x0, x1
    0.00 :   ffff800010125b74:       tbz     x0, #63, ffff800010125a70 <rebalance_domains+0x1b0>
         :                      spin_unlock():
         :                      raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
         :                      } while (0)
         :
         :                      static __always_inline void spin_unlock(spinlock_t *lock)
         :                      {
         :                      raw_spin_unlock(&lock->rlock);
    1.13 :   ffff800010125b78:       adrp    x0, ffff800011a7f000 <ucounts_hashtable+0x12f0>
    0.00 :   ffff800010125b7c:       add     x0, x0, #0xfc0
    0.00 :   ffff800010125b80:       add     x0, x0, #0x38
    0.00 :   ffff800010125b84:       str     x6, [x29, #104]
    0.00 :   ffff800010125b88:       str     x1, [x29, #120]
    0.00 :   ffff800010125b8c:       bl      ffff800010cb2408 <_raw_spin_unlock>
    0.00 :   ffff800010125b90:       ldr     x0, [x27, #64]
    0.00 :   ffff800010125b94:       ldr     x6, [x29, #104]
    0.00 :   ffff800010125b98:       ldr     x1, [x29, #120]
    0.00 :   ffff800010125b9c:       b       ffff800010125af8 <rebalance_domains+0x238>
         :                      rcu_read_unlock():
    0.00 :   ffff800010125ba0:       bl      ffff80001015eee0 <__rcu_read_unlock>
    0.00 :   ffff800010125ba4:       b       ffff8000101259e4 <rebalance_domains+0x124>
    0.00 :   ffff800010125ba8:       str     x23, [x29, #48]
    0.00 :   ffff800010125bac:       str     x25, [x29, #64]
    0.00 :   ffff800010125bb0:       str     x28, [x29, #88]
         :                      rebalance_domains():
         :                      }
    0.00 :   ffff800010125bb4:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (123 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010453550 <blk_start_plug>:
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    7.30 :   ffff800010453550:       mrs     x1, sp_el0
         :                      blk_start_plug():
         :                      struct task_struct *tsk = current;
         :
         :                      /*
         :                      * If this is a nested plug, don't actually assign it.
         :                      */
         :                      if (tsk->plug)
    6.54 :   ffff800010453554:       ldr     x2, [x1, #1840]
    0.00 :   ffff800010453558:       cbz     x2, ffff800010453560 <blk_start_plug+0x10>
         :                      /*
         :                      * Store ordering should not be needed here, since a potential
         :                      * preempt will imply a full memory barrier
         :                      */
         :                      tsk->plug = plug;
         :                      }
    0.00 :   ffff80001045355c:       ret
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
   38.98 :   ffff800010453560:       str     x0, [x0]
         :                      blk_start_plug():
         :                      INIT_LIST_HEAD(&plug->cb_list);
    0.00 :   ffff800010453564:       add     x2, x0, #0x10
         :                      INIT_LIST_HEAD():
         :                      struct list_head name = LIST_HEAD_INIT(name)
         :
         :                      static inline void INIT_LIST_HEAD(struct list_head *list)
         :                      {
         :                      WRITE_ONCE(list->next, list);
         :                      list->prev = list;
   11.35 :   ffff800010453568:       str     x0, [x0, #8]
         :                      __write_once_size():
    1.63 :   ffff80001045356c:       str     x2, [x0, #16]
         :                      INIT_LIST_HEAD():
    5.71 :   ffff800010453570:       str     x2, [x0, #24]
         :                      blk_start_plug():
         :                      plug->rq_count = 0;
    4.08 :   ffff800010453574:       strh    wzr, [x0, #32]
         :                      plug->multiple_queues = false;
   18.70 :   ffff800010453578:       strb    wzr, [x0, #34]
         :                      tsk->plug = plug;
    5.71 :   ffff80001045357c:       str     x0, [x1, #1840]
         :                      }
    0.00 :   ffff800010453580:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (119 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104ab160 <sbitmap_get>:
         :                      sbitmap_get():
         :
         :                      return nr;
         :                      }
         :
         :                      int sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint, bool round_robin)
         :                      {
    2.53 :   ffff8000104ab160:       stp     x29, x30, [sp, #-80]!
         :                      * Unless we're doing round robin tag allocation, just use the
         :                      * alloc_hint to find the right word index. No point in looping
         :                      * twice in find_next_zero_bit() for that case.
         :                      */
         :                      if (round_robin)
         :                      alloc_hint = SB_NR_TO_BIT(sb, alloc_hint);
    0.00 :   ffff8000104ab164:       mov     w3, #0xffffffff                 // #-1
    0.00 :   ffff8000104ab168:       ands    w2, w2, #0xff
         :                      {
    0.00 :   ffff8000104ab16c:       mov     x29, sp
    0.83 :   ffff8000104ab170:       str     x19, [sp, #16]
   14.34 :   ffff8000104ab174:       str     x21, [sp, #32]
         :                      else
         :                      alloc_hint = 0;
         :
         :                      for (i = 0; i < sb->map_nr; i++) {
    7.51 :   ffff8000104ab178:       ldp     w4, w5, [x0, #4]
         :                      alloc_hint = SB_NR_TO_BIT(sb, alloc_hint);
    0.00 :   ffff8000104ab17c:       lsl     w21, w3, w4
    1.69 :   ffff8000104ab180:       bic     w21, w1, w21
         :                      index = SB_NR_TO_INDEX(sb, alloc_hint);
    0.00 :   ffff8000104ab184:       lsr     w19, w1, w4
         :                      alloc_hint = SB_NR_TO_BIT(sb, alloc_hint);
    2.51 :   ffff8000104ab188:       csel    w21, w21, wzr, ne  // ne = any
         :                      for (i = 0; i < sb->map_nr; i++) {
    0.00 :   ffff8000104ab18c:       cbz     w5, ffff8000104ab240 <sbitmap_get+0xe0>
    0.00 :   ffff8000104ab190:       stp     x22, x23, [x29, #40]
    0.00 :   ffff8000104ab194:       eor     w22, w2, #0x1
   17.63 :   ffff8000104ab198:       str     x26, [x29, #72]
    0.00 :   ffff8000104ab19c:       mov     x26, x0
    0.83 :   ffff8000104ab1a0:       str     x20, [x29, #24]
         :                      __xchg_case_mb_64():
         :                      __XCHG_CASE(w,  , rel_, 32,        ,    ,  ,  , l, "memory")
         :                      __XCHG_CASE( ,  , rel_, 64,        ,    ,  ,  , l, "memory")
         :                      __XCHG_CASE(w, b,  mb_,  8, dmb ish, nop,  , a, l, "memory")
         :                      __XCHG_CASE(w, h,  mb_, 16, dmb ish, nop,  , a, l, "memory")
         :                      __XCHG_CASE(w,  ,  mb_, 32, dmb ish, nop,  , a, l, "memory")
         :                      __XCHG_CASE( ,  ,  mb_, 64, dmb ish, nop,  , a, l, "memory")
    0.00 :   ffff8000104ab1a4:       mov     x23, #0x0                       // #0
    1.66 :   ffff8000104ab1a8:       stp     x24, x25, [x29, #56]
         :                      sbitmap_get():
    0.00 :   ffff8000104ab1ac:       mov     w20, #0x0                       // #0
         :                      sbitmap_find_bit_in_index():
         :                      nr = __sbitmap_get_word(&sb->map[index].word,
    0.00 :   ffff8000104ab1b0:       mov     w24, #0xc0                      // #192
    0.00 :   ffff8000104ab1b4:       nop
    3.38 :   ffff8000104ab1b8:       smull   x25, w19, w24
    7.57 :   ffff8000104ab1bc:       ldr     x1, [x26, #16]
    1.68 :   ffff8000104ab1c0:       mov     w3, w22
    0.00 :   ffff8000104ab1c4:       mov     w2, w21
    0.00 :   ffff8000104ab1c8:       add     x0, x1, x25
    3.36 :   ffff8000104ab1cc:       ldr     x1, [x1, x25]
    0.00 :   ffff8000104ab1d0:       add     x0, x0, #0x40
    0.00 :   ffff8000104ab1d4:       bl      ffff8000104ab038 <__sbitmap_get_word>
         :                      if (nr != -1)
    1.67 :   ffff8000104ab1d8:       cmn     w0, #0x1
    0.00 :   ffff8000104ab1dc:       b.ne    ffff8000104ab2c8 <sbitmap_get+0x168>  // b.any
         :                      sbitmap_deferred_clear():
         :                      spin_lock_irqsave(&sb->map[index].swap_lock, flags);
    0.00 :   ffff8000104ab1e0:       ldr     x0, [x26, #16]
    0.00 :   ffff8000104ab1e4:       add     x0, x0, x25
    0.00 :   ffff8000104ab1e8:       add     x0, x0, #0x88
    0.00 :   ffff8000104ab1ec:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
    0.00 :   ffff8000104ab1f0:       mov     x6, x0
         :                      if (!sb->map[index].cleared)
    0.00 :   ffff8000104ab1f4:       ldr     x0, [x26, #16]
    0.00 :   ffff8000104ab1f8:       add     x0, x0, x25
    0.00 :   ffff8000104ab1fc:       ldr     x1, [x0, #128]
    0.00 :   ffff8000104ab200:       cbnz    x1, ffff8000104ab254 <sbitmap_get+0xf4>
         :                      spin_unlock_irqrestore():
         :                      raw_spin_unlock_irq(&lock->rlock);
         :                      }
         :
         :                      static __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
         :                      {
         :                      raw_spin_unlock_irqrestore(&lock->rlock, flags);
    0.00 :   ffff8000104ab204:       add     x0, x0, #0x88
    0.00 :   ffff8000104ab208:       mov     x1, x6
    0.00 :   ffff8000104ab20c:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      sbitmap_get():
         :                      break;
         :                      }
         :
         :                      /* Jump to next index. */
         :                      alloc_hint = 0;
         :                      if (++index >= sb->map_nr)
    0.00 :   ffff8000104ab210:       add     w19, w19, #0x1
    0.00 :   ffff8000104ab214:       ldr     w0, [x26, #8]
         :                      for (i = 0; i < sb->map_nr; i++) {
    0.00 :   ffff8000104ab218:       add     w20, w20, #0x1
         :                      alloc_hint = 0;
    0.00 :   ffff8000104ab21c:       mov     w21, #0x0                       // #0
         :                      index = 0;
    0.00 :   ffff8000104ab220:       cmp     w0, w19
    0.00 :   ffff8000104ab224:       csel    w19, w19, wzr, hi  // hi = pmore
         :                      for (i = 0; i < sb->map_nr; i++) {
    0.00 :   ffff8000104ab228:       cmp     w0, w20
    0.00 :   ffff8000104ab22c:       b.hi    ffff8000104ab1b8 <sbitmap_get+0x58>  // b.pmore
    0.00 :   ffff8000104ab230:       ldr     x20, [x29, #24]
    0.00 :   ffff8000104ab234:       ldp     x22, x23, [x29, #40]
    0.00 :   ffff8000104ab238:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff8000104ab23c:       ldr     x26, [x29, #72]
         :                      index = 0;
    0.00 :   ffff8000104ab240:       mov     w0, #0xffffffff                 // #-1
         :                      }
         :
         :                      return nr;
         :                      }
    0.00 :   ffff8000104ab244:       ldr     x19, [sp, #16]
    0.00 :   ffff8000104ab248:       ldr     x21, [sp, #32]
    0.00 :   ffff8000104ab24c:       ldp     x29, x30, [sp], #80
    0.00 :   ffff8000104ab250:       ret
         :                      __xchg_case_mb_64():
    0.00 :   ffff8000104ab254:       add     x0, x0, #0x80
    0.00 :   ffff8000104ab258:       prfm    pstl1strm, [x0]
    0.00 :   ffff8000104ab25c:       ldxr    x5, [x0]
    0.00 :   ffff8000104ab260:       stlxr   w1, x23, [x0]
    0.00 :   ffff8000104ab264:       cbnz    w1, ffff8000104ab25c <sbitmap_get+0xfc>
    0.00 :   ffff8000104ab268:       dmb     ish
    0.00 :   ffff8000104ab26c:       mvn     x5, x5
         :                      sbitmap_deferred_clear():
         :                      val = sb->map[index].word;
    0.00 :   ffff8000104ab270:       ldr     x3, [x26, #16]
    0.00 :   ffff8000104ab274:       add     x3, x3, x25
    0.00 :   ffff8000104ab278:       mov     x0, x3
    0.00 :   ffff8000104ab27c:       ldr     x4, [x0, #64]!
         :                      } while (cmpxchg(&sb->map[index].word, val, val & ~mask) != val);
    0.00 :   ffff8000104ab280:       and     x2, x4, x5
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000104ab284:       b       ffff8000104ab2bc <sbitmap_get+0x15c>
    0.00 :   ffff8000104ab288:       b       ffff8000104ab2bc <sbitmap_get+0x15c>
         :                      __lse__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,  l, "memory")
         :                      __CMPXCHG_CASE(x,  , rel_, 64,  l, "memory")
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, al, "memory")
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, al, "memory")
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, al, "memory")
         :                      __CMPXCHG_CASE(x,  ,  mb_, 64, al, "memory")
    0.00 :   ffff8000104ab28c:       mov     x1, x4
    0.00 :   ffff8000104ab290:       mov     x7, x1
    0.00 :   ffff8000104ab294:       casal   x7, x2, [x0]
    0.00 :   ffff8000104ab298:       mov     x0, x7
         :                      sbitmap_deferred_clear():
    0.00 :   ffff8000104ab29c:       cmp     x4, x0
    0.00 :   ffff8000104ab2a0:       b.ne    ffff8000104ab270 <sbitmap_get+0x110>  // b.any
         :                      spin_unlock_irqrestore(&sb->map[index].swap_lock, flags);
    0.00 :   ffff8000104ab2a4:       ldr     x0, [x26, #16]
         :                      spin_unlock_irqrestore():
    0.00 :   ffff8000104ab2a8:       mov     x1, x6
         :                      sbitmap_deferred_clear():
    0.00 :   ffff8000104ab2ac:       add     x0, x0, x25
         :                      spin_unlock_irqrestore():
    0.00 :   ffff8000104ab2b0:       add     x0, x0, #0x88
    0.00 :   ffff8000104ab2b4:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
    0.00 :   ffff8000104ab2b8:       b       ffff8000104ab1bc <sbitmap_get+0x5c>
         :                      __ll_sc__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  , rel_, 64,        ,  , l, "memory", L)
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  ,  mb_, 64, dmb ish,  , l, "memory", L)
    0.00 :   ffff8000104ab2bc:       add     x3, x3, #0x40
    0.00 :   ffff8000104ab2c0:       b       ffff8000104abd38 <__sbitmap_queue_get+0x188>
    0.00 :   ffff8000104ab2c4:       b       ffff8000104ab29c <sbitmap_get+0x13c>
         :                      sbitmap_get():
         :                      nr += index << sb->shift;
   10.96 :   ffff8000104ab2c8:       ldr     w1, [x26, #4]
    0.84 :   ffff8000104ab2cc:       ldr     x20, [x29, #24]
    0.00 :   ffff8000104ab2d0:       ldp     x22, x23, [x29, #40]
    0.00 :   ffff8000104ab2d4:       lsl     w19, w19, w1
    9.24 :   ffff8000104ab2d8:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff8000104ab2dc:       add     w0, w19, w0
    8.43 :   ffff8000104ab2e0:       ldr     x26, [x29, #72]
         :                      }
    0.85 :   ffff8000104ab2e4:       ldr     x19, [sp, #16]
    0.00 :   ffff8000104ab2e8:       ldr     x21, [sp, #32]
    2.49 :   ffff8000104ab2ec:       ldp     x29, x30, [sp], #80
    0.00 :   ffff8000104ab2f0:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (67 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001015f298 <rcu_idle_exit>:
         :                      rcu_idle_exit():
         :                      *
         :                      * If you add or remove a call to rcu_idle_exit(), be sure to test with
         :                      * CONFIG_RCU_EQS_DEBUG=y.
         :                      */
         :                      void rcu_idle_exit(void)
         :                      {
    4.53 :   ffff80001015f298:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff80001015f29c:       mov     x29, sp
         :                      arch_local_save_flags():
         :                      */
         :                      static inline unsigned long arch_local_save_flags(void)
         :                      {
         :                      unsigned long flags;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015f2a0:       mrs     x6, daif
         :                      arch_irqs_disabled_flags():
         :
         :                      static inline int arch_irqs_disabled_flags(unsigned long flags)
         :                      {
         :                      int res;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015f2a4:       and     w0, w6, #0x80
         :                      arch_local_irq_save():
         :
         :                      /*
         :                      * There are too many states with IRQs disabled, just keep the current
         :                      * state if interrupts are already disabled/masked.
         :                      */
         :                      if (!arch_irqs_disabled_flags(flags))
    0.00 :   ffff80001015f2a8:       cbnz    w0, ffff80001015f2b4 <rcu_idle_exit+0x1c>
         :                      arch_local_irq_disable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015f2ac:       mov     x0, #0x60                       // #96
    4.48 :   ffff80001015f2b0:       msr     daifset, #0x2
         :                      rcu_eqs_exit():
         :                      rdp = this_cpu_ptr(&rcu_data);
    0.00 :   ffff80001015f2b4:       adrp    x0, ffff8000114d6000 <runqueues+0x280>
    0.00 :   ffff80001015f2b8:       add     x0, x0, #0x9c0
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff80001015f2bc:       mrs     x5, tpidr_el1
         :                      rcu_eqs_exit():
    0.00 :   ffff80001015f2c0:       add     x5, x0, x5
         :                      oldval = rdp->dynticks_nesting;
    0.00 :   ffff80001015f2c4:       ldr     x0, [x5, #208]
         :                      if (oldval) {
    0.00 :   ffff80001015f2c8:       cbz     x0, ffff80001015f2e0 <rcu_idle_exit+0x48>
         :                      rdp->dynticks_nesting++;
    0.00 :   ffff80001015f2cc:       add     x0, x0, #0x1
    0.00 :   ffff80001015f2d0:       str     x0, [x5, #208]
         :                      arch_local_irq_restore():
         :                      /*
         :                      * restore saved IRQ state
         :                      */
         :                      static inline void arch_local_irq_restore(unsigned long flags)
         :                      {
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015f2d4:       msr     daif, x6
         :                      rcu_idle_exit():
         :                      unsigned long flags;
         :
         :                      local_irq_save(flags);
         :                      rcu_eqs_exit(false);
         :                      local_irq_restore(flags);
         :                      }
   90.99 :   ffff80001015f2d8:       ldp     x29, x30, [sp], #16
    0.00 :   ffff80001015f2dc:       ret
         :                      rcu_eqs_exit():
         :                      rcu_dynticks_eqs_exit();
    0.00 :   ffff80001015f2e0:       bl      ffff80001015c210 <rcu_dynticks_eqs_exit>
         :                      WARN_ON_ONCE(rdp->dynticks_nmi_nesting);
    0.00 :   ffff80001015f2e4:       ldr     x0, [x5, #216]
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff80001015f2e8:       mov     x1, #0x1                        // #1
    0.00 :   ffff80001015f2ec:       str     x1, [x5, #208]
         :                      rcu_eqs_exit():
    0.00 :   ffff80001015f2f0:       cbnz    x0, ffff80001015f300 <rcu_idle_exit+0x68>
         :                      __write_once_size():
    0.00 :   ffff80001015f2f4:       mov     x0, #0x4000000000000000         // #4611686018427387904
    0.00 :   ffff80001015f2f8:       str     x0, [x5, #216]
    0.00 :   ffff80001015f2fc:       b       ffff80001015f2d4 <rcu_idle_exit+0x3c>
         :                      rcu_eqs_exit():
    0.00 :   ffff80001015f300:       brk     #0x800
    0.00 :   ffff80001015f304:       b       ffff80001015f2f4 <rcu_idle_exit+0x5c>
 Percent |	Source code & Disassembly of vmlinux for cycles (65 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010125bb8 <_nohz_idle_balance>:
         :                      _nohz_idle_balance():
         :                      * The function returns false if the loop has stopped before running
         :                      * through all idle CPUs.
         :                      */
         :                      static bool _nohz_idle_balance(struct rq *this_rq, unsigned int flags,
         :                      enum cpu_idle_type idle)
         :                      {
    0.00 :   ffff800010125bb8:       stp     x29, x30, [sp, #-144]!
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010125bbc:       adrp    x3, ffff800011a7f000 <ucounts_hashtable+0x12f0>
         :                      _nohz_idle_balance():
         :                      /* Earliest time when we have to do rebalance again */
         :                      unsigned long now = jiffies;
         :                      unsigned long next_balance = now + 60*HZ;
    0.00 :   ffff800010125bc0:       mov     x4, #0x3a98                     // #15000
         :                      {
    0.00 :   ffff800010125bc4:       mov     x29, sp
    0.00 :   ffff800010125bc8:       stp     x19, x20, [sp, #16]
         :                      __write_once_size():
    0.00 :   ffff800010125bcc:       add     x20, x3, #0xfc0
         :                      _nohz_idle_balance():
         :                      unsigned long now = jiffies;
    0.00 :   ffff800010125bd0:       adrp    x3, ffff800011897000 <bit_wait_table+0xe80>
         :                      {
    0.00 :   ffff800010125bd4:       stp     x21, x22, [sp, #32]
    0.00 :   ffff800010125bd8:       stp     x23, x24, [sp, #48]
         :                      unsigned long now = jiffies;
    0.00 :   ffff800010125bdc:       ldr     x3, [x3, #2432]
         :                      {
    0.00 :   ffff800010125be0:       stp     x25, x26, [sp, #64]
    0.00 :   ffff800010125be4:       stp     x27, x28, [sp, #80]
         :                      unsigned long next_balance = now + 60*HZ;
    0.00 :   ffff800010125be8:       add     x19, x3, x4
         :                      __write_once_size():
    0.00 :   ffff800010125bec:       str     wzr, [x20, #36]
         :                      _nohz_idle_balance():
         :                      bool has_blocked_load = false;
         :                      int update_next_balance = 0;
         :                      int this_cpu = this_rq->cpu;
    0.00 :   ffff800010125bf0:       ldr     w23, [x0, #2568]
         :                      {
    0.00 :   ffff800010125bf4:       str     x0, [x29, #104]
    0.00 :   ffff800010125bf8:       str     w2, [x29, #116]
         :                      unsigned long now = jiffies;
    0.00 :   ffff800010125bfc:       str     x3, [x29, #120]
         :                      {
    0.00 :   ffff800010125c00:       str     w1, [x29, #136]
         :
         :                      /*
         :                      * Ensures that if we miss the CPU, we must see the has_blocked
         :                      * store from nohz_balance_enter_idle().
         :                      */
         :                      smp_mb();
    0.00 :   ffff800010125c04:       dmb     ish
         :                      if (need_resched()) {
         :                      has_blocked_load = true;
         :                      goto abort;
         :                      }
         :
         :                      rq = cpu_rq(balance_cpu);
   11.35 :   ffff800010125c08:       adrp    x25, ffff8000114d5000 <tegra_to+0x180>
    0.00 :   ffff800010125c0c:       adrp    x26, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010125c10:       add     x25, x25, #0xd80
    0.00 :   ffff800010125c14:       add     x26, x26, #0x8e8
         :                      for_each_cpu(balance_cpu, nohz.idle_cpus_mask) {
    0.00 :   ffff800010125c18:       mov     w27, #0xffffffff                // #-1
         :                      int update_next_balance = 0;
    0.00 :   ffff800010125c1c:       mov     w24, #0x0                       // #0
         :                      bool has_blocked_load = false;
    0.00 :   ffff800010125c20:       mov     w21, #0x0                       // #0
    0.00 :   ffff800010125c24:       adrp    x22, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      for_each_cpu(balance_cpu, nohz.idle_cpus_mask) {
    0.00 :   ffff800010125c28:       mov     x1, x20
    0.00 :   ffff800010125c2c:       mov     w0, w27
    0.00 :   ffff800010125c30:       bl      ffff800010c93a58 <cpumask_next>
    0.00 :   ffff800010125c34:       mov     w27, w0
    0.00 :   ffff800010125c38:       ldr     w1, [x22, #692]
    0.00 :   ffff800010125c3c:       cmp     w0, w1
    0.00 :   ffff800010125c40:       b.cs    ffff800010125cc4 <_nohz_idle_balance+0x10c>  // b.hs, b.nlast
         :                      if (balance_cpu == this_cpu || !idle_cpu(balance_cpu))
    6.48 :   ffff800010125c44:       cmp     w23, w27
    0.00 :   ffff800010125c48:       b.eq    ffff800010125c28 <_nohz_idle_balance+0x70>  // b.none
    7.45 :   ffff800010125c4c:       bl      ffff800010117488 <idle_cpu>
         :                      rq = cpu_rq(balance_cpu);
   17.54 :   ffff800010125c50:       mov     x3, x25
         :
         :                      has_blocked_load |= update_nohz_stats(rq, true);
    0.00 :   ffff800010125c54:       mov     w1, #0x1                        // #1
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    1.70 :   ffff800010125c58:       mrs     x4, sp_el0
         :                      _nohz_idle_balance():
         :                      if (balance_cpu == this_cpu || !idle_cpu(balance_cpu))
    0.00 :   ffff800010125c5c:       cbz     w0, ffff800010125c28 <_nohz_idle_balance+0x70>
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff800010125c60:       ldr     x0, [x4]
         :                      _nohz_idle_balance():
         :                      if (need_resched()) {
    0.00 :   ffff800010125c64:       tbnz    w0, #1, ffff800010125d54 <_nohz_idle_balance+0x19c>
         :                      rq = cpu_rq(balance_cpu);
    2.92 :   ffff800010125c68:       ldr     x0, [x26, w27, sxtw #3]
    0.00 :   ffff800010125c6c:       add     x28, x3, x0
         :                      has_blocked_load |= update_nohz_stats(rq, true);
    0.00 :   ffff800010125c70:       mov     x0, x28
    0.00 :   ffff800010125c74:       bl      ffff800010121940 <update_nohz_stats>
         :
         :                      /*
         :                      * If time for next balance is due,
         :                      * do the balance.
         :                      */
         :                      if (time_after_eq(jiffies, rq->next_balance)) {
    0.00 :   ffff800010125c78:       adrp    x1, ffff800011897000 <bit_wait_table+0xe80>
         :                      has_blocked_load |= update_nohz_stats(rq, true);
    0.00 :   ffff800010125c7c:       and     w0, w0, #0xff
    0.00 :   ffff800010125c80:       orr     w21, w21, w0
         :                      rq_lock_irqsave():
         :
         :                      static inline void
         :                      rq_lock_irqsave(struct rq *rq, struct rq_flags *rf)
         :                      __acquires(rq->lock)
         :                      {
         :                      raw_spin_lock_irqsave(&rq->lock, rf->flags);
    0.00 :   ffff800010125c84:       mov     x0, x28
         :                      _nohz_idle_balance():
         :                      if (time_after_eq(jiffies, rq->next_balance)) {
    1.29 :   ffff800010125c88:       ldr     x4, [x1, #2432]
    0.00 :   ffff800010125c8c:       ldr     x1, [x28, #2376]
    3.31 :   ffff800010125c90:       sub     x4, x4, x1
    0.00 :   ffff800010125c94:       tbz     x4, #63, ffff800010125d24 <_nohz_idle_balance+0x16c>
         :                      if (flags & NOHZ_BALANCE_KICK)
         :                      rebalance_domains(rq, CPU_IDLE);
         :                      }
         :
         :                      if (time_after(next_balance, rq->next_balance)) {
         :                      next_balance = rq->next_balance;
   34.35 :   ffff800010125c98:       subs    x0, x1, x19
    0.00 :   ffff800010125c9c:       csel    x19, x19, x1, pl  // pl = nfrst
    0.00 :   ffff800010125ca0:       cmp     x0, #0x0
    0.00 :   ffff800010125ca4:       csinc   w24, w24, wzr, ge  // ge = tcont
         :                      for_each_cpu(balance_cpu, nohz.idle_cpus_mask) {
    4.72 :   ffff800010125ca8:       mov     x1, x20
    0.00 :   ffff800010125cac:       mov     w0, w27
    0.00 :   ffff800010125cb0:       bl      ffff800010c93a58 <cpumask_next>
    1.58 :   ffff800010125cb4:       mov     w27, w0
    0.00 :   ffff800010125cb8:       ldr     w1, [x22, #692]
    0.00 :   ffff800010125cbc:       cmp     w0, w1
    0.00 :   ffff800010125cc0:       b.cc    ffff800010125c44 <_nohz_idle_balance+0x8c>  // b.lo, b.ul, b.last
         :                      update_next_balance = 1;
         :                      }
         :                      }
         :
         :                      /* Newly idle CPU doesn't need an update */
         :                      if (idle != CPU_NEWLY_IDLE) {
    0.00 :   ffff800010125cc4:       ldr     w0, [x29, #116]
    0.00 :   ffff800010125cc8:       cmp     w0, #0x2
    0.00 :   ffff800010125ccc:       b.ne    ffff800010125d80 <_nohz_idle_balance+0x1c8>  // b.any
         :                      update_blocked_averages(this_cpu);
         :                      has_blocked_load |= this_rq->has_blocked_load;
         :                      }
         :
         :                      if (flags & NOHZ_BALANCE_KICK)
    0.00 :   ffff800010125cd0:       ldr     x0, [x29, #136]
    0.00 :   ffff800010125cd4:       tbnz    w0, #0, ffff800010125da4 <_nohz_idle_balance+0x1ec>
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff800010125cd8:       adrp    x0, ffff800011a7f000 <ucounts_hashtable+0x12f0>
    0.00 :   ffff800010125cdc:       add     x1, x0, #0xfc0
         :                      _nohz_idle_balance():
         :                      rebalance_domains(this_rq, CPU_IDLE);
         :
         :                      WRITE_ONCE(nohz.next_blocked,
    0.00 :   ffff800010125ce0:       ldr     x0, [x29, #120]
    0.00 :   ffff800010125ce4:       add     x0, x0, #0x8
         :                      __write_once_size():
    0.00 :   ffff800010125ce8:       str     x0, [x1, #48]
         :                      _nohz_idle_balance():
         :                      /* The full idle balance loop has been done */
         :                      ret = true;
         :
         :                      abort:
         :                      /* There is still blocked load, enable periodic update */
         :                      if (has_blocked_load)
    0.00 :   ffff800010125cec:       cbnz    w21, ffff800010125d58 <_nohz_idle_balance+0x1a0>
    0.00 :   ffff800010125cf0:       mov     w21, #0x1                       // #1
         :                      /*
         :                      * next_balance will be updated only when there is a need.
         :                      * When the CPU is attached to null domain for ex, it will not be
         :                      * updated.
         :                      */
         :                      if (likely(update_next_balance))
    0.00 :   ffff800010125cf4:       cbz     w24, ffff800010125d04 <_nohz_idle_balance+0x14c>
         :                      nohz.next_balance = next_balance;
    0.00 :   ffff800010125cf8:       adrp    x0, ffff800011a7f000 <ucounts_hashtable+0x12f0>
    0.00 :   ffff800010125cfc:       add     x27, x0, #0xfc0
    0.00 :   ffff800010125d00:       str     x19, [x27, #40]
         :
         :                      return ret;
         :                      }
    0.00 :   ffff800010125d04:       mov     w0, w21
    0.00 :   ffff800010125d08:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010125d0c:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010125d10:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff800010125d14:       ldp     x25, x26, [sp, #64]
    0.00 :   ffff800010125d18:       ldp     x27, x28, [sp, #80]
    0.00 :   ffff800010125d1c:       ldp     x29, x30, [sp], #144
    0.00 :   ffff800010125d20:       ret
         :                      rq_lock_irqsave():
    4.12 :   ffff800010125d24:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
    0.00 :   ffff800010125d28:       mov     x1, x0
         :                      _nohz_idle_balance():
         :                      update_rq_clock(rq);
    0.00 :   ffff800010125d2c:       mov     x0, x28
         :                      rq_lock_irqsave():
    0.00 :   ffff800010125d30:       str     x1, [x29, #128]
         :                      _nohz_idle_balance():
    0.00 :   ffff800010125d34:       bl      ffff8000101133a8 <update_rq_clock>
         :                      rq_unlock_irqrestore():
         :                      static inline void
         :                      rq_unlock_irqrestore(struct rq *rq, struct rq_flags *rf)
         :                      __releases(rq->lock)
         :                      {
         :                      rq_unpin_lock(rq, rf);
         :                      raw_spin_unlock_irqrestore(&rq->lock, rf->flags);
    0.00 :   ffff800010125d38:       ldr     x1, [x29, #128]
    0.00 :   ffff800010125d3c:       mov     x0, x28
    0.00 :   ffff800010125d40:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      _nohz_idle_balance():
         :                      if (flags & NOHZ_BALANCE_KICK)
    3.19 :   ffff800010125d44:       ldr     x0, [x29, #136]
    0.00 :   ffff800010125d48:       tbnz    w0, #0, ffff800010125d6c <_nohz_idle_balance+0x1b4>
    0.00 :   ffff800010125d4c:       ldr     x1, [x28, #2376]
    0.00 :   ffff800010125d50:       b       ffff800010125c98 <_nohz_idle_balance+0xe0>
    0.00 :   ffff800010125d54:       mov     w21, #0x0                       // #0
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010125d58:       adrp    x0, ffff800011a7f000 <ucounts_hashtable+0x12f0>
    0.00 :   ffff800010125d5c:       add     x0, x0, #0xfc0
    0.00 :   ffff800010125d60:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010125d64:       str     w1, [x0, #36]
    0.00 :   ffff800010125d68:       b       ffff800010125cf4 <_nohz_idle_balance+0x13c>
         :                      _nohz_idle_balance():
         :                      rebalance_domains(rq, CPU_IDLE);
    0.00 :   ffff800010125d6c:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010125d70:       mov     x0, x28
    0.00 :   ffff800010125d74:       bl      ffff8000101258c0 <rebalance_domains>
    0.00 :   ffff800010125d78:       ldr     x1, [x28, #2376]
    0.00 :   ffff800010125d7c:       b       ffff800010125c98 <_nohz_idle_balance+0xe0>
         :                      update_blocked_averages(this_cpu);
    0.00 :   ffff800010125d80:       mov     w0, w23
    0.00 :   ffff800010125d84:       bl      ffff800010121268 <update_blocked_averages>
         :                      has_blocked_load |= this_rq->has_blocked_load;
    0.00 :   ffff800010125d88:       ldr     x0, [x29, #104]
    0.00 :   ffff800010125d8c:       ldr     w0, [x0, #40]
    0.00 :   ffff800010125d90:       orr     w21, w21, w0
         :                      if (flags & NOHZ_BALANCE_KICK)
    0.00 :   ffff800010125d94:       ldr     x0, [x29, #136]
         :                      has_blocked_load |= this_rq->has_blocked_load;
    0.00 :   ffff800010125d98:       cmp     w21, #0x0
    0.00 :   ffff800010125d9c:       cset    w21, ne  // ne = any
         :                      if (flags & NOHZ_BALANCE_KICK)
    0.00 :   ffff800010125da0:       tbz     w0, #0, ffff800010125cd8 <_nohz_idle_balance+0x120>
         :                      rebalance_domains(this_rq, CPU_IDLE);
    0.00 :   ffff800010125da4:       ldr     x0, [x29, #104]
    0.00 :   ffff800010125da8:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010125dac:       bl      ffff8000101258c0 <rebalance_domains>
    0.00 :   ffff800010125db0:       b       ffff800010125cd8 <_nohz_idle_balance+0x120>
 Percent |	Source code & Disassembly of vmlinux for cycles (100 samples, percent: local period)
----------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010482208 <iov_iter_init>:
         :                      iov_iter_init():
         :
         :                      void iov_iter_init(struct iov_iter *i, unsigned int direction,
         :                      const struct iovec *iov, unsigned long nr_segs,
         :                      size_t count)
         :                      {
         :                      WARN_ON(direction & ~(READ | WRITE));
   10.02 :   ffff800010482208:       tst     w1, #0xfffffffe
    0.00 :   ffff80001048220c:       b.ne    ffff800010482254 <iov_iter_init+0x4c>  // b.any
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    8.03 :   ffff800010482210:       mrs     x5, sp_el0
         :                      iov_iter_init():
         :                      direction &= READ | WRITE;
         :
         :                      /* It will get better.  Eventually... */
         :                      if (uaccess_kernel()) {
   24.00 :   ffff800010482214:       ldr     x5, [x5, #8]
         :                      direction &= READ | WRITE;
    0.00 :   ffff800010482218:       and     w1, w1, #0x1
         :                      if (uaccess_kernel()) {
    0.00 :   ffff80001048221c:       cmn     x5, #0x1
    0.00 :   ffff800010482220:       b.eq    ffff80001048223c <iov_iter_init+0x34>  // b.none
         :                      i->type = ITER_KVEC | direction;
         :                      i->kvec = (struct kvec *)iov;
         :                      } else {
         :                      i->type = ITER_IOVEC | direction;
   28.93 :   ffff800010482224:       orr     w1, w1, #0x4
   17.95 :   ffff800010482228:       str     w1, [x0]
         :                      i->iov = iov;
         :                      }
         :                      i->nr_segs = nr_segs;
         :                      i->iov_offset = 0;
         :                      i->count = count;
   10.04 :   ffff80001048222c:       stp     xzr, x4, [x0, #8]
         :                      i->iov = iov;
    0.00 :   ffff800010482230:       str     x2, [x0, #24]
         :                      i->nr_segs = nr_segs;
    1.02 :   ffff800010482234:       str     x3, [x0, #32]
         :                      }
    0.00 :   ffff800010482238:       ret
         :                      i->type = ITER_KVEC | direction;
    0.00 :   ffff80001048223c:       orr     w1, w1, #0x8
    0.00 :   ffff800010482240:       str     w1, [x0]
         :                      i->count = count;
    0.00 :   ffff800010482244:       stp     xzr, x4, [x0, #8]
         :                      i->kvec = (struct kvec *)iov;
    0.00 :   ffff800010482248:       str     x2, [x0, #24]
         :                      i->nr_segs = nr_segs;
    0.00 :   ffff80001048224c:       str     x3, [x0, #32]
         :                      }
    0.00 :   ffff800010482250:       ret
         :                      WARN_ON(direction & ~(READ | WRITE));
    0.00 :   ffff800010482254:       brk     #0x800
    0.00 :   ffff800010482258:       b       ffff800010482210 <iov_iter_init+0x8>
 Percent |	Source code & Disassembly of vmlinux for cycles (51 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000106fbb50 <iommu_unmap_fast>:
         :                      iommu_unmap_fast():
         :                      EXPORT_SYMBOL_GPL(iommu_unmap);
         :
         :                      size_t iommu_unmap_fast(struct iommu_domain *domain,
         :                      unsigned long iova, size_t size,
         :                      struct iommu_iotlb_gather *iotlb_gather)
         :                      {
   21.64 :   ffff8000106fbb50:       stp     x29, x30, [sp, #-16]!
    3.97 :   ffff8000106fbb54:       mov     x29, sp
         :                      return __iommu_unmap(domain, iova, size, iotlb_gather);
    1.74 :   ffff8000106fbb58:       bl      ffff8000106fb9b0 <__iommu_unmap>
         :                      }
   72.65 :   ffff8000106fbb5c:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000106fbb60:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (73 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101d30d0 <filemap_write_and_wait_range>:
         :                      filemap_write_and_wait_range():
         :                      *
         :                      * Return: error status of the address space.
         :                      */
         :                      int filemap_write_and_wait_range(struct address_space *mapping,
         :                      loff_t lstart, loff_t lend)
         :                      {
    0.00 :   ffff8000101d30d0:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff8000101d30d4:       mov     x29, sp
    2.75 :   ffff8000101d30d8:       str     x19, [sp, #16]
         :                      int err = 0;
         :
         :                      if (mapping_needs_writeback(mapping)) {
    0.00 :   ffff8000101d30dc:       ldr     x3, [x0, #88]
    1.38 :   ffff8000101d30e0:       cbz     x3, ffff8000101d3144 <filemap_write_and_wait_range+0x74>
    0.00 :   ffff8000101d30e4:       stp     x20, x21, [x29, #24]
         :                      err = __filemap_fdatawrite_range(mapping, lstart, lend,
    0.00 :   ffff8000101d30e8:       mov     w3, #0x1                        // #1
    0.00 :   ffff8000101d30ec:       str     x22, [x29, #40]
    0.00 :   ffff8000101d30f0:       mov     x20, x0
    0.00 :   ffff8000101d30f4:       mov     x21, x1
    0.00 :   ffff8000101d30f8:       mov     x22, x2
    0.00 :   ffff8000101d30fc:       bl      ffff8000101d2ec0 <__filemap_fdatawrite_range>
    0.00 :   ffff8000101d3100:       mov     w19, w0
         :                      WB_SYNC_ALL);
         :                      /* See comment of filemap_write_and_wait() */
         :                      if (err != -EIO) {
    0.00 :   ffff8000101d3104:       cmn     w0, #0x5
    0.00 :   ffff8000101d3108:       b.eq    ffff8000101d315c <filemap_write_and_wait_range+0x8c>  // b.none
         :                      filemap_fdatawait_range():
         :                      __filemap_fdatawait_range(mapping, start_byte, end_byte);
    0.00 :   ffff8000101d310c:       mov     x2, x22
    0.00 :   ffff8000101d3110:       mov     x1, x21
    0.00 :   ffff8000101d3114:       mov     x0, x20
    0.00 :   ffff8000101d3118:       bl      ffff8000101cefe0 <__filemap_fdatawait_range>
         :                      return filemap_check_errors(mapping);
    0.00 :   ffff8000101d311c:       mov     x0, x20
    0.00 :   ffff8000101d3120:       bl      ffff8000101d0140 <filemap_check_errors>
    0.00 :   ffff8000101d3124:       ldr     x22, [x29, #40]
         :                      filemap_write_and_wait_range():
         :                      int err2 = filemap_fdatawait_range(mapping,
         :                      lstart, lend);
         :                      if (!err)
         :                      err = err2;
    0.00 :   ffff8000101d3128:       cmp     w19, #0x0
    0.00 :   ffff8000101d312c:       csel    w19, w19, w0, ne  // ne = any
         :                      }
         :                      } else {
         :                      err = filemap_check_errors(mapping);
         :                      }
         :                      return err;
         :                      }
    0.00 :   ffff8000101d3130:       mov     w0, w19
    0.00 :   ffff8000101d3134:       ldr     x19, [sp, #16]
    0.00 :   ffff8000101d3138:       ldp     x20, x21, [x29, #24]
    0.00 :   ffff8000101d313c:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000101d3140:       ret
         :                      err = filemap_check_errors(mapping);
   25.94 :   ffff8000101d3144:       bl      ffff8000101d0140 <filemap_check_errors>
   46.54 :   ffff8000101d3148:       mov     w19, w0
         :                      }
    0.00 :   ffff8000101d314c:       mov     w0, w19
    4.09 :   ffff8000101d3150:       ldr     x19, [sp, #16]
   19.31 :   ffff8000101d3154:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000101d3158:       ret
         :                      filemap_check_errors(mapping);
    0.00 :   ffff8000101d315c:       mov     x0, x20
    0.00 :   ffff8000101d3160:       bl      ffff8000101d0140 <filemap_check_errors>
    0.00 :   ffff8000101d3164:       ldp     x20, x21, [x29, #24]
    0.00 :   ffff8000101d3168:       ldr     x22, [x29, #40]
    0.00 :   ffff8000101d316c:       b       ffff8000101d314c <filemap_write_and_wait_range+0x7c>
 Percent |	Source code & Disassembly of vmlinux for cycles (49 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001011ab98 <do_idle>:
         :                      do_idle():
         :                      * Generic idle loop implementation
         :                      *
         :                      * Called with polling cleared.
         :                      */
         :                      static void do_idle(void)
         :                      {
    6.21 :   ffff80001011ab98:       stp     x29, x30, [sp, #-112]!
         :                      int cpu = smp_processor_id();
    0.00 :   ffff80001011ab9c:       adrp    x0, ffff8000114ca000 <bp_hardening_data>
    0.00 :   ffff80001011aba0:       add     x0, x0, #0x18
         :                      {
    0.00 :   ffff80001011aba4:       mov     x29, sp
   11.59 :   ffff80001011aba8:       str     x20, [sp, #24]
    2.00 :   ffff80001011abac:       str     x22, [sp, #40]
    0.00 :   ffff80001011abb0:       adrp    x22, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff80001011abb4:       add     x1, x22, #0x8c8
    0.00 :   ffff80001011abb8:       ldr     x2, [x1]
    2.06 :   ffff80001011abbc:       str     x2, [x29, #104]
    0.00 :   ffff80001011abc0:       mov     x2, #0x0                        // #0
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff80001011abc4:       mrs     x1, tpidr_el1
         :                      do_idle():
         :                      int cpu = smp_processor_id();
    0.00 :   ffff80001011abc8:       ldr     w20, [x0, x1]
         :                      * then setting need_resched is guaranteed to cause the CPU to
         :                      * reschedule.
         :                      */
         :
         :                      __current_set_polling();
         :                      tick_nohz_idle_enter();
    0.00 :   ffff80001011abcc:       bl      ffff80001017a068 <tick_nohz_idle_enter>
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80001011abd0:       mrs     x0, sp_el0
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff80001011abd4:       ldr     x0, [x0]
         :                      do_idle():
         :
         :                      while (!need_resched()) {
    0.00 :   ffff80001011abd8:       tst     w0, #0x2
    0.00 :   ffff80001011abdc:       b.ne    ffff80001011ad18 <do_idle+0x180>  // b.any
         :                      test_bit():
    4.11 :   ffff80001011abe0:       cmp     w20, #0x0
    1.97 :   ffff80001011abe4:       str     x19, [x29, #16]
    0.00 :   ffff80001011abe8:       add     w19, w20, #0x3f
    0.00 :   ffff80001011abec:       adrp    x0, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff80001011abf0:       csel    w19, w19, w20, lt  // lt = tstop
    0.00 :   ffff80001011abf4:       add     x0, x0, #0x120
    0.00 :   ffff80001011abf8:       mov     x1, #0x1                        // #1
    0.00 :   ffff80001011abfc:       str     x21, [x29, #32]
    0.00 :   ffff80001011ac00:       asr     w19, w19, #6
    0.00 :   ffff80001011ac04:       stp     x23, x24, [x29, #48]
    0.00 :   ffff80001011ac08:       stp     x25, x26, [x29, #64]
         :                      cpuidle_get_device():
         :                      extern void cpuidle_disable_device(struct cpuidle_device *dev);
         :                      extern int cpuidle_play_dead(void);
         :
         :                      extern struct cpuidle_driver *cpuidle_get_cpu_driver(struct cpuidle_device *dev);
         :                      static inline struct cpuidle_device *cpuidle_get_device(void)
         :                      {return __this_cpu_read(cpuidle_devices); }
    0.00 :   ffff80001011ac0c:       adrp    x23, ffff8000114d4000 <bh_lrus+0x18>
         :                      idle_should_enter_s2idle():
         :
         :                      extern enum s2idle_states __read_mostly s2idle_state;
         :
         :                      static inline bool idle_should_enter_s2idle(void)
         :                      {
         :                      return unlikely(s2idle_state == S2IDLE_STATE_ENTER);
    0.00 :   ffff80001011ac10:       adrp    x24, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      cpuidle_get_device():
    0.00 :   ffff80001011ac14:       add     x23, x23, #0xaf8
         :                      idle_should_enter_s2idle():
    0.00 :   ffff80001011ac18:       add     x24, x24, #0x21c
         :                      test_bit():
    0.00 :   ffff80001011ac1c:       add     x19, x0, w19, sxtw #3
    0.00 :   ffff80001011ac20:       lsl     x20, x1, x20
    0.00 :   ffff80001011ac24:       adrp    x21, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff80001011ac28:       b       ffff80001011ac44 <do_idle+0xac>
         :                      do_idle():
         :                      * detected in the wakeup from idle path that the tick
         :                      * broadcast device expired for us, we don't want to go deep
         :                      * idle as we know that the IPI is going to arrive right away.
         :                      */
         :                      if (cpu_idle_force_poll || tick_check_broadcast_expired()) {
         :                      tick_nohz_idle_restart_tick();
    0.00 :   ffff80001011ac2c:       bl      ffff80001017a258 <tick_nohz_idle_restart_tick>
         :                      cpu_idle_poll();
    0.00 :   ffff80001011ac30:       bl      ffff800010cb2368 <cpu_idle_poll>
         :                      } else {
         :                      cpuidle_idle_call();
         :                      }
         :                      arch_cpu_idle_exit();
    0.00 :   ffff80001011ac34:       bl      ffff80001011ab70 <arch_cpu_idle_exit>
         :                      get_current():
    0.00 :   ffff80001011ac38:       mrs     x0, sp_el0
         :                      test_bit():
    0.00 :   ffff80001011ac3c:       ldr     x0, [x0]
         :                      do_idle():
         :                      while (!need_resched()) {
    0.00 :   ffff80001011ac40:       tbnz    w0, #1, ffff80001011ad08 <do_idle+0x170>
         :                      rmb();
    0.00 :   ffff80001011ac44:       dsb     ld
         :                      arch_local_irq_disable():
         :                      u32 pmr = read_sysreg_s(SYS_ICC_PMR_EL1);
         :
         :                      WARN_ON_ONCE(pmr != GIC_PRIO_IRQON && pmr != GIC_PRIO_IRQOFF);
         :                      }
         :
         :                      asm volatile(ALTERNATIVE(
   21.20 :   ffff80001011ac48:       mov     x0, #0x60                       // #96
    0.00 :   ffff80001011ac4c:       msr     daifset, #0x2
         :                      test_bit():
    0.00 :   ffff80001011ac50:       ldr     x0, [x19]
         :                      do_idle():
         :                      if (cpu_is_offline(cpu)) {
    0.00 :   ffff80001011ac54:       tst     x0, x20
    0.00 :   ffff80001011ac58:       b.eq    ffff80001011ad54 <do_idle+0x1bc>  // b.none
         :                      arch_cpu_idle_enter();
    0.00 :   ffff80001011ac5c:       bl      ffff80001011ab68 <arch_cpu_idle_enter>
         :                      if (cpu_idle_force_poll || tick_check_broadcast_expired()) {
    0.00 :   ffff80001011ac60:       ldr     w0, [x21, #504]
    0.00 :   ffff80001011ac64:       cbnz    w0, ffff80001011ac2c <do_idle+0x94>
    0.00 :   ffff80001011ac68:       bl      ffff8000101786d8 <tick_check_broadcast_expired>
    0.00 :   ffff80001011ac6c:       cbnz    w0, ffff80001011ac2c <do_idle+0x94>
         :                      __my_cpu_offset():
    0.00 :   ffff80001011ac70:       mrs     x1, tpidr_el1
         :                      cpuidle_get_device():
    0.00 :   ffff80001011ac74:       mov     x0, x23
    0.00 :   ffff80001011ac78:       ldr     x25, [x0, x1]
         :                      cpuidle_idle_call():
         :                      struct cpuidle_driver *drv = cpuidle_get_cpu_driver(dev);
    0.00 :   ffff80001011ac7c:       mov     x0, x25
    0.00 :   ffff80001011ac80:       bl      ffff800010a67150 <cpuidle_get_cpu_driver>
         :                      get_current():
    0.00 :   ffff80001011ac84:       mrs     x1, sp_el0
         :                      test_bit():
    0.00 :   ffff80001011ac88:       ldr     x1, [x1]
         :                      cpuidle_idle_call():
    0.00 :   ffff80001011ac8c:       mov     x26, x0
         :                      if (need_resched()) {
    0.00 :   ffff80001011ac90:       tbnz    w1, #1, ffff80001011adec <do_idle+0x254>
         :                      if (cpuidle_not_available(drv, dev)) {
    0.00 :   ffff80001011ac94:       mov     x1, x25
    0.00 :   ffff80001011ac98:       bl      ffff800010a66760 <cpuidle_not_available>
    0.00 :   ffff80001011ac9c:       tst     w0, #0xff
    0.00 :   ffff80001011aca0:       b.ne    ffff80001011ad64 <do_idle+0x1cc>  // b.any
         :                      if (idle_should_enter_s2idle() || dev->forced_idle_latency_limit_ns) {
    0.00 :   ffff80001011aca4:       ldr     w0, [x24]
    0.00 :   ffff80001011aca8:       str     x27, [x29, #80]
    0.00 :   ffff80001011acac:       cmp     w0, #0x1
    0.00 :   ffff80001011acb0:       b.eq    ffff80001011adf8 <do_idle+0x260>  // b.none
    0.00 :   ffff80001011acb4:       ldr     x27, [x25, #40]
    0.00 :   ffff80001011acb8:       cbz     x27, ffff80001011ad74 <do_idle+0x1dc>
         :                      tick_nohz_idle_stop_tick();
    0.00 :   ffff80001011acbc:       bl      ffff800010179e08 <tick_nohz_idle_stop_tick>
         :                      rcu_idle_enter();
    0.00 :   ffff80001011acc0:       bl      ffff80001015f158 <rcu_idle_enter>
         :                      next_state = cpuidle_find_deepest_state(drv, dev, max_latency_ns);
    0.00 :   ffff80001011acc4:       mov     x2, x27
    0.00 :   ffff80001011acc8:       mov     x1, x25
    0.00 :   ffff80001011accc:       mov     x0, x26
    0.00 :   ffff80001011acd0:       bl      ffff800010a66898 <cpuidle_find_deepest_state>
         :                      call_cpuidle(drv, dev, next_state);
    0.00 :   ffff80001011acd4:       mov     x1, x25
    0.00 :   ffff80001011acd8:       mov     w2, w0
    0.00 :   ffff80001011acdc:       mov     x0, x26
    0.00 :   ffff80001011ace0:       bl      ffff80001011aad0 <call_cpuidle>
    0.00 :   ffff80001011ace4:       ldr     x27, [x29, #80]
         :                      arch_local_save_flags():
         :                      */
         :                      static inline unsigned long arch_local_save_flags(void)
         :                      {
         :                      unsigned long flags;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001011ace8:       mrs     x1, daif
         :                      arch_irqs_disabled_flags():
         :
         :                      static inline int arch_irqs_disabled_flags(unsigned long flags)
         :                      {
         :                      int res;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001011acec:       and     w0, w1, #0x80
         :                      cpuidle_idle_call():
         :                      if (WARN_ON_ONCE(irqs_disabled()))
    0.00 :   ffff80001011acf0:       cbnz    w0, ffff80001011adc4 <do_idle+0x22c>
         :                      rcu_idle_exit();
    0.00 :   ffff80001011acf4:       bl      ffff80001015f298 <rcu_idle_exit>
         :                      do_idle():
         :                      arch_cpu_idle_exit();
   12.21 :   ffff80001011acf8:       bl      ffff80001011ab70 <arch_cpu_idle_exit>
         :                      get_current():
    0.00 :   ffff80001011acfc:       mrs     x0, sp_el0
         :                      test_bit():
    0.00 :   ffff80001011ad00:       ldr     x0, [x0]
         :                      do_idle():
         :                      while (!need_resched()) {
    0.00 :   ffff80001011ad04:       tbz     w0, #1, ffff80001011ac44 <do_idle+0xac>
    8.25 :   ffff80001011ad08:       ldr     x19, [x29, #16]
    2.01 :   ffff80001011ad0c:       ldr     x21, [x29, #32]
    0.00 :   ffff80001011ad10:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff80001011ad14:       ldp     x25, x26, [x29, #64]
         :                      get_current():
    0.00 :   ffff80001011ad18:       mrs     x0, sp_el0
         :                      set_preempt_need_resched():
         :                      task_thread_info(p)->preempt_count = PREEMPT_ENABLED; \
         :                      } while (0)
         :
         :                      static inline void set_preempt_need_resched(void)
         :                      {
         :                      current_thread_info()->preempt.need_resched = 0;
    0.00 :   ffff80001011ad1c:       str     wzr, [x0, #20]
         :                      do_idle():
         :                      *
         :                      * This is required because for polling idle loops we will not have had
         :                      * an IPI to fold the state for us.
         :                      */
         :                      preempt_set_need_resched();
         :                      tick_nohz_idle_exit();
    0.00 :   ffff80001011ad20:       bl      ffff80001017a298 <tick_nohz_idle_exit>
         :                      /*
         :                      * We promise to call sched_ttwu_pending() and reschedule if
         :                      * need_resched() is set while polling is set. That means that clearing
         :                      * polling needs to be visible before doing these things.
         :                      */
         :                      smp_mb__after_atomic();
    2.03 :   ffff80001011ad24:       dmb     ish
         :
         :                      sched_ttwu_pending();
    1.97 :   ffff80001011ad28:       bl      ffff800010115fa0 <sched_ttwu_pending>
         :                      schedule_idle();
         :
         :                      if (unlikely(klp_patch_pending(current)))
         :                      klp_update_patch_state(current);
         :                      }
    0.00 :   ffff80001011ad2c:       add     x22, x22, #0x8c8
         :                      schedule_idle();
    0.00 :   ffff80001011ad30:       bl      ffff800010cad6c8 <schedule_idle>
         :                      }
    2.06 :   ffff80001011ad34:       ldr     x1, [x29, #104]
    2.02 :   ffff80001011ad38:       ldr     x0, [x22]
    0.00 :   ffff80001011ad3c:       eor     x0, x1, x0
    0.00 :   ffff80001011ad40:       cbnz    x0, ffff80001011ae2c <do_idle+0x294>
    0.00 :   ffff80001011ad44:       ldr     x20, [sp, #24]
    0.00 :   ffff80001011ad48:       ldr     x22, [sp, #40]
    0.00 :   ffff80001011ad4c:       ldp     x29, x30, [sp], #112
    0.00 :   ffff80001011ad50:       ret
         :                      tick_nohz_idle_stop_tick();
    0.00 :   ffff80001011ad54:       bl      ffff800010179e08 <tick_nohz_idle_stop_tick>
         :                      cpuhp_report_idle_dead();
    0.00 :   ffff80001011ad58:       bl      ffff8000100e7b20 <cpuhp_report_idle_dead>
         :                      arch_cpu_idle_dead();
    0.00 :   ffff80001011ad5c:       bl      ffff800010087008 <arch_cpu_idle_dead>
    0.00 :   ffff80001011ad60:       b       ffff80001011ac5c <do_idle+0xc4>
         :                      cpuidle_idle_call():
         :                      tick_nohz_idle_stop_tick();
    0.00 :   ffff80001011ad64:       bl      ffff800010179e08 <tick_nohz_idle_stop_tick>
         :                      rcu_idle_enter();
    0.00 :   ffff80001011ad68:       bl      ffff80001015f158 <rcu_idle_enter>
         :                      default_idle_call();
    0.00 :   ffff80001011ad6c:       bl      ffff800010cb23d0 <default_idle_call>
   20.31 :   ffff80001011ad70:       b       ffff80001011ace8 <do_idle+0x150>
         :                      bool stop_tick = true;
    0.00 :   ffff80001011ad74:       mov     w0, #0x1                        // #1
         :                      next_state = cpuidle_select(drv, dev, &stop_tick);
    0.00 :   ffff80001011ad78:       add     x2, x29, #0x67
         :                      bool stop_tick = true;
    0.00 :   ffff80001011ad7c:       strb    w0, [x29, #103]
         :                      next_state = cpuidle_select(drv, dev, &stop_tick);
    0.00 :   ffff80001011ad80:       mov     x1, x25
    0.00 :   ffff80001011ad84:       mov     x0, x26
    0.00 :   ffff80001011ad88:       bl      ffff800010a66d30 <cpuidle_select>
    0.00 :   ffff80001011ad8c:       mov     w27, w0
         :                      if (stop_tick || tick_nohz_tick_stopped())
    0.00 :   ffff80001011ad90:       ldrb    w0, [x29, #103]
    0.00 :   ffff80001011ad94:       cbz     w0, ffff80001011add8 <do_idle+0x240>
         :                      tick_nohz_idle_stop_tick();
    0.00 :   ffff80001011ad98:       bl      ffff800010179e08 <tick_nohz_idle_stop_tick>
         :                      rcu_idle_enter();
    0.00 :   ffff80001011ad9c:       bl      ffff80001015f158 <rcu_idle_enter>
         :                      entered_state = call_cpuidle(drv, dev, next_state);
    0.00 :   ffff80001011ada0:       mov     w2, w27
    0.00 :   ffff80001011ada4:       mov     x1, x25
    0.00 :   ffff80001011ada8:       mov     x0, x26
    0.00 :   ffff80001011adac:       bl      ffff80001011aad0 <call_cpuidle>
         :                      cpuidle_reflect(dev, entered_state);
    0.00 :   ffff80001011adb0:       mov     w1, w0
    0.00 :   ffff80001011adb4:       mov     x0, x25
    0.00 :   ffff80001011adb8:       bl      ffff800010a66da0 <cpuidle_reflect>
    0.00 :   ffff80001011adbc:       ldr     x27, [x29, #80]
    0.00 :   ffff80001011adc0:       b       ffff80001011ace8 <do_idle+0x150>
         :                      if (WARN_ON_ONCE(irqs_disabled()))
    0.00 :   ffff80001011adc4:       brk     #0x800
         :                      arch_local_irq_enable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001011adc8:       mov     x0, #0xe0                       // #224
    0.00 :   ffff80001011adcc:       msr     daifclr, #0x2
         :                      cpuidle_idle_call():
         :                      rcu_idle_exit();
    0.00 :   ffff80001011add0:       bl      ffff80001015f298 <rcu_idle_exit>
    0.00 :   ffff80001011add4:       b       ffff80001011acf8 <do_idle+0x160>
         :                      if (stop_tick || tick_nohz_tick_stopped())
    0.00 :   ffff80001011add8:       bl      ffff800010179dc0 <tick_nohz_tick_stopped>
    0.00 :   ffff80001011addc:       tst     w0, #0xff
    0.00 :   ffff80001011ade0:       b.ne    ffff80001011ad98 <do_idle+0x200>  // b.any
         :                      tick_nohz_idle_retain_tick();
    0.00 :   ffff80001011ade4:       bl      ffff80001017a040 <tick_nohz_idle_retain_tick>
    0.00 :   ffff80001011ade8:       b       ffff80001011ad9c <do_idle+0x204>
         :                      arch_local_irq_enable():
    0.00 :   ffff80001011adec:       mov     x0, #0xe0                       // #224
    0.00 :   ffff80001011adf0:       msr     daifclr, #0x2
    0.00 :   ffff80001011adf4:       b       ffff80001011ac34 <do_idle+0x9c>
         :                      cpuidle_idle_call():
         :                      rcu_idle_enter();
    0.00 :   ffff80001011adf8:       bl      ffff80001015f158 <rcu_idle_enter>
         :                      entered_state = cpuidle_enter_s2idle(drv, dev);
    0.00 :   ffff80001011adfc:       mov     x1, x25
    0.00 :   ffff80001011ae00:       mov     x0, x26
    0.00 :   ffff80001011ae04:       bl      ffff800010a668f8 <cpuidle_enter_s2idle>
         :                      if (entered_state > 0) {
    0.00 :   ffff80001011ae08:       cmp     w0, #0x0
    0.00 :   ffff80001011ae0c:       b.le    ffff80001011ae20 <do_idle+0x288>
         :                      arch_local_irq_enable():
    0.00 :   ffff80001011ae10:       mov     x0, #0xe0                       // #224
    0.00 :   ffff80001011ae14:       msr     daifclr, #0x2
    0.00 :   ffff80001011ae18:       ldr     x27, [x29, #80]
    0.00 :   ffff80001011ae1c:       b       ffff80001011ace8 <do_idle+0x150>
         :                      cpuidle_idle_call():
         :                      max_latency_ns = U64_MAX;
    0.00 :   ffff80001011ae20:       mov     x27, #0xffffffffffffffff        // #-1
         :                      rcu_idle_exit();
    0.00 :   ffff80001011ae24:       bl      ffff80001015f298 <rcu_idle_exit>
    0.00 :   ffff80001011ae28:       b       ffff80001011acbc <do_idle+0x124>
    0.00 :   ffff80001011ae2c:       str     x19, [x29, #16]
    0.00 :   ffff80001011ae30:       str     x21, [x29, #32]
    0.00 :   ffff80001011ae34:       stp     x23, x24, [x29, #48]
    0.00 :   ffff80001011ae38:       stp     x25, x26, [x29, #64]
    0.00 :   ffff80001011ae3c:       str     x27, [x29, #80]
         :                      do_idle():
         :                      }
    0.00 :   ffff80001011ae40:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (74 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000106fff80 <__arm_lpae_init_pte>:
         :                      __arm_lpae_init_pte():
         :                      phys_addr_t paddr, arm_lpae_iopte prot,
         :                      int lvl, arm_lpae_iopte *ptep)
         :                      {
         :                      arm_lpae_iopte pte = prot;
         :
         :                      if (data->iop.cfg.quirks & IO_PGTABLE_QUIRK_ARM_NS)
    1.38 :   ffff8000106fff80:       ldr     x7, [x0, #16]
         :                      pte |= ARM_LPAE_PTE_NS;
    0.00 :   ffff8000106fff84:       orr     x6, x2, #0x20
         :
         :                      if (data->iop.fmt != ARM_MALI_LPAE && lvl == ARM_LPAE_MAX_LEVELS - 1)
    5.52 :   ffff8000106fff88:       ldr     w5, [x0]
         :                      pte |= ARM_LPAE_PTE_NS;
    0.00 :   ffff8000106fff8c:       tst     x7, #0x1
    0.00 :   ffff8000106fff90:       csel    x2, x6, x2, ne  // ne = any
         :                      if (data->iop.fmt != ARM_MALI_LPAE && lvl == ARM_LPAE_MAX_LEVELS - 1)
    0.00 :   ffff8000106fff94:       cmp     w5, #0x5
   16.19 :   ffff8000106fff98:       ccmp    w3, #0x3, #0x0, ne  // ne = any
         :                      pte |= ARM_LPAE_PTE_TYPE_PAGE;
    0.00 :   ffff8000106fff9c:       orr     x3, x2, #0x3
         :                      if (data->iop.fmt != ARM_MALI_LPAE && lvl == ARM_LPAE_MAX_LEVELS - 1)
    0.00 :   ffff8000106fffa0:       b.eq    ffff8000106fffb0 <__arm_lpae_init_pte+0x30>  // b.none
         :                      else
         :                      pte |= ARM_LPAE_PTE_TYPE_BLOCK;
    0.00 :   ffff8000106fffa4:       orr     x3, x2, #0x1
         :
         :                      if (data->iop.fmt != ARM_MALI_LPAE)
    0.00 :   ffff8000106fffa8:       cmp     w5, #0x5
    0.00 :   ffff8000106fffac:       b.eq    ffff8000106fffb4 <__arm_lpae_init_pte+0x34>  // b.none
         :                      pte |= ARM_LPAE_PTE_AF;
    0.00 :   ffff8000106fffb0:       orr     x3, x3, #0x400
         :                      paddr_to_iopte():
         :                      return (pte | (pte >> (48 - 12))) & ARM_LPAE_PTE_ADDR_MASK;
    0.00 :   ffff8000106fffb4:       orr     x1, x1, x1, lsr #36
    0.00 :   ffff8000106fffb8:       and     x2, x1, #0xfffffffff000
         :                      __arm_lpae_init_pte():
         :                      pte |= ARM_LPAE_PTE_SH_IS;
         :                      pte |= paddr_to_iopte(paddr, data);
    0.00 :   ffff8000106fffbc:       orr     x2, x2, #0x300
    4.12 :   ffff8000106fffc0:       orr     x2, x2, x3
         :                      __arm_lpae_set_pte():
         :                      *ptep = pte;
    0.00 :   ffff8000106fffc4:       str     x2, [x4]
         :                      if (!cfg->coherent_walk)
   52.59 :   ffff8000106fffc8:       ldrb    w1, [x0, #40]
    0.00 :   ffff8000106fffcc:       cbz     w1, ffff8000106fffd4 <__arm_lpae_init_pte+0x54>
   20.20 :   ffff8000106fffd0:       ret
         :                      __arm_lpae_init_pte():
         :                      {
    0.00 :   ffff8000106fffd4:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff8000106fffd8:       mov     x29, sp
         :                      __arm_lpae_set_pte():
         :                      __arm_lpae_sync_pte(ptep, cfg);
    0.00 :   ffff8000106fffdc:       ldr     x1, [x0, #56]
    0.00 :   ffff8000106fffe0:       mov     x0, x4
    0.00 :   ffff8000106fffe4:       bl      ffff8000106ffef8 <__arm_lpae_sync_pte.isra.21>
         :                      __arm_lpae_init_pte():
         :
         :                      __arm_lpae_set_pte(ptep, pte, &data->iop.cfg);
         :                      }
    0.00 :   ffff8000106fffe8:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000106fffec:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (77 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010487e20 <find_next_zero_bit>:
         :                      _find_next_bit():
         :                      const unsigned long *addr2, unsigned long nbits,
         :                      unsigned long start, unsigned long invert)
         :                      {
         :                      unsigned long tmp;
         :
         :                      if (unlikely(start >= nbits))
    0.00 :   ffff800010487e20:       cmp     x1, x2
    0.00 :   ffff800010487e24:       b.ls    ffff800010487e64 <find_next_zero_bit+0x44>  // b.plast
         :                      return nbits;
         :
         :                      tmp = addr1[start / BITS_PER_LONG];
   10.29 :   ffff800010487e28:       lsr     x5, x2, #6
         :                      if (addr2)
         :                      tmp &= addr2[start / BITS_PER_LONG];
         :                      tmp ^= invert;
         :
         :                      /* Handle 1st word. */
         :                      tmp &= BITMAP_FIRST_WORD_MASK(start);
    0.00 :   ffff800010487e2c:       mov     x3, #0xffffffffffffffff         // #-1
    0.00 :   ffff800010487e30:       lsl     x4, x3, x2
         :                      start = round_down(start, BITS_PER_LONG);
    0.00 :   ffff800010487e34:       and     x2, x2, #0xffffffffffffffc0
         :                      tmp = addr1[start / BITS_PER_LONG];
    6.52 :   ffff800010487e38:       ldr     x3, [x0, x5, lsl #3]
         :
         :                      while (!tmp) {
    0.00 :   ffff800010487e3c:       bics    x3, x4, x3
    2.58 :   ffff800010487e40:       b.eq    ffff800010487e58 <find_next_zero_bit+0x38>  // b.none
   66.22 :   ffff800010487e44:       b       ffff800010487e6c <find_next_zero_bit+0x4c>
         :                      start += BITS_PER_LONG;
         :                      if (start >= nbits)
         :                      return nbits;
         :
         :                      tmp = addr1[start / BITS_PER_LONG];
    0.00 :   ffff800010487e48:       lsr     x3, x2, #6
    0.00 :   ffff800010487e4c:       ldr     x3, [x0, x3, lsl #3]
         :                      if (addr2)
         :                      tmp &= addr2[start / BITS_PER_LONG];
         :                      tmp ^= invert;
    0.00 :   ffff800010487e50:       mvn     x3, x3
         :                      while (!tmp) {
    0.00 :   ffff800010487e54:       cbnz    x3, ffff800010487e6c <find_next_zero_bit+0x4c>
         :                      start += BITS_PER_LONG;
    2.59 :   ffff800010487e58:       add     x2, x2, #0x40
         :                      if (start >= nbits)
    0.00 :   ffff800010487e5c:       cmp     x1, x2
    0.00 :   ffff800010487e60:       b.hi    ffff800010487e48 <find_next_zero_bit+0x28>  // b.pmore
         :                      find_next_zero_bit():
         :                      #ifndef find_next_zero_bit
         :                      unsigned long find_next_zero_bit(const unsigned long *addr, unsigned long size,
         :                      unsigned long offset)
         :                      {
         :                      return _find_next_bit(addr, NULL, size, offset, ~0UL);
         :                      }
    0.00 :   ffff800010487e64:       mov     x0, x1
    0.00 :   ffff800010487e68:       ret
         :                      __ffs():
         :                      *
         :                      * Undefined if no bit exists, so code should check against 0 first.
         :                      */
         :                      static __always_inline unsigned long __ffs(unsigned long word)
         :                      {
         :                      return __builtin_ctzl(word);
    9.14 :   ffff800010487e6c:       rbit    x3, x3
    0.00 :   ffff800010487e70:       clz     x3, x3
         :                      _find_next_bit():
         :                      return min(start + __ffs(tmp), nbits);
    0.00 :   ffff800010487e74:       add     x2, x3, x2
    0.00 :   ffff800010487e78:       cmp     x1, x2
    0.00 :   ffff800010487e7c:       csel    x1, x1, x2, ls  // ls = plast
         :                      find_next_zero_bit():
         :                      }
    2.66 :   ffff800010487e80:       mov     x0, x1
    0.00 :   ffff800010487e84:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (40 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001010a8f8 <kthread_should_stop>:
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80001010a8f8:       mrs     x0, sp_el0
         :                      to_kthread():
         :                      current->set_child_tid = (__force void __user *)kthread;
         :                      }
         :
         :                      static inline struct kthread *to_kthread(struct task_struct *k)
         :                      {
         :                      WARN_ON(!(k->flags & PF_KTHREAD));
    0.00 :   ffff80001010a8fc:       ldr     w1, [x0, #44]
    0.00 :   ffff80001010a900:       tbz     w1, #21, ffff80001010a914 <kthread_should_stop+0x1c>
         :                      return (__force void *)k->set_child_tid;
    2.55 :   ffff80001010a904:       ldr     x0, [x0, #1344]
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
   97.45 :   ffff80001010a908:       ldr     x0, [x0]
         :                      kthread_should_stop():
         :                      * and this will return true.  You should then return, and your return
         :                      * value will be passed through to kthread_stop().
         :                      */
         :                      bool kthread_should_stop(void)
         :                      {
         :                      return test_bit(KTHREAD_SHOULD_STOP, &to_kthread(current)->flags);
    0.00 :   ffff80001010a90c:       ubfx    w0, w0, #1, #1
         :                      }
    0.00 :   ffff80001010a910:       ret
         :                      to_kthread():
         :                      WARN_ON(!(k->flags & PF_KTHREAD));
    0.00 :   ffff80001010a914:       brk     #0x800
         :                      return (__force void *)k->set_child_tid;
    0.00 :   ffff80001010a918:       ldr     x0, [x0, #1344]
         :                      test_bit():
    0.00 :   ffff80001010a91c:       ldr     x0, [x0]
         :                      kthread_should_stop():
         :                      return test_bit(KTHREAD_SHOULD_STOP, &to_kthread(current)->flags);
    0.00 :   ffff80001010a920:       ubfx    w0, w0, #1, #1
         :                      }
    0.00 :   ffff80001010a924:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (63 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001045db78 <hctx_lock>:
         :                      hctx_lock():
         :                      srcu_read_unlock(hctx->srcu, srcu_idx);
         :                      }
         :
         :                      static void hctx_lock(struct blk_mq_hw_ctx *hctx, int *srcu_idx)
         :                      __acquires(hctx->srcu)
         :                      {
    0.00 :   ffff80001045db78:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff80001045db7c:       mov     x29, sp
   26.99 :   ffff80001045db80:       str     x19, [sp, #16]
    0.00 :   ffff80001045db84:       mov     x19, x1
         :                      if (!(hctx->flags & BLK_MQ_F_BLOCKING)) {
   14.34 :   ffff80001045db88:       ldr     x1, [x0, #192]
    0.00 :   ffff80001045db8c:       tbz     w1, #5, ffff80001045dba8 <hctx_lock+0x30>
         :                      srcu_read_lock():
         :                      */
         :                      static inline int srcu_read_lock(struct srcu_struct *ssp) __acquires(ssp)
         :                      {
         :                      int retval;
         :
         :                      retval = __srcu_read_lock(ssp);
    0.00 :   ffff80001045db90:       add     x0, x0, #0x240
    0.00 :   ffff80001045db94:       bl      ffff8000101598d8 <__srcu_read_lock>
         :                      hctx_lock():
         :                      /* shut up gcc false positive */
         :                      *srcu_idx = 0;
         :                      rcu_read_lock();
         :                      } else
         :                      *srcu_idx = srcu_read_lock(hctx->srcu);
    0.00 :   ffff80001045db98:       str     w0, [x19]
         :                      }
    0.00 :   ffff80001045db9c:       ldr     x19, [sp, #16]
    0.00 :   ffff80001045dba0:       ldp     x29, x30, [sp], #32
    0.00 :   ffff80001045dba4:       ret
         :                      *srcu_idx = 0;
   14.18 :   ffff80001045dba8:       str     wzr, [x19]
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff80001045dbac:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      hctx_lock():
         :                      }
   19.11 :   ffff80001045dbb0:       ldr     x19, [sp, #16]
   25.39 :   ffff80001045dbb4:       ldp     x29, x30, [sp], #32
    0.00 :   ffff80001045dbb8:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (70 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101d5c38 <mempool_alloc>:
         :                      mempool_alloc():
         :                      * Note: using __GFP_ZERO is not supported.
         :                      *
         :                      * Return: pointer to the allocated element or %NULL on error.
         :                      */
         :                      void *mempool_alloc(mempool_t *pool, gfp_t gfp_mask)
         :                      {
    1.40 :   ffff8000101d5c38:       stp     x29, x30, [sp, #-144]!
         :                      spin_unlock_irqrestore(&pool->lock, flags);
         :                      return NULL;
         :                      }
         :
         :                      /* Let's wait for someone else to return an element to @pool */
         :                      init_wait(&wait);
    0.00 :   ffff8000101d5c3c:       adrp    x2, ffff80001012e000 <dl_cpu_busy+0x200>
         :                      {
    0.00 :   ffff8000101d5c40:       mov     x29, sp
    1.46 :   ffff8000101d5c44:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000101d5c48:       adrp    x20, ffff800011899000 <page_wait_table+0x1500>
   17.14 :   ffff8000101d5c4c:       stp     x21, x22, [sp, #32]
         :                      gfp_temp = gfp_mask & ~(__GFP_DIRECT_RECLAIM|__GFP_IO);
    0.00 :   ffff8000101d5c50:       mov     w19, #0xfffffbbf                // #-1089
         :                      {
    0.00 :   ffff8000101d5c54:       stp     x23, x24, [sp, #48]
    0.00 :   ffff8000101d5c58:       mov     x21, x0
    4.29 :   ffff8000101d5c5c:       stp     x25, x26, [sp, #64]
    0.00 :   ffff8000101d5c60:       add     x3, x20, #0x8c8
    1.38 :   ffff8000101d5c64:       str     x27, [sp, #80]
         :                      gfp_temp = gfp_mask & ~(__GFP_DIRECT_RECLAIM|__GFP_IO);
    0.00 :   ffff8000101d5c68:       and     w19, w1, w19
         :                      gfp_mask |= __GFP_NOWARN;       /* failures are OK */
    0.00 :   ffff8000101d5c6c:       mov     w0, #0x2000                     // #8192
         :                      might_sleep_if(gfp_mask & __GFP_DIRECT_RECLAIM);
    0.00 :   ffff8000101d5c70:       and     w24, w1, #0x400
         :                      gfp_mask |= __GFP_NOWARN;       /* failures are OK */
    9.92 :   ffff8000101d5c74:       movk    w0, #0x9, lsl #16
         :                      init_wait(&wait);
    0.00 :   ffff8000101d5c78:       add     x25, x2, #0xa78
         :                      gfp_mask |= __GFP_NOWARN;       /* failures are OK */
    0.00 :   ffff8000101d5c7c:       orr     w22, w1, w0
         :                      gfp_temp = gfp_mask & ~(__GFP_DIRECT_RECLAIM|__GFP_IO);
    0.00 :   ffff8000101d5c80:       orr     w19, w19, w0
         :                      prepare_to_wait(&pool->wait, &wait, TASK_UNINTERRUPTIBLE);
    0.00 :   ffff8000101d5c84:       add     x26, x21, #0x30
         :                      {
    0.00 :   ffff8000101d5c88:       ldr     x4, [x3]
    9.96 :   ffff8000101d5c8c:       str     x4, [x29, #136]
    0.00 :   ffff8000101d5c90:       mov     x4, #0x0                        // #0
         :                      INIT_LIST_HEAD():
         :                      #define LIST_HEAD(name) \
         :                      struct list_head name = LIST_HEAD_INIT(name)
         :
         :                      static inline void INIT_LIST_HEAD(struct list_head *list)
         :                      {
         :                      WRITE_ONCE(list->next, list);
    0.00 :   ffff8000101d5c94:       add     x27, x29, #0x78
         :                      mempool_alloc():
         :                      element = pool->alloc(gfp_temp, pool->pool_data);
   14.21 :   ffff8000101d5c98:       ldp     x1, x2, [x21, #24]
    0.00 :   ffff8000101d5c9c:       mov     w0, w19
    0.00 :   ffff8000101d5ca0:       blr     x2
    0.00 :   ffff8000101d5ca4:       mov     x23, x0
         :                      if (likely(element != NULL))
    0.00 :   ffff8000101d5ca8:       cbz     x0, ffff8000101d5ce0 <mempool_alloc+0xa8>
         :                      */
         :                      io_schedule_timeout(5*HZ);
         :
         :                      finish_wait(&pool->wait, &wait);
         :                      goto repeat_alloc;
         :                      }
    0.00 :   ffff8000101d5cac:       add     x20, x20, #0x8c8
    0.00 :   ffff8000101d5cb0:       mov     x0, x23
    0.00 :   ffff8000101d5cb4:       ldr     x2, [x29, #136]
   18.78 :   ffff8000101d5cb8:       ldr     x1, [x20]
    0.00 :   ffff8000101d5cbc:       eor     x1, x2, x1
    0.00 :   ffff8000101d5cc0:       cbnz    x1, ffff8000101d5d94 <mempool_alloc+0x15c>
    0.00 :   ffff8000101d5cc4:       ldp     x19, x20, [sp, #16]
    1.43 :   ffff8000101d5cc8:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000101d5ccc:       ldp     x23, x24, [sp, #48]
   20.02 :   ffff8000101d5cd0:       ldp     x25, x26, [sp, #64]
    0.00 :   ffff8000101d5cd4:       ldr     x27, [sp, #80]
    0.00 :   ffff8000101d5cd8:       ldp     x29, x30, [sp], #144
    0.00 :   ffff8000101d5cdc:       ret
    0.00 :   ffff8000101d5ce0:       str     x28, [x29, #88]
         :                      spin_lock_irqsave(&pool->lock, flags);
    0.00 :   ffff8000101d5ce4:       mov     x0, x21
    0.00 :   ffff8000101d5ce8:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
    0.00 :   ffff8000101d5cec:       mov     x28, x0
         :                      if (likely(pool->curr_nr)) {
    0.00 :   ffff8000101d5cf0:       ldr     w0, [x21, #8]
    0.00 :   ffff8000101d5cf4:       cbz     w0, ffff8000101d5d24 <mempool_alloc+0xec>
         :                      remove_element():
         :                      void *element = pool->elements[--pool->curr_nr];
    0.00 :   ffff8000101d5cf8:       sub     w0, w0, #0x1
         :                      mempool_alloc():
         :                      element = remove_element(pool);
    0.00 :   ffff8000101d5cfc:       ldr     x1, [x21, #16]
         :                      remove_element():
         :                      void *element = pool->elements[--pool->curr_nr];
    0.00 :   ffff8000101d5d00:       str     w0, [x21, #8]
    0.00 :   ffff8000101d5d04:       ldr     x23, [x1, w0, sxtw #3]
         :                      BUG_ON(pool->curr_nr < 0);
    0.00 :   ffff8000101d5d08:       tbnz    w0, #31, ffff8000101d5d44 <mempool_alloc+0x10c>
         :                      spin_unlock_irqrestore():
         :                      raw_spin_unlock_irq(&lock->rlock);
         :                      }
         :
         :                      static __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
         :                      {
         :                      raw_spin_unlock_irqrestore(&lock->rlock, flags);
    0.00 :   ffff8000101d5d0c:       mov     x1, x28
    0.00 :   ffff8000101d5d10:       mov     x0, x21
    0.00 :   ffff8000101d5d14:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      mempool_alloc():
         :                      smp_wmb();
    0.00 :   ffff8000101d5d18:       dmb     ishst
         :                      return element;
    0.00 :   ffff8000101d5d1c:       ldr     x28, [x29, #88]
    0.00 :   ffff8000101d5d20:       b       ffff8000101d5cac <mempool_alloc+0x74>
         :                      if (gfp_temp != gfp_mask) {
    0.00 :   ffff8000101d5d24:       cmp     w19, w22
    0.00 :   ffff8000101d5d28:       b.eq    ffff8000101d5d48 <mempool_alloc+0x110>  // b.none
         :                      spin_unlock_irqrestore():
    0.00 :   ffff8000101d5d2c:       mov     x1, x28
    0.00 :   ffff8000101d5d30:       mov     x0, x21
    0.00 :   ffff8000101d5d34:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      mempool_alloc():
         :                      gfp_mask |= __GFP_NOWARN;       /* failures are OK */
    0.00 :   ffff8000101d5d38:       mov     w19, w22
    0.00 :   ffff8000101d5d3c:       ldr     x28, [x29, #88]
    0.00 :   ffff8000101d5d40:       b       ffff8000101d5c98 <mempool_alloc+0x60>
         :                      remove_element():
    0.00 :   ffff8000101d5d44:       bl      ffff8000101d5c30 <remove_element.isra.9.part.10>
         :                      mempool_alloc():
         :                      if (!(gfp_mask & __GFP_DIRECT_RECLAIM)) {
    0.00 :   ffff8000101d5d48:       cbz     w24, ffff8000101d5d9c <mempool_alloc+0x164>
         :                      prepare_to_wait(&pool->wait, &wait, TASK_UNINTERRUPTIBLE);
    0.00 :   ffff8000101d5d4c:       mov     w2, #0x2                        // #2
    0.00 :   ffff8000101d5d50:       add     x1, x29, #0x60
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff8000101d5d54:       mrs     x0, sp_el0
         :                      mempool_alloc():
         :                      init_wait(&wait);
    0.00 :   ffff8000101d5d58:       str     wzr, [x29, #96]
    0.00 :   ffff8000101d5d5c:       stp     x0, x25, [x29, #104]
         :                      prepare_to_wait(&pool->wait, &wait, TASK_UNINTERRUPTIBLE);
    0.00 :   ffff8000101d5d60:       mov     x0, x26
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff8000101d5d64:       str     x27, [x29, #120]
         :                      INIT_LIST_HEAD():
         :                      list->prev = list;
    0.00 :   ffff8000101d5d68:       str     x27, [x29, #128]
         :                      mempool_alloc():
    0.00 :   ffff8000101d5d6c:       bl      ffff80001012e4e8 <prepare_to_wait>
         :                      spin_unlock_irqrestore():
    0.00 :   ffff8000101d5d70:       mov     x1, x28
    0.00 :   ffff8000101d5d74:       mov     x0, x21
    0.00 :   ffff8000101d5d78:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      mempool_alloc():
         :                      io_schedule_timeout(5*HZ);
    0.00 :   ffff8000101d5d7c:       mov     x0, #0x4e2                      // #1250
    0.00 :   ffff8000101d5d80:       bl      ffff800010cad7b0 <io_schedule_timeout>
         :                      finish_wait(&pool->wait, &wait);
    0.00 :   ffff8000101d5d84:       add     x1, x29, #0x60
    0.00 :   ffff8000101d5d88:       mov     x0, x26
    0.00 :   ffff8000101d5d8c:       bl      ffff80001012e670 <finish_wait>
         :                      goto repeat_alloc;
    0.00 :   ffff8000101d5d90:       b       ffff8000101d5d38 <mempool_alloc+0x100>
    0.00 :   ffff8000101d5d94:       str     x28, [x29, #88]
         :                      }
    0.00 :   ffff8000101d5d98:       bl      ffff8000100e5630 <__stack_chk_fail>
         :                      spin_unlock_irqrestore():
    0.00 :   ffff8000101d5d9c:       mov     x1, x28
    0.00 :   ffff8000101d5da0:       mov     x0, x21
    0.00 :   ffff8000101d5da4:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      mempool_alloc():
         :                      return NULL;
    0.00 :   ffff8000101d5da8:       ldr     x28, [x29, #88]
    0.00 :   ffff8000101d5dac:       b       ffff8000101d5cac <mempool_alloc+0x74>
 Percent |	Source code & Disassembly of vmlinux for cycles (33 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001044ff60 <bvec_free>:
         :                      bvec_free():
         :                      return bvec_slabs[--idx].nr_vecs;
         :                      }
         :
         :                      void bvec_free(mempool_t *pool, struct bio_vec *bv, unsigned int idx)
         :                      {
         :                      if (!idx)
   36.33 :   ffff80001044ff60:       cbz     w2, ffff80001044ff9c <bvec_free+0x3c>
         :                      {
    0.00 :   ffff80001044ff64:       stp     x29, x30, [sp, #-16]!
         :                      return;
         :                      idx--;
    0.00 :   ffff80001044ff68:       sub     w2, w2, #0x1
         :
         :                      BIO_BUG_ON(idx >= BVEC_POOL_NR);
    0.00 :   ffff80001044ff6c:       cmp     w2, #0x5
         :                      {
    0.00 :   ffff80001044ff70:       mov     x29, sp
         :                      BIO_BUG_ON(idx >= BVEC_POOL_NR);
    0.00 :   ffff80001044ff74:       b.hi    ffff80001044ffa0 <bvec_free+0x40>  // b.pmore
         :
         :                      if (idx == BVEC_POOL_MAX) {
    0.00 :   ffff80001044ff78:       b.eq    ffff80001044ffa4 <bvec_free+0x44>  // b.none
         :                      mempool_free(bv, pool);
         :                      } else {
         :                      struct biovec_slab *bvs = bvec_slabs + idx;
    0.00 :   ffff80001044ff7c:       adrp    x0, ffff80001189c000 <mm_slots_hash+0x1a28>
    0.00 :   ffff80001044ff80:       add     x0, x0, #0x888
    0.00 :   ffff80001044ff84:       mov     w3, #0x18                       // #24
         :
         :                      kmem_cache_free(bvs->slab, bv);
    0.00 :   ffff80001044ff88:       umaddl  x2, w2, w3, x0
    0.00 :   ffff80001044ff8c:       ldr     x0, [x2, #16]
    0.00 :   ffff80001044ff90:       bl      ffff800010250100 <kmem_cache_free>
         :                      }
         :                      }
    0.00 :   ffff80001044ff94:       ldp     x29, x30, [sp], #16
    0.00 :   ffff80001044ff98:       ret
   63.67 :   ffff80001044ff9c:       ret
         :                      BIO_BUG_ON(idx >= BVEC_POOL_NR);
    0.00 :   ffff80001044ffa0:       brk     #0x800
    0.00 :   ffff80001044ffa4:       mov     x3, x1
         :                      mempool_free(bv, pool);
    0.00 :   ffff80001044ffa8:       mov     x1, x0
    0.00 :   ffff80001044ffac:       mov     x0, x3
    0.00 :   ffff80001044ffb0:       bl      ffff8000101d5b40 <mempool_free>
         :                      }
    0.00 :   ffff80001044ffb4:       ldp     x29, x30, [sp], #16
    0.00 :   ffff80001044ffb8:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (27 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010465f40 <blk_mq_sched_restart>:
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff800010465f40:       ldr     x1, [x0, #24]
         :                      blk_mq_sched_restart():
         :                      }
         :                      EXPORT_SYMBOL_GPL(blk_mq_sched_mark_restart_hctx);
         :
         :                      void blk_mq_sched_restart(struct blk_mq_hw_ctx *hctx)
         :                      {
         :                      if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
    0.00 :   ffff800010465f44:       tst     w1, #0x4
    0.00 :   ffff800010465f48:       b.ne    ffff800010465f50 <blk_mq_sched_restart+0x10>  // b.any
  100.00 :   ffff800010465f4c:       ret
         :                      {
    0.00 :   ffff800010465f50:       stp     x29, x30, [sp, #-16]!
         :                      return;
         :                      clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
    0.00 :   ffff800010465f54:       add     x2, x0, #0x18
         :                      {
    0.00 :   ffff800010465f58:       mov     x29, sp
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010465f5c:       b       ffff800010465f7c <blk_mq_sched_restart+0x3c>
    0.00 :   ffff800010465f60:       b       ffff800010465f7c <blk_mq_sched_restart+0x3c>
         :                      __lse_atomic64_andnot():
         :                      "       " #asm_op "     %[i], %[v]\n"                                   \
         :                      : [i] "+r" (i), [v] "+Q" (v->counter)                           \
         :                      : "r" (v));                                                     \
         :                      }
         :
         :                      ATOMIC64_OP(andnot, stclr)
    0.00 :   ffff800010465f64:       mov     x1, #0x4                        // #4
    0.00 :   ffff800010465f68:       stclr   x1, [x2]
         :                      blk_mq_sched_restart():
         :
         :                      blk_mq_run_hw_queue(hctx, true);
    0.00 :   ffff800010465f6c:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010465f70:       bl      ffff80001045ef88 <blk_mq_run_hw_queue>
         :                      }
    0.00 :   ffff800010465f74:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010465f78:       ret
         :                      __ll_sc_atomic64_andnot():
         :                      /*
         :                      * GAS converts the mysterious and undocumented BIC (immediate) alias to
         :                      * an AND (immediate) instruction with the immediate inverted. We don't
         :                      * have a constraint for this, so fall back to register.
         :                      */
         :                      ATOMIC64_OPS(andnot, bic, )
    0.00 :   ffff800010465f7c:       mov     x1, #0x4                        // #4
    0.00 :   ffff800010465f80:       add     x4, x0, #0x18
    0.00 :   ffff800010465f84:       b       ffff800010466924 <blk_mq_init_sched+0x204>
         :                      blk_mq_sched_restart():
         :                      blk_mq_run_hw_queue(hctx, true);
    0.00 :   ffff800010465f88:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010465f8c:       bl      ffff80001045ef88 <blk_mq_run_hw_queue>
         :                      }
    0.00 :   ffff800010465f90:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010465f94:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (30 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010487db8 <find_next_bit>:
         :                      _find_next_bit():
         :                      const unsigned long *addr2, unsigned long nbits,
         :                      unsigned long start, unsigned long invert)
         :                      {
         :                      unsigned long tmp;
         :
         :                      if (unlikely(start >= nbits))
    6.87 :   ffff800010487db8:       cmp     x1, x2
    0.00 :   ffff800010487dbc:       b.ls    ffff800010487df8 <find_next_bit+0x40>  // b.plast
         :                      return nbits;
         :
         :                      tmp = addr1[start / BITS_PER_LONG];
    3.13 :   ffff800010487dc0:       lsr     x4, x2, #6
         :                      if (addr2)
         :                      tmp &= addr2[start / BITS_PER_LONG];
         :                      tmp ^= invert;
         :
         :                      /* Handle 1st word. */
         :                      tmp &= BITMAP_FIRST_WORD_MASK(start);
    0.00 :   ffff800010487dc4:       mov     x3, #0xffffffffffffffff         // #-1
    0.00 :   ffff800010487dc8:       lsl     x3, x3, x2
         :                      start = round_down(start, BITS_PER_LONG);
    0.00 :   ffff800010487dcc:       and     x2, x2, #0xffffffffffffffc0
         :                      tmp = addr1[start / BITS_PER_LONG];
    3.47 :   ffff800010487dd0:       ldr     x4, [x0, x4, lsl #3]
         :
         :                      while (!tmp) {
   16.77 :   ffff800010487dd4:       ands    x3, x3, x4
    9.17 :   ffff800010487dd8:       b.eq    ffff800010487dec <find_next_bit+0x34>  // b.none
   46.85 :   ffff800010487ddc:       b       ffff800010487e00 <find_next_bit+0x48>
         :                      start += BITS_PER_LONG;
         :                      if (start >= nbits)
         :                      return nbits;
         :
         :                      tmp = addr1[start / BITS_PER_LONG];
    0.00 :   ffff800010487de0:       lsr     x3, x2, #6
    0.00 :   ffff800010487de4:       ldr     x3, [x0, x3, lsl #3]
         :                      while (!tmp) {
    0.00 :   ffff800010487de8:       cbnz    x3, ffff800010487e00 <find_next_bit+0x48>
         :                      start += BITS_PER_LONG;
    0.00 :   ffff800010487dec:       add     x2, x2, #0x40
         :                      if (start >= nbits)
    0.00 :   ffff800010487df0:       cmp     x1, x2
    0.00 :   ffff800010487df4:       b.hi    ffff800010487de0 <find_next_bit+0x28>  // b.pmore
         :                      find_next_bit():
         :                      */
         :                      unsigned long find_next_bit(const unsigned long *addr, unsigned long size,
         :                      unsigned long offset)
         :                      {
         :                      return _find_next_bit(addr, NULL, size, offset, 0UL);
         :                      }
    0.00 :   ffff800010487df8:       mov     x0, x1
    0.00 :   ffff800010487dfc:       ret
         :                      __ffs():
         :                      *
         :                      * Undefined if no bit exists, so code should check against 0 first.
         :                      */
         :                      static __always_inline unsigned long __ffs(unsigned long word)
         :                      {
         :                      return __builtin_ctzl(word);
   10.77 :   ffff800010487e00:       rbit    x3, x3
    0.00 :   ffff800010487e04:       clz     x3, x3
         :                      _find_next_bit():
         :                      return min(start + __ffs(tmp), nbits);
    0.00 :   ffff800010487e08:       add     x2, x3, x2
    0.00 :   ffff800010487e0c:       cmp     x1, x2
    2.97 :   ffff800010487e10:       csel    x1, x1, x2, ls  // ls = plast
         :                      find_next_bit():
         :                      }
    0.00 :   ffff800010487e14:       mov     x0, x1
    0.00 :   ffff800010487e18:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (46 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000102d5908 <aio_setup_rw.isra.26>:
         :                      aio_setup_rw():
         :
         :                      req->ki_flags &= ~IOCB_HIPRI; /* no one is going to poll for this I/O */
         :                      return 0;
         :                      }
         :
         :                      static ssize_t aio_setup_rw(int rw, const struct iocb *iocb,
    2.19 :   ffff8000102d5908:       stp     x29, x30, [sp, #-32]!
         :                      struct iov_iter *iter)
         :                      {
         :                      void __user *buf = (void __user *)(uintptr_t)iocb->aio_buf;
         :                      size_t len = iocb->aio_nbytes;
         :
         :                      if (!vectored) {
    0.00 :   ffff8000102d590c:       tst     w4, #0xff
         :                      static ssize_t aio_setup_rw(int rw, const struct iocb *iocb,
    0.00 :   ffff8000102d5910:       mov     x29, sp
   43.54 :   ffff8000102d5914:       str     x19, [sp, #16]
    0.00 :   ffff8000102d5918:       mov     x19, x3
         :                      if (!vectored) {
    0.00 :   ffff8000102d591c:       b.eq    ffff8000102d5960 <aio_setup_rw.isra.26+0x58>  // b.none
    0.00 :   ffff8000102d5920:       and     w5, w5, #0xff
         :                      ssize_t ret = import_single_range(rw, buf, len, *iovec, iter);
         :                      *iovec = NULL;
         :                      return ret;
         :                      }
         :                      #ifdef CONFIG_COMPAT
         :                      if (compat)
    0.00 :   ffff8000102d5924:       cbnz    w5, ffff8000102d5944 <aio_setup_rw.isra.26+0x3c>
         :                      return compat_import_iovec(rw, buf, len, UIO_FASTIOV, iovec,
         :                      iter);
         :                      #endif
         :                      return import_iovec(rw, buf, len, UIO_FASTIOV, iovec, iter);
    0.00 :   ffff8000102d5928:       mov     x4, x3
    0.00 :   ffff8000102d592c:       mov     x5, x6
    0.00 :   ffff8000102d5930:       mov     w3, #0x8                        // #8
    0.00 :   ffff8000102d5934:       bl      ffff8000104825e0 <import_iovec>
         :                      }
    0.00 :   ffff8000102d5938:       ldr     x19, [sp, #16]
    0.00 :   ffff8000102d593c:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000102d5940:       ret
         :                      return compat_import_iovec(rw, buf, len, UIO_FASTIOV, iovec,
    0.00 :   ffff8000102d5944:       mov     x4, x3
    0.00 :   ffff8000102d5948:       mov     x5, x6
    0.00 :   ffff8000102d594c:       mov     w3, #0x8                        // #8
    0.00 :   ffff8000102d5950:       bl      ffff8000104826b0 <compat_import_iovec>
         :                      }
    0.00 :   ffff8000102d5954:       ldr     x19, [sp, #16]
    0.00 :   ffff8000102d5958:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000102d595c:       ret
         :                      ssize_t ret = import_single_range(rw, buf, len, *iovec, iter);
   49.97 :   ffff8000102d5960:       ldr     x3, [x3]
    0.00 :   ffff8000102d5964:       mov     x4, x6
    0.00 :   ffff8000102d5968:       bl      ffff8000104828b8 <import_single_range>
         :                      *iovec = NULL;
    0.00 :   ffff8000102d596c:       str     xzr, [x19]
         :                      ssize_t ret = import_single_range(rw, buf, len, *iovec, iter);
    0.00 :   ffff8000102d5970:       sxtw    x0, w0
         :                      }
    0.00 :   ffff8000102d5974:       ldr     x19, [sp, #16]
    4.31 :   ffff8000102d5978:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000102d597c:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (27 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010c93b70 <cpumask_next_and>:
         :                      cpumask_next_and():
         :                      *
         :                      * Returns >= nr_cpu_ids if no further cpus set in both.
         :                      */
         :                      int cpumask_next_and(int n, const struct cpumask *src1p,
         :                      const struct cpumask *src2p)
         :                      {
   88.11 :   ffff800010c93b70:       stp     x29, x30, [sp, #-16]!
         :                      /* -1 is a legal arg here. */
         :                      if (n != -1)
         :                      cpumask_check(n);
         :                      return find_next_and_bit(cpumask_bits(src1p), cpumask_bits(src2p),
         :                      nr_cpumask_bits, n + 1);
    0.00 :   ffff800010c93b74:       add     w3, w0, #0x1
         :                      {
    0.00 :   ffff800010c93b78:       mov     x0, x1
    0.00 :   ffff800010c93b7c:       mov     x1, x2
    0.00 :   ffff800010c93b80:       mov     x29, sp
         :                      return find_next_and_bit(cpumask_bits(src1p), cpumask_bits(src2p),
    0.00 :   ffff800010c93b84:       sxtw    x3, w3
    0.00 :   ffff800010c93b88:       mov     x2, #0x100                      // #256
    0.00 :   ffff800010c93b8c:       bl      ffff800010487cf0 <find_next_and_bit>
         :                      }
    7.90 :   ffff800010c93b90:       ldp     x29, x30, [sp], #16
    3.99 :   ffff800010c93b94:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (21 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010115fa0 <sched_ttwu_pending>:
         :                      sched_ttwu_pending():
         :                      return ret;
         :                      }
         :
         :                      #ifdef CONFIG_SMP
         :                      void sched_ttwu_pending(void)
         :                      {
    0.00 :   ffff800010115fa0:       stp     x29, x30, [sp, #-48]!
         :                      __xchg_case_mb_64():
         :                      __XCHG_CASE(w,  , rel_, 32,        ,    ,  ,  , l, "memory")
         :                      __XCHG_CASE( ,  , rel_, 64,        ,    ,  ,  , l, "memory")
         :                      __XCHG_CASE(w, b,  mb_,  8, dmb ish, nop,  , a, l, "memory")
         :                      __XCHG_CASE(w, h,  mb_, 16, dmb ish, nop,  , a, l, "memory")
         :                      __XCHG_CASE(w,  ,  mb_, 32, dmb ish, nop,  , a, l, "memory")
         :                      __XCHG_CASE( ,  ,  mb_, 64, dmb ish, nop,  , a, l, "memory")
    0.00 :   ffff800010115fa4:       mov     x0, #0x0                        // #0
         :                      sched_ttwu_pending():
    0.00 :   ffff800010115fa8:       mov     x29, sp
    0.00 :   ffff800010115fac:       stp     x19, x20, [sp, #16]
         :                      struct rq *rq = this_rq();
    0.00 :   ffff800010115fb0:       adrp    x19, ffff8000114d5000 <tegra_to+0x180>
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    4.72 :   ffff800010115fb4:       mrs     x1, tpidr_el1
         :                      sched_ttwu_pending():
    0.00 :   ffff800010115fb8:       add     x19, x19, #0xd80
    0.00 :   ffff800010115fbc:       add     x19, x19, x1
         :                      __xchg_case_mb_64():
    0.00 :   ffff800010115fc0:       add     x2, x19, #0xba0
    0.00 :   ffff800010115fc4:       prfm    pstl1strm, [x2]
   95.28 :   ffff800010115fc8:       ldxr    x20, [x2]
    0.00 :   ffff800010115fcc:       stlxr   w1, x0, [x2]
    0.00 :   ffff800010115fd0:       cbnz    w1, ffff800010115fc8 <sched_ttwu_pending+0x28>
    0.00 :   ffff800010115fd4:       dmb     ish
         :                      sched_ttwu_pending():
         :                      struct llist_node *llist = llist_del_all(&rq->wake_list);
         :                      struct task_struct *p, *t;
         :                      struct rq_flags rf;
         :
         :                      if (!llist)
    0.00 :   ffff800010115fd8:       cbnz    x20, ffff800010115fe8 <sched_ttwu_pending+0x48>
         :
         :                      llist_for_each_entry_safe(p, t, llist, wake_entry)
         :                      ttwu_do_activate(rq, p, p->sched_remote_wakeup ? WF_MIGRATED : 0, &rf);
         :
         :                      rq_unlock_irqrestore(rq, &rf);
         :                      }
    0.00 :   ffff800010115fdc:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010115fe0:       ldp     x29, x30, [sp], #48
    0.00 :   ffff800010115fe4:       ret
    0.00 :   ffff800010115fe8:       str     x21, [x29, #32]
         :                      rq_lock_irqsave():
         :
         :                      static inline void
         :                      rq_lock_irqsave(struct rq *rq, struct rq_flags *rf)
         :                      __acquires(rq->lock)
         :                      {
         :                      raw_spin_lock_irqsave(&rq->lock, rf->flags);
    0.00 :   ffff800010115fec:       mov     x0, x19
    0.00 :   ffff800010115ff0:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
    0.00 :   ffff800010115ff4:       mov     x21, x0
         :                      update_rq_clock():
         :                      if (rq->clock_update_flags & RQCF_ACT_SKIP)
    0.00 :   ffff800010115ff8:       ldr     w0, [x19, #2392]
    0.00 :   ffff800010115ffc:       tbz     w0, #1, ffff80001011604c <sched_ttwu_pending+0xac>
         :                      sched_ttwu_pending():
         :                      llist_for_each_entry_safe(p, t, llist, wake_entry)
    0.00 :   ffff800010116000:       sub     x1, x20, #0x38
    0.00 :   ffff800010116004:       cbz     x20, ffff800010116030 <sched_ttwu_pending+0x90>
         :                      ttwu_do_activate(rq, p, p->sched_remote_wakeup ? WF_MIGRATED : 0, &rf);
    0.00 :   ffff800010116008:       ldrb    w2, [x1, #1060]
    0.00 :   ffff80001011600c:       mov     x0, x19
         :                      llist_for_each_entry_safe(p, t, llist, wake_entry)
    0.00 :   ffff800010116010:       ldr     x20, [x1, #56]
         :                      ttwu_do_activate(rq, p, p->sched_remote_wakeup ? WF_MIGRATED : 0, &rf);
    0.00 :   ffff800010116014:       lsr     w2, w2, #1
         :                      llist_for_each_entry_safe(p, t, llist, wake_entry)
    0.00 :   ffff800010116018:       sub     x20, x20, #0x38
         :                      ttwu_do_activate(rq, p, p->sched_remote_wakeup ? WF_MIGRATED : 0, &rf);
    0.00 :   ffff80001011601c:       and     w2, w2, #0x4
    0.00 :   ffff800010116020:       bl      ffff800010114bc8 <ttwu_do_activate.isra.95>
    0.00 :   ffff800010116024:       mov     x1, x20
         :                      llist_for_each_entry_safe(p, t, llist, wake_entry)
    0.00 :   ffff800010116028:       cmn     x20, #0x38
    0.00 :   ffff80001011602c:       b.ne    ffff800010116008 <sched_ttwu_pending+0x68>  // b.any
         :                      rq_unlock_irqrestore():
         :                      static inline void
         :                      rq_unlock_irqrestore(struct rq *rq, struct rq_flags *rf)
         :                      __releases(rq->lock)
         :                      {
         :                      rq_unpin_lock(rq, rf);
         :                      raw_spin_unlock_irqrestore(&rq->lock, rf->flags);
    0.00 :   ffff800010116030:       mov     x1, x21
    0.00 :   ffff800010116034:       mov     x0, x19
    0.00 :   ffff800010116038:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
    0.00 :   ffff80001011603c:       ldr     x21, [x29, #32]
         :                      sched_ttwu_pending():
         :                      }
    0.00 :   ffff800010116040:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010116044:       ldp     x29, x30, [sp], #48
    0.00 :   ffff800010116048:       ret
         :                      update_rq_clock():
    0.00 :   ffff80001011604c:       mov     x0, x19
    0.00 :   ffff800010116050:       bl      ffff800010112988 <update_rq_clock.part.89>
    0.00 :   ffff800010116054:       b       ffff800010116000 <sched_ttwu_pending+0x60>
 Percent |	Source code & Disassembly of vmlinux for cycles (29 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000104694e8 <part_inc_in_flight>:
         :                      part_inc_in_flight():
         :                      static void disk_del_events(struct gendisk *disk);
         :                      static void disk_release_events(struct gendisk *disk);
         :
         :                      void part_inc_in_flight(struct request_queue *q, struct hd_struct *part, int rw)
         :                      {
         :                      if (queue_is_mq(q))
    0.00 :   ffff8000104694e8:       ldr     x0, [x0, #48]
    0.00 :   ffff8000104694ec:       cbz     x0, ffff8000104694f4 <part_inc_in_flight+0xc>
         :                      return;
         :
         :                      part_stat_local_inc(part, in_flight[rw]);
         :                      if (part->partno)
         :                      part_stat_local_inc(&part_to_disk(part)->part0, in_flight[rw]);
         :                      }
  100.00 :   ffff8000104694f0:       ret
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000104694f4:       mrs     x5, tpidr_el1
         :                      part_inc_in_flight():
         :                      part_stat_local_inc(part, in_flight[rw]);
    0.00 :   ffff8000104694f8:       adrp    x3, ffff8000114ca000 <bp_hardening_data>
    0.00 :   ffff8000104694fc:       add     x0, x3, #0x18
    0.00 :   ffff800010469500:       ldrsw   x6, [x0, x5]
    0.00 :   ffff800010469504:       adrp    x4, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010469508:       add     x5, x4, #0x8e8
    0.00 :   ffff80001046950c:       sxtw    x2, w2
    0.00 :   ffff800010469510:       add     x2, x2, #0x12
    0.00 :   ffff800010469514:       ldr     x0, [x1, #840]
    0.00 :   ffff800010469518:       ldr     x5, [x5, x6, lsl #3]
    0.00 :   ffff80001046951c:       lsl     x2, x2, #3
    0.00 :   ffff800010469520:       add     x0, x0, x5
    0.00 :   ffff800010469524:       add     x0, x0, x2
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010469528:       b       ffff800010469588 <part_inc_in_flight+0xa0>
    0.00 :   ffff80001046952c:       b       ffff800010469588 <part_inc_in_flight+0xa0>
         :                      __lse_atomic64_add():
         :                      }
         :
         :                      ATOMIC64_OP(andnot, stclr)
         :                      ATOMIC64_OP(or, stset)
         :                      ATOMIC64_OP(xor, steor)
         :                      ATOMIC64_OP(add, stadd)
    0.00 :   ffff800010469530:       mov     x5, #0x1                        // #1
    0.00 :   ffff800010469534:       stadd   x5, [x0]
         :                      part_inc_in_flight():
         :                      if (part->partno)
    0.00 :   ffff800010469538:       ldr     w0, [x1, #820]
    0.00 :   ffff80001046953c:       cbz     w0, ffff8000104694f0 <part_inc_in_flight+0x8>
         :                      part_to_disk():
         :                      struct lockdep_map lockdep_map;
         :                      };
         :
         :                      static inline struct gendisk *part_to_disk(struct hd_struct *part)
         :                      {
         :                      if (likely(part)) {
    0.00 :   ffff800010469540:       cbz     x1, ffff800010469590 <part_inc_in_flight+0xa8>
         :                      if (part->partno)
         :                      return dev_to_disk(part_to_dev(part)->parent);
    0.00 :   ffff800010469544:       ldr     x0, [x1, #104]
    0.00 :   ffff800010469548:       sub     x0, x0, #0x70
         :                      part_inc_in_flight():
         :                      part_stat_local_inc(&part_to_disk(part)->part0, in_flight[rw]);
    0.00 :   ffff80001046954c:       add     x3, x3, #0x18
    0.00 :   ffff800010469550:       add     x4, x4, #0x8e8
         :                      __my_cpu_offset():
    0.00 :   ffff800010469554:       mrs     x1, tpidr_el1
         :                      part_inc_in_flight():
    0.00 :   ffff800010469558:       ldrsw   x1, [x3, x1]
    0.00 :   ffff80001046955c:       ldr     x0, [x0, #912]
    0.00 :   ffff800010469560:       ldr     x1, [x4, x1, lsl #3]
    0.00 :   ffff800010469564:       add     x0, x0, x1
    0.00 :   ffff800010469568:       add     x2, x0, x2
         :                      arch_static_branch_jump():
    0.00 :   ffff80001046956c:       b       ffff800010469580 <part_inc_in_flight+0x98>
    0.00 :   ffff800010469570:       b       ffff800010469580 <part_inc_in_flight+0x98>
         :                      __lse_atomic64_add():
    0.00 :   ffff800010469574:       mov     x0, #0x1                        // #1
    0.00 :   ffff800010469578:       stadd   x0, [x2]
         :                      part_inc_in_flight():
         :                      }
    0.00 :   ffff80001046957c:       ret
         :                      __ll_sc_atomic64_add():
         :                      ATOMIC64_FETCH_OP (, dmb ish,  , l, "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(add, add, I)
    0.00 :   ffff800010469580:       b       ffff80001046add4 <disk_clear_events+0x134>
         :                      part_inc_in_flight():
    0.00 :   ffff800010469584:       ret
         :                      __ll_sc_atomic64_add():
    0.00 :   ffff800010469588:       b       ffff80001046adec <disk_clear_events+0x14c>
    0.00 :   ffff80001046958c:       b       ffff800010469538 <part_inc_in_flight+0x50>
         :                      part_to_disk():
         :                      else
         :                      return dev_to_disk(part_to_dev(part));
         :                      }
         :                      return NULL;
    0.00 :   ffff800010469590:       mov     x0, #0x0                        // #0
    0.00 :   ffff800010469594:       b       ffff80001046954c <part_inc_in_flight+0x64>
 Percent |	Source code & Disassembly of vmlinux for cycles (22 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010216478 <change_protection>:
         :                      change_protection():
         :                      }
         :
         :                      unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,
         :                      unsigned long end, pgprot_t newprot,
         :                      int dirty_accountable, int prot_numa)
         :                      {
    0.00 :   ffff800010216478:       stp     x29, x30, [sp, #-304]!
    0.00 :   ffff80001021647c:       adrp    x6, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010216480:       add     x6, x6, #0x8c8
    0.00 :   ffff800010216484:       mov     x29, sp
    0.00 :   ffff800010216488:       ldr     x7, [x6]
    0.00 :   ffff80001021648c:       str     x7, [x29, #296]
    0.00 :   ffff800010216490:       mov     x7, #0x0                        // #0
         :                      is_vm_hugetlb_page():
         :
         :                      #include <linux/mm.h>
         :
         :                      static inline bool is_vm_hugetlb_page(struct vm_area_struct *vma)
         :                      {
         :                      return !!(vma->vm_flags & VM_HUGETLB);
    0.00 :   ffff800010216494:       ldr     x6, [x0, #80]
         :                      change_protection():
    0.00 :   ffff800010216498:       str     x1, [x29, #128]
    0.00 :   ffff80001021649c:       str     x2, [x29, #144]
    0.00 :   ffff8000102164a0:       str     x3, [x29, #168]
    0.00 :   ffff8000102164a4:       str     w4, [x29, #188]
         :                      unsigned long pages;
         :
         :                      if (is_vm_hugetlb_page(vma))
    0.00 :   ffff8000102164a8:       tbnz    w6, #22, ffff800010216bf0 <change_protection+0x778>
    0.00 :   ffff8000102164ac:       str     x28, [x29, #88]
    0.00 :   ffff8000102164b0:       mov     x28, x0
         :                      change_protection_range():
         :                      BUG_ON(addr >= end);
    0.00 :   ffff8000102164b4:       ldr     x1, [x29, #144]
         :                      struct mm_struct *mm = vma->vm_mm;
    0.00 :   ffff8000102164b8:       ldr     x0, [x0, #64]
    0.00 :   ffff8000102164bc:       stp     x19, x20, [x29, #16]
    0.00 :   ffff8000102164c0:       stp     x21, x22, [x29, #32]
    0.00 :   ffff8000102164c4:       stp     x23, x24, [x29, #48]
    0.00 :   ffff8000102164c8:       stp     x25, x26, [x29, #64]
    0.00 :   ffff8000102164cc:       str     x27, [x29, #80]
    0.00 :   ffff8000102164d0:       str     x0, [x29, #120]
         :                      BUG_ON(addr >= end);
    0.00 :   ffff8000102164d4:       ldr     x0, [x29, #128]
    0.00 :   ffff8000102164d8:       cmp     x0, x1
    0.00 :   ffff8000102164dc:       b.cs    ffff800010216c80 <change_protection+0x808>  // b.hs, b.nlast
         :                      pgd = pgd_offset(mm, addr);
    0.00 :   ffff8000102164e0:       ldp     x2, x0, [x29, #120]
    0.00 :   ffff8000102164e4:       mov     w26, w5
         :                      inc_tlb_flush_pending():
         :                      atomic_set(&mm->tlb_flush_pending, 0);
         :                      }
         :
         :                      static inline void inc_tlb_flush_pending(struct mm_struct *mm)
         :                      {
         :                      atomic_inc(&mm->tlb_flush_pending);
    0.00 :   ffff8000102164e8:       add     x3, x2, #0x34c
    0.00 :   ffff8000102164ec:       str     x3, [x29, #112]
         :                      change_protection_range():
    0.00 :   ffff8000102164f0:       ldr     x1, [x2, #64]
    0.00 :   ffff8000102164f4:       ubfx    x0, x0, #39, #9
    0.00 :   ffff8000102164f8:       add     x0, x1, x0, lsl #3
    0.00 :   ffff8000102164fc:       str     x0, [x29, #160]
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010216500:       b       ffff800010216bcc <change_protection+0x754>
    0.00 :   ffff800010216504:       b       ffff800010216bcc <change_protection+0x754>
         :                      __lse_atomic_add():
         :                      }
         :
         :                      ATOMIC_OP(andnot, stclr)
         :                      ATOMIC_OP(or, stset)
         :                      ATOMIC_OP(xor, steor)
         :                      ATOMIC_OP(add, stadd)
    0.00 :   ffff800010216508:       mov     w0, #0x1                        // #1
    0.00 :   ffff80001021650c:       stadd   w0, [x3]
    0.00 :   ffff800010216510:       ldr     x1, [x29, #144]
         :                      pte_modify():
         :                      const pteval_t mask = PTE_USER | PTE_PXN | PTE_UXN | PTE_RDONLY |
         :                      PTE_PROT_NONE | PTE_VALID | PTE_WRITE;
         :                      /* preserve the hardware dirty information */
         :                      if (pte_hw_dirty(pte))
         :                      pte = pte_mkdirty(pte);
         :                      pte_val(pte) = (pte_val(pte) & ~mask) | (pgprot_val(newprot) & mask);
    0.00 :   ffff800010216514:       mov     x0, #0xc1                       // #193
         :                      change_protection():
         :                      {
    0.00 :   ffff800010216518:       ldr     x25, [x29, #128]
    0.00 :   ffff80001021651c:       mov     x27, x28
    0.00 :   ffff800010216520:       sub     x1, x1, #0x1
    0.00 :   ffff800010216524:       str     x1, [x29, #136]
         :                      pte_modify():
    0.00 :   ffff800010216528:       ldr     x1, [x29, #168]
         :                      change_protection():
    0.00 :   ffff80001021652c:       mov     x28, x25
         :                      pte_modify():
    0.00 :   ffff800010216530:       movk    x0, #0x468, lsl #48
         :                      change_protection():
    0.00 :   ffff800010216534:       str     xzr, [x29, #152]
         :                      pte_modify():
    0.00 :   ffff800010216538:       and     x0, x1, x0
    0.00 :   ffff80001021653c:       str     x0, [x29, #176]
         :                      change_protection_range():
         :                      next = pgd_addr_end(addr, end);
    0.00 :   ffff800010216540:       mov     x22, #0x8000000000              // #549755813888
    0.00 :   ffff800010216544:       ldr     x0, [x29, #160]
    0.00 :   ffff800010216548:       add     x22, x28, x22
    0.00 :   ffff80001021654c:       ldr     x2, [x29, #136]
    0.00 :   ffff800010216550:       and     x1, x22, #0xffffff8000000000
    0.00 :   ffff800010216554:       str     x1, [x29, #240]
    0.00 :   ffff800010216558:       sub     x1, x1, #0x1
    0.00 :   ffff80001021655c:       ldr     x0, [x0]
    0.00 :   ffff800010216560:       cmp     x1, x2
    0.00 :   ffff800010216564:       b.cs    ffff800010216b98 <change_protection+0x720>  // b.hs, b.nlast
         :                      pgd_none_or_clear_bad():
         :
         :                      void pmd_clear_bad(pmd_t *);
         :
         :                      static inline int pgd_none_or_clear_bad(pgd_t *pgd)
         :                      {
         :                      if (pgd_none(*pgd))
    0.00 :   ffff800010216568:       cbz     x0, ffff80001021687c <change_protection+0x404>
         :                      return 1;
         :                      if (unlikely(pgd_bad(*pgd))) {
    0.00 :   ffff80001021656c:       tbz     w0, #1, ffff800010216ba8 <change_protection+0x730>
    0.00 :   ffff800010216570:       ldr     x1, [x29, #240]
         :                      change_pud_range():
         :                      pud = pud_offset(p4d, addr);
    0.00 :   ffff800010216574:       ubfx    x21, x28, #30, #9
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010216578:       ldr     x0, [x29, #160]
         :                      change_pud_range():
    0.00 :   ffff80001021657c:       mov     x25, x27
    0.00 :   ffff800010216580:       sub     x1, x1, #0x1
    0.00 :   ffff800010216584:       str     x1, [x29, #192]
    0.00 :   ffff800010216588:       adrp    x1, ffff8000112ae000 <cpu_ops+0x248>
    0.00 :   ffff80001021658c:       mov     x27, x28
         :                      __read_once_size():
    0.00 :   ffff800010216590:       ldr     x0, [x0]
         :                      change_pud_range():
    0.00 :   ffff800010216594:       ldr     x1, [x1, #1872]
         :                      pgd_page_paddr():
         :                      return __pgd_to_phys(pgd);
    0.00 :   ffff800010216598:       and     x0, x0, #0xfffffffff000
         :                      change_pud_range():
         :                      unsigned long pages = 0;
    0.00 :   ffff80001021659c:       str     xzr, [x29, #224]
         :                      pud = pud_offset(p4d, addr);
    0.00 :   ffff8000102165a0:       sub     x0, x0, x1
    0.00 :   ffff8000102165a4:       add     x21, x0, x21, lsl #3
         :                      next = pud_addr_end(addr, end);
    0.00 :   ffff8000102165a8:       mov     x8, #0x40000000                 // #1073741824
    0.00 :   ffff8000102165ac:       add     x8, x27, x8
    0.00 :   ffff8000102165b0:       and     x28, x8, #0xffffffffc0000000
    0.00 :   ffff8000102165b4:       ldr     x2, [x29, #192]
    0.00 :   ffff8000102165b8:       sub     x1, x28, #0x1
    0.00 :   ffff8000102165bc:       ldr     x0, [x21]
    0.00 :   ffff8000102165c0:       cmp     x1, x2
    0.00 :   ffff8000102165c4:       b.cs    ffff800010216ac4 <change_protection+0x64c>  // b.hs, b.nlast
         :                      pud_none_or_clear_bad():
         :                      return 0;
         :                      }
         :
         :                      static inline int pud_none_or_clear_bad(pud_t *pud)
         :                      {
         :                      if (pud_none(*pud))
    0.00 :   ffff8000102165c8:       cbz     x0, ffff800010216848 <change_protection+0x3d0>
         :                      return 1;
         :                      if (unlikely(pud_bad(*pud))) {
    0.00 :   ffff8000102165cc:       tbz     w0, #1, ffff800010216ad0 <change_protection+0x658>
         :                      change_pmd_range():
         :                      pmd = pmd_offset(pud, addr);
    0.00 :   ffff8000102165d0:       adrp    x0, ffff8000112ae000 <cpu_ops+0x248>
         :                      range.start = 0;
    0.00 :   ffff8000102165d4:       str     xzr, [x29, #272]
         :                      pmd = pmd_offset(pud, addr);
    0.00 :   ffff8000102165d8:       ubfx    x7, x27, #21, #9
    0.00 :   ffff8000102165dc:       sub     x20, x28, #0x1
    0.00 :   ffff8000102165e0:       ldr     x1, [x0, #1872]
    0.00 :   ffff8000102165e4:       mov     x2, #0x0                        // #0
         :                      __read_once_size():
    0.00 :   ffff8000102165e8:       ldr     x0, [x21]
         :                      change_pmd_range():
    0.00 :   ffff8000102165ec:       str     x21, [x29, #216]
         :                      pud_page_paddr():
         :                      return __pud_to_phys(pud);
    0.00 :   ffff8000102165f0:       and     x0, x0, #0xfffffffff000
         :                      change_pmd_range():
         :                      unsigned long nr_huge_updates = 0;
    0.00 :   ffff8000102165f4:       str     xzr, [x29, #232]
         :                      pmd = pmd_offset(pud, addr);
    0.00 :   ffff8000102165f8:       sub     x0, x0, x1
         :                      unsigned long pages = 0;
    0.00 :   ffff8000102165fc:       str     xzr, [x29, #248]
         :                      pmd = pmd_offset(pud, addr);
    0.00 :   ffff800010216600:       add     x24, x0, x7, lsl #3
    0.00 :   ffff800010216604:       mov     x0, x25
    0.00 :   ffff800010216608:       mov     x25, x27
    0.00 :   ffff80001021660c:       mov     x27, x0
    0.00 :   ffff800010216610:       b       ffff800010216628 <change_protection+0x1b0>
         :                      is_swap_pmd():
         :                      extern spinlock_t *__pud_trans_huge_lock(pud_t *pud,
         :                      struct vm_area_struct *vma);
         :
         :                      static inline int is_swap_pmd(pmd_t pmd)
         :                      {
         :                      return !pmd_none(pmd) && !pmd_present(pmd);
    0.00 :   ffff800010216614:       cbnz    x0, ffff800010216648 <change_protection+0x1d0>
    0.00 :   ffff800010216618:       mov     x25, x21
         :                      change_pmd_range():
         :                      } while (pmd++, addr = next, addr != end);
    0.00 :   ffff80001021661c:       add     x24, x24, #0x8
    0.00 :   ffff800010216620:       cmp     x21, x28
    0.00 :   ffff800010216624:       b.eq    ffff800010216a8c <change_protection+0x614>  // b.none
         :                      next = pmd_addr_end(addr, end);
    0.00 :   ffff800010216628:       add     x5, x25, #0x200, lsl #12
    0.00 :   ffff80001021662c:       and     x21, x5, #0xffffffffffe00000
    0.00 :   ffff800010216630:       sub     x0, x21, #0x1
    0.00 :   ffff800010216634:       cmp     x0, x20
    4.63 :   ffff800010216638:       ldr     x0, [x24]
    0.00 :   ffff80001021663c:       b.cc    ffff800010216614 <change_protection+0x19c>  // b.lo, b.ul, b.last
         :                      is_swap_pmd():
    0.00 :   ffff800010216640:       cbz     x0, ffff800010216a8c <change_protection+0x614>
    0.00 :   ffff800010216644:       mov     x21, x28
    0.00 :   ffff800010216648:       and     x1, x0, #0x7ffffffffffffff
    0.00 :   ffff80001021664c:       and     x1, x1, #0xfc00000000000001
    0.00 :   ffff800010216650:       cbnz    x1, ffff800010216924 <change_protection+0x4ac>
         :                      change_pmd_range():
         :                      if (!range.start) {
    0.00 :   ffff800010216654:       cbz     x2, ffff800010216934 <change_protection+0x4bc>
         :                      if (next - addr != HPAGE_PMD_SIZE) {
    0.00 :   ffff800010216658:       sub     x0, x21, x25
    0.00 :   ffff80001021665c:       cmp     x0, #0x200, lsl #12
    0.00 :   ffff800010216660:       b.eq    ffff80001021698c <change_protection+0x514>  // b.none
         :                      __split_huge_pmd(vma, pmd, addr, false, NULL);
    0.00 :   ffff800010216664:       mov     x0, x27
    0.00 :   ffff800010216668:       mov     x4, #0x0                        // #0
    0.00 :   ffff80001021666c:       mov     w3, #0x0                        // #0
    0.00 :   ffff800010216670:       mov     x2, x25
    0.00 :   ffff800010216674:       mov     x1, x24
    0.00 :   ffff800010216678:       bl      ffff80001025c228 <__split_huge_pmd>
    0.00 :   ffff80001021667c:       ldr     x0, [x24]
         :                      pmd_none_or_trans_huge_or_clear_bad():
         :                      *
         :                      * pmd_none() is preseved for future condition checks on pmd migration
         :                      * entries and not confusing with this function name, although it is
         :                      * redundant with !pmd_present().
         :                      */
         :                      if (pmd_none(pmdval) || pmd_trans_huge(pmdval) ||
    0.00 :   ffff800010216680:       cbz     x0, ffff800010216688 <change_protection+0x210>
    0.00 :   ffff800010216684:       tbnz    w0, #1, ffff800010216690 <change_protection+0x218>
    0.00 :   ffff800010216688:       ldr     x2, [x29, #272]
    0.00 :   ffff80001021668c:       b       ffff800010216618 <change_protection+0x1a0>
         :                      change_pte_range():
         :                      pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
    0.00 :   ffff800010216690:       adrp    x1, ffff8000112ae000 <cpu_ops+0x248>
         :                      __read_once_size():
    0.00 :   ffff800010216694:       ldr     x0, [x24]
         :                      pte_lockptr():
         :                      }
         :                      #endif /* ALLOC_SPLIT_PTLOCKS */
         :
         :                      static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
         :                      {
         :                      return ptlock_ptr(pmd_page(*pmd));
    0.00 :   ffff800010216698:       ldr     x22, [x24]
         :                      change_pte_range():
    0.00 :   ffff80001021669c:       ubfx    x3, x25, #12, #9
    0.00 :   ffff8000102166a0:       ldr     x2, [x1, #1872]
         :                      pte_lockptr():
    0.00 :   ffff8000102166a4:       adrp    x1, ffff8000112ae000 <cpu_ops+0x248>
         :                      pmd_page_paddr():
         :                      return __pmd_to_phys(pmd);
    0.00 :   ffff8000102166a8:       and     x0, x0, #0xfffffffff000
         :                      change_pte_range():
    0.00 :   ffff8000102166ac:       sub     x0, x0, x2
         :                      pte_lockptr():
    0.00 :   ffff8000102166b0:       ldr     x1, [x1, #1880]
         :                      change_pte_range():
    0.00 :   ffff8000102166b4:       add     x3, x0, x3, lsl #3
         :                      pte_lockptr():
    0.00 :   ffff8000102166b8:       ubfx    x22, x22, #12, #36
    0.00 :   ffff8000102166bc:       add     x22, x1, x22, lsl #6
         :                      spin_lock():
         :                      raw_spin_lock_init(&(_lock)->rlock);            \
         :                      } while (0)
         :
         :                      static __always_inline void spin_lock(spinlock_t *lock)
         :                      {
         :                      raw_spin_lock(&lock->rlock);
    0.00 :   ffff8000102166c0:       add     x0, x22, #0x28
    0.00 :   ffff8000102166c4:       stp     x3, x0, [x29, #200]
    0.00 :   ffff8000102166c8:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      change_pte_range():
         :                      int target_node = NUMA_NO_NODE;
    0.00 :   ffff8000102166cc:       mov     w0, #0xffffffff                 // #-1
    0.00 :   ffff8000102166d0:       str     w0, [x29, #184]
         :                      if (prot_numa && !(vma->vm_flags & VM_SHARED) &&
    0.00 :   ffff8000102166d4:       ldr     x3, [x29, #200]
    0.00 :   ffff8000102166d8:       cbz     w26, ffff800010216708 <change_protection+0x290>
    0.00 :   ffff8000102166dc:       ldr     x0, [x27, #80]
    0.00 :   ffff8000102166e0:       tbnz    w0, #3, ffff800010216708 <change_protection+0x290>
         :                      atomic_read(&vma->vm_mm->mm_users) == 1)
    0.00 :   ffff8000102166e4:       ldr     x0, [x27, #64]
         :                      __read_once_size():
    0.00 :   ffff8000102166e8:       ldr     w0, [x0, #76]
         :                      change_pte_range():
         :                      if (prot_numa && !(vma->vm_flags & VM_SHARED) &&
    0.00 :   ffff8000102166ec:       cmp     w0, #0x1
    0.00 :   ffff8000102166f0:       b.ne    ffff800010216708 <change_protection+0x290>  // b.any
         :                      numa_node_id():
         :
         :                      #ifndef numa_node_id
         :                      /* Returns the number of the current Node. */
         :                      static inline int numa_node_id(void)
         :                      {
         :                      return raw_cpu_read(numa_node);
    0.00 :   ffff8000102166f4:       adrp    x0, ffff8000114d3000 <pmu_sb_events>
    0.00 :   ffff8000102166f8:       add     x0, x0, #0xc58
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000102166fc:       mrs     x1, tpidr_el1
         :                      numa_node_id():
    0.00 :   ffff800010216700:       ldr     w0, [x0, x1]
    0.00 :   ffff800010216704:       str     w0, [x29, #184]
         :                      change_pte_range():
         :                      unsigned long pages = 0;
    0.00 :   ffff800010216708:       mov     x23, #0x0                       // #0
         :                      if (pte_present(oldpte)) {
    0.00 :   ffff80001021670c:       mov     x22, #0x1                       // #1
         :                      pte_modify():
         :                      pte_val(pte) = (pte_val(pte) & ~mask) | (pgprot_val(newprot) & mask);
    0.00 :   ffff800010216710:       str     x20, [x29, #200]
    0.00 :   ffff800010216714:       mov     x20, x27
    0.00 :   ffff800010216718:       mov     w27, w26
    0.00 :   ffff80001021671c:       mov     x26, x23
    0.00 :   ffff800010216720:       mov     x23, x21
    0.00 :   ffff800010216724:       mov     x21, x25
    0.00 :   ffff800010216728:       mov     x25, x3
         :                      change_pte_range():
    0.00 :   ffff80001021672c:       movk    x22, #0x400, lsl #48
         :                      oldpte = *pte;
    0.00 :   ffff800010216730:       ldr     x19, [x25]
         :                      if (pte_present(oldpte)) {
    0.00 :   ffff800010216734:       ands    x0, x19, x22
    0.00 :   ffff800010216738:       b.eq    ffff8000102168f8 <change_protection+0x480>  // b.none
         :                      bool preserve_write = prot_numa && pte_write(oldpte);
    0.00 :   ffff80001021673c:       cbnz    w27, ffff8000102169b0 <change_protection+0x538>
    0.00 :   ffff800010216740:       mov     w19, #0x0                       // #0
         :                      __xchg_case_64():
         :                      }
         :
         :                      __XCHG_CASE(w, b,     ,  8,        ,    ,  ,  ,  ,         )
         :                      __XCHG_CASE(w, h,     , 16,        ,    ,  ,  ,  ,         )
         :                      __XCHG_CASE(w,  ,     , 32,        ,    ,  ,  ,  ,         )
         :                      __XCHG_CASE( ,  ,     , 64,        ,    ,  ,  ,  ,         )
    0.00 :   ffff800010216744:       mov     x1, #0x0                        // #0
    0.00 :   ffff800010216748:       prfm    pstl1strm, [x25]
    0.00 :   ffff80001021674c:       ldxr    x0, [x25]
    0.00 :   ffff800010216750:       stxr    w2, x1, [x25]
    0.00 :   ffff800010216754:       cbnz    w2, ffff80001021674c <change_protection+0x2d4>
         :                      pte_modify():
         :                      if (pte_hw_dirty(pte))
    0.00 :   ffff800010216758:       mov     x2, #0x80                       // #128
    0.00 :   ffff80001021675c:       mov     x1, #0x8000000000000            // #2251799813685248
    0.00 :   ffff800010216760:       movk    x2, #0x8, lsl #48
    0.00 :   ffff800010216764:       and     x2, x0, x2
    0.00 :   ffff800010216768:       cmp     x2, x1
    0.00 :   ffff80001021676c:       b.ne    ffff800010216784 <change_protection+0x30c>  // b.any
         :                      set_pte_bit():
         :                      pte_val(pte) |= pgprot_val(prot);
    0.00 :   ffff800010216770:       and     x1, x0, #0xffffffffffffff7f
    0.00 :   ffff800010216774:       tst     x0, #0x8000000000000
    0.00 :   ffff800010216778:       orr     x1, x1, #0x80000000000000
    0.00 :   ffff80001021677c:       orr     x0, x0, #0x80000000000000
    0.00 :   ffff800010216780:       csel    x0, x1, x0, ne  // ne = any
         :                      pte_modify():
         :                      pte_val(pte) = (pte_val(pte) & ~mask) | (pgprot_val(newprot) & mask);
    0.00 :   ffff800010216784:       mov     x1, #0xffffffffffffff3e         // #-194
         :                      clear_pte_bit():
         :                      pte_val(pte) &= ~pgprot_val(prot);
    0.00 :   ffff800010216788:       cmp     w19, #0x0
         :                      pte_modify():
         :                      pte_val(pte) = (pte_val(pte) & ~mask) | (pgprot_val(newprot) & mask);
    0.00 :   ffff80001021678c:       movk    x1, #0xfb97, lsl #48
    0.00 :   ffff800010216790:       and     x19, x0, x1
    0.00 :   ffff800010216794:       ldr     x0, [x29, #176]
    0.00 :   ffff800010216798:       orr     x19, x19, x0
         :                      clear_pte_bit():
         :                      pte_val(pte) &= ~pgprot_val(prot);
    0.00 :   ffff80001021679c:       and     x0, x19, #0xffffffffffffff7f
    0.00 :   ffff8000102167a0:       orr     x0, x0, #0x8000000000000
    0.00 :   ffff8000102167a4:       csel    x19, x19, x0, eq  // eq = none
         :                      change_pte_range():
         :                      if (dirty_accountable && pte_dirty(ptent) &&
    0.00 :   ffff8000102167a8:       ldr     w0, [x29, #188]
    0.00 :   ffff8000102167ac:       cbz     w0, ffff8000102167d0 <change_protection+0x358>
    0.00 :   ffff8000102167b0:       tbnz    x19, #55, ffff800010216b08 <change_protection+0x690>
    0.00 :   ffff8000102167b4:       mov     x0, #0x80                       // #128
    0.00 :   ffff8000102167b8:       movk    x0, #0x8, lsl #48
    0.00 :   ffff8000102167bc:       and     x1, x19, x0
    0.00 :   ffff8000102167c0:       mov     x0, #0x8000000000000            // #2251799813685248
    0.00 :   ffff8000102167c4:       cmp     x1, x0
    0.00 :   ffff8000102167c8:       b.eq    ffff800010216b08 <change_protection+0x690>  // b.none
    0.00 :   ffff8000102167cc:       nop
         :                      set_pte_at():
         :                      if (pte_present(pte) && pte_user_exec(pte) && !pte_special(pte))
    0.00 :   ffff8000102167d0:       tst     x19, x22
    0.00 :   ffff8000102167d4:       b.eq    ffff8000102167e4 <change_protection+0x36c>  // b.none
    0.00 :   ffff8000102167d8:       mov     x0, #0x140000000000000          // #90071992547409920
    0.00 :   ffff8000102167dc:       tst     x19, x0
    0.00 :   ffff8000102167e0:       b.eq    ffff800010216b8c <change_protection+0x714>  // b.none
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff8000102167e4:       str     x19, [x25]
         :                      set_pte():
         :                      if (pte_valid_not_user(pte)) {
    0.00 :   ffff8000102167e8:       mov     x0, #0x41                       // #65
    0.00 :   ffff8000102167ec:       and     x19, x19, x0
    0.00 :   ffff8000102167f0:       cmp     x19, #0x1
    0.00 :   ffff8000102167f4:       b.eq    ffff80001021683c <change_protection+0x3c4>  // b.none
         :                      change_pte_range():
         :                      pages++;
    0.00 :   ffff8000102167f8:       add     x26, x26, #0x1
         :                      } while (pte++, addr += PAGE_SIZE, addr != end);
    0.00 :   ffff8000102167fc:       add     x21, x21, #0x1, lsl #12
    0.00 :   ffff800010216800:       add     x25, x25, #0x8
    0.00 :   ffff800010216804:       cmp     x21, x23
    0.00 :   ffff800010216808:       b.ne    ffff800010216730 <change_protection+0x2b8>  // b.any
    0.00 :   ffff80001021680c:       ldr     x0, [x29, #248]
    0.00 :   ffff800010216810:       mov     x21, x23
    0.00 :   ffff800010216814:       mov     x23, x26
    0.00 :   ffff800010216818:       mov     w26, w27
    0.00 :   ffff80001021681c:       add     x0, x0, x23
    0.00 :   ffff800010216820:       str     x0, [x29, #248]
         :                      spin_unlock():
         :                      raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
         :                      } while (0)
         :
         :                      static __always_inline void spin_unlock(spinlock_t *lock)
         :                      {
         :                      raw_spin_unlock(&lock->rlock);
    0.00 :   ffff800010216824:       ldr     x0, [x29, #208]
    0.00 :   ffff800010216828:       mov     x27, x20
    0.00 :   ffff80001021682c:       ldr     x20, [x29, #200]
    0.00 :   ffff800010216830:       bl      ffff800010cb2408 <_raw_spin_unlock>
    0.00 :   ffff800010216834:       ldr     x2, [x29, #272]
    0.00 :   ffff800010216838:       b       ffff800010216618 <change_protection+0x1a0>
         :                      set_pte():
         :                      dsb(ishst);
    0.00 :   ffff80001021683c:       dsb     ishst
         :                      isb();
    0.00 :   ffff800010216840:       isb
    0.00 :   ffff800010216844:       b       ffff8000102167f8 <change_protection+0x380>
         :                      change_pud_range():
         :                      next = pud_addr_end(addr, end);
    0.00 :   ffff800010216848:       mov     x27, x28
         :                      } while (pud++, addr = next, addr != end);
    0.00 :   ffff80001021684c:       ldr     x0, [x29, #240]
    0.00 :   ffff800010216850:       add     x21, x21, #0x8
    0.00 :   ffff800010216854:       cmp     x0, x28
    0.00 :   ffff800010216858:       b.ne    ffff8000102165a8 <change_protection+0x130>  // b.any
    0.00 :   ffff80001021685c:       nop
         :                      change_protection_range():
         :                      pages += change_p4d_range(vma, pgd, addr, next, newprot,
    0.00 :   ffff800010216860:       ldr     x0, [x29, #152]
    0.00 :   ffff800010216864:       mov     x27, x25
    0.00 :   ffff800010216868:       ldr     x1, [x29, #224]
    0.00 :   ffff80001021686c:       ldr     x28, [x29, #240]
    0.00 :   ffff800010216870:       add     x0, x0, x1
    0.00 :   ffff800010216874:       str     x0, [x29, #152]
    0.00 :   ffff800010216878:       b       ffff800010216880 <change_protection+0x408>
         :                      next = pgd_addr_end(addr, end);
    0.00 :   ffff80001021687c:       ldr     x28, [x29, #240]
         :                      } while (pgd++, addr = next, addr != end);
    0.00 :   ffff800010216880:       ldr     x0, [x29, #160]
    0.00 :   ffff800010216884:       ldr     x1, [x29, #240]
    0.00 :   ffff800010216888:       add     x0, x0, #0x8
    0.00 :   ffff80001021688c:       str     x0, [x29, #160]
    0.00 :   ffff800010216890:       ldr     x0, [x29, #144]
    0.00 :   ffff800010216894:       cmp     x0, x1
    0.00 :   ffff800010216898:       b.ne    ffff800010216540 <change_protection+0xc8>  // b.any
         :                      if (pages)
    0.00 :   ffff80001021689c:       ldr     x0, [x29, #152]
    0.00 :   ffff8000102168a0:       cbnz    x0, ffff800010216c0c <change_protection+0x794>
         :                      arch_static_branch_jump():
    0.00 :   ffff8000102168a4:       b       ffff800010216bdc <change_protection+0x764>
    0.00 :   ffff8000102168a8:       b       ffff800010216bdc <change_protection+0x764>
         :                      __lse_atomic_sub():
         :
         :                      #undef ATOMIC_FETCH_OP_AND
         :
         :                      static inline void __lse_atomic_sub(int i, atomic_t *v)
         :                      {
         :                      asm volatile(
    0.00 :   ffff8000102168ac:       ldp     x2, x1, [x29, #112]
    0.00 :   ffff8000102168b0:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000102168b4:       add     x1, x1, #0x34c
    0.00 :   ffff8000102168b8:       neg     w0, w0
    0.00 :   ffff8000102168bc:       stadd   w0, [x1]
    0.00 :   ffff8000102168c0:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff8000102168c4:       ldp     x21, x22, [x29, #32]
    0.00 :   ffff8000102168c8:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff8000102168cc:       ldp     x25, x26, [x29, #64]
    0.00 :   ffff8000102168d0:       ldp     x27, x28, [x29, #80]
         :                      change_protection():
         :                      pages = hugetlb_change_protection(vma, start, end, newprot);
         :                      else
         :                      pages = change_protection_range(vma, start, end, newprot, dirty_accountable, prot_numa);
         :
         :                      return pages;
         :                      }
    0.00 :   ffff8000102168d4:       adrp    x0, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000102168d8:       add     x1, x0, #0x8c8
    0.00 :   ffff8000102168dc:       ldr     x0, [x29, #152]
    0.00 :   ffff8000102168e0:       ldr     x2, [x29, #296]
    0.00 :   ffff8000102168e4:       ldr     x1, [x1]
    0.00 :   ffff8000102168e8:       eor     x1, x2, x1
    0.00 :   ffff8000102168ec:       cbnz    x1, ffff800010216cec <change_protection+0x874>
    0.00 :   ffff8000102168f0:       ldp     x29, x30, [sp], #304
    0.00 :   ffff8000102168f4:       ret
         :                      pte_to_swp_entry():
         :                      swp_entry_t arch_entry;
         :
         :                      if (pte_swp_soft_dirty(pte))
         :                      pte = pte_swp_clear_soft_dirty(pte);
         :                      arch_entry = __pte_to_swp_entry(pte);
         :                      return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));
    0.00 :   ffff8000102168f8:       lsr     x0, x19, #2
    0.00 :   ffff8000102168fc:       ubfx    x19, x19, #8, #50
         :                      swp_entry():
         :                      ret.val = (type << SWP_TYPE_SHIFT) | (offset & SWP_OFFSET_MASK);
    0.00 :   ffff800010216900:       orr     x19, x19, x0, lsl #58
         :                      swp_type():
         :                      return (entry.val >> SWP_TYPE_SHIFT);
    0.00 :   ffff800010216904:       lsr     x0, x19, #58
         :                      change_pte_range():
         :                      if (is_write_migration_entry(entry)) {
    0.00 :   ffff800010216908:       cmp     x0, #0x1f
    0.00 :   ffff80001021690c:       b.ne    ffff8000102167fc <change_protection+0x384>  // b.any
         :                      swp_entry_to_pte():
         :                      */
         :                      static inline pte_t swp_entry_to_pte(swp_entry_t entry)
         :                      {
         :                      swp_entry_t arch_entry;
         :
         :                      arch_entry = __swp_entry(swp_type(entry), swp_offset(entry));
    0.00 :   ffff800010216910:       lsl     x19, x19, #8
         :                      change_pte_range():
         :                      pages++;
    0.00 :   ffff800010216914:       add     x26, x26, #0x1
         :                      swp_entry_to_pte():
    0.00 :   ffff800010216918:       orr     x19, x19, #0x78
         :                      __write_once_size():
    0.00 :   ffff80001021691c:       str     x19, [x25]
    0.00 :   ffff800010216920:       b       ffff8000102167fc <change_protection+0x384>
         :                      change_pmd_range():
         :                      if (!is_swap_pmd(*pmd) && !pmd_trans_huge(*pmd) && !pmd_devmap(*pmd)
    0.00 :   ffff800010216924:       and     x3, x0, #0x2
    0.00 :   ffff800010216928:       tbz     w0, #1, ffff800010216930 <change_protection+0x4b8>
    0.00 :   ffff80001021692c:       tbz     x0, #57, ffff800010216a54 <change_protection+0x5dc>
         :                      if (!range.start) {
    0.00 :   ffff800010216930:       cbnz    x2, ffff800010216978 <change_protection+0x500>
         :                      mmu_notifier_range_init(&range,
    0.00 :   ffff800010216934:       ldr     x0, [x27, #64]
         :                      mmu_notifier_range_init():
         :                      struct mm_struct *mm,
         :                      unsigned long start,
         :                      unsigned long end)
         :                      {
         :                      range->vma = vma;
         :                      range->event = event;
    0.00 :   ffff800010216938:       mov     w1, #0x2                        // #2
         :                      range->mm = mm;
    0.00 :   ffff80001021693c:       stp     x27, x0, [x29, #256]
         :                      range->start = start;
         :                      range->end = end;
    0.00 :   ffff800010216940:       stp     x25, x28, [x29, #272]
         :                      mmu_notifier_invalidate_range_start():
         :                      if (mm_has_notifiers(range->mm)) {
    0.00 :   ffff800010216944:       ldr     x0, [x0, #816]
         :                      mmu_notifier_range_init():
         :                      range->flags = flags;
    0.00 :   ffff800010216948:       str     wzr, [x29, #288]
         :                      range->event = event;
    0.00 :   ffff80001021694c:       str     w1, [x29, #292]
         :                      mmu_notifier_invalidate_range_start():
         :                      if (mm_has_notifiers(range->mm)) {
    0.00 :   ffff800010216950:       cbnz    x0, ffff800010216a78 <change_protection+0x600>
    0.00 :   ffff800010216954:       ldr     x0, [x24]
         :                      is_swap_pmd():
    0.00 :   ffff800010216958:       cbnz    x0, ffff800010216964 <change_protection+0x4ec>
         :                      pmd_none_or_trans_huge_or_clear_bad():
         :                      barrier();
    0.00 :   ffff80001021695c:       ldr     x2, [x29, #272]
    0.00 :   ffff800010216960:       b       ffff800010216618 <change_protection+0x1a0>
    0.00 :   ffff800010216964:       and     x1, x0, #0x7ffffffffffffff
    0.00 :   ffff800010216968:       and     x1, x1, #0xfc00000000000001
    0.00 :   ffff80001021696c:       nop
         :                      is_swap_pmd():
    0.00 :   ffff800010216970:       cbz     x1, ffff800010216658 <change_protection+0x1e0>
    0.00 :   ffff800010216974:       and     x3, x0, #0x2
         :                      change_pmd_range():
         :                      if (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {
    0.00 :   ffff800010216978:       cbz     x3, ffff800010216658 <change_protection+0x1e0>
    0.00 :   ffff80001021697c:       tbz     x0, #57, ffff800010216680 <change_protection+0x208>
         :                      if (next - addr != HPAGE_PMD_SIZE) {
    0.00 :   ffff800010216980:       sub     x0, x21, x25
    0.00 :   ffff800010216984:       cmp     x0, #0x200, lsl #12
    0.00 :   ffff800010216988:       b.ne    ffff800010216664 <change_protection+0x1ec>  // b.any
         :                      int nr_ptes = change_huge_pmd(vma, pmd, addr,
    0.00 :   ffff80001021698c:       ldr     x3, [x29, #168]
    0.00 :   ffff800010216990:       mov     w4, w26
    0.00 :   ffff800010216994:       mov     x2, x25
    0.00 :   ffff800010216998:       mov     x1, x24
    0.00 :   ffff80001021699c:       mov     x0, x27
    0.00 :   ffff8000102169a0:       bl      ffff80001025c008 <change_huge_pmd>
         :                      if (nr_ptes) {
    0.00 :   ffff8000102169a4:       cbnz    w0, ffff800010216ae0 <change_protection+0x668>
    0.00 :   ffff8000102169a8:       ldr     x0, [x24]
    0.00 :   ffff8000102169ac:       b       ffff800010216680 <change_protection+0x208>
         :                      change_pte_range():
         :                      if (pte_protnone(oldpte))
    0.00 :   ffff8000102169b0:       mov     x1, #0x400000000000000          // #288230376151711744
    0.00 :   ffff8000102169b4:       cmp     x0, x1
    0.00 :   ffff8000102169b8:       b.eq    ffff8000102167fc <change_protection+0x384>  // b.none
         :                      page = vm_normal_page(vma, addr, oldpte);
    0.00 :   ffff8000102169bc:       mov     x1, x21
    0.00 :   ffff8000102169c0:       mov     x2, x19
    0.00 :   ffff8000102169c4:       mov     x0, x20
    0.00 :   ffff8000102169c8:       bl      ffff800010208180 <vm_normal_page>
    0.00 :   ffff8000102169cc:       mov     x1, x0
         :                      if (!page || PageKsm(page))
    0.00 :   ffff8000102169d0:       cbz     x0, ffff8000102167fc <change_protection+0x384>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000102169d4:       ldr     x0, [x0, #8]
         :                      compound_head():
         :                      static inline struct page *compound_head(struct page *page)
         :                      {
         :                      unsigned long head = READ_ONCE(page->compound_head);
         :
         :                      if (unlikely(head & 1))
         :                      return (struct page *) (head - 1);
    0.00 :   ffff8000102169d8:       tst     x0, #0x1
    8.90 :   ffff8000102169dc:       sub     x0, x0, #0x1
    0.00 :   ffff8000102169e0:       csel    x0, x0, x1, ne  // ne = any
         :                      PageKsm():
         :                      * anon_vma, but to that page's node of the stable tree.
         :                      */
         :                      static __always_inline int PageKsm(struct page *page)
         :                      {
         :                      page = compound_head(page);
         :                      return ((unsigned long)page->mapping & PAGE_MAPPING_FLAGS) ==
   27.46 :   ffff8000102169e4:       ldr     x0, [x0, #24]
    0.00 :   ffff8000102169e8:       and     x0, x0, #0x3
         :                      change_pte_range():
    0.00 :   ffff8000102169ec:       cmp     x0, #0x3
    0.00 :   ffff8000102169f0:       b.eq    ffff8000102167fc <change_protection+0x384>  // b.none
         :                      is_cow_mapping():
         :                      */
         :                      #define page_order_unsafe(page)         READ_ONCE(page_private(page))
         :
         :                      static inline bool is_cow_mapping(vm_flags_t flags)
         :                      {
         :                      return (flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;
    4.51 :   ffff8000102169f4:       ldr     x0, [x20, #80]
    0.00 :   ffff8000102169f8:       mov     x2, #0x28                       // #40
    0.00 :   ffff8000102169fc:       and     x0, x0, x2
         :                      change_pte_range():
         :                      if (is_cow_mapping(vma->vm_flags) &&
    0.00 :   ffff800010216a00:       cmp     x0, #0x20
    0.00 :   ffff800010216a04:       b.eq    ffff800010216b68 <change_protection+0x6f0>  // b.none
         :                      __read_once_size():
    0.00 :   ffff800010216a08:       ldr     x0, [x1, #8]
         :                      compound_head():
         :                      if (unlikely(head & 1))
    0.00 :   ffff800010216a0c:       tst     x0, #0x1
    0.00 :   ffff800010216a10:       sub     x0, x0, #0x1
    0.00 :   ffff800010216a14:       csel    x0, x0, x1, ne  // ne = any
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff800010216a18:       ldr     x0, [x0]
         :                      change_pte_range():
         :                      if (page_is_file_cache(page) && PageDirty(page))
    0.00 :   ffff800010216a1c:       tbnz    w0, #19, ffff800010216a38 <change_protection+0x5c0>
         :                      __read_once_size():
    0.00 :   ffff800010216a20:       ldr     x0, [x1, #8]
         :                      compound_head():
         :                      return (struct page *) (head - 1);
    0.00 :   ffff800010216a24:       tst     x0, #0x1
    0.00 :   ffff800010216a28:       sub     x0, x0, #0x1
    0.00 :   ffff800010216a2c:       csel    x0, x0, x1, ne  // ne = any
         :                      test_bit():
    0.00 :   ffff800010216a30:       ldr     x0, [x0]
         :                      change_pte_range():
    0.00 :   ffff800010216a34:       tbnz    w0, #3, ffff8000102167fc <change_protection+0x384>
         :                      page_to_nid():
         :                      return (PF_POISONED_CHECK(p)->flags >> NODES_PGSHIFT) & NODES_MASK;
    0.00 :   ffff800010216a38:       ldr     x0, [x1]
         :                      change_pte_range():
         :                      if (target_node == page_to_nid(page))
    0.00 :   ffff800010216a3c:       ldr     w1, [x29, #184]
         :                      page_to_nid():
    0.00 :   ffff800010216a40:       lsr     x0, x0, #62
         :                      change_pte_range():
    0.00 :   ffff800010216a44:       cmp     w1, w0
    0.00 :   ffff800010216a48:       b.eq    ffff8000102167fc <change_protection+0x384>  // b.none
         :                      bool preserve_write = prot_numa && pte_write(oldpte);
    0.00 :   ffff800010216a4c:       ubfx    x19, x19, #51, #1
    0.00 :   ffff800010216a50:       b       ffff800010216744 <change_protection+0x2cc>
         :                      change_pmd_range():
         :                      if (!range.start) {
    0.00 :   ffff800010216a54:       cbnz    x2, ffff800010216970 <change_protection+0x4f8>
         :                      mmu_notifier_range_init(&range,
    0.00 :   ffff800010216a58:       ldr     x0, [x27, #64]
         :                      mmu_notifier_range_init():
         :                      range->event = event;
    0.00 :   ffff800010216a5c:       mov     w1, #0x2                        // #2
         :                      range->mm = mm;
    0.00 :   ffff800010216a60:       stp     x27, x0, [x29, #256]
         :                      range->end = end;
    0.00 :   ffff800010216a64:       stp     x25, x28, [x29, #272]
         :                      mmu_notifier_invalidate_range_start():
         :                      if (mm_has_notifiers(range->mm)) {
    0.00 :   ffff800010216a68:       ldr     x0, [x0, #816]
         :                      mmu_notifier_range_init():
         :                      range->flags = flags;
    0.00 :   ffff800010216a6c:       str     wzr, [x29, #288]
         :                      range->event = event;
    0.00 :   ffff800010216a70:       str     w1, [x29, #292]
         :                      mmu_notifier_invalidate_range_start():
         :                      if (mm_has_notifiers(range->mm)) {
    0.00 :   ffff800010216a74:       cbz     x0, ffff800010216954 <change_protection+0x4dc>
         :                      range->flags |= MMU_NOTIFIER_RANGE_BLOCKABLE;
    0.00 :   ffff800010216a78:       mov     w0, #0x1                        // #1
    0.00 :   ffff800010216a7c:       str     w0, [x29, #288]
         :                      __mmu_notifier_invalidate_range_start(range);
    0.00 :   ffff800010216a80:       add     x0, x29, #0x100
    0.00 :   ffff800010216a84:       bl      ffff800010245f98 <__mmu_notifier_invalidate_range_start>
    0.00 :   ffff800010216a88:       b       ffff800010216954 <change_protection+0x4dc>
    0.00 :   ffff800010216a8c:       mov     x25, x27
    0.00 :   ffff800010216a90:       ldr     x21, [x29, #216]
         :                      change_pmd_range():
         :                      if (range.start)
    0.00 :   ffff800010216a94:       cbz     x2, ffff800010216aa4 <change_protection+0x62c>
         :                      mm_has_notifiers():
         :                      return unlikely(mm->mmu_notifier_mm);
    0.00 :   ffff800010216a98:       ldr     x0, [x29, #264]
         :                      mmu_notifier_invalidate_range_end():
         :                      if (mm_has_notifiers(range->mm))
    0.00 :   ffff800010216a9c:       ldr     x0, [x0, #816]
    0.00 :   ffff800010216aa0:       cbnz    x0, ffff800010216bfc <change_protection+0x784>
         :                      change_pmd_range():
         :                      if (nr_huge_updates)
    0.00 :   ffff800010216aa4:       ldr     x0, [x29, #232]
    0.00 :   ffff800010216aa8:       cbnz    x0, ffff800010216b14 <change_protection+0x69c>
         :                      change_pud_range():
         :                      pages += change_pmd_range(vma, pud, addr, next, newprot,
    0.00 :   ffff800010216aac:       ldr     x0, [x29, #224]
    0.00 :   ffff800010216ab0:       mov     x27, x28
    0.00 :   ffff800010216ab4:       ldr     x1, [x29, #248]
    0.00 :   ffff800010216ab8:       add     x0, x0, x1
    0.00 :   ffff800010216abc:       str     x0, [x29, #224]
    0.00 :   ffff800010216ac0:       b       ffff80001021684c <change_protection+0x3d4>
         :                      pud_none_or_clear_bad():
         :                      if (pud_none(*pud))
    0.00 :   ffff800010216ac4:       cbz     x0, ffff800010216860 <change_protection+0x3e8>
    0.00 :   ffff800010216ac8:       ldr     x28, [x29, #240]
         :                      if (unlikely(pud_bad(*pud))) {
    0.00 :   ffff800010216acc:       tbnz    w0, #1, ffff8000102165d0 <change_protection+0x158>
         :                      pud_clear_bad(pud);
    0.00 :   ffff800010216ad0:       mov     x0, x21
    0.00 :   ffff800010216ad4:       mov     x27, x28
    0.00 :   ffff800010216ad8:       bl      ffff800010219378 <pud_clear_bad>
    0.00 :   ffff800010216adc:       b       ffff80001021684c <change_protection+0x3d4>
         :                      change_pmd_range():
         :                      if (nr_ptes == HPAGE_PMD_NR) {
    0.00 :   ffff800010216ae0:       cmp     w0, #0x200
    0.00 :   ffff800010216ae4:       ldr     x2, [x29, #272]
    0.00 :   ffff800010216ae8:       b.ne    ffff800010216618 <change_protection+0x1a0>  // b.any
         :                      pages += HPAGE_PMD_NR;
    0.00 :   ffff800010216aec:       ldr     x0, [x29, #248]
    0.00 :   ffff800010216af0:       add     x0, x0, #0x200
    0.00 :   ffff800010216af4:       str     x0, [x29, #248]
         :                      nr_huge_updates++;
    0.00 :   ffff800010216af8:       ldr     x0, [x29, #232]
    0.00 :   ffff800010216afc:       add     x0, x0, #0x1
    0.00 :   ffff800010216b00:       str     x0, [x29, #232]
    0.00 :   ffff800010216b04:       b       ffff800010216618 <change_protection+0x1a0>
         :                      clear_pte_bit():
         :                      pte_val(pte) &= ~pgprot_val(prot);
    0.00 :   ffff800010216b08:       and     x19, x19, #0xffffffffffffff7f
    0.00 :   ffff800010216b0c:       orr     x19, x19, #0x8000000000000
    0.00 :   ffff800010216b10:       b       ffff8000102167d0 <change_protection+0x358>
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800010216b14:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.00 :   ffff800010216b18:       ldr     w0, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff800010216b1c:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010216b20:       str     w0, [x1, #16]
         :                      count_vm_events():
         :                      raw_cpu_add(vm_event_states.event[item], delta);
         :                      }
         :
         :                      static inline void count_vm_events(enum vm_event_item item, long delta)
         :                      {
         :                      this_cpu_add(vm_event_states.event[item], delta);
    0.00 :   ffff800010216b24:       adrp    x0, ffff8000114d3000 <pmu_sb_events>
    0.00 :   ffff800010216b28:       add     x0, x0, #0xa00
         :                      __my_cpu_offset():
    0.00 :   ffff800010216b2c:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :
         :                      PERCPU_RW_OPS(8)
         :                      PERCPU_RW_OPS(16)
         :                      PERCPU_RW_OPS(32)
         :                      PERCPU_RW_OPS(64)
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff800010216b30:       add     x0, x0, x2
    0.00 :   ffff800010216b34:       ldr     x2, [x29, #232]
    0.00 :   ffff800010216b38:       ldxr    x4, [x0]
    0.00 :   ffff800010216b3c:       add     x4, x4, x2
    0.00 :   ffff800010216b40:       stxr    w3, x4, [x0]
    0.00 :   ffff800010216b44:       cbnz    w3, ffff800010216b38 <change_protection+0x6c0>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010216b48:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800010216b4c:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010216b50:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800010216b54:       cbz     x0, ffff800010216b60 <change_protection+0x6e8>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010216b58:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff800010216b5c:       cbnz    x0, ffff800010216aac <change_protection+0x634>
         :                      count_vm_events():
    0.00 :   ffff800010216b60:       bl      ffff800010cad668 <preempt_schedule_notrace>
    0.00 :   ffff800010216b64:       b       ffff800010216aac <change_protection+0x634>
         :                      test_bit():
    0.00 :   ffff800010216b68:       ldr     x0, [x1]
         :                      PageCompound():
         :                      return test_bit(PG_head, &page->flags) || PageTail(page);
    0.00 :   ffff800010216b6c:       tbz     w0, #16, ffff800010216bb8 <change_protection+0x740>
         :                      page_mapcount():
         :                      return __page_mapcount(page);
    0.00 :   ffff800010216b70:       mov     x0, x1
    0.00 :   ffff800010216b74:       str     x1, [x29, #104]
    0.00 :   ffff800010216b78:       bl      ffff8000101f13f0 <__page_mapcount>
    0.00 :   ffff800010216b7c:       ldr     x1, [x29, #104]
         :                      change_pte_range():
         :                      if (is_cow_mapping(vma->vm_flags) &&
    0.00 :   ffff800010216b80:       cmp     w0, #0x1
    0.00 :   ffff800010216b84:       b.eq    ffff800010216a08 <change_protection+0x590>  // b.none
    0.00 :   ffff800010216b88:       b       ffff8000102167fc <change_protection+0x384>
         :                      set_pte_at():
         :                      __sync_icache_dcache(pte);
    0.00 :   ffff800010216b8c:       mov     x0, x19
    0.00 :   ffff800010216b90:       bl      ffff8000100a2690 <__sync_icache_dcache>
    0.00 :   ffff800010216b94:       b       ffff8000102167e4 <change_protection+0x36c>
         :                      pgd_none_or_clear_bad():
         :                      if (pgd_none(*pgd))
    0.00 :   ffff800010216b98:       cbz     x0, ffff80001021689c <change_protection+0x424>
    0.00 :   ffff800010216b9c:       ldr     x1, [x29, #144]
    0.00 :   ffff800010216ba0:       str     x1, [x29, #240]
         :                      if (unlikely(pgd_bad(*pgd))) {
    0.00 :   ffff800010216ba4:       tbnz    w0, #1, ffff800010216570 <change_protection+0xf8>
         :                      pgd_clear_bad(pgd);
    0.00 :   ffff800010216ba8:       ldr     x0, [x29, #160]
    0.00 :   ffff800010216bac:       ldr     x28, [x29, #240]
    0.00 :   ffff800010216bb0:       bl      ffff800010219310 <pgd_clear_bad>
    0.00 :   ffff800010216bb4:       b       ffff800010216880 <change_protection+0x408>
         :                      __read_once_size():
    0.00 :   ffff800010216bb8:       ldr     x0, [x1, #8]
         :                      PageCompound():
    0.00 :   ffff800010216bbc:       tbnz    w0, #0, ffff800010216b70 <change_protection+0x6f8>
         :                      __read_once_size():
    0.00 :   ffff800010216bc0:       ldr     w0, [x1, #48]
         :                      page_mapcount():
         :                      return atomic_read(&page->_mapcount) + 1;
    0.00 :   ffff800010216bc4:       add     w0, w0, #0x1
    0.00 :   ffff800010216bc8:       b       ffff800010216b80 <change_protection+0x708>
         :                      __ll_sc_atomic_add():
         :                      ATOMIC_FETCH_OP (        , dmb ish,  , l, "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
    0.00 :   ffff800010216bcc:       ldr     x0, [x29, #120]
    0.00 :   ffff800010216bd0:       add     x2, x0, #0x34c
    0.00 :   ffff800010216bd4:       b       ffff8000102171d4 <__arm64_sys_mprotect+0x234>
    0.00 :   ffff800010216bd8:       b       ffff800010216510 <change_protection+0x98>
         :                      __ll_sc_atomic_sub():
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff800010216bdc:       ldr     x1, [x29, #120]
    0.00 :   ffff800010216be0:       mov     w0, #0x1                        // #1
    0.00 :   ffff800010216be4:       add     x3, x1, #0x34c
    0.00 :   ffff800010216be8:       b       ffff8000102171ec <__arm64_sys_mprotect+0x24c>
    0.00 :   ffff800010216bec:       b       ffff8000102168c0 <change_protection+0x448>
         :                      change_protection():
         :                      pages = hugetlb_change_protection(vma, start, end, newprot);
    0.00 :   ffff800010216bf0:       bl      ffff80001023f738 <hugetlb_change_protection>
    0.00 :   ffff800010216bf4:       str     x0, [x29, #152]
    0.00 :   ffff800010216bf8:       b       ffff8000102168d4 <change_protection+0x45c>
         :                      mmu_notifier_invalidate_range_end():
         :                      __mmu_notifier_invalidate_range_end(range, false);
    0.00 :   ffff800010216bfc:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010216c00:       add     x0, x29, #0x100
    0.00 :   ffff800010216c04:       bl      ffff800010246190 <__mmu_notifier_invalidate_range_end>
    0.00 :   ffff800010216c08:       b       ffff800010216aa4 <change_protection+0x62c>
         :                      __flush_tlb_range():
         :                      {
         :                      unsigned long asid = ASID(vma->vm_mm);
         :                      unsigned long addr;
         :
         :                      start = round_down(start, stride);
         :                      end = round_up(end, stride);
    0.00 :   ffff800010216c0c:       ldp     x1, x0, [x29, #128]
         :
         :                      if ((end - start) >= (MAX_TLBI_OPS * stride)) {
    0.00 :   ffff800010216c10:       mov     x3, #0x1fffff                   // #2097151
         :                      unsigned long asid = ASID(vma->vm_mm);
    0.00 :   ffff800010216c14:       ldr     x2, [x27, #64]
         :                      end = round_up(end, stride);
    0.00 :   ffff800010216c18:       orr     x0, x0, #0xfff
         :                      start = round_down(start, stride);
    0.00 :   ffff800010216c1c:       and     x1, x1, #0xfffffffffffff000
         :                      end = round_up(end, stride);
    0.00 :   ffff800010216c20:       add     x0, x0, #0x1
    0.00 :   ffff800010216c24:       ldr     x2, [x2, #736]
         :                      if ((end - start) >= (MAX_TLBI_OPS * stride)) {
    0.00 :   ffff800010216c28:       sub     x4, x0, x1
    0.00 :   ffff800010216c2c:       cmp     x4, x3
    0.00 :   ffff800010216c30:       lsl     x2, x2, #48
    0.00 :   ffff800010216c34:       b.hi    ffff800010216ca4 <change_protection+0x82c>  // b.pmore
         :                      }
         :
         :                      /* Convert the stride into units of 4k */
         :                      stride >>= 12;
         :
         :                      start = __TLBI_VADDR(start, asid);
    0.00 :   ffff800010216c38:       ubfx    x1, x1, #12, #44
         :                      end = __TLBI_VADDR(end, asid);
    0.00 :   ffff800010216c3c:       ubfx    x0, x0, #12, #44
         :                      start = __TLBI_VADDR(start, asid);
    0.00 :   ffff800010216c40:       orr     x1, x1, x2
         :                      end = __TLBI_VADDR(end, asid);
    0.00 :   ffff800010216c44:       orr     x0, x0, x2
         :
         :                      dsb(ishst);
    0.00 :   ffff800010216c48:       dsb     ishst
         :                      for (addr = start; addr < end; addr += stride) {
    0.00 :   ffff800010216c4c:       cmp     x1, x0
    0.00 :   ffff800010216c50:       b.cs    ffff800010216c78 <change_protection+0x800>  // b.hs, b.nlast
         :                      test_bit():
    0.00 :   ffff800010216c54:       adrp    x3, ffff800011a76000 <reset_devices>
         :                      __flush_tlb_range():
         :                      if (last_level) {
         :                      __tlbi(vale1is, addr);
         :                      __tlbi_user(vale1is, addr);
         :                      } else {
         :                      __tlbi(vae1is, addr);
    0.00 :   ffff800010216c58:       tlbi    vae1is, x1
   22.62 :   ffff800010216c5c:       nop
    0.00 :   ffff800010216c60:       nop
         :                      arch_static_branch_jump():
    0.00 :   ffff800010216c64:       b       ffff800010216c84 <change_protection+0x80c>
         :                      arch_static_branch():
         :                      asm_volatile_goto(
    0.00 :   ffff800010216c68:       nop
         :                      __flush_tlb_range():
         :                      for (addr = start; addr < end; addr += stride) {
    0.00 :   ffff800010216c6c:       add     x1, x1, #0x1
    0.00 :   ffff800010216c70:       cmp     x0, x1
    0.00 :   ffff800010216c74:       b.ne    ffff800010216c58 <change_protection+0x7e0>  // b.any
         :                      __tlbi_user(vae1is, addr);
         :                      }
         :                      }
         :                      dsb(ish);
    0.00 :   ffff800010216c78:       dsb     ish
    4.49 :   ffff800010216c7c:       b       ffff8000102168a4 <change_protection+0x42c>
         :                      change_protection_range():
         :                      BUG_ON(addr >= end);
    0.00 :   ffff800010216c80:       brk     #0x800
         :                      test_bit():
    0.00 :   ffff800010216c84:       ldr     x2, [x3, #1320]
         :                      __flush_tlb_range():
         :                      __tlbi_user(vae1is, addr);
    0.00 :   ffff800010216c88:       tst     w2, #0x800000
    0.00 :   ffff800010216c8c:       b.eq    ffff800010216c6c <change_protection+0x7f4>  // b.none
    0.00 :   ffff800010216c90:       orr     x2, x1, #0x1000000000000
    0.00 :   ffff800010216c94:       tlbi    vae1is, x2
   27.39 :   ffff800010216c98:       nop
    0.00 :   ffff800010216c9c:       nop
    0.00 :   ffff800010216ca0:       b       ffff800010216c6c <change_protection+0x7f4>
         :                      flush_tlb_mm():
         :                      dsb(ishst);
    0.00 :   ffff800010216ca4:       dsb     ishst
         :                      __tlbi(aside1is, asid);
    0.00 :   ffff800010216ca8:       tlbi    aside1is, x2
    0.00 :   ffff800010216cac:       nop
    0.00 :   ffff800010216cb0:       nop
         :                      arch_static_branch_jump():
         :                      asm_volatile_goto(
    0.00 :   ffff800010216cb4:       b       ffff800010216cc4 <change_protection+0x84c>
         :                      arch_static_branch():
         :                      asm_volatile_goto(
    0.00 :   ffff800010216cb8:       nop
         :                      __flush_tlb_range():
         :                      dsb(ish);
    0.00 :   ffff800010216cbc:       dsb     ish
    0.00 :   ffff800010216cc0:       b       ffff8000102168a4 <change_protection+0x42c>
         :                      test_bit():
    0.00 :   ffff800010216cc4:       adrp    x0, ffff800011a76000 <reset_devices>
    0.00 :   ffff800010216cc8:       ldr     x0, [x0, #1320]
         :                      flush_tlb_mm():
         :                      __tlbi_user(aside1is, asid);
    0.00 :   ffff800010216ccc:       tst     w0, #0x800000
    0.00 :   ffff800010216cd0:       b.eq    ffff800010216c78 <change_protection+0x800>  // b.none
    0.00 :   ffff800010216cd4:       orr     x0, x2, #0x1000000000000
    0.00 :   ffff800010216cd8:       tlbi    aside1is, x0
    0.00 :   ffff800010216cdc:       nop
    0.00 :   ffff800010216ce0:       nop
         :                      __flush_tlb_range():
         :                      dsb(ish);
    0.00 :   ffff800010216ce4:       dsb     ish
    0.00 :   ffff800010216ce8:       b       ffff8000102168a4 <change_protection+0x42c>
    0.00 :   ffff800010216cec:       stp     x19, x20, [x29, #16]
    0.00 :   ffff800010216cf0:       stp     x21, x22, [x29, #32]
    0.00 :   ffff800010216cf4:       stp     x23, x24, [x29, #48]
    0.00 :   ffff800010216cf8:       stp     x25, x26, [x29, #64]
    0.00 :   ffff800010216cfc:       stp     x27, x28, [x29, #80]
         :                      change_protection():
         :                      }
    0.00 :   ffff800010216d00:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (25 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001044e8e0 <__bio_try_merge_page>:
         :                      __bio_try_merge_page():
         :                      *
         :                      * Return %true on success or %false on failure.
         :                      */
         :                      bool __bio_try_merge_page(struct bio *bio, struct page *page,
         :                      unsigned int len, unsigned int off, bool *same_page)
         :                      {
    0.00 :   ffff80001044e8e0:       stp     x29, x30, [sp, #-80]!
    0.00 :   ffff80001044e8e4:       mov     x29, sp
         :                      bio_flagged():
         :                      atomic_set(&bio->__bi_cnt, count);
         :                      }
         :
         :                      static inline bool bio_flagged(struct bio *bio, unsigned int bit)
         :                      {
         :                      return (bio->bi_flags & (1U << bit)) != 0;
    0.00 :   ffff80001044e8e8:       ldrh    w5, [x0, #20]
         :                      __bio_try_merge_page():
         :                      if (WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED)))
    0.00 :   ffff80001044e8ec:       tbnz    w5, #1, ffff80001044ea10 <__bio_try_merge_page+0x130>
         :                      return false;
         :
         :                      if (bio->bi_vcnt > 0) {
    0.00 :   ffff80001044e8f0:       ldrh    w5, [x0, #96]
    0.00 :   ffff80001044e8f4:       cbnz    w5, ffff80001044e904 <__bio_try_merge_page+0x24>
         :                      bv->bv_len += len;
         :                      bio->bi_iter.bi_size += len;
         :                      return true;
         :                      }
         :                      }
         :                      return false;
   88.05 :   ffff80001044e8f8:       mov     w0, #0x0                        // #0
         :                      }
   11.95 :   ffff80001044e8fc:       ldp     x29, x30, [sp], #80
    0.00 :   ffff80001044e900:       ret
    0.00 :   ffff80001044e904:       stp     x19, x20, [x29, #16]
         :                      struct bio_vec *bv = &bio->bi_io_vec[bio->bi_vcnt - 1];
    0.00 :   ffff80001044e908:       mov     x7, #0xfffffffffffffff0         // #-16
    0.00 :   ffff80001044e90c:       str     x21, [x29, #32]
    0.00 :   ffff80001044e910:       add     x5, x7, w5, uxtw #4
    0.00 :   ffff80001044e914:       stp     x24, x25, [x29, #56]
         :                      page_is_mergeable():
         :                      phys_addr_t vec_end_addr = page_to_phys(bv->bv_page) +
    0.00 :   ffff80001044e918:       adrp    x25, ffff8000112ae000 <cpu_ops+0x248>
    0.00 :   ffff80001044e91c:       mov     x21, x1
         :                      __bio_try_merge_page():
         :                      struct bio_vec *bv = &bio->bi_io_vec[bio->bi_vcnt - 1];
    0.00 :   ffff80001044e920:       ldr     x8, [x0, #104]
         :                      page_is_mergeable():
         :                      phys_addr_t vec_end_addr = page_to_phys(bv->bv_page) +
    0.00 :   ffff80001044e924:       ldr     x7, [x25, #1880]
         :                      __bio_try_merge_page():
         :                      struct bio_vec *bv = &bio->bi_io_vec[bio->bi_vcnt - 1];
    0.00 :   ffff80001044e928:       add     x24, x8, x5
         :                      page_is_mergeable():
         :                      phys_addr_t vec_end_addr = page_to_phys(bv->bv_page) +
    0.00 :   ffff80001044e92c:       ldr     x19, [x8, x5]
         :                      phys_addr_t page_addr = page_to_phys(page);
    0.00 :   ffff80001044e930:       sub     x20, x1, x7
         :                      phys_addr_t vec_end_addr = page_to_phys(bv->bv_page) +
    0.00 :   ffff80001044e934:       sub     x19, x19, x7
         :                      phys_addr_t page_addr = page_to_phys(page);
    0.00 :   ffff80001044e938:       asr     x20, x20, #6
         :                      bv->bv_offset + bv->bv_len - 1;
    0.00 :   ffff80001044e93c:       ldp     w7, w5, [x24, #8]
         :                      phys_addr_t vec_end_addr = page_to_phys(bv->bv_page) +
    0.00 :   ffff80001044e940:       asr     x19, x19, #6
         :                      phys_addr_t page_addr = page_to_phys(page);
    0.00 :   ffff80001044e944:       lsl     x20, x20, #12
         :                      bv->bv_offset + bv->bv_len - 1;
    0.00 :   ffff80001044e948:       add     x5, x5, x7
         :                      if (vec_end_addr + 1 != page_addr + off)
    0.00 :   ffff80001044e94c:       add     x3, x20, w3, uxtw
         :                      bv->bv_offset + bv->bv_len - 1;
    0.00 :   ffff80001044e950:       add     x19, x5, x19, lsl #12
         :                      if (vec_end_addr + 1 != page_addr + off)
    0.00 :   ffff80001044e954:       cmp     x19, x3
    0.00 :   ffff80001044e958:       b.eq    ffff80001044e96c <__bio_try_merge_page+0x8c>  // b.none
    0.00 :   ffff80001044e95c:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff80001044e960:       ldr     x21, [x29, #32]
    0.00 :   ffff80001044e964:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff80001044e968:       b       ffff80001044e8f8 <__bio_try_merge_page+0x18>
    0.00 :   ffff80001044e96c:       str     x26, [x29, #72]
    0.00 :   ffff80001044e970:       mov     x26, x0
         :                      if (xen_domain() && !xen_biovec_phys_mergeable(bv, page))
    0.00 :   ffff80001044e974:       adrp    x0, ffff800011a7c000 <cpu_release_addr+0x748>
    0.00 :   ffff80001044e978:       stp     x22, x23, [x29, #40]
    0.00 :   ffff80001044e97c:       mov     x23, x4
    0.00 :   ffff80001044e980:       mov     w22, w2
    0.00 :   ffff80001044e984:       ldr     w0, [x0, #1400]
    0.00 :   ffff80001044e988:       cbnz    w0, ffff80001044ea20 <__bio_try_merge_page+0x140>
         :                      phys_addr_t vec_end_addr = page_to_phys(bv->bv_page) +
    0.00 :   ffff80001044e98c:       sub     x19, x19, #0x1
         :                      *same_page = ((vec_end_addr & PAGE_MASK) == page_addr);
    0.00 :   ffff80001044e990:       and     x0, x19, #0xfffffffffffff000
    0.00 :   ffff80001044e994:       cmp     x20, x0
    0.00 :   ffff80001044e998:       cset    w1, eq  // eq = none
    0.00 :   ffff80001044e99c:       strb    w1, [x23]
         :                      if (!*same_page && pfn_to_page(PFN_DOWN(vec_end_addr)) + 1 != page)
    0.00 :   ffff80001044e9a0:       b.eq    ffff80001044e9d0 <__bio_try_merge_page+0xf0>  // b.none
    0.00 :   ffff80001044e9a4:       lsr     x19, x19, #12
    0.00 :   ffff80001044e9a8:       ldr     x1, [x25, #1880]
    0.00 :   ffff80001044e9ac:       add     x0, x19, #0x1
    0.00 :   ffff80001044e9b0:       add     x0, x1, x0, lsl #6
    0.00 :   ffff80001044e9b4:       cmp     x21, x0
    0.00 :   ffff80001044e9b8:       b.eq    ffff80001044e9d0 <__bio_try_merge_page+0xf0>  // b.none
    0.00 :   ffff80001044e9bc:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff80001044e9c0:       ldp     x21, x22, [x29, #32]
    0.00 :   ffff80001044e9c4:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff80001044e9c8:       ldp     x25, x26, [x29, #64]
    0.00 :   ffff80001044e9cc:       b       ffff80001044e8f8 <__bio_try_merge_page+0x18>
         :                      __bio_try_merge_page():
         :                      if (bio->bi_iter.bi_size > UINT_MAX - len)
    0.00 :   ffff80001044e9d0:       ldr     w1, [x26, #40]
    0.00 :   ffff80001044e9d4:       mvn     w0, w22
    0.00 :   ffff80001044e9d8:       cmp     w1, w0
    0.00 :   ffff80001044e9dc:       b.hi    ffff80001044e9bc <__bio_try_merge_page+0xdc>  // b.pmore
         :                      bv->bv_len += len;
    0.00 :   ffff80001044e9e0:       ldr     w1, [x24, #8]
         :                      return true;
    0.00 :   ffff80001044e9e4:       mov     w0, #0x1                        // #1
         :                      bv->bv_len += len;
    0.00 :   ffff80001044e9e8:       add     w1, w1, w22
    0.00 :   ffff80001044e9ec:       str     w1, [x24, #8]
         :                      bio->bi_iter.bi_size += len;
    0.00 :   ffff80001044e9f0:       ldr     w2, [x26, #40]
    0.00 :   ffff80001044e9f4:       add     w2, w2, w22
    0.00 :   ffff80001044e9f8:       str     w2, [x26, #40]
         :                      return true;
    0.00 :   ffff80001044e9fc:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff80001044ea00:       ldp     x21, x22, [x29, #32]
    0.00 :   ffff80001044ea04:       ldp     x23, x24, [x29, #48]
    0.00 :   ffff80001044ea08:       ldp     x25, x26, [x29, #64]
    0.00 :   ffff80001044ea0c:       b       ffff80001044e8fc <__bio_try_merge_page+0x1c>
         :                      if (WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED)))
    0.00 :   ffff80001044ea10:       brk     #0x800
         :                      return false;
    0.00 :   ffff80001044ea14:       mov     w0, #0x0                        // #0
         :                      }
    0.00 :   ffff80001044ea18:       ldp     x29, x30, [sp], #80
    0.00 :   ffff80001044ea1c:       ret
         :                      page_is_mergeable():
         :                      if (xen_domain() && !xen_biovec_phys_mergeable(bv, page))
    0.00 :   ffff80001044ea20:       mov     x0, x24
    0.00 :   ffff80001044ea24:       bl      ffff80001066ce38 <xen_biovec_phys_mergeable>
    0.00 :   ffff80001044ea28:       tst     w0, #0xff
    0.00 :   ffff80001044ea2c:       b.ne    ffff80001044e98c <__bio_try_merge_page+0xac>  // b.any
    0.00 :   ffff80001044ea30:       b       ffff80001044e9bc <__bio_try_merge_page+0xdc>
 Percent |	Source code & Disassembly of vmlinux for cycles (18 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010121940 <update_nohz_stats>:
         :                      update_nohz_stats():
         :                      static bool update_nohz_stats(struct rq *rq, bool force)
         :                      {
         :                      #ifdef CONFIG_NO_HZ_COMMON
         :                      unsigned int cpu = rq->cpu;
         :
         :                      if (!rq->has_blocked_load)
    0.00 :   ffff800010121940:       ldr     w2, [x0, #40]
    0.00 :   ffff800010121944:       cbz     w2, ffff8000101219cc <update_nohz_stats+0x8c>
         :                      unsigned int cpu = rq->cpu;
    0.00 :   ffff800010121948:       ldr     w3, [x0, #2568]
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff80001012194c:       adrp    x5, ffff800011a7f000 <ucounts_hashtable+0x12f0>
    0.00 :   ffff800010121950:       add     x5, x5, #0xfc0
         :                      update_nohz_stats():
         :                      return false;
    0.00 :   ffff800010121954:       mov     w4, #0x0                        // #0
         :                      test_bit():
   56.95 :   ffff800010121958:       add     w2, w3, #0x3f
    0.00 :   ffff80001012195c:       cmp     w3, #0x0
    0.00 :   ffff800010121960:       csel    w2, w2, w3, lt  // lt = tstop
    0.00 :   ffff800010121964:       asr     w2, w2, #6
    0.00 :   ffff800010121968:       sxtw    x2, w2
    0.00 :   ffff80001012196c:       ldr     x2, [x5, x2, lsl #3]
    0.00 :   ffff800010121970:       lsr     x2, x2, x3
         :                      update_nohz_stats():
         :
         :                      if (!cpumask_test_cpu(cpu, nohz.idle_cpus_mask))
    0.00 :   ffff800010121974:       tbz     w2, #0, ffff8000101219d0 <update_nohz_stats+0x90>
   32.85 :   ffff800010121978:       and     w1, w1, #0xff
         :                      return false;
         :
         :                      if (!force && !time_after(jiffies, rq->last_blocked_load_update_tick))
    0.00 :   ffff80001012197c:       cbnz    w1, ffff800010121998 <update_nohz_stats+0x58>
    0.00 :   ffff800010121980:       adrp    x2, ffff800011897000 <bit_wait_table+0xe80>
    0.00 :   ffff800010121984:       ldr     x1, [x0, #32]
         :                      return true;
    0.00 :   ffff800010121988:       mov     w4, #0x1                        // #1
         :                      if (!force && !time_after(jiffies, rq->last_blocked_load_update_tick))
    0.00 :   ffff80001012198c:       ldr     x2, [x2, #2432]
    0.00 :   ffff800010121990:       sub     x1, x1, x2
    0.00 :   ffff800010121994:       tbz     x1, #63, ffff8000101219d0 <update_nohz_stats+0x90>
         :                      {
    0.00 :   ffff800010121998:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff80001012199c:       mov     x29, sp
    0.00 :   ffff8000101219a0:       str     x19, [sp, #16]
    0.00 :   ffff8000101219a4:       mov     x19, x0
         :
         :                      update_blocked_averages(cpu);
    0.00 :   ffff8000101219a8:       mov     w0, w3
    0.00 :   ffff8000101219ac:       bl      ffff800010121268 <update_blocked_averages>
         :
         :                      return rq->has_blocked_load;
    0.00 :   ffff8000101219b0:       ldr     w0, [x19, #40]
         :                      #else
         :                      return false;
         :                      #endif
         :                      }
    0.00 :   ffff8000101219b4:       ldr     x19, [sp, #16]
         :                      return rq->has_blocked_load;
    0.00 :   ffff8000101219b8:       cmp     w0, #0x0
    0.00 :   ffff8000101219bc:       cset    w4, ne  // ne = any
         :                      }
    0.00 :   ffff8000101219c0:       mov     w0, w4
    0.00 :   ffff8000101219c4:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000101219c8:       ret
         :                      return false;
   10.20 :   ffff8000101219cc:       mov     w4, #0x0                        // #0
         :                      }
    0.00 :   ffff8000101219d0:       mov     w0, w4
    0.00 :   ffff8000101219d4:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (18 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010125db8 <run_rebalance_domains>:
         :                      run_rebalance_domains():
         :                      /*
         :                      * run_rebalance_domains is triggered when needed from the scheduler tick.
         :                      * Also triggered for nohz idle balancing (with nohz_balancing_kick set).
         :                      */
         :                      static __latent_entropy void run_rebalance_domains(struct softirq_action *h)
         :                      {
    0.47 :   ffff800010125db8:       stp     x29, x30, [sp, #-32]!
         :                      struct rq *this_rq = this_rq();
    0.00 :   ffff800010125dbc:       adrp    x2, ffff8000114d5000 <tegra_to+0x180>
    0.00 :   ffff800010125dc0:       add     x2, x2, #0xd80
         :                      nohz_idle_balance():
         :                      if (!(atomic_read(nohz_flags(this_cpu)) & NOHZ_KICK_MASK))
    0.00 :   ffff800010125dc4:       adrp    x1, ffff800011899000 <page_wait_table+0x1500>
         :                      run_rebalance_domains():
         :                      {
    0.00 :   ffff800010125dc8:       mov     x29, sp
    0.00 :   ffff800010125dcc:       stp     x19, x20, [sp, #16]
         :                      nohz_idle_balance():
         :                      if (!(atomic_read(nohz_flags(this_cpu)) & NOHZ_KICK_MASK))
    0.00 :   ffff800010125dd0:       add     x1, x1, #0x8e8
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800010125dd4:       mrs     x19, tpidr_el1
         :                      run_rebalance_domains():
         :                      struct rq *this_rq = this_rq();
    0.00 :   ffff800010125dd8:       add     x19, x2, x19
         :                      nohz_idle_balance():
         :                      int this_cpu = this_rq->cpu;
    0.00 :   ffff800010125ddc:       ldr     w0, [x19, #2568]
         :                      run_rebalance_domains():
         :                      enum cpu_idle_type idle = this_rq->idle_balance ?
    6.69 :   ffff800010125de0:       ldrb    w20, [x19, #2504]
         :                      nohz_idle_balance():
         :                      if (!(atomic_read(nohz_flags(this_cpu)) & NOHZ_KICK_MASK))
    0.00 :   ffff800010125de4:       ldr     x1, [x1, w0, sxtw #3]
    0.00 :   ffff800010125de8:       add     x2, x2, x1
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010125dec:       ldr     w1, [x2, #48]
         :                      nohz_idle_balance():
    0.00 :   ffff800010125df0:       tst     x1, #0x3
    0.00 :   ffff800010125df4:       b.eq    ffff800010125e1c <run_rebalance_domains+0x64>  // b.none
         :                      if (idle != CPU_IDLE) {
    0.00 :   ffff800010125df8:       cbz     w20, ffff800010125e3c <run_rebalance_domains+0x84>
         :                      flags = atomic_fetch_andnot(NOHZ_KICK_MASK, nohz_flags(this_cpu));
    0.00 :   ffff800010125dfc:       add     x0, x2, #0x30
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010125e00:       b       ffff800010125e58 <run_rebalance_domains+0xa0>
    0.00 :   ffff800010125e04:       b       ffff800010125e58 <run_rebalance_domains+0xa0>
         :                      __lse_atomic_fetch_andnot():
         :                      ATOMIC_FETCH_OP(_relaxed,   , op, asm_op)                       \
         :                      ATOMIC_FETCH_OP(_acquire,  a, op, asm_op, "memory")             \
         :                      ATOMIC_FETCH_OP(_release,  l, op, asm_op, "memory")             \
         :                      ATOMIC_FETCH_OP(        , al, op, asm_op, "memory")
         :
         :                      ATOMIC_FETCH_OPS(andnot, ldclr)
    0.00 :   ffff800010125e08:       mov     w1, #0x3                        // #3
    0.00 :   ffff800010125e0c:       ldclral w1, w1, [x0]
         :                      nohz_idle_balance():
         :                      if (!(flags & NOHZ_KICK_MASK))
   92.84 :   ffff800010125e10:       tst     x1, #0x3
    0.00 :   ffff800010125e14:       b.ne    ffff800010125e6c <run_rebalance_domains+0xb4>  // b.any
    0.00 :   ffff800010125e18:       ldr     w0, [x19, #2568]
         :                      run_rebalance_domains():
         :                      */
         :                      if (nohz_idle_balance(this_rq, idle))
         :                      return;
         :
         :                      /* normal load balance */
         :                      update_blocked_averages(this_rq->cpu);
    0.00 :   ffff800010125e1c:       bl      ffff800010121268 <update_blocked_averages>
         :                      CPU_IDLE : CPU_NOT_IDLE;
    0.00 :   ffff800010125e20:       cmp     w20, #0x0
         :                      rebalance_domains(this_rq, idle);
    0.00 :   ffff800010125e24:       mov     x0, x19
    0.00 :   ffff800010125e28:       cset    w1, eq  // eq = none
    0.00 :   ffff800010125e2c:       bl      ffff8000101258c0 <rebalance_domains>
         :                      }
    0.00 :   ffff800010125e30:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010125e34:       ldp     x29, x30, [sp], #32
    0.00 :   ffff800010125e38:       ret
         :                      nohz_idle_balance():
         :                      atomic_andnot(NOHZ_KICK_MASK, nohz_flags(this_cpu));
    0.00 :   ffff800010125e3c:       add     x1, x2, #0x30
         :                      arch_static_branch_jump():
    0.00 :   ffff800010125e40:       b       ffff800010125e84 <run_rebalance_domains+0xcc>
    0.00 :   ffff800010125e44:       b       ffff800010125e84 <run_rebalance_domains+0xcc>
         :                      __lse_atomic_andnot():
         :                      ATOMIC_OP(andnot, stclr)
    0.00 :   ffff800010125e48:       mov     w0, #0x3                        // #3
    0.00 :   ffff800010125e4c:       stclr   w0, [x1]
    0.00 :   ffff800010125e50:       ldr     w0, [x19, #2568]
    0.00 :   ffff800010125e54:       b       ffff800010125e1c <run_rebalance_domains+0x64>
         :                      __ll_sc_atomic_fetch_andnot():
         :                      /*
         :                      * GAS converts the mysterious and undocumented BIC (immediate) alias to
         :                      * an AND (immediate) instruction with the immediate inverted. We don't
         :                      * have a constraint for this, so fall back to register.
         :                      */
         :                      ATOMIC_OPS(andnot, bic, )
    0.00 :   ffff800010125e58:       mov     w0, #0x3                        // #3
    0.00 :   ffff800010125e5c:       add     x2, x2, #0x30
    0.00 :   ffff800010125e60:       b       ffff8000101275f8 <sched_group_set_shares+0x710>
         :                      nohz_idle_balance():
         :                      if (!(flags & NOHZ_KICK_MASK))
    0.00 :   ffff800010125e64:       tst     x1, #0x3
    0.00 :   ffff800010125e68:       b.eq    ffff800010125e18 <run_rebalance_domains+0x60>  // b.none
         :                      _nohz_idle_balance(this_rq, flags, idle);
    0.00 :   ffff800010125e6c:       mov     w2, #0x0                        // #0
    0.00 :   ffff800010125e70:       mov     x0, x19
    0.00 :   ffff800010125e74:       bl      ffff800010125bb8 <_nohz_idle_balance>
         :                      run_rebalance_domains():
         :                      }
    0.00 :   ffff800010125e78:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010125e7c:       ldp     x29, x30, [sp], #32
    0.00 :   ffff800010125e80:       ret
         :                      __ll_sc_atomic_andnot():
    0.00 :   ffff800010125e84:       mov     w0, #0x3                        // #3
    0.00 :   ffff800010125e88:       add     x2, x2, #0x30
    0.00 :   ffff800010125e8c:       b       ffff800010127614 <sched_group_set_shares+0x72c>
    0.00 :   ffff800010125e90:       b       ffff800010125e18 <run_rebalance_domains+0x60>
 Percent |	Source code & Disassembly of vmlinux for cycles (13 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010ca29e8 <memchr_inv>:
         :                      memchr_inv():
         :                      * returns the address of the first character other than @c, or %NULL
         :                      * if the whole buffer contains just @c.
         :                      */
         :                      void *memchr_inv(const void *start, int c, size_t bytes)
         :                      {
         :                      u8 value = c;
    0.00 :   ffff800010ca29e8:       and     w1, w1, #0xff
         :                      u64 value64;
         :                      unsigned int words, prefix;
         :
         :                      if (bytes <= 16)
    0.00 :   ffff800010ca29ec:       cmp     x2, #0x10
    0.00 :   ffff800010ca29f0:       b.hi    ffff800010ca2a3c <memchr_inv+0x54>  // b.pmore
         :                      return check_bytes8(start, value, bytes);
    0.00 :   ffff800010ca29f4:       mov     w3, w2
         :                      check_bytes8():
         :                      while (bytes) {
    0.00 :   ffff800010ca29f8:       cbz     x2, ffff800010ca2a30 <memchr_inv+0x48>
         :                      if (*start != value)
    0.00 :   ffff800010ca29fc:       ldrb    w2, [x0]
   15.08 :   ffff800010ca2a00:       cmp     w2, w1
    0.00 :   ffff800010ca2a04:       b.ne    ffff800010ca2b10 <memchr_inv+0x128>  // b.any
   53.23 :   ffff800010ca2a08:       sub     w2, w3, #0x1
    0.00 :   ffff800010ca2a0c:       add     x2, x2, #0x1
    0.00 :   ffff800010ca2a10:       add     x2, x0, x2
    0.00 :   ffff800010ca2a14:       b       ffff800010ca2a24 <memchr_inv+0x3c>
    8.43 :   ffff800010ca2a18:       ldrb    w3, [x0]
    0.00 :   ffff800010ca2a1c:       cmp     w3, w1
    0.00 :   ffff800010ca2a20:       b.ne    ffff800010ca2b10 <memchr_inv+0x128>  // b.any
         :                      start++;
   23.27 :   ffff800010ca2a24:       add     x0, x0, #0x1
         :                      while (bytes) {
    0.00 :   ffff800010ca2a28:       cmp     x0, x2
    0.00 :   ffff800010ca2a2c:       b.ne    ffff800010ca2a18 <memchr_inv+0x30>  // b.any
         :                      return NULL;
    0.00 :   ffff800010ca2a30:       mov     x3, #0x0                        // #0
         :                      memchr_inv():
         :                      start += 8;
         :                      words--;
         :                      }
         :
         :                      return check_bytes8(start, value, bytes % 8);
         :                      }
    0.00 :   ffff800010ca2a34:       mov     x0, x3
    0.00 :   ffff800010ca2a38:       ret
         :                      if (prefix) {
    0.00 :   ffff800010ca2a3c:       ands    w6, w0, #0x7
    0.00 :   ffff800010ca2a40:       b.ne    ffff800010ca2acc <memchr_inv+0xe4>  // b.any
         :                      words = bytes / 8;
    0.00 :   ffff800010ca2a44:       lsr     x3, x2, #3
         :                      while (words) {
    0.00 :   ffff800010ca2a48:       cbz     w3, ffff800010ca2a8c <memchr_inv+0xa4>
         :                      value64 = value;
    0.00 :   ffff800010ca2a4c:       and     x5, x1, #0xff
         :                      value64 *= 0x0101010101010101ULL;
    0.00 :   ffff800010ca2a50:       mov     x6, #0x101010101010101          // #72340172838076673
         :                      if (*(u64 *)start != value64)
    0.00 :   ffff800010ca2a54:       ldr     x4, [x0]
         :                      value64 *= 0x0101010101010101ULL;
    0.00 :   ffff800010ca2a58:       mul     x5, x5, x6
         :                      if (*(u64 *)start != value64)
    0.00 :   ffff800010ca2a5c:       cmp     x5, x4
    0.00 :   ffff800010ca2a60:       b.ne    ffff800010ca2b1c <memchr_inv+0x134>  // b.any
    0.00 :   ffff800010ca2a64:       sub     w3, w3, #0x1
    0.00 :   ffff800010ca2a68:       add     x3, x3, #0x1
    0.00 :   ffff800010ca2a6c:       add     x3, x0, x3, lsl #3
    0.00 :   ffff800010ca2a70:       b       ffff800010ca2a80 <memchr_inv+0x98>
    0.00 :   ffff800010ca2a74:       ldr     x4, [x0]
    0.00 :   ffff800010ca2a78:       cmp     x4, x5
    0.00 :   ffff800010ca2a7c:       b.ne    ffff800010ca2b1c <memchr_inv+0x134>  // b.any
         :                      start += 8;
    0.00 :   ffff800010ca2a80:       add     x0, x0, #0x8
         :                      while (words) {
    0.00 :   ffff800010ca2a84:       cmp     x0, x3
    0.00 :   ffff800010ca2a88:       b.ne    ffff800010ca2a74 <memchr_inv+0x8c>  // b.any
         :                      check_bytes8():
         :                      while (bytes) {
    0.00 :   ffff800010ca2a8c:       ands    w2, w2, #0x7
    0.00 :   ffff800010ca2a90:       b.eq    ffff800010ca2a30 <memchr_inv+0x48>  // b.none
         :                      if (*start != value)
    0.00 :   ffff800010ca2a94:       ldrb    w3, [x0]
    0.00 :   ffff800010ca2a98:       cmp     w3, w1
    0.00 :   ffff800010ca2a9c:       b.ne    ffff800010ca2b10 <memchr_inv+0x128>  // b.any
    0.00 :   ffff800010ca2aa0:       sub     w2, w2, #0x1
    0.00 :   ffff800010ca2aa4:       add     x2, x2, #0x1
    0.00 :   ffff800010ca2aa8:       add     x2, x0, x2
    0.00 :   ffff800010ca2aac:       b       ffff800010ca2abc <memchr_inv+0xd4>
    0.00 :   ffff800010ca2ab0:       ldrb    w3, [x0]
    0.00 :   ffff800010ca2ab4:       cmp     w3, w1
    0.00 :   ffff800010ca2ab8:       b.ne    ffff800010ca2b10 <memchr_inv+0x128>  // b.any
         :                      start++;
    0.00 :   ffff800010ca2abc:       add     x0, x0, #0x1
         :                      while (bytes) {
    0.00 :   ffff800010ca2ac0:       cmp     x0, x2
    0.00 :   ffff800010ca2ac4:       b.ne    ffff800010ca2ab0 <memchr_inv+0xc8>  // b.any
    0.00 :   ffff800010ca2ac8:       b       ffff800010ca2a30 <memchr_inv+0x48>
    0.00 :   ffff800010ca2acc:       mov     w4, #0x7                        // #7
    0.00 :   ffff800010ca2ad0:       sub     w4, w4, w6
         :                      memchr_inv():
         :                      prefix = 8 - prefix;
    0.00 :   ffff800010ca2ad4:       mov     w3, #0x8                        // #8
    0.00 :   ffff800010ca2ad8:       add     x4, x4, #0x1
    0.00 :   ffff800010ca2adc:       sub     w6, w3, w6
    0.00 :   ffff800010ca2ae0:       add     x4, x0, x4
    0.00 :   ffff800010ca2ae4:       mov     x3, x0
         :                      check_bytes8():
         :                      if (*start != value)
    0.00 :   ffff800010ca2ae8:       ldrb    w5, [x3]
    0.00 :   ffff800010ca2aec:       cmp     w5, w1
    0.00 :   ffff800010ca2af0:       b.ne    ffff800010ca2b3c <memchr_inv+0x154>  // b.any
         :                      start++;
    0.00 :   ffff800010ca2af4:       add     x3, x3, #0x1
         :                      while (bytes) {
    0.00 :   ffff800010ca2af8:       cmp     x3, x4
    0.00 :   ffff800010ca2afc:       b.ne    ffff800010ca2ae8 <memchr_inv+0x100>  // b.any
         :                      memchr_inv():
         :                      start += prefix;
    0.00 :   ffff800010ca2b00:       mov     w3, w6
    0.00 :   ffff800010ca2b04:       add     x0, x0, x3
         :                      bytes -= prefix;
    0.00 :   ffff800010ca2b08:       sub     x2, x2, x3
    0.00 :   ffff800010ca2b0c:       b       ffff800010ca2a44 <memchr_inv+0x5c>
         :                      check_bytes8():
         :                      start++;
    0.00 :   ffff800010ca2b10:       mov     x3, x0
         :                      memchr_inv():
         :                      }
    0.00 :   ffff800010ca2b14:       mov     x0, x3
    0.00 :   ffff800010ca2b18:       ret
    0.00 :   ffff800010ca2b1c:       add     x3, x0, #0x8
         :                      check_bytes8():
         :                      if (*start != value)
    0.00 :   ffff800010ca2b20:       ldrb    w2, [x0]
    0.00 :   ffff800010ca2b24:       cmp     w2, w1
    0.00 :   ffff800010ca2b28:       b.ne    ffff800010ca2b10 <memchr_inv+0x128>  // b.any
         :                      start++;
    0.00 :   ffff800010ca2b2c:       add     x0, x0, #0x1
         :                      while (bytes) {
    0.00 :   ffff800010ca2b30:       cmp     x0, x3
    0.00 :   ffff800010ca2b34:       b.ne    ffff800010ca2b20 <memchr_inv+0x138>  // b.any
    0.00 :   ffff800010ca2b38:       b       ffff800010ca2a30 <memchr_inv+0x48>
         :                      memchr_inv():
         :                      if (r)
    0.00 :   ffff800010ca2b3c:       cbz     x3, ffff800010ca2b00 <memchr_inv+0x118>
         :                      }
    0.00 :   ffff800010ca2b40:       mov     x0, x3
    0.00 :   ffff800010ca2b44:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (12 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101f2f80 <vmstat_shepherd>:
         :                      vmstat_shepherd():
         :                      static void vmstat_shepherd(struct work_struct *w);
         :
         :                      static DECLARE_DEFERRABLE_WORK(shepherd, vmstat_shepherd);
         :
         :                      static void vmstat_shepherd(struct work_struct *w)
         :                      {
    0.00 :   ffff8000101f2f80:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff8000101f2f84:       mov     x29, sp
    0.00 :   ffff8000101f2f88:       stp     x19, x20, [sp, #16]
         :                      int cpu;
         :
         :                      get_online_cpus();
         :                      /* Check processors whose vmstat worker threads have been disabled */
         :                      for_each_online_cpu(cpu) {
         :                      struct delayed_work *dw = &per_cpu(vmstat_work, cpu);
    0.00 :   ffff8000101f2f8c:       adrp    x20, ffff8000114d3000 <pmu_sb_events>
    0.00 :   ffff8000101f2f90:       add     x20, x20, #0x8b8
         :                      {
    0.00 :   ffff8000101f2f94:       stp     x21, x22, [sp, #32]
         :                      struct delayed_work *dw = &per_cpu(vmstat_work, cpu);
    0.00 :   ffff8000101f2f98:       add     x20, x20, #0x2a0
    0.00 :   ffff8000101f2f9c:       adrp    x21, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000101f2fa0:       add     x21, x21, #0x8e8
         :                      {
    0.00 :   ffff8000101f2fa4:       stp     x23, x24, [sp, #48]
         :                      for_each_online_cpu(cpu) {
    0.00 :   ffff8000101f2fa8:       mov     w19, #0xffffffff                // #-1
    0.00 :   ffff8000101f2fac:       adrp    x23, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff8000101f2fb0:       adrp    x22, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      get_online_cpus():
         :                      #endif  /* !CONFIG_HOTPLUG_CPU */
         :
         :                      /* Wrappers which go away once all code is converted */
         :                      static inline void cpu_hotplug_begin(void) { cpus_write_lock(); }
         :                      static inline void cpu_hotplug_done(void) { cpus_write_unlock(); }
         :                      static inline void get_online_cpus(void) { cpus_read_lock(); }
    0.00 :   ffff8000101f2fb4:       bl      ffff8000100e6d30 <cpus_read_lock>
         :                      vmstat_shepherd():
   91.63 :   ffff8000101f2fb8:       add     x1, x23, #0x120
    0.00 :   ffff8000101f2fbc:       mov     w0, w19
    0.00 :   ffff8000101f2fc0:       bl      ffff800010c93a58 <cpumask_next>
    0.00 :   ffff8000101f2fc4:       mov     w19, w0
    0.00 :   ffff8000101f2fc8:       ldr     w2, [x22, #692]
         :                      struct delayed_work *dw = &per_cpu(vmstat_work, cpu);
    0.00 :   ffff8000101f2fcc:       mov     x1, x20
         :                      for_each_online_cpu(cpu) {
    0.00 :   ffff8000101f2fd0:       cmp     w0, w2
    0.00 :   ffff8000101f2fd4:       b.cs    ffff8000101f302c <vmstat_shepherd+0xac>  // b.hs, b.nlast
         :                      struct delayed_work *dw = &per_cpu(vmstat_work, cpu);
    0.00 :   ffff8000101f2fd8:       ldr     x3, [x21, w19, sxtw #3]
    0.00 :   ffff8000101f2fdc:       add     x24, x1, x3
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff8000101f2fe0:       ldr     x1, [x1, x3]
         :                      vmstat_shepherd():
         :
         :                      if (!delayed_work_pending(dw) && need_update(cpu))
    0.00 :   ffff8000101f2fe4:       tbnz    w1, #0, ffff8000101f2fb8 <vmstat_shepherd+0x38>
    8.37 :   ffff8000101f2fe8:       bl      ffff8000101f2510 <need_update>
    0.00 :   ffff8000101f2fec:       tst     w0, #0xff
    0.00 :   ffff8000101f2ff0:       b.eq    ffff8000101f2fb8 <vmstat_shepherd+0x38>  // b.none
         :                      queue_delayed_work_on(cpu, mm_percpu_wq, dw, 0);
    0.00 :   ffff8000101f2ff4:       adrp    x1, ffff800011aaa000 <pmus_srcu+0x18>
    0.00 :   ffff8000101f2ff8:       mov     x2, x24
    0.00 :   ffff8000101f2ffc:       mov     x3, #0x0                        // #0
    0.00 :   ffff8000101f3000:       mov     w0, w19
    0.00 :   ffff8000101f3004:       ldr     x1, [x1, #2600]
    0.00 :   ffff8000101f3008:       bl      ffff800010104848 <queue_delayed_work_on>
         :                      for_each_online_cpu(cpu) {
    0.00 :   ffff8000101f300c:       add     x1, x23, #0x120
    0.00 :   ffff8000101f3010:       mov     w0, w19
    0.00 :   ffff8000101f3014:       bl      ffff800010c93a58 <cpumask_next>
    0.00 :   ffff8000101f3018:       mov     w19, w0
    0.00 :   ffff8000101f301c:       ldr     w2, [x22, #692]
         :                      struct delayed_work *dw = &per_cpu(vmstat_work, cpu);
    0.00 :   ffff8000101f3020:       mov     x1, x20
         :                      for_each_online_cpu(cpu) {
    0.00 :   ffff8000101f3024:       cmp     w0, w2
    0.00 :   ffff8000101f3028:       b.cc    ffff8000101f2fd8 <vmstat_shepherd+0x58>  // b.lo, b.ul, b.last
         :                      put_online_cpus():
         :                      static inline void put_online_cpus(void) { cpus_read_unlock(); }
    0.00 :   ffff8000101f302c:       bl      ffff8000100e6e48 <cpus_read_unlock>
         :                      vmstat_shepherd():
         :                      }
         :                      put_online_cpus();
         :
         :                      schedule_delayed_work(&shepherd,
    0.00 :   ffff8000101f3030:       adrp    x0, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff8000101f3034:       ldrsw   x0, [x0, #908]
    0.00 :   ffff8000101f3038:       bl      ffff800010167ad0 <round_jiffies_relative>
         :                      queue_delayed_work():
         :                      */
         :                      static inline bool queue_delayed_work(struct workqueue_struct *wq,
         :                      struct delayed_work *dwork,
         :                      unsigned long delay)
         :                      {
         :                      return queue_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);
    0.00 :   ffff8000101f303c:       mov     x3, x0
         :                      schedule_delayed_work():
         :                      * workqueue.
         :                      */
         :                      static inline bool schedule_delayed_work(struct delayed_work *dwork,
         :                      unsigned long delay)
         :                      {
         :                      return queue_delayed_work(system_wq, dwork, delay);
    0.00 :   ffff8000101f3040:       adrp    x1, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      queue_delayed_work():
         :                      return queue_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);
    0.00 :   ffff8000101f3044:       adrp    x2, ffff8000118bb000 <pmus_lock+0x18>
    0.00 :   ffff8000101f3048:       mov     w0, #0x100                      // #256
    0.00 :   ffff8000101f304c:       add     x2, x2, #0x9e0
    0.00 :   ffff8000101f3050:       ldr     x1, [x1, #432]
    0.00 :   ffff8000101f3054:       bl      ffff800010104848 <queue_delayed_work_on>
         :                      vmstat_shepherd():
         :                      round_jiffies_relative(sysctl_stat_interval));
         :                      }
    0.00 :   ffff8000101f3058:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000101f305c:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000101f3060:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff8000101f3064:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000101f3068:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (16 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000106fcd00 <iommu_map_atomic>:
         :                      iommu_map_atomic():
         :                      }
         :                      EXPORT_SYMBOL_GPL(iommu_map);
         :
         :                      int iommu_map_atomic(struct iommu_domain *domain, unsigned long iova,
         :                      phys_addr_t paddr, size_t size, int prot)
         :                      {
    6.22 :   ffff8000106fcd00:       stp     x29, x30, [sp, #-16]!
         :                      return __iommu_map(domain, iova, paddr, size, prot, GFP_ATOMIC);
    0.00 :   ffff8000106fcd04:       mov     w5, #0xa20                      // #2592
         :                      {
    0.00 :   ffff8000106fcd08:       mov     x29, sp
         :                      return __iommu_map(domain, iova, paddr, size, prot, GFP_ATOMIC);
    0.00 :   ffff8000106fcd0c:       bl      ffff8000106fcb28 <__iommu_map>
         :                      }
   93.78 :   ffff8000106fcd10:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000106fcd14:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (14 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001015d1d8 <note_gp_changes>:
         :                      note_gp_changes():
         :                      rcu_gpnum_ovf(rnp, rdp);
         :                      return ret;
         :                      }
         :
         :                      static void note_gp_changes(struct rcu_data *rdp)
         :                      {
    7.43 :   ffff80001015d1d8:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff80001015d1dc:       mov     x29, sp
    0.00 :   ffff80001015d1e0:       stp     x20, x21, [sp, #24]
         :                      arch_local_save_flags():
         :                      */
         :                      static inline unsigned long arch_local_save_flags(void)
         :                      {
         :                      unsigned long flags;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015d1e4:       mrs     x20, daif
         :                      arch_irqs_disabled_flags():
         :
         :                      static inline int arch_irqs_disabled_flags(unsigned long flags)
         :                      {
         :                      int res;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015d1e8:       and     w1, w20, #0x80
         :                      arch_local_irq_save():
         :
         :                      /*
         :                      * There are too many states with IRQs disabled, just keep the current
         :                      * state if interrupts are already disabled/masked.
         :                      */
         :                      if (!arch_irqs_disabled_flags(flags))
    0.00 :   ffff80001015d1ec:       cbnz    w1, ffff80001015d1f8 <note_gp_changes+0x20>
         :                      arch_local_irq_disable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015d1f0:       mov     x1, #0x60                       // #96
    7.33 :   ffff80001015d1f4:       msr     daifset, #0x2
         :                      note_gp_changes():
         :                      unsigned long flags;
         :                      bool needwake;
         :                      struct rcu_node *rnp;
         :
         :                      local_irq_save(flags);
         :                      rnp = rdp->mynode;
    0.00 :   ffff80001015d1f8:       ldr     x21, [x0, #24]
         :                      if ((rdp->gp_seq == rcu_seq_current(&rnp->gp_seq) &&
    0.00 :   ffff80001015d1fc:       ldr     x2, [x0]
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001015d200:       ldr     x1, [x21, #8]
         :                      note_gp_changes():
    0.00 :   ffff80001015d204:       cmp     x2, x1
    0.00 :   ffff80001015d208:       b.ne    ffff80001015d228 <note_gp_changes+0x50>  // b.any
         :                      __read_once_size():
    0.00 :   ffff80001015d20c:       ldrb    w1, [x0, #20]
         :                      note_gp_changes():
    0.00 :   ffff80001015d210:       tst     w1, #0xff
    0.00 :   ffff80001015d214:       b.ne    ffff80001015d228 <note_gp_changes+0x50>  // b.any
         :                      arch_local_irq_restore():
         :                      /*
         :                      * restore saved IRQ state
         :                      */
         :                      static inline void arch_local_irq_restore(unsigned long flags)
         :                      {
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015d218:       msr     daif, x20
         :                      note_gp_changes():
         :                      }
         :                      needwake = __note_gp_changes(rnp, rdp);
         :                      raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
         :                      if (needwake)
         :                      rcu_gp_kthread_wake();
         :                      }
   78.24 :   ffff80001015d21c:       ldp     x20, x21, [sp, #24]
    0.00 :   ffff80001015d220:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001015d224:       ret
    0.00 :   ffff80001015d228:       str     x19, [x29, #16]
    0.00 :   ffff80001015d22c:       mov     x19, x0
         :                      !raw_spin_trylock_rcu_node(rnp)) { /* irqs already off, so later. */
    0.00 :   ffff80001015d230:       mov     x0, x21
    0.00 :   ffff80001015d234:       bl      ffff800010cb2670 <_raw_spin_trylock>
         :                      !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    0.00 :   ffff80001015d238:       cbnz    w0, ffff80001015d244 <note_gp_changes+0x6c>
    0.00 :   ffff80001015d23c:       ldr     x19, [x29, #16]
    0.00 :   ffff80001015d240:       b       ffff80001015d218 <note_gp_changes+0x40>
         :                      needwake = __note_gp_changes(rnp, rdp);
    0.00 :   ffff80001015d244:       mov     x1, x19
    0.00 :   ffff80001015d248:       mov     x0, x21
    0.00 :   ffff80001015d24c:       bl      ffff80001015d068 <__note_gp_changes>
    0.00 :   ffff80001015d250:       and     w19, w0, #0xff
         :                      raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    0.00 :   ffff80001015d254:       mov     x1, x20
    0.00 :   ffff80001015d258:       mov     x0, x21
    0.00 :   ffff80001015d25c:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :                      if (needwake)
    0.00 :   ffff80001015d260:       cbnz    w19, ffff80001015d274 <note_gp_changes+0x9c>
    0.00 :   ffff80001015d264:       ldr     x19, [x29, #16]
         :                      }
    0.00 :   ffff80001015d268:       ldp     x20, x21, [sp, #24]
    7.00 :   ffff80001015d26c:       ldp     x29, x30, [sp], #48
    0.00 :   ffff80001015d270:       ret
         :                      rcu_gp_kthread_wake();
    0.00 :   ffff80001015d274:       bl      ffff80001015af98 <rcu_gp_kthread_wake>
    0.00 :   ffff80001015d278:       ldr     x19, [x29, #16]
    0.00 :   ffff80001015d27c:       b       ffff80001015d21c <note_gp_changes+0x44>
 Percent |	Source code & Disassembly of vmlinux for cycles (15 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001045dbc0 <hctx_unlock>:
         :                      hctx_unlock():
         :                      put_cpu();
         :                      }
         :
         :                      static void hctx_unlock(struct blk_mq_hw_ctx *hctx, int srcu_idx)
         :                      __releases(hctx->srcu)
         :                      {
   26.82 :   ffff80001045dbc0:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff80001045dbc4:       mov     x29, sp
         :                      if (!(hctx->flags & BLK_MQ_F_BLOCKING))
   33.32 :   ffff80001045dbc8:       ldr     x2, [x0, #192]
    0.00 :   ffff80001045dbcc:       tbz     w2, #5, ffff80001045dbe8 <hctx_unlock+0x28>
         :                      rcu_read_unlock();
         :                      else
         :                      srcu_read_unlock(hctx->srcu, srcu_idx);
    0.00 :   ffff80001045dbd0:       add     x0, x0, #0x240
         :                      srcu_read_unlock():
         :                      * Exit an SRCU read-side critical section.
         :                      */
         :                      static inline void srcu_read_unlock(struct srcu_struct *ssp, int idx)
         :                      __releases(ssp)
         :                      {
         :                      WARN_ON_ONCE(idx & ~0x1);
    0.00 :   ffff80001045dbd4:       tst     w1, #0xfffffffe
    0.00 :   ffff80001045dbd8:       b.ne    ffff80001045dbf4 <hctx_unlock+0x34>  // b.any
         :                      rcu_lock_release(&(ssp)->dep_map);
         :                      __srcu_read_unlock(ssp, idx);
    0.00 :   ffff80001045dbdc:       bl      ffff800010159868 <__srcu_read_unlock>
         :                      hctx_unlock():
         :                      }
    0.00 :   ffff80001045dbe0:       ldp     x29, x30, [sp], #16
    0.00 :   ffff80001045dbe4:       ret
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
   19.93 :   ffff80001045dbe8:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      hctx_unlock():
   19.93 :   ffff80001045dbec:       ldp     x29, x30, [sp], #16
    0.00 :   ffff80001045dbf0:       ret
         :                      srcu_read_unlock():
         :                      WARN_ON_ONCE(idx & ~0x1);
    0.00 :   ffff80001045dbf4:       brk     #0x800
    0.00 :   ffff80001045dbf8:       b       ffff80001045dbdc <hctx_unlock+0x1c>
 Percent |	Source code & Disassembly of vmlinux for cycles (15 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010453da8 <should_fail_bio.isra.39>:
         :                      should_fail_bio():
         :                      static noinline int should_fail_bio(struct bio *bio)
         :                      {
         :                      if (should_fail_request(&bio->bi_disk->part0, bio->bi_iter.bi_size))
         :                      return -EIO;
         :                      return 0;
         :                      }
  100.00 :   ffff800010453da8:       mov     w0, #0x0                        // #0
    0.00 :   ffff800010453dac:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (15 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001045dd00 <blk_mq_update_dispatch_busy.isra.47.part.48>:
         :                      blk_mq_update_dispatch_busy():
         :                      unsigned int ewma;
         :
         :                      if (hctx->queue->elevator)
         :                      return;
         :
         :                      ewma = hctx->dispatch_busy;
   20.04 :   ffff80001045dd00:       ldr     w3, [x0]
         :                      static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
    0.00 :   ffff80001045dd04:       and     w1, w1, #0xff
         :
         :                      if (!ewma && !busy)
    0.00 :   ffff80001045dd08:       eor     w2, w1, #0x1
    0.00 :   ffff80001045dd0c:       cmp     w3, #0x0
   59.95 :   ffff80001045dd10:       cset    w4, eq  // eq = none
    0.00 :   ffff80001045dd14:       tst     w4, w2
    0.00 :   ffff80001045dd18:       b.ne    ffff80001045dd38 <blk_mq_update_dispatch_busy.isra.47.part.48+0x38>  // b.any
         :                      return;
         :
         :                      ewma *= BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT - 1;
    0.00 :   ffff80001045dd1c:       lsl     w2, w3, #3
         :                      if (busy)
         :                      ewma += 1 << BLK_MQ_DISPATCH_BUSY_EWMA_FACTOR;
    0.00 :   ffff80001045dd20:       cmp     w1, #0x0
         :                      ewma *= BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT - 1;
    0.00 :   ffff80001045dd24:       sub     w1, w2, w3
         :                      ewma += 1 << BLK_MQ_DISPATCH_BUSY_EWMA_FACTOR;
    0.00 :   ffff80001045dd28:       add     w2, w1, #0x10
    0.00 :   ffff80001045dd2c:       csel    w1, w2, w1, ne  // ne = any
         :                      ewma /= BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT;
    0.00 :   ffff80001045dd30:       lsr     w1, w1, #3
         :
         :                      hctx->dispatch_busy = ewma;
    0.00 :   ffff80001045dd34:       str     w1, [x0]
         :                      }
   20.00 :   ffff80001045dd38:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (9 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001026bfb0 <memcg_kmem_put_cache>:
         :                      memcg_kmem_put_cache():
         :                      * memcg_kmem_put_cache: drop reference taken by memcg_kmem_get_cache
         :                      * @cachep: the cache returned by memcg_kmem_get_cache
         :                      */
         :                      void memcg_kmem_put_cache(struct kmem_cache *cachep)
         :                      {
         :                      if (!is_root_cache(cachep))
    0.00 :   ffff80001026bfb0:       ldr     x1, [x0, #208]
    0.00 :   ffff80001026bfb4:       cbz     x1, ffff80001026bfd4 <memcg_kmem_put_cache+0x24>
         :                      {
    0.00 :   ffff80001026bfb8:       stp     x29, x30, [sp, #-16]!
         :                      percpu_ref_put():
         :                      *
         :                      * This function is safe to call as long as @ref is between init and exit.
         :                      */
         :                      static inline void percpu_ref_put(struct percpu_ref *ref)
         :                      {
         :                      percpu_ref_put_many(ref, 1);
    0.00 :   ffff80001026bfbc:       mov     x1, #0x1                        // #1
    0.00 :   ffff80001026bfc0:       add     x0, x0, #0x100
         :                      memcg_kmem_put_cache():
    0.00 :   ffff80001026bfc4:       mov     x29, sp
         :                      percpu_ref_put():
    0.00 :   ffff80001026bfc8:       bl      ffff800010264530 <percpu_ref_put_many>
         :                      memcg_kmem_put_cache():
         :                      percpu_ref_put(&cachep->memcg_params.refcnt);
         :                      }
    0.00 :   ffff80001026bfcc:       ldp     x29, x30, [sp], #16
    0.00 :   ffff80001026bfd0:       ret
  100.00 :   ffff80001026bfd4:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (7 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010167b58 <run_timer_softirq>:
         :                      run_timer_softirq():
         :
         :                      /*
         :                      * This function runs timers and the timer-tq in bottom half context.
         :                      */
         :                      static __latent_entropy void run_timer_softirq(struct softirq_action *h)
         :                      {
    0.00 :   ffff800010167b58:       stp     x29, x30, [sp, #-160]!
    0.00 :   ffff800010167b5c:       mov     x29, sp
    0.00 :   ffff800010167b60:       str     x20, [sp, #24]
         :                      struct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);
    0.00 :   ffff800010167b64:       adrp    x20, ffff8000114d0000 <safe_print_seq+0x1e38>
         :                      {
   14.30 :   ffff800010167b68:       stp     x22, x23, [sp, #40]
    0.00 :   ffff800010167b6c:       adrp    x23, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010167b70:       add     x0, x23, #0x8c8
         :                      __run_timers():
         :                      if (!time_after_eq(jiffies, base->clk))
    0.00 :   ffff800010167b74:       adrp    x22, ffff800011897000 <bit_wait_table+0xe80>
         :                      run_timer_softirq():
         :                      {
    0.00 :   ffff800010167b78:       ldr     x1, [x0]
    0.00 :   ffff800010167b7c:       str     x1, [x29, #152]
    0.00 :   ffff800010167b80:       mov     x1, #0x0                        // #0
         :                      struct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);
    0.00 :   ffff800010167b84:       add     x20, x20, #0x380
         :                      __my_cpu_offset():
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
         :                      "mrs %0, tpidr_el2",
         :                      ARM64_HAS_VIRT_HOST_EXTN)
         :                      : "=r" (off) :
         :                      "Q" (*(const unsigned long *)current_stack_pointer));
    0.00 :   ffff800010167b88:       mov     x1, sp
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800010167b8c:       mrs     x0, tpidr_el1
         :                      run_timer_softirq():
    0.00 :   ffff800010167b90:       add     x20, x20, x0
         :                      __run_timers():
         :                      if (!time_after_eq(jiffies, base->clk))
    0.00 :   ffff800010167b94:       ldr     x0, [x22, #2432]
    0.00 :   ffff800010167b98:       ldr     x2, [x20, #16]
    0.00 :   ffff800010167b9c:       sub     x0, x0, x2
    0.00 :   ffff800010167ba0:       tbz     x0, #63, ffff800010167d7c <run_timer_softirq+0x224>
         :                      __my_cpu_offset():
    0.00 :   ffff800010167ba4:       mrs     x1, tpidr_el1
         :                      run_timer_softirq():
         :
         :                      __run_timers(base);
         :                      if (IS_ENABLED(CONFIG_NO_HZ_COMMON))
         :                      __run_timers(this_cpu_ptr(&timer_bases[BASE_DEF]));
    0.00 :   ffff800010167ba8:       adrp    x20, ffff8000114d1000 <timer_bases+0xc80>
    0.00 :   ffff800010167bac:       add     x20, x20, #0x600
    0.00 :   ffff800010167bb0:       add     x20, x20, x1
         :                      __run_timers():
         :                      if (!time_after_eq(jiffies, base->clk))
    0.00 :   ffff800010167bb4:       ldr     x0, [x22, #2432]
    0.00 :   ffff800010167bb8:       ldr     x1, [x20, #16]
    0.00 :   ffff800010167bbc:       sub     x0, x0, x1
    0.00 :   ffff800010167bc0:       tbnz    x0, #63, ffff800010167d58 <run_timer_softirq+0x200>
   14.53 :   ffff800010167bc4:       stp     x24, x25, [x29, #56]
         :                      detach_timer():
         :                      entry->next = LIST_POISON2;
    0.00 :   ffff800010167bc8:       mov     x24, #0x122                     // #290
         :                      __run_timers():
         :                      raw_spin_lock_irq(&base->lock);
    0.00 :   ffff800010167bcc:       mov     x0, x20
    0.00 :   ffff800010167bd0:       str     x19, [x29, #16]
    0.00 :   ffff800010167bd4:       str     x21, [x29, #32]
         :                      detach_timer():
         :                      entry->next = LIST_POISON2;
    0.00 :   ffff800010167bd8:       movk    x24, #0xdead, lsl #48
    0.00 :   ffff800010167bdc:       str     x26, [x29, #72]
         :                      __run_timers():
         :                      raw_spin_lock_irq(&base->lock);
    0.00 :   ffff800010167be0:       bl      ffff800010cb2cd0 <_raw_spin_lock_irq>
         :                      base->must_forward_clk = false;
    0.00 :   ffff800010167be4:       strb    wzr, [x20, #37]
         :                      while (time_after_eq(jiffies, base->clk)) {
    0.00 :   ffff800010167be8:       ldr     x1, [x22, #2432]
    0.00 :   ffff800010167bec:       ldr     x0, [x20, #16]
    0.00 :   ffff800010167bf0:       sub     x1, x1, x0
    0.00 :   ffff800010167bf4:       tbnz    x1, #63, ffff800010167d40 <run_timer_softirq+0x1e8>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010167bf8:       ldr     x19, [x22, #2432]
         :                      collect_expired_timers():
         :                      if ((long)(now - base->clk) > 2) {
    0.00 :   ffff800010167bfc:       sub     x1, x19, x0
    0.00 :   ffff800010167c00:       cmp     x1, #0x2
    0.00 :   ffff800010167c04:       b.le    ffff800010167c20 <run_timer_softirq+0xc8>
         :                      unsigned long next = __next_timer_interrupt(base);
    0.00 :   ffff800010167c08:       mov     x0, x20
         :                      return 0;
    0.00 :   ffff800010167c0c:       mov     w25, #0x0                       // #0
         :                      unsigned long next = __next_timer_interrupt(base);
    0.00 :   ffff800010167c10:       bl      ffff8000101675c8 <__next_timer_interrupt>
         :                      if (time_after(next, now)) {
    0.00 :   ffff800010167c14:       sub     x1, x19, x0
    0.00 :   ffff800010167c18:       tbnz    x1, #63, ffff800010167c9c <run_timer_softirq+0x144>
         :                      base->clk = next;
    0.00 :   ffff800010167c1c:       str     x0, [x20, #16]
    0.00 :   ffff800010167c20:       add     x6, x20, #0x28
         :                      __collect_expired_timers():
         :                      int i, levels = 0;
    0.00 :   ffff800010167c24:       add     x7, x29, #0x50
         :                      vec = base->vectors + idx;
    0.00 :   ffff800010167c28:       add     x10, x20, #0x70
         :                      collect_expired_timers():
         :                      return 0;
    0.00 :   ffff800010167c2c:       mov     w3, #0x0                        // #0
         :                      __collect_expired_timers():
         :                      int i, levels = 0;
    0.00 :   ffff800010167c30:       mov     w25, #0x0                       // #0
         :                      __test_and_clear_bit():
         :                      * If two examples of this operation race, one can appear to succeed
         :                      * but actually fail.  You must protect multiple accesses with a lock.
         :                      */
         :                      static inline int __test_and_clear_bit(int nr, volatile unsigned long *addr)
         :                      {
         :                      unsigned long mask = BIT_MASK(nr);
    0.00 :   ffff800010167c34:       mov     x9, #0x1                        // #1
         :                      __collect_expired_timers():
         :                      idx = (clk & LVL_MASK) + i * LVL_SIZE;
    0.00 :   ffff800010167c38:       and     w1, w0, #0x3f
    0.00 :   ffff800010167c3c:       add     w1, w1, w3
         :                      __test_and_clear_bit():
         :                      unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
    0.00 :   ffff800010167c40:       asr     w2, w1, #6
         :                      unsigned long mask = BIT_MASK(nr);
    0.00 :   ffff800010167c44:       lsl     x5, x9, x1
         :                      unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
    0.00 :   ffff800010167c48:       sxtw    x2, w2
         :                      unsigned long old = *p;
    0.00 :   ffff800010167c4c:       ldr     x4, [x6, x2, lsl #3]
         :
         :                      *p = old & ~mask;
    0.00 :   ffff800010167c50:       bic     x8, x4, x5
    0.00 :   ffff800010167c54:       str     x8, [x6, x2, lsl #3]
         :                      __collect_expired_timers():
         :                      if (__test_and_clear_bit(idx, base->pending_map)) {
    0.00 :   ffff800010167c58:       tst     x5, x4
    0.00 :   ffff800010167c5c:       b.eq    ffff800010167c80 <run_timer_softirq+0x128>  // b.none
         :                      hlist_move_list():
         :                      * reference of the first entry if it exists.
         :                      */
         :                      static inline void hlist_move_list(struct hlist_head *old,
         :                      struct hlist_head *new)
         :                      {
         :                      new->first = old->first;
    0.00 :   ffff800010167c60:       ldr     x2, [x10, x1, lsl #3]
         :                      __collect_expired_timers():
         :                      hlist_move_list(vec, heads++);
    0.00 :   ffff800010167c64:       add     x4, x7, #0x8
         :                      hlist_move_list():
    0.00 :   ffff800010167c68:       str     x2, [x7]
         :                      if (new->first)
    0.00 :   ffff800010167c6c:       cbz     x2, ffff800010167c74 <run_timer_softirq+0x11c>
         :                      new->first->pprev = &new->first;
    0.00 :   ffff800010167c70:       str     x7, [x2, #8]
         :                      __collect_expired_timers():
         :                      levels++;
    0.00 :   ffff800010167c74:       add     w25, w25, #0x1
         :                      hlist_move_list(vec, heads++);
    0.00 :   ffff800010167c78:       mov     x7, x4
         :                      hlist_move_list():
         :                      old->first = NULL;
    0.00 :   ffff800010167c7c:       str     xzr, [x10, x1, lsl #3]
         :                      __collect_expired_timers():
         :                      if (clk & LVL_CLK_MASK)
    0.00 :   ffff800010167c80:       tst     x0, #0x7
    0.00 :   ffff800010167c84:       b.ne    ffff800010167c98 <run_timer_softirq+0x140>  // b.any
    0.00 :   ffff800010167c88:       add     w3, w3, #0x40
         :                      clk >>= LVL_CLK_SHIFT;
    0.00 :   ffff800010167c8c:       lsr     x0, x0, #3
         :                      for (i = 0; i < LVL_DEPTH; i++) {
    0.00 :   ffff800010167c90:       cmp     w3, #0x240
    0.00 :   ffff800010167c94:       b.ne    ffff800010167c38 <run_timer_softirq+0xe0>  // b.any
    0.00 :   ffff800010167c98:       ldr     x19, [x20, #16]
    0.00 :   ffff800010167c9c:       sub     w25, w25, #0x1
    0.00 :   ffff800010167ca0:       add     x0, x29, #0x50
         :                      __run_timers():
         :                      base->clk++;
    0.00 :   ffff800010167ca4:       add     x19, x19, #0x1
    0.00 :   ffff800010167ca8:       str     x19, [x20, #16]
    0.00 :   ffff800010167cac:       add     x21, x0, w25, sxtw #3
         :                      while (levels--)
    0.00 :   ffff800010167cb0:       cmn     w25, #0x1
    0.00 :   ffff800010167cb4:       b.eq    ffff800010167be8 <run_timer_softirq+0x90>  // b.none
         :                      __read_once_size():
    0.00 :   ffff800010167cb8:       ldr     x0, [x21]
         :                      expire_timers():
         :                      while (!hlist_empty(head)) {
    0.00 :   ffff800010167cbc:       cbz     x0, ffff800010167d10 <run_timer_softirq+0x1b8>
         :                      timer = hlist_entry(head->first, struct timer_list, entry);
    0.00 :   ffff800010167cc0:       ldr     x19, [x21]
         :                      base->running_timer = timer;
    0.00 :   ffff800010167cc4:       str     x19, [x20, #8]
         :                      __hlist_del():
         :                      struct hlist_node **pprev = n->pprev;
    0.00 :   ffff800010167cc8:       ldp     x0, x1, [x19]
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff800010167ccc:       str     x0, [x1]
         :                      __hlist_del():
         :                      if (next)
    0.00 :   ffff800010167cd0:       cbz     x0, ffff800010167cd8 <run_timer_softirq+0x180>
         :                      next->pprev = pprev;
    0.00 :   ffff800010167cd4:       str     x1, [x0, #8]
         :                      expire_timers():
         :                      if (timer->flags & TIMER_IRQSAFE) {
    0.00 :   ffff800010167cd8:       ldr     w0, [x19, #32]
         :                      detach_timer():
         :                      entry->pprev = NULL;
    0.00 :   ffff800010167cdc:       stp     x24, xzr, [x19]
         :                      expire_timers():
         :                      fn = timer->function;
    0.00 :   ffff800010167ce0:       ldr     x26, [x19, #24]
         :                      if (timer->flags & TIMER_IRQSAFE) {
    0.00 :   ffff800010167ce4:       tbz     w0, #21, ffff800010167d1c <run_timer_softirq+0x1c4>
         :                      raw_spin_unlock(&base->lock);
    0.00 :   ffff800010167ce8:       mov     x0, x20
    0.00 :   ffff800010167cec:       bl      ffff800010cb2408 <_raw_spin_unlock>
         :                      call_timer_fn(timer, fn, baseclk);
    0.00 :   ffff800010167cf0:       mov     x1, x26
    0.00 :   ffff800010167cf4:       mov     x0, x19
    0.00 :   ffff800010167cf8:       bl      ffff800010167798 <call_timer_fn.isra.32>
         :                      base->running_timer = NULL;
    0.00 :   ffff800010167cfc:       str     xzr, [x20, #8]
         :                      raw_spin_lock(&base->lock);
    0.00 :   ffff800010167d00:       mov     x0, x20
    0.00 :   ffff800010167d04:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010167d08:       ldr     x0, [x21]
         :                      expire_timers():
         :                      while (!hlist_empty(head)) {
    0.00 :   ffff800010167d0c:       cbnz    x0, ffff800010167cc0 <run_timer_softirq+0x168>
    0.00 :   ffff800010167d10:       sub     x21, x21, #0x8
    0.00 :   ffff800010167d14:       sub     w25, w25, #0x1
    0.00 :   ffff800010167d18:       b       ffff800010167cb0 <run_timer_softirq+0x158>
         :                      raw_spin_unlock_irq(&base->lock);
    0.00 :   ffff800010167d1c:       mov     x0, x20
    0.00 :   ffff800010167d20:       bl      ffff800010cb2bf0 <_raw_spin_unlock_irq>
         :                      call_timer_fn(timer, fn, baseclk);
    0.00 :   ffff800010167d24:       mov     x1, x26
    0.00 :   ffff800010167d28:       mov     x0, x19
    0.00 :   ffff800010167d2c:       bl      ffff800010167798 <call_timer_fn.isra.32>
         :                      base->running_timer = NULL;
    0.00 :   ffff800010167d30:       str     xzr, [x20, #8]
         :                      raw_spin_lock_irq(&base->lock);
    0.00 :   ffff800010167d34:       mov     x0, x20
    0.00 :   ffff800010167d38:       bl      ffff800010cb2cd0 <_raw_spin_lock_irq>
    0.00 :   ffff800010167d3c:       b       ffff800010167cb8 <run_timer_softirq+0x160>
         :                      __run_timers():
         :                      raw_spin_unlock_irq(&base->lock);
    0.00 :   ffff800010167d40:       mov     x0, x20
    0.00 :   ffff800010167d44:       bl      ffff800010cb2bf0 <_raw_spin_unlock_irq>
   14.64 :   ffff800010167d48:       ldr     x19, [x29, #16]
    0.00 :   ffff800010167d4c:       ldr     x21, [x29, #32]
    0.00 :   ffff800010167d50:       ldp     x24, x25, [x29, #56]
   27.89 :   ffff800010167d54:       ldr     x26, [x29, #72]
         :                      run_timer_softirq():
         :                      }
    0.00 :   ffff800010167d58:       add     x23, x23, #0x8c8
    0.00 :   ffff800010167d5c:       ldr     x1, [x29, #152]
    0.00 :   ffff800010167d60:       ldr     x0, [x23]
    0.00 :   ffff800010167d64:       eor     x0, x1, x0
    0.00 :   ffff800010167d68:       cbnz    x0, ffff800010167f18 <run_timer_softirq+0x3c0>
    0.00 :   ffff800010167d6c:       ldr     x20, [sp, #24]
    0.00 :   ffff800010167d70:       ldp     x22, x23, [sp, #40]
   14.65 :   ffff800010167d74:       ldp     x29, x30, [sp], #160
    0.00 :   ffff800010167d78:       ret
   13.99 :   ffff800010167d7c:       stp     x24, x25, [x29, #56]
         :                      detach_timer():
         :                      entry->next = LIST_POISON2;
    0.00 :   ffff800010167d80:       mov     x24, #0x122                     // #290
         :                      __run_timers():
         :                      raw_spin_lock_irq(&base->lock);
    0.00 :   ffff800010167d84:       mov     x0, x20
    0.00 :   ffff800010167d88:       str     x19, [x29, #16]
    0.00 :   ffff800010167d8c:       str     x21, [x29, #32]
         :                      detach_timer():
         :                      entry->next = LIST_POISON2;
    0.00 :   ffff800010167d90:       movk    x24, #0xdead, lsl #48
    0.00 :   ffff800010167d94:       str     x26, [x29, #72]
         :                      __run_timers():
         :                      raw_spin_lock_irq(&base->lock);
    0.00 :   ffff800010167d98:       bl      ffff800010cb2cd0 <_raw_spin_lock_irq>
         :                      base->must_forward_clk = false;
    0.00 :   ffff800010167d9c:       strb    wzr, [x20, #37]
         :                      while (time_after_eq(jiffies, base->clk)) {
    0.00 :   ffff800010167da0:       ldr     x1, [x22, #2432]
    0.00 :   ffff800010167da4:       ldr     x0, [x20, #16]
    0.00 :   ffff800010167da8:       sub     x1, x1, x0
    0.00 :   ffff800010167dac:       tbnz    x1, #63, ffff800010167ef8 <run_timer_softirq+0x3a0>
         :                      __read_once_size():
    0.00 :   ffff800010167db0:       ldr     x19, [x22, #2432]
         :                      collect_expired_timers():
         :                      if ((long)(now - base->clk) > 2) {
    0.00 :   ffff800010167db4:       sub     x1, x19, x0
    0.00 :   ffff800010167db8:       cmp     x1, #0x2
    0.00 :   ffff800010167dbc:       b.le    ffff800010167dd8 <run_timer_softirq+0x280>
         :                      unsigned long next = __next_timer_interrupt(base);
    0.00 :   ffff800010167dc0:       mov     x0, x20
         :                      return 0;
    0.00 :   ffff800010167dc4:       mov     w25, #0x0                       // #0
         :                      unsigned long next = __next_timer_interrupt(base);
    0.00 :   ffff800010167dc8:       bl      ffff8000101675c8 <__next_timer_interrupt>
         :                      if (time_after(next, now)) {
    0.00 :   ffff800010167dcc:       sub     x1, x19, x0
    0.00 :   ffff800010167dd0:       tbnz    x1, #63, ffff800010167e54 <run_timer_softirq+0x2fc>
         :                      base->clk = next;
    0.00 :   ffff800010167dd4:       str     x0, [x20, #16]
    0.00 :   ffff800010167dd8:       add     x6, x20, #0x28
         :                      __collect_expired_timers():
         :                      int i, levels = 0;
    0.00 :   ffff800010167ddc:       add     x7, x29, #0x50
         :                      vec = base->vectors + idx;
    0.00 :   ffff800010167de0:       add     x10, x20, #0x70
         :                      run_timer_softirq():
         :                      {
    0.00 :   ffff800010167de4:       mov     w3, #0x0                        // #0
         :                      __collect_expired_timers():
         :                      int i, levels = 0;
    0.00 :   ffff800010167de8:       mov     w25, #0x0                       // #0
         :                      __test_and_clear_bit():
         :                      unsigned long mask = BIT_MASK(nr);
    0.00 :   ffff800010167dec:       mov     x9, #0x1                        // #1
         :                      __collect_expired_timers():
         :                      idx = (clk & LVL_MASK) + i * LVL_SIZE;
    0.00 :   ffff800010167df0:       and     w1, w0, #0x3f
    0.00 :   ffff800010167df4:       add     w1, w1, w3
         :                      __test_and_clear_bit():
         :                      unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
    0.00 :   ffff800010167df8:       asr     w2, w1, #6
         :                      unsigned long mask = BIT_MASK(nr);
    0.00 :   ffff800010167dfc:       lsl     x5, x9, x1
         :                      unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
    0.00 :   ffff800010167e00:       sxtw    x2, w2
         :                      unsigned long old = *p;
    0.00 :   ffff800010167e04:       ldr     x4, [x6, x2, lsl #3]
         :                      *p = old & ~mask;
    0.00 :   ffff800010167e08:       bic     x8, x4, x5
    0.00 :   ffff800010167e0c:       str     x8, [x6, x2, lsl #3]
         :                      __collect_expired_timers():
         :                      if (__test_and_clear_bit(idx, base->pending_map)) {
    0.00 :   ffff800010167e10:       tst     x5, x4
    0.00 :   ffff800010167e14:       b.eq    ffff800010167e38 <run_timer_softirq+0x2e0>  // b.none
         :                      hlist_move_list():
         :                      new->first = old->first;
    0.00 :   ffff800010167e18:       ldr     x2, [x10, x1, lsl #3]
         :                      __collect_expired_timers():
         :                      hlist_move_list(vec, heads++);
    0.00 :   ffff800010167e1c:       add     x4, x7, #0x8
         :                      hlist_move_list():
    0.00 :   ffff800010167e20:       str     x2, [x7]
         :                      if (new->first)
    0.00 :   ffff800010167e24:       cbz     x2, ffff800010167e2c <run_timer_softirq+0x2d4>
         :                      new->first->pprev = &new->first;
    0.00 :   ffff800010167e28:       str     x7, [x2, #8]
         :                      __collect_expired_timers():
         :                      levels++;
    0.00 :   ffff800010167e2c:       add     w25, w25, #0x1
         :                      hlist_move_list(vec, heads++);
    0.00 :   ffff800010167e30:       mov     x7, x4
         :                      hlist_move_list():
         :                      old->first = NULL;
    0.00 :   ffff800010167e34:       str     xzr, [x10, x1, lsl #3]
         :                      __collect_expired_timers():
         :                      if (clk & LVL_CLK_MASK)
    0.00 :   ffff800010167e38:       tst     x0, #0x7
    0.00 :   ffff800010167e3c:       b.ne    ffff800010167e50 <run_timer_softirq+0x2f8>  // b.any
    0.00 :   ffff800010167e40:       add     w3, w3, #0x40
         :                      clk >>= LVL_CLK_SHIFT;
    0.00 :   ffff800010167e44:       lsr     x0, x0, #3
         :                      for (i = 0; i < LVL_DEPTH; i++) {
    0.00 :   ffff800010167e48:       cmp     w3, #0x240
    0.00 :   ffff800010167e4c:       b.ne    ffff800010167df0 <run_timer_softirq+0x298>  // b.any
    0.00 :   ffff800010167e50:       ldr     x19, [x20, #16]
    0.00 :   ffff800010167e54:       sub     w25, w25, #0x1
    0.00 :   ffff800010167e58:       add     x0, x29, #0x50
         :                      __run_timers():
         :                      base->clk++;
    0.00 :   ffff800010167e5c:       add     x19, x19, #0x1
    0.00 :   ffff800010167e60:       str     x19, [x20, #16]
    0.00 :   ffff800010167e64:       add     x21, x0, w25, sxtw #3
         :                      while (levels--)
    0.00 :   ffff800010167e68:       cmn     w25, #0x1
    0.00 :   ffff800010167e6c:       b.eq    ffff800010167da0 <run_timer_softirq+0x248>  // b.none
         :                      __read_once_size():
    0.00 :   ffff800010167e70:       ldr     x0, [x21]
         :                      expire_timers():
         :                      while (!hlist_empty(head)) {
    0.00 :   ffff800010167e74:       cbz     x0, ffff800010167ec8 <run_timer_softirq+0x370>
         :                      timer = hlist_entry(head->first, struct timer_list, entry);
    0.00 :   ffff800010167e78:       ldr     x19, [x21]
         :                      base->running_timer = timer;
    0.00 :   ffff800010167e7c:       str     x19, [x20, #8]
         :                      __hlist_del():
         :                      struct hlist_node **pprev = n->pprev;
    0.00 :   ffff800010167e80:       ldp     x0, x1, [x19]
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff800010167e84:       str     x0, [x1]
         :                      __hlist_del():
         :                      if (next)
    0.00 :   ffff800010167e88:       cbz     x0, ffff800010167e90 <run_timer_softirq+0x338>
         :                      next->pprev = pprev;
    0.00 :   ffff800010167e8c:       str     x1, [x0, #8]
         :                      expire_timers():
         :                      if (timer->flags & TIMER_IRQSAFE) {
    0.00 :   ffff800010167e90:       ldr     w0, [x19, #32]
         :                      detach_timer():
         :                      entry->pprev = NULL;
    0.00 :   ffff800010167e94:       stp     x24, xzr, [x19]
         :                      expire_timers():
         :                      fn = timer->function;
    0.00 :   ffff800010167e98:       ldr     x26, [x19, #24]
         :                      if (timer->flags & TIMER_IRQSAFE) {
    0.00 :   ffff800010167e9c:       tbz     w0, #21, ffff800010167ed4 <run_timer_softirq+0x37c>
         :                      raw_spin_unlock(&base->lock);
    0.00 :   ffff800010167ea0:       mov     x0, x20
    0.00 :   ffff800010167ea4:       bl      ffff800010cb2408 <_raw_spin_unlock>
         :                      call_timer_fn(timer, fn, baseclk);
    0.00 :   ffff800010167ea8:       mov     x1, x26
    0.00 :   ffff800010167eac:       mov     x0, x19
    0.00 :   ffff800010167eb0:       bl      ffff800010167798 <call_timer_fn.isra.32>
         :                      base->running_timer = NULL;
    0.00 :   ffff800010167eb4:       str     xzr, [x20, #8]
         :                      raw_spin_lock(&base->lock);
    0.00 :   ffff800010167eb8:       mov     x0, x20
    0.00 :   ffff800010167ebc:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010167ec0:       ldr     x0, [x21]
         :                      expire_timers():
         :                      while (!hlist_empty(head)) {
    0.00 :   ffff800010167ec4:       cbnz    x0, ffff800010167e78 <run_timer_softirq+0x320>
    0.00 :   ffff800010167ec8:       sub     x21, x21, #0x8
    0.00 :   ffff800010167ecc:       sub     w25, w25, #0x1
    0.00 :   ffff800010167ed0:       b       ffff800010167e68 <run_timer_softirq+0x310>
         :                      raw_spin_unlock_irq(&base->lock);
    0.00 :   ffff800010167ed4:       mov     x0, x20
    0.00 :   ffff800010167ed8:       bl      ffff800010cb2bf0 <_raw_spin_unlock_irq>
         :                      call_timer_fn(timer, fn, baseclk);
    0.00 :   ffff800010167edc:       mov     x1, x26
    0.00 :   ffff800010167ee0:       mov     x0, x19
    0.00 :   ffff800010167ee4:       bl      ffff800010167798 <call_timer_fn.isra.32>
         :                      base->running_timer = NULL;
    0.00 :   ffff800010167ee8:       str     xzr, [x20, #8]
         :                      raw_spin_lock_irq(&base->lock);
    0.00 :   ffff800010167eec:       mov     x0, x20
    0.00 :   ffff800010167ef0:       bl      ffff800010cb2cd0 <_raw_spin_lock_irq>
    0.00 :   ffff800010167ef4:       b       ffff800010167e70 <run_timer_softirq+0x318>
         :                      __run_timers():
         :                      raw_spin_unlock_irq(&base->lock);
    0.00 :   ffff800010167ef8:       mov     x0, x20
    0.00 :   ffff800010167efc:       bl      ffff800010cb2bf0 <_raw_spin_unlock_irq>
    0.00 :   ffff800010167f00:       ldr     x19, [x29, #16]
    0.00 :   ffff800010167f04:       mov     x1, sp
    0.00 :   ffff800010167f08:       ldr     x21, [x29, #32]
    0.00 :   ffff800010167f0c:       ldp     x24, x25, [x29, #56]
    0.00 :   ffff800010167f10:       ldr     x26, [x29, #72]
    0.00 :   ffff800010167f14:       b       ffff800010167ba4 <run_timer_softirq+0x4c>
    0.00 :   ffff800010167f18:       str     x19, [x29, #16]
    0.00 :   ffff800010167f1c:       str     x21, [x29, #32]
    0.00 :   ffff800010167f20:       stp     x24, x25, [x29, #56]
    0.00 :   ffff800010167f24:       str     x26, [x29, #72]
         :                      run_timer_softirq():
         :                      }
    0.00 :   ffff800010167f28:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (11 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010cad640 <preempt_schedule>:
         :                      preempt_schedule():
         :                      /*
         :                      * This is the entry point to schedule() from in-kernel preemption
         :                      * off of preempt_enable.
         :                      */
         :                      asmlinkage __visible void __sched notrace preempt_schedule(void)
         :                      {
   27.34 :   ffff800010cad640:       stp     x29, x30, [sp, #-16]!
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
   27.37 :   ffff800010cad644:       mrs     x0, sp_el0
         :                      preempt_schedule():
    0.00 :   ffff800010cad648:       mov     x29, sp
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010cad64c:       ldr     w0, [x0, #16]
         :                      preempt_schedule():
         :                      /*
         :                      * If there is a non-zero preempt_count or interrupts are disabled,
         :                      * we do not want to preempt the current task. Just return..
         :                      */
         :                      if (likely(!preemptible()))
    0.00 :   ffff800010cad650:       cbz     w0, ffff800010cad65c <preempt_schedule+0x1c>
         :                      return;
         :
         :                      preempt_schedule_common();
         :                      }
    0.00 :   ffff800010cad654:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010cad658:       ret
   45.30 :   ffff800010cad65c:       bl      ffff800010cad618 <preempt_schedule.part.91>
    0.00 :   ffff800010cad660:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010cad664:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (8 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010c93a58 <cpumask_next>:
         :                      cpumask_next():
         :                      * @srcp: the cpumask pointer
         :                      *
         :                      * Returns >= nr_cpu_ids if no further cpus set.
         :                      */
         :                      unsigned int cpumask_next(int n, const struct cpumask *srcp)
         :                      {
    0.00 :   ffff800010c93a58:       stp     x29, x30, [sp, #-16]!
         :                      /* -1 is a legal arg here. */
         :                      if (n != -1)
         :                      cpumask_check(n);
         :                      return find_next_bit(cpumask_bits(srcp), nr_cpumask_bits, n + 1);
   12.71 :   ffff800010c93a5c:       add     w2, w0, #0x1
         :                      {
    0.00 :   ffff800010c93a60:       mov     x0, x1
         :                      return find_next_bit(cpumask_bits(srcp), nr_cpumask_bits, n + 1);
    0.00 :   ffff800010c93a64:       mov     x1, #0x100                      // #256
         :                      {
   61.33 :   ffff800010c93a68:       mov     x29, sp
         :                      return find_next_bit(cpumask_bits(srcp), nr_cpumask_bits, n + 1);
   13.44 :   ffff800010c93a6c:       sxtw    x2, w2
    0.00 :   ffff800010c93a70:       bl      ffff800010487db8 <find_next_bit>
         :                      }
   12.52 :   ffff800010c93a74:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010c93a78:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (9 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010cad5d0 <preempt_schedule_common>:
         :                      preempt_schedule_common():
         :                      schedule();
         :                      preempt_disable();
         :                      }
         :
         :                      static void __sched notrace preempt_schedule_common(void)
         :                      {
   33.23 :   ffff800010cad5d0:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff800010cad5d4:       mov     x29, sp
    0.00 :   ffff800010cad5d8:       str     x19, [sp, #16]
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800010cad5dc:       mrs     x19, sp_el0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010cad5e0:       ldr     w0, [x19, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff800010cad5e4:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
   33.56 :   ffff800010cad5e8:       str     w0, [x19, #16]
         :                      preempt_schedule_common():
         :                      * traced. The other to still record the preemption latency,
         :                      * which can also be traced by the function tracer.
         :                      */
         :                      preempt_disable_notrace();
         :                      preempt_latency_start(1);
         :                      __schedule(true);
    0.00 :   ffff800010cad5ec:       mov     w0, #0x1                        // #1
    0.00 :   ffff800010cad5f0:       bl      ffff800010caccd8 <__schedule>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010cad5f4:       ldr     w0, [x19, #16]
         :                      __preempt_count_sub():
         :                      }
         :
         :                      static inline void __preempt_count_sub(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc -= val;
    0.00 :   ffff800010cad5f8:       sub     w0, w0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
   22.27 :   ffff800010cad5fc:       str     w0, [x19, #16]
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff800010cad600:       ldr     x0, [x19]
         :                      preempt_schedule_common():
         :
         :                      /*
         :                      * Check again in case we missed a preemption opportunity
         :                      * between schedule and now.
         :                      */
         :                      } while (need_resched());
    0.00 :   ffff800010cad604:       tbnz    w0, #1, ffff800010cad5dc <preempt_schedule_common+0xc>
         :                      }
    0.00 :   ffff800010cad608:       ldr     x19, [sp, #16]
   10.93 :   ffff800010cad60c:       ldp     x29, x30, [sp], #32
    0.00 :   ffff800010cad610:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (10 samples, percent: local period)
---------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010cad618 <preempt_schedule.part.91>:
         :                      preempt_schedule():
         :                      #ifdef CONFIG_PREEMPTION
         :                      /*
         :                      * This is the entry point to schedule() from in-kernel preemption
         :                      * off of preempt_enable.
         :                      */
         :                      asmlinkage __visible void __sched notrace preempt_schedule(void)
   40.19 :   ffff800010cad618:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff800010cad61c:       mov     x29, sp
         :                      arch_local_save_flags():
         :                      */
         :                      static inline unsigned long arch_local_save_flags(void)
         :                      {
         :                      unsigned long flags;
         :
         :                      asm volatile(ALTERNATIVE(
   40.12 :   ffff800010cad620:       mrs     x1, daif
         :                      arch_irqs_disabled_flags():
         :
         :                      static inline int arch_irqs_disabled_flags(unsigned long flags)
         :                      {
         :                      int res;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010cad624:       and     w0, w1, #0x80
         :                      preempt_schedule():
         :                      {
         :                      /*
         :                      * If there is a non-zero preempt_count or interrupts are disabled,
         :                      * we do not want to preempt the current task. Just return..
         :                      */
         :                      if (likely(!preemptible()))
    0.00 :   ffff800010cad628:       cbz     w0, ffff800010cad634 <preempt_schedule.part.91+0x1c>
         :                      return;
         :
         :                      preempt_schedule_common();
         :                      }
    0.00 :   ffff800010cad62c:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010cad630:       ret
         :                      preempt_schedule_common();
    0.00 :   ffff800010cad634:       bl      ffff800010cad5d0 <preempt_schedule_common>
         :                      }
   19.69 :   ffff800010cad638:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010cad63c:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (6 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001047f4e0 <__bitmap_and>:
         :                      __bitmap_and():
         :
         :                      int __bitmap_and(unsigned long *dst, const unsigned long *bitmap1,
         :                      const unsigned long *bitmap2, unsigned int bits)
         :                      {
         :                      unsigned int k;
         :                      unsigned int lim = bits/BITS_PER_LONG;
   36.17 :   ffff80001047f4e0:       lsr     w8, w3, #6
         :                      unsigned long result = 0;
         :
         :                      for (k = 0; k < lim; k++)
    0.00 :   ffff80001047f4e4:       cbz     w8, ffff80001047f548 <__bitmap_and+0x68>
    0.00 :   ffff80001047f4e8:       mov     x4, #0x0                        // #0
         :                      unsigned long result = 0;
    0.00 :   ffff80001047f4ec:       mov     x6, #0x0                        // #0
         :                      result |= (dst[k] = bitmap1[k] & bitmap2[k]);
    0.00 :   ffff80001047f4f0:       ldr     x5, [x1, x4, lsl #3]
   32.91 :   ffff80001047f4f4:       ldr     x7, [x2, x4, lsl #3]
    0.00 :   ffff80001047f4f8:       and     x5, x5, x7
    0.00 :   ffff80001047f4fc:       str     x5, [x0, x4, lsl #3]
    0.00 :   ffff80001047f500:       add     x4, x4, #0x1
    0.00 :   ffff80001047f504:       orr     x6, x6, x5
         :                      for (k = 0; k < lim; k++)
    0.00 :   ffff80001047f508:       cmp     w8, w4
   30.92 :   ffff80001047f50c:       b.hi    ffff80001047f4f0 <__bitmap_and+0x10>  // b.pmore
         :                      if (bits % BITS_PER_LONG)
    0.00 :   ffff80001047f510:       tst     x3, #0x3f
    0.00 :   ffff80001047f514:       b.eq    ffff80001047f53c <__bitmap_and+0x5c>  // b.none
         :                      result |= (dst[k] = bitmap1[k] & bitmap2[k] &
         :                      BITMAP_LAST_WORD_MASK(bits));
    0.00 :   ffff80001047f518:       neg     w3, w3
    0.00 :   ffff80001047f51c:       mov     x4, #0xffffffffffffffff         // #-1
    0.00 :   ffff80001047f520:       lsr     x4, x4, x3
         :                      result |= (dst[k] = bitmap1[k] & bitmap2[k] &
    0.00 :   ffff80001047f524:       ldr     x3, [x1, x8, lsl #3]
    0.00 :   ffff80001047f528:       ldr     x1, [x2, x8, lsl #3]
    0.00 :   ffff80001047f52c:       and     x3, x3, x1
    0.00 :   ffff80001047f530:       and     x3, x3, x4
    0.00 :   ffff80001047f534:       str     x3, [x0, x8, lsl #3]
    0.00 :   ffff80001047f538:       orr     x6, x6, x3
         :                      return result != 0;
    0.00 :   ffff80001047f53c:       cmp     x6, #0x0
         :                      }
    0.00 :   ffff80001047f540:       cset    w0, ne  // ne = any
    0.00 :   ffff80001047f544:       ret
         :                      unsigned long result = 0;
    0.00 :   ffff80001047f548:       mov     x6, #0x0                        // #0
    0.00 :   ffff80001047f54c:       b       ffff80001047f510 <__bitmap_and+0x30>
 Percent |	Source code & Disassembly of vmlinux for cycles (5 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010cad668 <preempt_schedule_notrace>:
         :                      preempt_schedule_notrace():
         :                      * To prevent this, the preempt_enable_notrace will use this function
         :                      * instead of preempt_schedule() to exit user context if needed before
         :                      * calling the scheduler.
         :                      */
         :                      asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)
         :                      {
    0.00 :   ffff800010cad668:       stp     x29, x30, [sp, #-32]!
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
   20.36 :   ffff800010cad66c:       mrs     x0, sp_el0
         :                      preempt_schedule_notrace():
    0.00 :   ffff800010cad670:       mov     x29, sp
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010cad674:       ldr     w0, [x0, #16]
         :                      preempt_schedule_notrace():
         :                      enum ctx_state prev_ctx;
         :
         :                      if (likely(!preemptible()))
    0.00 :   ffff800010cad678:       cbz     w0, ffff800010cad684 <preempt_schedule_notrace+0x1c>
         :                      exception_exit(prev_ctx);
         :
         :                      preempt_latency_stop(1);
         :                      preempt_enable_no_resched_notrace();
         :                      } while (need_resched());
         :                      }
    0.00 :   ffff800010cad67c:       ldp     x29, x30, [sp], #32
    0.00 :   ffff800010cad680:       ret
         :                      arch_local_save_flags():
         :                      */
         :                      static inline unsigned long arch_local_save_flags(void)
         :                      {
         :                      unsigned long flags;
         :
         :                      asm volatile(ALTERNATIVE(
   39.27 :   ffff800010cad684:       mrs     x1, daif
         :                      arch_irqs_disabled_flags():
         :
         :                      static inline int arch_irqs_disabled_flags(unsigned long flags)
         :                      {
         :                      int res;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010cad688:       and     w0, w1, #0x80
         :                      preempt_schedule_notrace():
         :                      if (likely(!preemptible()))
    0.00 :   ffff800010cad68c:       cbnz    w0, ffff800010cad67c <preempt_schedule_notrace+0x14>
    0.00 :   ffff800010cad690:       str     x19, [x29, #16]
         :                      get_current():
    0.00 :   ffff800010cad694:       mrs     x19, sp_el0
         :                      __read_once_size():
    0.00 :   ffff800010cad698:       ldr     w0, [x19, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff800010cad69c:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
   20.27 :   ffff800010cad6a0:       str     w0, [x19, #16]
         :                      preempt_schedule_notrace():
         :                      __schedule(true);
    0.00 :   ffff800010cad6a4:       mov     w0, #0x1                        // #1
    0.00 :   ffff800010cad6a8:       bl      ffff800010caccd8 <__schedule>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010cad6ac:       ldr     w0, [x19, #16]
         :                      __preempt_count_sub():
         :                      }
         :
         :                      static inline void __preempt_count_sub(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc -= val;
    0.00 :   ffff800010cad6b0:       sub     w0, w0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
   20.10 :   ffff800010cad6b4:       str     w0, [x19, #16]
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff800010cad6b8:       ldr     x0, [x19]
         :                      preempt_schedule_notrace():
         :                      } while (need_resched());
    0.00 :   ffff800010cad6bc:       tbnz    w0, #1, ffff800010cad694 <preempt_schedule_notrace+0x2c>
    0.00 :   ffff800010cad6c0:       ldr     x19, [x29, #16]
    0.00 :   ffff800010cad6c4:       b       ffff800010cad67c <preempt_schedule_notrace+0x14>
 Percent |	Source code & Disassembly of vmlinux for cycles (8 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001029ecc8 <fget>:
         :                      fget():
         :                      {
         :                      return __fget(fd, FMODE_PATH, refs);
         :                      }
         :
         :                      struct file *fget(unsigned int fd)
         :                      {
    0.00 :   ffff80001029ecc8:       stp     x29, x30, [sp, #-16]!
         :                      return __fget(fd, FMODE_PATH, 1);
    0.00 :   ffff80001029eccc:       mov     w2, #0x1                        // #1
    0.00 :   ffff80001029ecd0:       mov     w1, #0x4000                     // #16384
         :                      {
    0.00 :   ffff80001029ecd4:       mov     x29, sp
         :                      return __fget(fd, FMODE_PATH, 1);
  100.00 :   ffff80001029ecd8:       bl      ffff80001029ebd8 <__fget>
         :                      }
    0.00 :   ffff80001029ecdc:       ldp     x29, x30, [sp], #16
    0.00 :   ffff80001029ece0:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (3 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001011dbe8 <task_numa_work>:
         :                      task_numa_work():
         :                      /*
         :                      * The expensive part of numa migration is done from task_work context.
         :                      * Triggered from task_tick_numa().
         :                      */
         :                      static void task_numa_work(struct callback_head *work)
         :                      {
    0.00 :   ffff80001011dbe8:       stp     x29, x30, [sp, #-112]!
         :                      unsigned long migrate, next_scan, now = jiffies;
    0.00 :   ffff80001011dbec:       adrp    x1, ffff800011897000 <bit_wait_table+0xe80>
         :                      {
    0.00 :   ffff80001011dbf0:       mov     x29, sp
    0.00 :   ffff80001011dbf4:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001011dbf8:       str     x21, [sp, #32]
    0.00 :   ffff80001011dbfc:       str     x23, [sp, #48]
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80001011dc00:       mrs     x19, sp_el0
         :                      task_numa_work():
         :                      unsigned long migrate, next_scan, now = jiffies;
    0.00 :   ffff80001011dc04:       ldr     x21, [x1, #2432]
         :                      struct task_struct *p = current;
         :                      struct mm_struct *mm = p->mm;
         :                      u64 runtime = p->se.sum_exec_runtime;
    0.00 :   ffff80001011dc08:       ldr     x23, [x19, #272]
         :                      struct mm_struct *mm = p->mm;
    0.00 :   ffff80001011dc0c:       ldr     x20, [x19, #952]
         :                      unsigned long nr_pte_updates = 0;
         :                      long pages, virtpages;
         :
         :                      SCHED_WARN_ON(p != container_of(work, struct task_struct, numa_work));
         :
         :                      work->next = work;
    0.00 :   ffff80001011dc10:       str     x0, [x0]
         :                      * NOTE: make sure not to dereference p->mm before this check,
         :                      * exit_task_work() happens _after_ exit_mm() so we could be called
         :                      * without p->mm even though we still had it when we enqueued this
         :                      * work.
         :                      */
         :                      if (p->flags & PF_EXITING)
    0.00 :   ffff80001011dc14:       ldr     w0, [x19, #44]
    0.00 :   ffff80001011dc18:       tbnz    w0, #2, ffff80001011dc80 <task_numa_work+0x98>
    0.00 :   ffff80001011dc1c:       str     x22, [x29, #40]
         :                      return;
         :
         :                      if (!mm->numa_next_scan) {
    0.00 :   ffff80001011dc20:       ldr     x22, [x20, #824]
    0.00 :   ffff80001011dc24:       cbnz    x22, ffff80001011dc3c <task_numa_work+0x54>
         :                      mm->numa_next_scan = now +
         :                      msecs_to_jiffies(sysctl_numa_balancing_scan_delay);
    0.00 :   ffff80001011dc28:       adrp    x0, ffff8000118b0000 <user_table+0x188>
         :                      msecs_to_jiffies():
         :                      if (__builtin_constant_p(m)) {
         :                      if ((int)m < 0)
         :                      return MAX_JIFFY_OFFSET;
         :                      return _msecs_to_jiffies(m);
         :                      } else {
         :                      return __msecs_to_jiffies(m);
    0.00 :   ffff80001011dc2c:       ldr     w0, [x0, #2060]
    0.00 :   ffff80001011dc30:       bl      ffff8000101653f0 <__msecs_to_jiffies>
         :                      task_numa_work():
         :                      mm->numa_next_scan = now +
    0.00 :   ffff80001011dc34:       add     x22, x21, x0
    0.00 :   ffff80001011dc38:       str     x22, [x20, #824]
         :
         :                      /*
         :                      * Enforce maximal scan/migration frequency..
         :                      */
         :                      migrate = mm->numa_next_scan;
         :                      if (time_before(now, migrate))
    0.00 :   ffff80001011dc3c:       sub     x0, x21, x22
    0.00 :   ffff80001011dc40:       tbnz    x0, #63, ffff80001011dc7c <task_numa_work+0x94>
         :                      return;
         :
         :                      if (p->numa_scan_period == 0) {
    0.00 :   ffff80001011dc44:       ldr     w0, [x19, #2184]
    0.00 :   ffff80001011dc48:       cbz     w0, ffff80001011dcd4 <task_numa_work+0xec>
         :                      msecs_to_jiffies():
    0.00 :   ffff80001011dc4c:       bl      ffff8000101653f0 <__msecs_to_jiffies>
         :                      task_numa_work():
         :                      p->numa_scan_period_max = task_scan_max(p);
         :                      p->numa_scan_period = task_scan_start(p);
         :                      }
         :
         :                      next_scan = now + msecs_to_jiffies(p->numa_scan_period);
    0.00 :   ffff80001011dc50:       add     x2, x21, x0
         :                      if (cmpxchg(&mm->numa_next_scan, migrate, next_scan) != migrate)
    0.00 :   ffff80001011dc54:       add     x1, x20, #0x338
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff80001011dc58:       b       ffff80001011dc94 <task_numa_work+0xac>
    0.00 :   ffff80001011dc5c:       b       ffff80001011dc94 <task_numa_work+0xac>
         :                      __lse__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,  l, "memory")
         :                      __CMPXCHG_CASE(x,  , rel_, 64,  l, "memory")
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, al, "memory")
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, al, "memory")
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, al, "memory")
         :                      __CMPXCHG_CASE(x,  ,  mb_, 64, al, "memory")
    0.00 :   ffff80001011dc60:       mov     x0, x1
    0.00 :   ffff80001011dc64:       mov     x1, x22
    0.00 :   ffff80001011dc68:       mov     x4, x1
    0.00 :   ffff80001011dc6c:       casal   x4, x2, [x0]
    0.00 :   ffff80001011dc70:       mov     x0, x4
         :                      task_numa_work():
    0.00 :   ffff80001011dc74:       cmp     x0, x22
    0.00 :   ffff80001011dc78:       b.eq    ffff80001011dca4 <task_numa_work+0xbc>  // b.none
    0.00 :   ffff80001011dc7c:       ldr     x22, [x29, #40]
         :                      */
         :                      if (unlikely(p->se.sum_exec_runtime != runtime)) {
         :                      u64 diff = p->se.sum_exec_runtime - runtime;
         :                      p->node_stamp += 32 * diff;
         :                      }
         :                      }
    0.00 :   ffff80001011dc80:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001011dc84:       ldr     x21, [sp, #32]
    0.00 :   ffff80001011dc88:       ldr     x23, [sp, #48]
    0.00 :   ffff80001011dc8c:       ldp     x29, x30, [sp], #112
    0.00 :   ffff80001011dc90:       ret
         :                      __ll_sc__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  , rel_, 64,        ,  , l, "memory", L)
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  ,  mb_, 64, dmb ish,  , l, "memory", L)
    0.00 :   ffff80001011dc94:       add     x3, x20, #0x338
    0.00 :   ffff80001011dc98:       b       ffff8000101273dc <sched_group_set_shares+0x4f4>
         :                      task_numa_work():
         :                      if (cmpxchg(&mm->numa_next_scan, migrate, next_scan) != migrate)
    0.00 :   ffff80001011dc9c:       cmp     x0, x22
    0.00 :   ffff80001011dca0:       b.ne    ffff80001011dc7c <task_numa_work+0x94>  // b.any
    0.00 :   ffff80001011dca4:       str     x27, [x29, #80]
         :                      pages = sysctl_numa_balancing_scan_size;
    0.00 :   ffff80001011dca8:       adrp    x1, ffff8000118b0000 <user_table+0x188>
         :                      p->node_stamp += 2 * TICK_NSEC;
    0.00 :   ffff80001011dcac:       ldr     x0, [x19, #2208]
         :                      pages = sysctl_numa_balancing_scan_size;
    0.00 :   ffff80001011dcb0:       ldr     w6, [x1, #2032]
         :                      p->node_stamp += 2 * TICK_NSEC;
    0.00 :   ffff80001011dcb4:       add     x0, x0, #0x7a1, lsl #12
    0.00 :   ffff80001011dcb8:       add     x0, x0, #0x200
    0.00 :   ffff80001011dcbc:       str     x0, [x19, #2208]
         :                      pages <<= 20 - PAGE_SHIFT; /* MB in pages */
    0.00 :   ffff80001011dcc0:       lsl     x27, x6, #8
         :                      if (!pages)
    0.00 :   ffff80001011dcc4:       cbnz    x27, ffff80001011dcf0 <task_numa_work+0x108>
    0.00 :   ffff80001011dcc8:       ldr     x22, [x29, #40]
    0.00 :   ffff80001011dccc:       ldr     x27, [x29, #80]
    0.00 :   ffff80001011dcd0:       b       ffff80001011dc80 <task_numa_work+0x98>
         :                      p->numa_scan_period_max = task_scan_max(p);
    0.00 :   ffff80001011dcd4:       mov     x0, x19
    0.00 :   ffff80001011dcd8:       bl      ffff80001011b860 <task_scan_max>
    0.00 :   ffff80001011dcdc:       str     w0, [x19, #2188]
         :                      p->numa_scan_period = task_scan_start(p);
    0.00 :   ffff80001011dce0:       mov     x0, x19
    0.00 :   ffff80001011dce4:       bl      ffff80001011cb28 <task_scan_start>
    0.00 :   ffff80001011dce8:       str     w0, [x19, #2184]
    0.00 :   ffff80001011dcec:       b       ffff80001011dc4c <task_numa_work+0x64>
    0.00 :   ffff80001011dcf0:       str     x28, [x29, #88]
         :                      if (!down_read_trylock(&mm->mmap_sem))
    0.00 :   ffff80001011dcf4:       add     x0, x20, #0x68
    0.00 :   ffff80001011dcf8:       str     x0, [x29, #104]
         :                      start = mm->numa_scan_offset;
    0.00 :   ffff80001011dcfc:       ldr     x28, [x20, #832]
         :                      if (!down_read_trylock(&mm->mmap_sem))
    0.00 :   ffff80001011dd00:       bl      ffff800010136fb0 <down_read_trylock>
    0.00 :   ffff80001011dd04:       cbnz    w0, ffff80001011dd14 <task_numa_work+0x12c>
    0.00 :   ffff80001011dd08:       ldr     x22, [x29, #40]
    0.00 :   ffff80001011dd0c:       ldp     x27, x28, [x29, #80]
    0.00 :   ffff80001011dd10:       b       ffff80001011dc80 <task_numa_work+0x98>
    0.00 :   ffff80001011dd14:       str     x26, [x29, #72]
         :                      vma = find_vma(mm, start);
    0.00 :   ffff80001011dd18:       mov     x1, x28
    0.00 :   ffff80001011dd1c:       mov     x0, x20
    0.00 :   ffff80001011dd20:       bl      ffff800010210d80 <find_vma>
    0.00 :   ffff80001011dd24:       mov     x26, x0
         :                      if (!vma) {
    0.00 :   ffff80001011dd28:       cbz     x0, ffff80001011de88 <task_numa_work+0x2a0>
    0.00 :   ffff80001011dd2c:       str     x24, [x29, #56]
         :                      vma_migratable():
         :                      * do so then migration (at least from node to node) is not
         :                      * possible.
         :                      */
         :                      if (vma->vm_file &&
         :                      gfp_zone(mapping_gfp_mask(vma->vm_file->f_mapping))
         :                      < policy_zone)
    0.00 :   ffff80001011dd30:       adrp    x24, ffff800011aaf000 <hstates+0x1038>
         :                      task_numa_work():
         :                      virtpages = pages * 8;     /* Scan up to this much virtual space */
    0.00 :   ffff80001011dd34:       lsl     x21, x27, #3
         :                      vma_migratable():
    0.00 :   ffff80001011dd38:       add     x24, x24, #0x448
         :                      if (vma->vm_flags & (VM_IO | VM_PFNMAP))
    0.00 :   ffff80001011dd3c:       mov     x22, #0x4400                    // #17408
    0.00 :   ffff80001011dd40:       b       ffff80001011dd4c <task_numa_work+0x164>
         :                      task_numa_work():
         :                      for (; vma; vma = vma->vm_next) {
    0.00 :   ffff80001011dd44:       ldr     x26, [x26, #16]
    0.00 :   ffff80001011dd48:       cbz     x26, ffff80001011de68 <task_numa_work+0x280>
         :                      vma_migratable():
    0.00 :   ffff80001011dd4c:       ldr     x0, [x26, #80]
    0.00 :   ffff80001011dd50:       tst     x0, x22
    0.00 :   ffff80001011dd54:       b.ne    ffff80001011dd44 <task_numa_work+0x15c>  // b.any
         :                      if (vma->vm_file &&
    0.00 :   ffff80001011dd58:       ldr     x0, [x26, #160]
    0.00 :   ffff80001011dd5c:       cbz     x0, ffff80001011dd88 <task_numa_work+0x1a0>
         :                      gfp_zone(mapping_gfp_mask(vma->vm_file->f_mapping))
    0.00 :   ffff80001011dd60:       ldr     x0, [x0, #240]
         :                      gfp_zone():
         :                      static inline enum zone_type gfp_zone(gfp_t flags)
         :                      {
         :                      enum zone_type z;
         :                      int bit = (__force int) (flags & GFP_ZONEMASK);
         :
         :                      z = (GFP_ZONE_TABLE >> (bit * GFP_ZONES_SHIFT)) &
    0.00 :   ffff80001011dd64:       mov     w2, #0x122                      // #290
    0.00 :   ffff80001011dd68:       movk    w2, #0x132, lsl #16
         :                      vma_migratable():
         :                      if (vma->vm_file &&
    0.00 :   ffff80001011dd6c:       ldr     w1, [x24]
         :                      gfp_zone():
         :                      int bit = (__force int) (flags & GFP_ZONEMASK);
    0.00 :   ffff80001011dd70:       ldr     w0, [x0, #24]
         :                      z = (GFP_ZONE_TABLE >> (bit * GFP_ZONES_SHIFT)) &
    0.00 :   ffff80001011dd74:       ubfiz   w0, w0, #1, #4
    0.00 :   ffff80001011dd78:       asr     w0, w2, w0
    0.00 :   ffff80001011dd7c:       and     w0, w0, #0x3
         :                      vma_migratable():
   67.45 :   ffff80001011dd80:       cmp     w0, w1
    0.00 :   ffff80001011dd84:       b.cc    ffff80001011dd44 <task_numa_work+0x15c>  // b.lo, b.ul, b.last
         :                      task_numa_work():
         :                      if (!vma_migratable(vma) || !vma_policy_mof(vma) ||
   32.55 :   ffff80001011dd88:       mov     x0, x26
    0.00 :   ffff80001011dd8c:       bl      ffff8000102434d0 <vma_policy_mof>
    0.00 :   ffff80001011dd90:       tst     w0, #0xff
    0.00 :   ffff80001011dd94:       b.eq    ffff80001011dd44 <task_numa_work+0x15c>  // b.none
         :                      is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_MIXEDMAP)) {
    0.00 :   ffff80001011dd98:       ldr     x0, [x26, #80]
    0.00 :   ffff80001011dd9c:       mov     x1, #0x10400000                 // #272629760
    0.00 :   ffff80001011dda0:       tst     x0, x1
    0.00 :   ffff80001011dda4:       b.ne    ffff80001011dd44 <task_numa_work+0x15c>  // b.any
         :                      if (!vma->vm_mm ||
    0.00 :   ffff80001011dda8:       ldr     x1, [x26, #64]
    0.00 :   ffff80001011ddac:       cbz     x1, ffff80001011dd44 <task_numa_work+0x15c>
    0.00 :   ffff80001011ddb0:       ldr     x1, [x26, #160]
    0.00 :   ffff80001011ddb4:       cbz     x1, ffff80001011ddc4 <task_numa_work+0x1dc>
         :                      (vma->vm_file && (vma->vm_flags & (VM_READ|VM_WRITE)) == (VM_READ)))
    0.00 :   ffff80001011ddb8:       and     x1, x0, #0x3
    0.00 :   ffff80001011ddbc:       cmp     x1, #0x1
    0.00 :   ffff80001011ddc0:       b.eq    ffff80001011dd44 <task_numa_work+0x15c>  // b.none
         :                      if (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))
    0.00 :   ffff80001011ddc4:       tst     x0, #0x7
    0.00 :   ffff80001011ddc8:       b.eq    ffff80001011dd44 <task_numa_work+0x15c>  // b.none
    0.00 :   ffff80001011ddcc:       str     x25, [x29, #64]
    0.00 :   ffff80001011ddd0:       ldr     x2, [x26, #8]
    0.00 :   ffff80001011ddd4:       b       ffff80001011dde4 <task_numa_work+0x1fc>
         :                      } while (end != vma->vm_end);
    0.00 :   ffff80001011ddd8:       ldr     x2, [x26, #8]
    0.00 :   ffff80001011dddc:       cmp     x2, x28
    0.00 :   ffff80001011dde0:       b.eq    ffff80001011deb4 <task_numa_work+0x2cc>  // b.none
         :                      start = max(start, vma->vm_start);
    0.00 :   ffff80001011dde4:       ldr     x5, [x26]
         :                      end = ALIGN(start + (pages << PAGE_SHIFT), HPAGE_SIZE);
    0.00 :   ffff80001011dde8:       mov     x1, #0x1fffff                   // #2097151
         :                      nr_pte_updates = change_prot_numa(vma, start, end);
    0.00 :   ffff80001011ddec:       mov     x0, x26
         :                      start = max(start, vma->vm_start);
    0.00 :   ffff80001011ddf0:       cmp     x5, x28
    0.00 :   ffff80001011ddf4:       csel    x25, x5, x28, cs  // cs = hs, nlast
         :                      end = ALIGN(start + (pages << PAGE_SHIFT), HPAGE_SIZE);
    0.00 :   ffff80001011ddf8:       add     x4, x25, x1
         :                      nr_pte_updates = change_prot_numa(vma, start, end);
    0.00 :   ffff80001011ddfc:       mov     x1, x25
         :                      end = ALIGN(start + (pages << PAGE_SHIFT), HPAGE_SIZE);
    0.00 :   ffff80001011de00:       add     x4, x4, x27, lsl #12
    0.00 :   ffff80001011de04:       and     x4, x4, #0xffffffffffe00000
         :                      end = min(end, vma->vm_end);
    0.00 :   ffff80001011de08:       cmp     x4, x2
    0.00 :   ffff80001011de0c:       csel    x28, x4, x2, ls  // ls = plast
         :                      nr_pte_updates = change_prot_numa(vma, start, end);
    0.00 :   ffff80001011de10:       mov     x2, x28
    0.00 :   ffff80001011de14:       bl      ffff8000102428a8 <change_prot_numa>
         :                      pages -= (end - start) >> PAGE_SHIFT;
    0.00 :   ffff80001011de18:       cmp     x0, #0x0
    0.00 :   ffff80001011de1c:       sub     x5, x28, x25
    0.00 :   ffff80001011de20:       lsr     x5, x5, #12
    0.00 :   ffff80001011de24:       sub     x0, x27, x5
         :                      virtpages -= (end - start) >> PAGE_SHIFT;
    0.00 :   ffff80001011de28:       sub     x21, x21, x5
         :                      pages -= (end - start) >> PAGE_SHIFT;
    0.00 :   ffff80001011de2c:       csel    x27, x0, x27, ne  // ne = any
         :                      if (pages <= 0 || virtpages <= 0)
    0.00 :   ffff80001011de30:       cmp     x27, #0x0
    0.00 :   ffff80001011de34:       ccmp    x21, #0x0, #0x4, gt
    0.00 :   ffff80001011de38:       b.gt    ffff80001011ddd8 <task_numa_work+0x1f0>
         :                      mm->numa_scan_offset = start;
    0.00 :   ffff80001011de3c:       str     x28, [x20, #832]
    0.00 :   ffff80001011de40:       ldp     x24, x25, [x29, #56]
         :                      up_read(&mm->mmap_sem);
    0.00 :   ffff80001011de44:       ldr     x0, [x29, #104]
    0.00 :   ffff80001011de48:       bl      ffff8000101376e8 <up_read>
         :                      if (unlikely(p->se.sum_exec_runtime != runtime)) {
    0.00 :   ffff80001011de4c:       ldr     x0, [x19, #272]
    0.00 :   ffff80001011de50:       cmp     x0, x23
    0.00 :   ffff80001011de54:       b.ne    ffff80001011debc <task_numa_work+0x2d4>  // b.any
    0.00 :   ffff80001011de58:       ldr     x22, [x29, #40]
    0.00 :   ffff80001011de5c:       ldp     x26, x27, [x29, #72]
    0.00 :   ffff80001011de60:       ldr     x28, [x29, #88]
    0.00 :   ffff80001011de64:       b       ffff80001011dc80 <task_numa_work+0x98>
         :                      out:
    0.00 :   ffff80001011de68:       ldr     x24, [x29, #56]
         :                      reset_ptenuma_scan():
         :                      WRITE_ONCE(p->mm->numa_scan_seq, READ_ONCE(p->mm->numa_scan_seq) + 1);
    0.00 :   ffff80001011de6c:       ldr     x1, [x19, #952]
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001011de70:       ldr     w0, [x1, #840]
         :                      reset_ptenuma_scan():
    0.00 :   ffff80001011de74:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001011de78:       str     w0, [x1, #840]
         :                      reset_ptenuma_scan():
         :                      p->mm->numa_scan_offset = 0;
    0.00 :   ffff80001011de7c:       ldr     x0, [x19, #952]
    0.00 :   ffff80001011de80:       str     xzr, [x0, #832]
    0.00 :   ffff80001011de84:       b       ffff80001011de44 <task_numa_work+0x25c>
         :                      WRITE_ONCE(p->mm->numa_scan_seq, READ_ONCE(p->mm->numa_scan_seq) + 1);
    0.00 :   ffff80001011de88:       ldr     x1, [x19, #952]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001011de8c:       ldr     w0, [x1, #840]
         :                      reset_ptenuma_scan():
    0.00 :   ffff80001011de90:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001011de94:       str     w0, [x1, #840]
         :                      reset_ptenuma_scan():
         :                      p->mm->numa_scan_offset = 0;
    0.00 :   ffff80001011de98:       ldr     x0, [x19, #952]
    0.00 :   ffff80001011de9c:       str     xzr, [x0, #832]
         :                      task_numa_work():
         :                      vma = mm->mmap;
    0.00 :   ffff80001011dea0:       ldr     x26, [x20]
         :                      for (; vma; vma = vma->vm_next) {
    0.00 :   ffff80001011dea4:       cbz     x26, ffff80001011de6c <task_numa_work+0x284>
         :                      start = 0;
    0.00 :   ffff80001011dea8:       mov     x28, #0x0                       // #0
    0.00 :   ffff80001011deac:       str     x24, [x29, #56]
    0.00 :   ffff80001011deb0:       b       ffff80001011dd30 <task_numa_work+0x148>
    0.00 :   ffff80001011deb4:       ldr     x25, [x29, #64]
    0.00 :   ffff80001011deb8:       b       ffff80001011dd44 <task_numa_work+0x15c>
         :                      p->node_stamp += 32 * diff;
    0.00 :   ffff80001011debc:       ldr     x1, [x19, #2208]
         :                      u64 diff = p->se.sum_exec_runtime - runtime;
    0.00 :   ffff80001011dec0:       sub     x0, x0, x23
         :                      p->node_stamp += 32 * diff;
    0.00 :   ffff80001011dec4:       add     x0, x1, x0, lsl #5
    0.00 :   ffff80001011dec8:       str     x0, [x19, #2208]
    0.00 :   ffff80001011decc:       ldr     x22, [x29, #40]
    0.00 :   ffff80001011ded0:       ldp     x26, x27, [x29, #72]
    0.00 :   ffff80001011ded4:       ldr     x28, [x29, #88]
    0.00 :   ffff80001011ded8:       b       ffff80001011dc80 <task_numa_work+0x98>
 Percent |	Source code & Disassembly of vmlinux for cycles (3 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001015e158 <rcu_preempt_deferred_qs_irqrestore>:
         :                      rcu_preempt_deferred_qs_irqrestore():
         :                      * be quite short, for example, in the case of the call from
         :                      * rcu_read_unlock_special().
         :                      */
         :                      static void
         :                      rcu_preempt_deferred_qs_irqrestore(struct task_struct *t, unsigned long flags)
         :                      {
    0.00 :   ffff80001015e158:       stp     x29, x30, [sp, #-64]!
         :                      * If RCU core is waiting for this CPU to exit its critical section,
         :                      * report the fact that it has exited.  Because irqs are disabled,
         :                      * t->rcu_read_unlock_special cannot change.
         :                      */
         :                      special = t->rcu_read_unlock_special;
         :                      rdp = this_cpu_ptr(&rcu_data);
    0.00 :   ffff80001015e15c:       adrp    x2, ffff8000114d6000 <runqueues+0x280>
    0.00 :   ffff80001015e160:       add     x2, x2, #0x9c0
         :                      {
    0.00 :   ffff80001015e164:       mov     x29, sp
    0.00 :   ffff80001015e168:       str     x20, [sp, #24]
         :                      special = t->rcu_read_unlock_special;
    0.00 :   ffff80001015e16c:       ldr     w20, [x0, #780]
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff80001015e170:       mrs     x4, tpidr_el1
         :                      rcu_preempt_deferred_qs_irqrestore():
         :                      rdp = this_cpu_ptr(&rcu_data);
    0.00 :   ffff80001015e174:       add     x4, x2, x4
         :                      if (!special.s && !rdp->exp_deferred_qs) {
    0.00 :   ffff80001015e178:       cbnz    w20, ffff80001015e184 <rcu_preempt_deferred_qs_irqrestore+0x2c>
    0.00 :   ffff80001015e17c:       ldrb    w2, [x4, #21]
    0.00 :   ffff80001015e180:       cbz     w2, ffff80001015e2d0 <rcu_preempt_deferred_qs_irqrestore+0x178>
    0.00 :   ffff80001015e184:       str     x19, [x29, #16]
         :                      local_irq_restore(flags);
         :                      return;
         :                      }
         :                      t->rcu_read_unlock_special.b.deferred_qs = false;
         :                      if (special.b.need_qs) {
    0.00 :   ffff80001015e188:       tst     x20, #0xff00
    0.00 :   ffff80001015e18c:       str     x21, [x29, #32]
    0.00 :   ffff80001015e190:       mov     x19, x0
         :                      t->rcu_read_unlock_special.b.deferred_qs = false;
    0.00 :   ffff80001015e194:       strb    wzr, [x0, #783]
    0.00 :   ffff80001015e198:       mov     x21, x1
         :                      if (special.b.need_qs) {
    0.00 :   ffff80001015e19c:       b.ne    ffff80001015e284 <rcu_preempt_deferred_qs_irqrestore+0x12c>  // b.any
         :                      * Respond to a request by an expedited grace period for a
         :                      * quiescent state from this CPU.  Note that requests from
         :                      * tasks are handled when removing the task from the
         :                      * blocked-tasks list below.
         :                      */
         :                      if (rdp->exp_deferred_qs) {
    0.00 :   ffff80001015e1a0:       ldrb    w0, [x4, #21]
    0.00 :   ffff80001015e1a4:       cbnz    w0, ffff80001015e2b4 <rcu_preempt_deferred_qs_irqrestore+0x15c>
         :                      return;
         :                      }
         :                      }
         :
         :                      /* Clean up if blocked during RCU read-side critical section. */
         :                      if (special.b.blocked) {
    0.00 :   ffff80001015e1a8:       and     w20, w20, #0xff
    0.00 :   ffff80001015e1ac:       cbz     w20, ffff80001015e29c <rcu_preempt_deferred_qs_irqrestore+0x144>
         :                      * Remove this task from the list it blocked on.  The task
         :                      * now remains queued on the rcu_node corresponding to the
         :                      * CPU it first blocked on, so there is no longer any need
         :                      * to loop.  Retain a WARN_ON_ONCE() out of sheer paranoia.
         :                      */
         :                      rnp = t->rcu_blocked_node;
    0.00 :   ffff80001015e1b0:       ldr     x20, [x19, #800]
         :                      t->rcu_read_unlock_special.b.blocked = false;
    0.00 :   ffff80001015e1b4:       strb    wzr, [x19, #780]
         :                      raw_spin_lock_rcu_node(rnp); /* irqs already disabled. */
    0.00 :   ffff80001015e1b8:       mov     x0, x20
    0.00 :   ffff80001015e1bc:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      WARN_ON_ONCE(rnp != t->rcu_blocked_node);
    0.00 :   ffff80001015e1c0:       ldr     x0, [x19, #800]
    0.00 :   ffff80001015e1c4:       cmp     x0, x20
    0.00 :   ffff80001015e1c8:       b.ne    ffff80001015e3b4 <rcu_preempt_deferred_qs_irqrestore+0x25c>  // b.any
         :                      WARN_ON_ONCE(!rcu_is_leaf_node(rnp));
    0.00 :   ffff80001015e1cc:       adrp    x0, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff80001015e1d0:       ldrb    w1, [x20, #113]
    0.00 :   ffff80001015e1d4:       ldr     w0, [x0, #612]
    0.00 :   ffff80001015e1d8:       sub     w0, w0, #0x1
    0.00 :   ffff80001015e1dc:       cmp     w1, w0
    0.00 :   ffff80001015e1e0:       b.ne    ffff80001015e3ac <rcu_preempt_deferred_qs_irqrestore+0x254>  // b.any
         :                      empty_norm = !rcu_preempt_blocked_readers_cgp(rnp);
         :                      WARN_ON_ONCE(rnp->completedqs == rnp->gp_seq &&
    0.00 :   ffff80001015e1e4:       ldr     x0, [x20, #8]
    0.00 :   ffff80001015e1e8:       ldr     x1, [x20, #24]
         :                      empty_norm = !rcu_preempt_blocked_readers_cgp(rnp);
    0.00 :   ffff80001015e1ec:       ldr     x5, [x20, #144]
         :                      WARN_ON_ONCE(rnp->completedqs == rnp->gp_seq &&
    0.00 :   ffff80001015e1f0:       cmp     x1, x0
    0.00 :   ffff80001015e1f4:       b.eq    ffff80001015e31c <rcu_preempt_deferred_qs_irqrestore+0x1c4>  // b.none
         :                      sync_rcu_preempt_exp_done():
         :                      */
         :                      static bool sync_rcu_preempt_exp_done(struct rcu_node *rnp)
         :                      {
         :                      raw_lockdep_assert_held_rcu_node(rnp);
         :
         :                      return rnp->exp_tasks == NULL &&
    0.00 :   ffff80001015e1f8:       ldr     x0, [x20, #152]
    0.00 :   ffff80001015e1fc:       mov     w6, #0x1                        // #1
    0.00 :   ffff80001015e200:       cbz     x0, ffff80001015e33c <rcu_preempt_deferred_qs_irqrestore+0x1e4>
         :                      rcu_preempt_deferred_qs_irqrestore():
         :                      (!empty_norm || rnp->qsmask));
         :                      empty_exp = sync_rcu_preempt_exp_done(rnp);
         :                      smp_mb(); /* ensure expedited fastpath sees end of RCU c-s. */
    0.00 :   ffff80001015e204:       dmb     ish
         :                      rcu_next_node_entry():
         :                      np = t->rcu_node_entry.next;
    0.00 :   ffff80001015e208:       ldr     x0, [x19, #784]
         :                      rcu_preempt_deferred_qs_irqrestore():
         :                      np = rcu_next_node_entry(t, rnp);
         :                      list_del_init(&t->rcu_node_entry);
    0.00 :   ffff80001015e20c:       add     x1, x19, #0x310
         :                      __list_del_entry():
         :                      static inline void __list_del_entry(struct list_head *entry)
         :                      {
         :                      if (!__list_del_entry_valid(entry))
         :                      return;
         :
         :                      __list_del(entry->prev, entry->next);
    0.00 :   ffff80001015e210:       ldr     x4, [x19, #792]
         :                      rcu_next_node_entry():
         :                      if (np == &rnp->blkd_tasks)
    0.00 :   ffff80001015e214:       add     x2, x20, #0x80
    0.00 :   ffff80001015e218:       cmp     x0, x2
         :                      __list_del():
         :                      next->prev = prev;
    0.00 :   ffff80001015e21c:       str     x4, [x0, #8]
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff80001015e220:       str     x0, [x4]
         :                      rcu_next_node_entry():
    0.00 :   ffff80001015e224:       csel    x0, x0, xzr, ne  // ne = any
         :                      __write_once_size():
    0.00 :   ffff80001015e228:       str     x1, [x19, #784]
         :                      INIT_LIST_HEAD():
         :                      list->prev = list;
    0.00 :   ffff80001015e22c:       str     x1, [x19, #792]
         :                      rcu_preempt_deferred_qs_irqrestore():
         :                      t->rcu_blocked_node = NULL;
    0.00 :   ffff80001015e230:       str     xzr, [x19, #800]
         :                      trace_rcu_unlock_preempted_task(TPS("rcu_preempt"),
         :                      rnp->gp_seq, t->pid);
         :                      if (&t->rcu_node_entry == rnp->gp_tasks)
    0.00 :   ffff80001015e234:       ldr     x2, [x20, #144]
    0.00 :   ffff80001015e238:       cmp     x1, x2
    0.00 :   ffff80001015e23c:       b.eq    ffff80001015e334 <rcu_preempt_deferred_qs_irqrestore+0x1dc>  // b.none
         :                      rnp->gp_tasks = np;
         :                      if (&t->rcu_node_entry == rnp->exp_tasks)
    0.00 :   ffff80001015e240:       ldr     x2, [x20, #152]
    0.00 :   ffff80001015e244:       cmp     x1, x2
    0.00 :   ffff80001015e248:       b.eq    ffff80001015e328 <rcu_preempt_deferred_qs_irqrestore+0x1d0>  // b.none
         :                      sync_rcu_preempt_exp_done():
    0.00 :   ffff80001015e24c:       mov     w19, #0x0                       // #0
    0.00 :   ffff80001015e250:       cbz     x2, ffff80001015e2e0 <rcu_preempt_deferred_qs_irqrestore+0x188>
         :                      rcu_preempt_deferred_qs_irqrestore():
         :                      * we aren't waiting on any CPUs, report the quiescent state.
         :                      * Note that rcu_report_unblock_qs_rnp() releases rnp->lock,
         :                      * so we must take a snapshot of the expedited state.
         :                      */
         :                      empty_exp_now = sync_rcu_preempt_exp_done(rnp);
         :                      if (!empty_norm && !rcu_preempt_blocked_readers_cgp(rnp)) {
    0.00 :   ffff80001015e254:       cbz     x5, ffff80001015e260 <rcu_preempt_deferred_qs_irqrestore+0x108>
    0.00 :   ffff80001015e258:       ldr     x0, [x20, #144]
    0.00 :   ffff80001015e25c:       cbz     x0, ffff80001015e34c <rcu_preempt_deferred_qs_irqrestore+0x1f4>
         :                      rnp->grplo,
         :                      rnp->grphi,
         :                      !!rnp->gp_tasks);
         :                      rcu_report_unblock_qs_rnp(rnp, flags);
         :                      } else {
         :                      raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    0.00 :   ffff80001015e260:       mov     x1, x21
    0.00 :   ffff80001015e264:       mov     x0, x20
    0.00 :   ffff80001015e268:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
         :
         :                      /*
         :                      * If this was the last task on the expedited lists,
         :                      * then we need to report up the rcu_node hierarchy.
         :                      */
         :                      if (!empty_exp && empty_exp_now)
  100.00 :   ffff80001015e26c:       cbnz    w19, ffff80001015e2f0 <rcu_preempt_deferred_qs_irqrestore+0x198>
    0.00 :   ffff80001015e270:       ldr     x19, [x29, #16]
    0.00 :   ffff80001015e274:       ldr     x21, [x29, #32]
         :                      rcu_report_exp_rnp(rnp, true);
         :                      } else {
         :                      local_irq_restore(flags);
         :                      }
         :                      }
    0.00 :   ffff80001015e278:       ldr     x20, [sp, #24]
    0.00 :   ffff80001015e27c:       ldp     x29, x30, [sp], #64
    0.00 :   ffff80001015e280:       ret
         :                      rcu_qs();
    0.00 :   ffff80001015e284:       bl      ffff80001015c1b0 <rcu_qs>
         :                      t->rcu_read_unlock_special.b.need_qs = false;
    0.00 :   ffff80001015e288:       strb    wzr, [x19, #781]
         :                      if (!t->rcu_read_unlock_special.s && !rdp->exp_deferred_qs) {
    0.00 :   ffff80001015e28c:       ldr     w0, [x19, #780]
    0.00 :   ffff80001015e290:       cbnz    w0, ffff80001015e1a0 <rcu_preempt_deferred_qs_irqrestore+0x48>
    0.00 :   ffff80001015e294:       ldrb    w0, [x4, #21]
    0.00 :   ffff80001015e298:       cbnz    w0, ffff80001015e2b4 <rcu_preempt_deferred_qs_irqrestore+0x15c>
         :                      arch_local_irq_restore():
         :                      /*
         :                      * restore saved IRQ state
         :                      */
         :                      static inline void arch_local_irq_restore(unsigned long flags)
         :                      {
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015e29c:       msr     daif, x21
    0.00 :   ffff80001015e2a0:       ldr     x19, [x29, #16]
    0.00 :   ffff80001015e2a4:       ldr     x21, [x29, #32]
         :                      rcu_preempt_deferred_qs_irqrestore():
         :                      }
    0.00 :   ffff80001015e2a8:       ldr     x20, [sp, #24]
    0.00 :   ffff80001015e2ac:       ldp     x29, x30, [sp], #64
    0.00 :   ffff80001015e2b0:       ret
         :                      rcu_report_exp_rdp():
         :                      * Report expedited quiescent state for specified rcu_data (CPU).
         :                      */
         :                      static void rcu_report_exp_rdp(struct rcu_data *rdp)
         :                      {
         :                      WRITE_ONCE(rdp->exp_deferred_qs, false);
         :                      rcu_report_exp_cpu_mult(rdp->mynode, rdp->grpmask, true);
    0.00 :   ffff80001015e2b4:       ldp     x0, x1, [x4, #24]
         :                      __write_once_size():
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
    0.00 :   ffff80001015e2b8:       strb    wzr, [x4, #21]
         :                      rcu_report_exp_rdp():
    0.00 :   ffff80001015e2bc:       mov     w2, #0x1                        // #1
    0.00 :   ffff80001015e2c0:       bl      ffff80001015a828 <rcu_report_exp_cpu_mult>
         :                      rcu_preempt_deferred_qs_irqrestore():
         :                      if (!t->rcu_read_unlock_special.s) {
    0.00 :   ffff80001015e2c4:       ldr     w0, [x19, #780]
    0.00 :   ffff80001015e2c8:       cbnz    w0, ffff80001015e1a8 <rcu_preempt_deferred_qs_irqrestore+0x50>
    0.00 :   ffff80001015e2cc:       b       ffff80001015e29c <rcu_preempt_deferred_qs_irqrestore+0x144>
         :                      arch_local_irq_restore():
    0.00 :   ffff80001015e2d0:       msr     daif, x1
         :                      rcu_preempt_deferred_qs_irqrestore():
         :                      }
    0.00 :   ffff80001015e2d4:       ldr     x20, [sp, #24]
    0.00 :   ffff80001015e2d8:       ldp     x29, x30, [sp], #64
    0.00 :   ffff80001015e2dc:       ret
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001015e2e0:       ldr     x0, [x20, #64]
         :                      sync_rcu_preempt_exp_done():
         :                      return rnp->exp_tasks == NULL &&
    0.00 :   ffff80001015e2e4:       cmp     x0, #0x0
    0.00 :   ffff80001015e2e8:       csel    w19, w6, wzr, eq  // eq = none
    0.00 :   ffff80001015e2ec:       b       ffff80001015e254 <rcu_preempt_deferred_qs_irqrestore+0xfc>
         :                      rcu_report_exp_rnp():
         :                      raw_spin_lock_irqsave_rcu_node(rnp, flags);
    0.00 :   ffff80001015e2f0:       mov     x0, x20
    0.00 :   ffff80001015e2f4:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
         :                      __rcu_report_exp_rnp(rnp, wake, flags);
    0.00 :   ffff80001015e2f8:       mov     w1, #0x1                        // #1
    0.00 :   ffff80001015e2fc:       mov     x2, x0
    0.00 :   ffff80001015e300:       mov     x0, x20
    0.00 :   ffff80001015e304:       bl      ffff80001015a720 <__rcu_report_exp_rnp>
    0.00 :   ffff80001015e308:       ldr     x19, [x29, #16]
    0.00 :   ffff80001015e30c:       ldr     x21, [x29, #32]
         :                      rcu_preempt_deferred_qs_irqrestore():
    0.00 :   ffff80001015e310:       ldr     x20, [sp, #24]
    0.00 :   ffff80001015e314:       ldp     x29, x30, [sp], #64
    0.00 :   ffff80001015e318:       ret
         :                      WARN_ON_ONCE(rnp->completedqs == rnp->gp_seq &&
    0.00 :   ffff80001015e31c:       cbz     x5, ffff80001015e3a0 <rcu_preempt_deferred_qs_irqrestore+0x248>
    0.00 :   ffff80001015e320:       brk     #0x800
    0.00 :   ffff80001015e324:       b       ffff80001015e1f8 <rcu_preempt_deferred_qs_irqrestore+0xa0>
         :                      rnp->exp_tasks = np;
    0.00 :   ffff80001015e328:       mov     x2, x0
    0.00 :   ffff80001015e32c:       str     x0, [x20, #152]
    0.00 :   ffff80001015e330:       b       ffff80001015e24c <rcu_preempt_deferred_qs_irqrestore+0xf4>
         :                      rnp->gp_tasks = np;
    0.00 :   ffff80001015e334:       str     x0, [x20, #144]
    0.00 :   ffff80001015e338:       b       ffff80001015e240 <rcu_preempt_deferred_qs_irqrestore+0xe8>
         :                      __read_once_size():
    0.00 :   ffff80001015e33c:       ldr     x0, [x20, #64]
    0.00 :   ffff80001015e340:       cmp     x0, #0x0
    0.00 :   ffff80001015e344:       cset    w6, ne  // ne = any
    0.00 :   ffff80001015e348:       b       ffff80001015e204 <rcu_preempt_deferred_qs_irqrestore+0xac>
         :                      rcu_report_unblock_qs_rnp():
         :                      unsigned long mask;
         :                      struct rcu_node *rnp_p;
         :
         :                      raw_lockdep_assert_held_rcu_node(rnp);
         :                      if (WARN_ON_ONCE(!IS_ENABLED(CONFIG_PREEMPTION)) ||
         :                      WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp)) ||
    0.00 :   ffff80001015e34c:       ldr     x0, [x20, #32]
    0.00 :   ffff80001015e350:       cbnz    x0, ffff80001015e260 <rcu_preempt_deferred_qs_irqrestore+0x108>
    0.00 :   ffff80001015e354:       stp     x22, x23, [x29, #40]
         :                      raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
         :                      return;  /* Still need more quiescent states! */
         :                      }
         :
         :                      rnp->completedqs = rnp->gp_seq;
         :                      rnp_p = rnp->parent;
    0.00 :   ffff80001015e358:       ldr     x22, [x20, #120]
         :                      rnp->completedqs = rnp->gp_seq;
    0.00 :   ffff80001015e35c:       ldr     x23, [x20, #8]
    0.00 :   ffff80001015e360:       str     x23, [x20, #24]
         :                      if (rnp_p == NULL) {
    0.00 :   ffff80001015e364:       cbz     x22, ffff80001015e3bc <rcu_preempt_deferred_qs_irqrestore+0x264>
    0.00 :   ffff80001015e368:       str     x24, [x29, #56]
         :                      }
         :
         :                      /* Report up the rest of the hierarchy, tracking current ->gp_seq. */
         :                      gps = rnp->gp_seq;
         :                      mask = rnp->grpmask;
         :                      raw_spin_unlock_rcu_node(rnp);  /* irqs remain disabled. */
    0.00 :   ffff80001015e36c:       mov     x0, x20
         :                      mask = rnp->grpmask;
    0.00 :   ffff80001015e370:       ldr     x24, [x20, #96]
         :                      raw_spin_unlock_rcu_node(rnp);  /* irqs remain disabled. */
    0.00 :   ffff80001015e374:       bl      ffff800010cb2408 <_raw_spin_unlock>
         :                      raw_spin_lock_rcu_node(rnp_p);  /* irqs already disabled. */
    0.00 :   ffff80001015e378:       mov     x0, x22
    0.00 :   ffff80001015e37c:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      rcu_report_qs_rnp(mask, rnp_p, gps, flags);
    0.00 :   ffff80001015e380:       mov     x2, x23
    0.00 :   ffff80001015e384:       mov     x1, x22
    0.00 :   ffff80001015e388:       mov     x0, x24
    0.00 :   ffff80001015e38c:       mov     x3, x21
    0.00 :   ffff80001015e390:       bl      ffff80001015b070 <rcu_report_qs_rnp>
    0.00 :   ffff80001015e394:       ldr     x24, [x29, #56]
    0.00 :   ffff80001015e398:       ldp     x22, x23, [x29, #40]
    0.00 :   ffff80001015e39c:       b       ffff80001015e26c <rcu_preempt_deferred_qs_irqrestore+0x114>
         :                      rcu_preempt_deferred_qs_irqrestore():
         :                      WARN_ON_ONCE(rnp->completedqs == rnp->gp_seq &&
    0.00 :   ffff80001015e3a0:       ldr     x0, [x20, #32]
    0.00 :   ffff80001015e3a4:       cbz     x0, ffff80001015e1f8 <rcu_preempt_deferred_qs_irqrestore+0xa0>
    0.00 :   ffff80001015e3a8:       b       ffff80001015e320 <rcu_preempt_deferred_qs_irqrestore+0x1c8>
         :                      WARN_ON_ONCE(!rcu_is_leaf_node(rnp));
    0.00 :   ffff80001015e3ac:       brk     #0x800
    0.00 :   ffff80001015e3b0:       b       ffff80001015e1e4 <rcu_preempt_deferred_qs_irqrestore+0x8c>
         :                      WARN_ON_ONCE(rnp != t->rcu_blocked_node);
    0.00 :   ffff80001015e3b4:       brk     #0x800
    0.00 :   ffff80001015e3b8:       b       ffff80001015e1cc <rcu_preempt_deferred_qs_irqrestore+0x74>
         :                      rcu_report_unblock_qs_rnp():
         :                      rcu_report_qs_rsp(flags);
    0.00 :   ffff80001015e3bc:       mov     x0, x21
    0.00 :   ffff80001015e3c0:       bl      ffff80001015b020 <rcu_report_qs_rsp>
    0.00 :   ffff80001015e3c4:       ldp     x22, x23, [x29, #40]
    0.00 :   ffff80001015e3c8:       b       ffff80001015e26c <rcu_preempt_deferred_qs_irqrestore+0x114>
 Percent |	Source code & Disassembly of vmlinux for cycles (3 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010104848 <queue_delayed_work_on>:
         :                      queue_delayed_work_on():
         :                      * @delay is zero and @dwork is idle, it will be scheduled for immediate
         :                      * execution.
         :                      */
         :                      bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
         :                      struct delayed_work *dwork, unsigned long delay)
         :                      {
    0.00 :   ffff800010104848:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff80001010484c:       mov     x29, sp
    0.00 :   ffff800010104850:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010104854:       adrp    x19, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010104858:       add     x4, x19, #0x8c8
    0.00 :   ffff80001010485c:       ldr     x5, [x4]
    0.00 :   ffff800010104860:       str     x5, [x29, #40]
    0.00 :   ffff800010104864:       mov     x5, #0x0                        // #0
         :                      arch_local_save_flags():
         :                      */
         :                      static inline unsigned long arch_local_save_flags(void)
         :                      {
         :                      unsigned long flags;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010104868:       mrs     x20, daif
         :                      arch_irqs_disabled_flags():
         :
         :                      static inline int arch_irqs_disabled_flags(unsigned long flags)
         :                      {
         :                      int res;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001010486c:       and     w4, w20, #0x80
         :                      arch_local_irq_save():
         :
         :                      /*
         :                      * There are too many states with IRQs disabled, just keep the current
         :                      * state if interrupts are already disabled/masked.
         :                      */
         :                      if (!arch_irqs_disabled_flags(flags))
    0.00 :   ffff800010104870:       cbnz    w4, ffff80001010487c <queue_delayed_work_on+0x34>
         :                      arch_local_irq_disable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010104874:       mov     x4, #0x60                       // #96
    0.00 :   ffff800010104878:       msr     daifset, #0x2
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001010487c:       ldr     x4, [x2]
         :                      test_and_set_bit():
         :                      {
         :                      long old;
         :                      unsigned long mask = BIT_MASK(nr);
         :
         :                      p += BIT_WORD(nr);
         :                      if (READ_ONCE(*p) & mask)
    0.00 :   ffff800010104880:       tbz     w4, #0, ffff8000101048ac <queue_delayed_work_on+0x64>
         :                      queue_delayed_work_on():
         :                      struct work_struct *work = &dwork->work;
         :                      bool ret = false;
    0.00 :   ffff800010104884:       mov     w0, #0x0                        // #0
         :                      arch_local_irq_restore():
         :                      /*
         :                      * restore saved IRQ state
         :                      */
         :                      static inline void arch_local_irq_restore(unsigned long flags)
         :                      {
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010104888:       msr     daif, x20
         :                      queue_delayed_work_on():
         :                      ret = true;
         :                      }
         :
         :                      local_irq_restore(flags);
         :                      return ret;
         :                      }
  100.00 :   ffff80001010488c:       add     x19, x19, #0x8c8
    0.00 :   ffff800010104890:       ldr     x2, [x29, #40]
    0.00 :   ffff800010104894:       ldr     x1, [x19]
    0.00 :   ffff800010104898:       eor     x1, x2, x1
    0.00 :   ffff80001010489c:       cbnz    x1, ffff8000101048d4 <queue_delayed_work_on+0x8c>
    0.00 :   ffff8000101048a0:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000101048a4:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000101048a8:       ret
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000101048ac:       b       ffff8000101048cc <queue_delayed_work_on+0x84>
    0.00 :   ffff8000101048b0:       b       ffff8000101048cc <queue_delayed_work_on+0x84>
         :                      __lse_atomic64_fetch_or():
         :                      ATOMIC64_FETCH_OP(_acquire,  a, op, asm_op, "memory")           \
         :                      ATOMIC64_FETCH_OP(_release,  l, op, asm_op, "memory")           \
         :                      ATOMIC64_FETCH_OP(        , al, op, asm_op, "memory")
         :
         :                      ATOMIC64_FETCH_OPS(andnot, ldclr)
         :                      ATOMIC64_FETCH_OPS(or, ldset)
    0.00 :   ffff8000101048b4:       mov     x4, #0x1                        // #1
    0.00 :   ffff8000101048b8:       ldsetal x4, x4, [x2]
         :                      queue_delayed_work_on():
         :                      if (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {
    0.00 :   ffff8000101048bc:       tbnz    w4, #0, ffff800010104884 <queue_delayed_work_on+0x3c>
         :                      __queue_delayed_work(cpu, wq, dwork, delay);
    0.00 :   ffff8000101048c0:       bl      ffff800010104780 <__queue_delayed_work>
         :                      ret = true;
    0.00 :   ffff8000101048c4:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000101048c8:       b       ffff800010104888 <queue_delayed_work_on+0x40>
         :                      __ll_sc_atomic64_fetch_or():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(and, and, L)
         :                      ATOMIC64_OPS(or, orr, L)
    0.00 :   ffff8000101048cc:       b       ffff800010107e80 <alloc_workqueue+0x560>
    0.00 :   ffff8000101048d0:       b       ffff8000101048bc <queue_delayed_work_on+0x74>
         :                      queue_delayed_work_on():
         :                      }
    0.00 :   ffff8000101048d4:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (3 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001015e4c8 <rcu_core>:
         :                      rcu_core():
         :                      }
         :                      EXPORT_SYMBOL_GPL(rcu_force_quiescent_state);
         :
         :                      /* Perform RCU core processing work for the current CPU.  */
         :                      static __latent_entropy void rcu_core(void)
         :                      {
    0.00 :   ffff80001015e4c8:       stp     x29, x30, [sp, #-144]!
    0.00 :   ffff80001015e4cc:       mov     x29, sp
    0.00 :   ffff80001015e4d0:       stp     x19, x20, [sp, #16]
         :                      unsigned long flags;
         :                      struct rcu_data *rdp = raw_cpu_ptr(&rcu_data);
    0.00 :   ffff80001015e4d4:       adrp    x20, ffff8000114d6000 <runqueues+0x280>
         :                      {
    0.00 :   ffff80001015e4d8:       stp     x21, x22, [sp, #32]
         :                      struct rcu_node *rnp = rdp->mynode;
         :                      const bool offloaded = IS_ENABLED(CONFIG_RCU_NOCB_CPU) &&
         :                      rcu_segcblist_is_offloaded(&rdp->cblist);
         :
         :                      if (cpu_is_offline(smp_processor_id()))
    0.00 :   ffff80001015e4dc:       adrp    x22, ffff8000114ca000 <bp_hardening_data>
         :                      {
    0.00 :   ffff80001015e4e0:       stp     x23, x24, [sp, #48]
         :                      if (cpu_is_offline(smp_processor_id()))
    0.00 :   ffff80001015e4e4:       add     x0, x22, #0x18
         :                      {
    0.00 :   ffff80001015e4e8:       str     x26, [sp, #72]
    0.00 :   ffff80001015e4ec:       adrp    x21, ffff800011899000 <page_wait_table+0x1500>
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff80001015e4f0:       mrs     x24, tpidr_el1
         :                      rcu_core():
         :                      if (cpu_is_offline(smp_processor_id()))
    0.00 :   ffff80001015e4f4:       ldr     w1, [x0, x24]
         :                      {
    0.00 :   ffff80001015e4f8:       add     x0, x21, #0x8c8
    0.00 :   ffff80001015e4fc:       ldr     x2, [x0]
   33.03 :   ffff80001015e500:       str     x2, [x29, #136]
    0.00 :   ffff80001015e504:       mov     x2, #0x0                        // #0
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff80001015e508:       adrp    x23, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff80001015e50c:       add     w0, w1, #0x3f
    0.00 :   ffff80001015e510:       cmp     w1, #0x0
    0.00 :   ffff80001015e514:       csel    w0, w0, w1, lt  // lt = tstop
    0.00 :   ffff80001015e518:       add     x2, x23, #0x120
         :                      rcu_core():
         :                      struct rcu_data *rdp = raw_cpu_ptr(&rcu_data);
    0.00 :   ffff80001015e51c:       add     x20, x20, #0x9c0
         :                      test_bit():
    0.00 :   ffff80001015e520:       asr     w0, w0, #6
    0.00 :   ffff80001015e524:       sxtw    x0, w0
         :                      rcu_core():
    0.00 :   ffff80001015e528:       add     x19, x20, x24
         :                      test_bit():
    0.00 :   ffff80001015e52c:       ldr     x0, [x2, x0, lsl #3]
         :                      rcu_core():
         :                      struct rcu_node *rnp = rdp->mynode;
   34.17 :   ffff80001015e530:       ldr     x26, [x19, #24]
         :                      test_bit():
    0.00 :   ffff80001015e534:       lsr     x1, x0, x1
         :                      rcu_core():
         :                      if (cpu_is_offline(smp_processor_id()))
    0.00 :   ffff80001015e538:       tbz     w1, #0, ffff80001015e60c <rcu_core+0x144>
    0.00 :   ffff80001015e53c:       str     x25, [x29, #64]
         :                      return;
         :                      trace_rcu_utilization(TPS("Start RCU core"));
         :                      WARN_ON_ONCE(!rdp->beenonline);
    0.00 :   ffff80001015e540:       ldrb    w0, [x19, #19]
    0.00 :   ffff80001015e544:       cbz     w0, ffff80001015e894 <rcu_core+0x3cc>
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80001015e548:       mrs     x3, sp_el0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001015e54c:       ldr     w0, [x3, #16]
         :                      rcu_core():
         :
         :                      /* Report any deferred quiescent states if preemption enabled. */
         :                      if (!(preempt_count() & PREEMPT_MASK)) {
    0.00 :   ffff80001015e550:       and     w0, w0, #0xff
    0.00 :   ffff80001015e554:       cbz     w0, ffff80001015e638 <rcu_core+0x170>
         :                      rcu_preempt_deferred_qs(current);
         :                      } else if (rcu_preempt_need_deferred_qs(current)) {
    0.00 :   ffff80001015e558:       mov     x0, x3
    0.00 :   ffff80001015e55c:       bl      ffff80001015c1d8 <rcu_preempt_need_deferred_qs>
    0.00 :   ffff80001015e560:       tst     w0, #0xff
    0.00 :   ffff80001015e564:       b.ne    ffff80001015e848 <rcu_core+0x380>  // b.any
         :                      rcu_check_quiescent_state():
         :                      note_gp_changes(rdp);
    0.00 :   ffff80001015e568:       mov     x0, x19
    0.00 :   ffff80001015e56c:       bl      ffff80001015d1d8 <note_gp_changes>
         :                      if (!rdp->core_needs_qs)
    0.00 :   ffff80001015e570:       ldrb    w0, [x19, #18]
    0.00 :   ffff80001015e574:       cbz     w0, ffff80001015e580 <rcu_core+0xb8>
         :                      if (rdp->cpu_no_qs.b.norm)
    0.00 :   ffff80001015e578:       ldrb    w0, [x19, #16]
    0.00 :   ffff80001015e57c:       cbz     w0, ffff80001015e660 <rcu_core+0x198>
         :                      __read_once_size():
    0.00 :   ffff80001015e580:       adrp    x25, ffff8000118b2000 <irq_gc_syscore_ops+0x18>
    0.00 :   ffff80001015e584:       add     x0, x25, #0xb40
    0.00 :   ffff80001015e588:       ldr     x0, [x0, #8776]
         :                      rcu_core():
         :
         :                      /* Update RCU state based on any recent quiescent states. */
         :                      rcu_check_quiescent_state(rdp);
         :
         :                      /* No grace period and unregistered callbacks? */
         :                      if (!rcu_gp_in_progress() &&
    0.00 :   ffff80001015e58c:       tst     x0, #0x3
    0.00 :   ffff80001015e590:       b.ne    ffff80001015e59c <rcu_core+0xd4>  // b.any
    0.00 :   ffff80001015e594:       ldrb    w0, [x19, #168]
    0.00 :   ffff80001015e598:       cbnz    w0, ffff80001015e864 <rcu_core+0x39c>
         :                      __read_once_size():
    0.00 :   ffff80001015e59c:       adrp    x0, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff80001015e5a0:       ldr     w1, [x0, #596]
         :                      rcu_jiffies_till_stall_check():
         :
         :                      /*
         :                      * Limit check must be consistent with the Kconfig limits
         :                      * for CONFIG_RCU_CPU_STALL_TIMEOUT.
         :                      */
         :                      if (till_stall_check < 3) {
    0.00 :   ffff80001015e5a4:       cmp     w1, #0x2
    0.00 :   ffff80001015e5a8:       b.gt    ffff80001015e64c <rcu_core+0x184>
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001015e5ac:       mov     w1, #0x3                        // #3
    0.00 :   ffff80001015e5b0:       str     w1, [x0, #596]
         :                      rcu_core():
         :                      }
         :
         :                      rcu_check_gp_start_stall(rnp, rdp, rcu_jiffies_till_stall_check());
         :
         :                      /* If there are callbacks ready, invoke them. */
         :                      if (!offloaded && rcu_segcblist_ready_cbs(&rdp->cblist) &&
    0.00 :   ffff80001015e5b4:       add     x24, x19, #0x50
    0.00 :   ffff80001015e5b8:       mov     x0, x24
    0.00 :   ffff80001015e5bc:       bl      ffff800010160d80 <rcu_segcblist_ready_cbs>
    0.00 :   ffff80001015e5c0:       tst     w0, #0xff
    0.00 :   ffff80001015e5c4:       b.eq    ffff80001015e644 <rcu_core+0x17c>  // b.none
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001015e5c8:       adrp    x0, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff80001015e5cc:       add     x0, x0, #0x260
    0.00 :   ffff80001015e5d0:       ldr     w0, [x0, #8]
         :                      rcu_core():
    0.00 :   ffff80001015e5d4:       cbz     w0, ffff80001015e644 <rcu_core+0x17c>
         :                      rcu_do_batch():
         :                      struct rcu_cblist rcl = RCU_CBLIST_INITIALIZER(rcl);
    0.00 :   ffff80001015e5d8:       stp     xzr, xzr, [x29, #104]
    0.00 :   ffff80001015e5dc:       add     x20, x29, #0x68
         :                      if (!rcu_segcblist_ready_cbs(&rdp->cblist)) {
    0.00 :   ffff80001015e5e0:       mov     x0, x24
         :                      struct rcu_cblist rcl = RCU_CBLIST_INITIALIZER(rcl);
    0.00 :   ffff80001015e5e4:       str     x20, [x29, #112]
    0.00 :   ffff80001015e5e8:       stp     xzr, xzr, [x29, #120]
         :                      if (!rcu_segcblist_ready_cbs(&rdp->cblist)) {
    0.00 :   ffff80001015e5ec:       bl      ffff800010160d80 <rcu_segcblist_ready_cbs>
    0.00 :   ffff80001015e5f0:       tst     w0, #0xff
    0.00 :   ffff80001015e5f4:       b.ne    ffff80001015e6a8 <rcu_core+0x1e0>  // b.any
         :                      __read_once_size():
    0.00 :   ffff80001015e5f8:       ldr     x0, [x19, #152]
    0.00 :   ffff80001015e5fc:       ldr     x0, [x19, #80]
    0.00 :   ffff80001015e600:       ldr     x25, [x29, #64]
         :                      get_current():
    0.00 :   ffff80001015e604:       mrs     x0, sp_el0
         :                      test_bit():
    0.00 :   ffff80001015e608:       ldr     x0, [x0]
         :                      rcu_core():
         :                      rcu_do_batch(rdp);
         :
         :                      /* Do any needed deferred wakeups of rcuo kthreads. */
         :                      do_nocb_deferred_wakeup(rdp);
         :                      trace_rcu_utilization(TPS("End RCU core"));
         :                      }
    0.00 :   ffff80001015e60c:       add     x21, x21, #0x8c8
    0.00 :   ffff80001015e610:       ldr     x1, [x29, #136]
    0.00 :   ffff80001015e614:       ldr     x0, [x21]
    0.00 :   ffff80001015e618:       eor     x0, x1, x0
    0.00 :   ffff80001015e61c:       cbnz    x0, ffff80001015e984 <rcu_core+0x4bc>
    0.00 :   ffff80001015e620:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001015e624:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff80001015e628:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff80001015e62c:       ldr     x26, [sp, #72]
   32.80 :   ffff80001015e630:       ldp     x29, x30, [sp], #144
    0.00 :   ffff80001015e634:       ret
         :                      rcu_preempt_deferred_qs(current);
    0.00 :   ffff80001015e638:       mov     x0, x3
    0.00 :   ffff80001015e63c:       bl      ffff80001015e3d0 <rcu_preempt_deferred_qs>
    0.00 :   ffff80001015e640:       b       ffff80001015e568 <rcu_core+0xa0>
    0.00 :   ffff80001015e644:       ldr     x25, [x29, #64]
    0.00 :   ffff80001015e648:       b       ffff80001015e60c <rcu_core+0x144>
         :                      rcu_jiffies_till_stall_check():
         :                      WRITE_ONCE(rcu_cpu_stall_timeout, 3);
         :                      till_stall_check = 3;
         :                      } else if (till_stall_check > 300) {
    0.00 :   ffff80001015e64c:       cmp     w1, #0x12c
    0.00 :   ffff80001015e650:       b.le    ffff80001015e5b4 <rcu_core+0xec>
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001015e654:       mov     w1, #0x12c                      // #300
    0.00 :   ffff80001015e658:       str     w1, [x0, #596]
    0.00 :   ffff80001015e65c:       b       ffff80001015e5b4 <rcu_core+0xec>
    0.00 :   ffff80001015e660:       str     x27, [x29, #80]
         :                      rcu_report_qs_rdp():
         :                      rnp = rdp->mynode;
    0.00 :   ffff80001015e664:       ldr     x25, [x19, #24]
         :                      raw_spin_lock_irqsave_rcu_node(rnp, flags);
    0.00 :   ffff80001015e668:       mov     x0, x25
    0.00 :   ffff80001015e66c:       bl      ffff800010cb2db8 <_raw_spin_lock_irqsave>
    0.00 :   ffff80001015e670:       mov     x27, x0
         :                      if (rdp->cpu_no_qs.b.norm || rdp->gp_seq != rnp->gp_seq ||
    0.00 :   ffff80001015e674:       ldrb    w0, [x19, #16]
    0.00 :   ffff80001015e678:       cbnz    w0, ffff80001015e68c <rcu_core+0x1c4>
    0.00 :   ffff80001015e67c:       ldr     x1, [x20, x24]
    0.00 :   ffff80001015e680:       ldr     x0, [x25, #8]
    0.00 :   ffff80001015e684:       cmp     x1, x0
    0.00 :   ffff80001015e688:       b.eq    ffff80001015e8f4 <rcu_core+0x42c>  // b.none
         :                      rdp->cpu_no_qs.b.norm = true;   /* need qs for new gp. */
    0.00 :   ffff80001015e68c:       mov     w0, #0x1                        // #1
    0.00 :   ffff80001015e690:       strb    w0, [x19, #16]
         :                      raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    0.00 :   ffff80001015e694:       mov     x1, x27
    0.00 :   ffff80001015e698:       mov     x0, x25
    0.00 :   ffff80001015e69c:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
    0.00 :   ffff80001015e6a0:       ldr     x27, [x29, #80]
    0.00 :   ffff80001015e6a4:       b       ffff80001015e580 <rcu_core+0xb8>
    0.00 :   ffff80001015e6a8:       stp     x27, x28, [x29, #80]
         :                      arch_local_save_flags():
         :                      */
         :                      static inline unsigned long arch_local_save_flags(void)
         :                      {
         :                      unsigned long flags;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015e6ac:       mrs     x27, daif
         :                      arch_irqs_disabled_flags():
         :
         :                      static inline int arch_irqs_disabled_flags(unsigned long flags)
         :                      {
         :                      int res;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015e6b0:       and     w0, w27, #0x80
         :                      arch_local_irq_save():
         :
         :                      /*
         :                      * There are too many states with IRQs disabled, just keep the current
         :                      * state if interrupts are already disabled/masked.
         :                      */
         :                      if (!arch_irqs_disabled_flags(flags))
    0.00 :   ffff80001015e6b4:       cbnz    w0, ffff80001015e6c0 <rcu_core+0x1f8>
         :                      arch_local_irq_disable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015e6b8:       mov     x0, #0x60                       // #96
    0.00 :   ffff80001015e6bc:       msr     daifset, #0x2
         :                      __my_cpu_offset():
    0.00 :   ffff80001015e6c0:       mrs     x0, tpidr_el1
         :                      rcu_do_batch():
         :                      WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    0.00 :   ffff80001015e6c4:       add     x22, x22, #0x18
         :                      test_bit():
    0.00 :   ffff80001015e6c8:       add     x23, x23, #0x120
         :                      rcu_do_batch():
    0.00 :   ffff80001015e6cc:       ldr     w1, [x22, x0]
         :                      test_bit():
    0.00 :   ffff80001015e6d0:       add     w0, w1, #0x3f
    0.00 :   ffff80001015e6d4:       cmp     w1, #0x0
    0.00 :   ffff80001015e6d8:       csel    w0, w0, w1, lt  // lt = tstop
    0.00 :   ffff80001015e6dc:       asr     w0, w0, #6
    0.00 :   ffff80001015e6e0:       sxtw    x0, w0
    0.00 :   ffff80001015e6e4:       ldr     x0, [x23, x0, lsl #3]
    0.00 :   ffff80001015e6e8:       lsr     x1, x0, x1
         :                      rcu_do_batch():
    0.00 :   ffff80001015e6ec:       tbz     w1, #0, ffff80001015e95c <rcu_core+0x494>
         :                      bl = max(rdp->blimit, pending >> rcu_divisor);
    0.00 :   ffff80001015e6f0:       adrp    x26, ffff8000118b2000 <irq_gc_syscore_ops+0x18>
    0.00 :   ffff80001015e6f4:       add     x28, x26, #0x9c0
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001015e6f8:       ldr     x22, [x19, #152]
         :                      rcu_do_batch():
         :                      long pending, tlimit = 0;
    0.00 :   ffff80001015e6fc:       mov     x23, #0x0                       // #0
         :                      bl = max(rdp->blimit, pending >> rcu_divisor);
    0.00 :   ffff80001015e700:       ldr     x0, [x19, #192]
    0.00 :   ffff80001015e704:       ldr     w1, [x28, #192]
    0.00 :   ffff80001015e708:       asr     x22, x22, x1
    0.00 :   ffff80001015e70c:       cmp     x22, x0
    0.00 :   ffff80001015e710:       csel    x22, x22, x0, ge  // ge = tcont
         :                      if (unlikely(bl > 100))
    0.00 :   ffff80001015e714:       cmp     x22, #0x64
    0.00 :   ffff80001015e718:       b.gt    ffff80001015e94c <rcu_core+0x484>
         :                      __read_once_size():
    0.00 :   ffff80001015e71c:       ldr     x0, [x19, #152]
         :                      rcu_do_batch():
         :                      rcu_segcblist_extract_done_cbs(&rdp->cblist, &rcl);
    0.00 :   ffff80001015e720:       mov     x1, x20
    0.00 :   ffff80001015e724:       mov     x0, x24
    0.00 :   ffff80001015e728:       bl      ffff800010160f50 <rcu_segcblist_extract_done_cbs>
         :                      arch_local_irq_restore():
         :                      /*
         :                      * restore saved IRQ state
         :                      */
         :                      static inline void arch_local_irq_restore(unsigned long flags)
         :                      {
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015e72c:       msr     daif, x27
         :                      rcu_do_batch():
         :                      rhp = rcu_cblist_dequeue(&rcl);
    0.00 :   ffff80001015e730:       mov     x0, x20
    0.00 :   ffff80001015e734:       bl      ffff800010160cc0 <rcu_cblist_dequeue>
         :                      for (; rhp; rhp = rcu_cblist_dequeue(&rcl)) {
    0.00 :   ffff80001015e738:       cbz     x0, ffff80001015e790 <rcu_core+0x2c8>
         :                      __rcu_reclaim():
         :                      * or freeing it directly (lazy case).  Return true if lazy, false otherwise.
         :                      */
         :                      static inline bool __rcu_reclaim(const char *rn, struct rcu_head *head)
         :                      {
         :                      rcu_callback_t f;
         :                      unsigned long offset = (unsigned long)head->func;
    0.00 :   ffff80001015e73c:       ldr     x1, [x0, #8]
         :
         :                      rcu_lock_acquire(&rcu_callback_map);
         :                      if (__is_kfree_rcu_offset(offset)) {
    0.00 :   ffff80001015e740:       cmp     x1, #0xfff
    0.00 :   ffff80001015e744:       b.ls    ffff80001015e830 <rcu_core+0x368>  // b.plast
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff80001015e748:       str     xzr, [x0, #8]
         :                      __rcu_reclaim():
         :                      return true;
         :                      } else {
         :                      trace_rcu_invoke_callback(rn, head);
         :                      f = head->func;
         :                      WRITE_ONCE(head->func, (rcu_callback_t)0L);
         :                      f(head);
    0.00 :   ffff80001015e74c:       blr     x1
         :                      rcu_do_batch():
         :                      if (-rcl.len >= bl && !offloaded &&
    0.00 :   ffff80001015e750:       ldr     x0, [x29, #120]
    0.00 :   ffff80001015e754:       neg     x0, x0
    0.00 :   ffff80001015e758:       cmp     x22, x0
    0.00 :   ffff80001015e75c:       b.gt    ffff80001015e774 <rcu_core+0x2ac>
         :                      get_current():
    0.00 :   ffff80001015e760:       mrs     x1, sp_el0
         :                      test_bit():
    0.00 :   ffff80001015e764:       ldr     x2, [x1]
         :                      rcu_do_batch():
    0.00 :   ffff80001015e768:       tbnz    w2, #1, ffff80001015e790 <rcu_core+0x2c8>
         :                      is_idle_task():
         :                      *
         :                      * Return: 1 if @p is an idle task. 0 otherwise.
         :                      */
         :                      static inline bool is_idle_task(const struct task_struct *p)
         :                      {
         :                      return !!(p->flags & PF_IDLE);
    0.00 :   ffff80001015e76c:       ldr     w1, [x1, #44]
         :                      rcu_do_batch():
         :                      (need_resched() ||
    0.00 :   ffff80001015e770:       tbz     w1, #1, ffff80001015e790 <rcu_core+0x2c8>
         :                      if (unlikely(tlimit)) {
    0.00 :   ffff80001015e774:       cbz     x23, ffff80001015e730 <rcu_core+0x268>
         :                      if (likely((-rcl.len & 31) || local_clock() < tlimit))
    0.00 :   ffff80001015e778:       tst     x0, #0x1f
    0.00 :   ffff80001015e77c:       b.ne    ffff80001015e730 <rcu_core+0x268>  // b.any
         :                      local_clock():
         :                      return sched_clock();
         :                      }
         :
         :                      static inline u64 local_clock(void)
         :                      {
         :                      return sched_clock();
    0.00 :   ffff80001015e780:       bl      ffff800010179198 <sched_clock>
         :                      rcu_do_batch():
    0.00 :   ffff80001015e784:       cmp     x0, x23
    0.00 :   ffff80001015e788:       b.cc    ffff80001015e730 <rcu_core+0x268>  // b.lo, b.ul, b.last
    0.00 :   ffff80001015e78c:       nop
         :                      arch_local_save_flags():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015e790:       mrs     x22, daif
         :                      arch_irqs_disabled_flags():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015e794:       and     w0, w22, #0x80
         :                      arch_local_irq_save():
         :                      if (!arch_irqs_disabled_flags(flags))
    0.00 :   ffff80001015e798:       cbnz    w0, ffff80001015e7a4 <rcu_core+0x2dc>
         :                      arch_local_irq_disable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015e79c:       mov     x0, #0x60                       // #96
    0.00 :   ffff80001015e7a0:       msr     daifset, #0x2
         :                      get_current():
    0.00 :   ffff80001015e7a4:       mrs     x0, sp_el0
         :                      test_bit():
    0.00 :   ffff80001015e7a8:       ldr     x0, [x0]
         :                      rcu_do_batch():
         :                      rcu_segcblist_insert_done_cbs(&rdp->cblist, &rcl);
    0.00 :   ffff80001015e7ac:       mov     x1, x20
    0.00 :   ffff80001015e7b0:       mov     x0, x24
    0.00 :   ffff80001015e7b4:       bl      ffff800010161030 <rcu_segcblist_insert_done_cbs>
         :                      smp_mb(); /* List handling before counting for rcu_barrier(). */
    0.00 :   ffff80001015e7b8:       dmb     ish
         :                      rcu_segcblist_insert_count(&rdp->cblist, &rcl);
    0.00 :   ffff80001015e7bc:       mov     x1, x20
    0.00 :   ffff80001015e7c0:       mov     x0, x24
    0.00 :   ffff80001015e7c4:       bl      ffff800010161000 <rcu_segcblist_insert_count>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001015e7c8:       ldr     x0, [x19, #152]
         :                      rcu_do_batch():
         :                      if (rdp->blimit >= DEFAULT_MAX_RCU_BLIMIT && count <= qlowmark)
    0.00 :   ffff80001015e7cc:       mov     x1, #0x270f                     // #9999
    0.00 :   ffff80001015e7d0:       ldr     x2, [x19, #192]
    0.00 :   ffff80001015e7d4:       cmp     x2, x1
    0.00 :   ffff80001015e7d8:       b.le    ffff80001015e7ec <rcu_core+0x324>
    0.00 :   ffff80001015e7dc:       add     x1, x26, #0x9c0
    0.00 :   ffff80001015e7e0:       ldr     x2, [x1, #208]
    0.00 :   ffff80001015e7e4:       cmp     x0, x2
    0.00 :   ffff80001015e7e8:       b.le    ffff80001015e8d8 <rcu_core+0x410>
    0.00 :   ffff80001015e7ec:       ldr     x1, [x19, #176]
         :                      if (count == 0 && rdp->qlen_last_fqs_check != 0) {
    0.00 :   ffff80001015e7f0:       cbnz    x0, ffff80001015e89c <rcu_core+0x3d4>
    0.00 :   ffff80001015e7f4:       cbz     x1, ffff80001015e808 <rcu_core+0x340>
         :                      rdp->n_force_qs_snap = rcu_state.n_force_qs;
    0.00 :   ffff80001015e7f8:       add     x25, x25, #0xb40
         :                      rdp->qlen_last_fqs_check = 0;
    0.00 :   ffff80001015e7fc:       str     xzr, [x19, #176]
         :                      rdp->n_force_qs_snap = rcu_state.n_force_qs;
    0.00 :   ffff80001015e800:       ldr     x0, [x25, #9048]
    0.00 :   ffff80001015e804:       str     x0, [x19, #184]
         :                      __read_once_size():
    0.00 :   ffff80001015e808:       ldr     x0, [x19, #80]
         :                      rcu_do_batch():
         :                      WARN_ON_ONCE(count == 0 && !rcu_segcblist_empty(&rdp->cblist));
    0.00 :   ffff80001015e80c:       cbnz    x0, ffff80001015e8d0 <rcu_core+0x408>
         :                      arch_local_irq_restore():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015e810:       msr     daif, x22
         :                      rcu_do_batch():
         :                      if (!offloaded && rcu_segcblist_ready_cbs(&rdp->cblist))
    0.00 :   ffff80001015e814:       mov     x0, x24
    0.00 :   ffff80001015e818:       bl      ffff800010160d80 <rcu_segcblist_ready_cbs>
    0.00 :   ffff80001015e81c:       tst     w0, #0xff
    0.00 :   ffff80001015e820:       b.ne    ffff80001015e8c0 <rcu_core+0x3f8>  // b.any
    0.00 :   ffff80001015e824:       ldr     x25, [x29, #64]
    0.00 :   ffff80001015e828:       ldp     x27, x28, [x29, #80]
    0.00 :   ffff80001015e82c:       b       ffff80001015e60c <rcu_core+0x144>
         :                      __rcu_reclaim():
         :                      kfree((void *)head - offset);
    0.00 :   ffff80001015e830:       sub     x0, x0, x1
    0.00 :   ffff80001015e834:       bl      ffff80001024fe88 <kfree>
         :                      rcu_cblist_dequeued_lazy():
         :                      * Account for the fact that a previously dequeued callback turned out
         :                      * to be marked as lazy.
         :                      */
         :                      static inline void rcu_cblist_dequeued_lazy(struct rcu_cblist *rclp)
         :                      {
         :                      rclp->len_lazy--;
    0.00 :   ffff80001015e838:       ldr     x0, [x29, #128]
    0.00 :   ffff80001015e83c:       sub     x0, x0, #0x1
    0.00 :   ffff80001015e840:       str     x0, [x29, #128]
    0.00 :   ffff80001015e844:       b       ffff80001015e750 <rcu_core+0x288>
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff80001015e848:       b       ffff80001015e88c <rcu_core+0x3c4>
    0.00 :   ffff80001015e84c:       b       ffff80001015e88c <rcu_core+0x3c4>
         :                      __lse_atomic64_or():
         :                      : [i] "+r" (i), [v] "+Q" (v->counter)                           \
         :                      : "r" (v));                                                     \
         :                      }
         :
         :                      ATOMIC64_OP(andnot, stclr)
         :                      ATOMIC64_OP(or, stset)
    0.00 :   ffff80001015e850:       mov     x0, #0x2                        // #2
    0.00 :   ffff80001015e854:       stset   x0, [x3]
         :                      get_current():
    0.00 :   ffff80001015e858:       mrs     x0, sp_el0
         :                      set_preempt_need_resched():
         :                      task_thread_info(p)->preempt_count = PREEMPT_ENABLED; \
         :                      } while (0)
         :
         :                      static inline void set_preempt_need_resched(void)
         :                      {
         :                      current_thread_info()->preempt.need_resched = 0;
    0.00 :   ffff80001015e85c:       str     wzr, [x0, #20]
    0.00 :   ffff80001015e860:       b       ffff80001015e568 <rcu_core+0xa0>
         :                      arch_local_save_flags():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015e864:       mrs     x20, daif
         :                      arch_irqs_disabled_flags():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015e868:       and     w0, w20, #0x80
         :                      arch_local_irq_save():
         :                      if (!arch_irqs_disabled_flags(flags))
    0.00 :   ffff80001015e86c:       cbnz    w0, ffff80001015e878 <rcu_core+0x3b0>
         :                      arch_local_irq_disable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015e870:       mov     x0, #0x60                       // #96
    0.00 :   ffff80001015e874:       msr     daifset, #0x2
         :                      __read_once_size():
    0.00 :   ffff80001015e878:       ldr     x0, [x19, #104]
    0.00 :   ffff80001015e87c:       ldr     x0, [x0]
         :                      rcu_core():
         :                      if (!rcu_segcblist_restempty(&rdp->cblist, RCU_NEXT_READY_TAIL))
    0.00 :   ffff80001015e880:       cbnz    x0, ffff80001015e8e4 <rcu_core+0x41c>
         :                      arch_local_irq_restore():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001015e884:       msr     daif, x20
    0.00 :   ffff80001015e888:       b       ffff80001015e59c <rcu_core+0xd4>
         :                      __ll_sc_atomic64_or():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(and, and, L)
         :                      ATOMIC64_OPS(or, orr, L)
    0.00 :   ffff80001015e88c:       b       ffff80001016079c <rcu_needs_cpu+0x17c>
    0.00 :   ffff80001015e890:       b       ffff80001015e858 <rcu_core+0x390>
         :                      rcu_core():
         :                      WARN_ON_ONCE(!rdp->beenonline);
    0.00 :   ffff80001015e894:       brk     #0x800
    0.00 :   ffff80001015e898:       b       ffff80001015e548 <rcu_core+0x80>
         :                      rcu_do_batch():
         :                      } else if (count < rdp->qlen_last_fqs_check - qhimark)
    0.00 :   ffff80001015e89c:       add     x26, x26, #0x9c0
    0.00 :   ffff80001015e8a0:       ldr     x2, [x26, #184]
    0.00 :   ffff80001015e8a4:       sub     x1, x1, x2
    0.00 :   ffff80001015e8a8:       cmp     x1, x0
    0.00 :   ffff80001015e8ac:       b.gt    ffff80001015e944 <rcu_core+0x47c>
         :                      __read_once_size():
    0.00 :   ffff80001015e8b0:       ldr     x0, [x19, #80]
         :                      rcu_do_batch():
         :                      WARN_ON_ONCE(!IS_ENABLED(CONFIG_RCU_NOCB_CPU) &&
    0.00 :   ffff80001015e8b4:       cbnz    x0, ffff80001015e810 <rcu_core+0x348>
    0.00 :   ffff80001015e8b8:       brk     #0x800
    0.00 :   ffff80001015e8bc:       b       ffff80001015e810 <rcu_core+0x348>
         :                      invoke_rcu_core();
    0.00 :   ffff80001015e8c0:       bl      ffff80001015cf88 <invoke_rcu_core>
    0.00 :   ffff80001015e8c4:       ldr     x25, [x29, #64]
    0.00 :   ffff80001015e8c8:       ldp     x27, x28, [x29, #80]
    0.00 :   ffff80001015e8cc:       b       ffff80001015e60c <rcu_core+0x144>
         :                      WARN_ON_ONCE(count == 0 && !rcu_segcblist_empty(&rdp->cblist));
    0.00 :   ffff80001015e8d0:       brk     #0x800
    0.00 :   ffff80001015e8d4:       b       ffff80001015e810 <rcu_core+0x348>
         :                      rdp->blimit = blimit;
    0.00 :   ffff80001015e8d8:       ldr     x1, [x1, #216]
    0.00 :   ffff80001015e8dc:       str     x1, [x19, #192]
    0.00 :   ffff80001015e8e0:       b       ffff80001015e7ec <rcu_core+0x324>
         :                      rcu_core():
         :                      rcu_accelerate_cbs_unlocked(rnp, rdp);
    0.00 :   ffff80001015e8e4:       mov     x1, x19
    0.00 :   ffff80001015e8e8:       mov     x0, x26
    0.00 :   ffff80001015e8ec:       bl      ffff80001015b368 <rcu_accelerate_cbs_unlocked>
    0.00 :   ffff80001015e8f0:       b       ffff80001015e884 <rcu_core+0x3bc>
         :                      rcu_report_qs_rdp():
         :                      if (rdp->cpu_no_qs.b.norm || rdp->gp_seq != rnp->gp_seq ||
    0.00 :   ffff80001015e8f4:       ldrb    w0, [x19, #20]
    0.00 :   ffff80001015e8f8:       cbnz    w0, ffff80001015e68c <rcu_core+0x1c4>
         :                      mask = rdp->grpmask;
    0.00 :   ffff80001015e8fc:       ldr     x20, [x19, #32]
         :                      if ((rnp->qsmask & mask) == 0) {
    0.00 :   ffff80001015e900:       ldr     x0, [x25, #32]
    0.00 :   ffff80001015e904:       tst     x20, x0
    0.00 :   ffff80001015e908:       b.eq    ffff80001015e964 <rcu_core+0x49c>  // b.none
         :                      needwake = rcu_accelerate_cbs(rnp, rdp);
    0.00 :   ffff80001015e90c:       mov     x1, x19
    0.00 :   ffff80001015e910:       mov     x0, x25
    0.00 :   ffff80001015e914:       bl      ffff80001015aca0 <rcu_accelerate_cbs>
         :                      __write_once_size():
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
    0.00 :   ffff80001015e918:       strb    wzr, [x19, #229]
    0.00 :   ffff80001015e91c:       strb    wzr, [x19, #228]
         :                      rcu_report_qs_rdp():
    0.00 :   ffff80001015e920:       and     w24, w0, #0xff
         :                      rcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);
    0.00 :   ffff80001015e924:       mov     x3, x27
    0.00 :   ffff80001015e928:       mov     x1, x25
    0.00 :   ffff80001015e92c:       ldr     x2, [x25, #8]
    0.00 :   ffff80001015e930:       mov     x0, x20
    0.00 :   ffff80001015e934:       bl      ffff80001015b070 <rcu_report_qs_rnp>
         :                      if (needwake)
    0.00 :   ffff80001015e938:       cbnz    w24, ffff80001015e978 <rcu_core+0x4b0>
    0.00 :   ffff80001015e93c:       ldr     x27, [x29, #80]
    0.00 :   ffff80001015e940:       b       ffff80001015e580 <rcu_core+0xb8>
         :                      rcu_do_batch():
         :                      rdp->qlen_last_fqs_check = count;
    0.00 :   ffff80001015e944:       str     x0, [x19, #176]
    0.00 :   ffff80001015e948:       b       ffff80001015e8b0 <rcu_core+0x3e8>
         :                      local_clock():
    0.00 :   ffff80001015e94c:       bl      ffff800010179198 <sched_clock>
         :                      rcu_do_batch():
         :                      tlimit = local_clock() + rcu_resched_ns;
    0.00 :   ffff80001015e950:       ldr     x23, [x28, #200]
    0.00 :   ffff80001015e954:       add     x23, x0, x23
    0.00 :   ffff80001015e958:       b       ffff80001015e71c <rcu_core+0x254>
         :                      WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    0.00 :   ffff80001015e95c:       brk     #0x800
    0.00 :   ffff80001015e960:       b       ffff80001015e6f0 <rcu_core+0x228>
         :                      rcu_report_qs_rdp():
         :                      raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    0.00 :   ffff80001015e964:       mov     x1, x27
    0.00 :   ffff80001015e968:       mov     x0, x25
    0.00 :   ffff80001015e96c:       bl      ffff800010cb2bb0 <_raw_spin_unlock_irqrestore>
    0.00 :   ffff80001015e970:       ldr     x27, [x29, #80]
    0.00 :   ffff80001015e974:       b       ffff80001015e580 <rcu_core+0xb8>
         :                      rcu_gp_kthread_wake();
    0.00 :   ffff80001015e978:       bl      ffff80001015af98 <rcu_gp_kthread_wake>
    0.00 :   ffff80001015e97c:       ldr     x27, [x29, #80]
    0.00 :   ffff80001015e980:       b       ffff80001015e580 <rcu_core+0xb8>
    0.00 :   ffff80001015e984:       str     x25, [x29, #64]
    0.00 :   ffff80001015e988:       stp     x27, x28, [x29, #80]
         :                      rcu_core():
         :                      }
    0.00 :   ffff80001015e98c:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (3 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101f2510 <need_update>:
         :                      need_update():
         :                      /*
         :                      * Check if the diffs for a certain cpu indicate that
         :                      * an update is needed.
         :                      */
         :                      static bool need_update(int cpu)
         :                      {
   30.86 :   ffff8000101f2510:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff8000101f2514:       mov     x29, sp
   37.95 :   ffff8000101f2518:       str     x21, [sp, #32]
    0.00 :   ffff8000101f251c:       sxtw    x21, w0
         :                      struct zone *zone;
         :
         :                      for_each_populated_zone(zone) {
    0.00 :   ffff8000101f2520:       bl      ffff8000101f2140 <first_online_pgdat>
    0.00 :   ffff8000101f2524:       cbz     x0, ffff8000101f2594 <need_update+0x84>
    0.00 :   ffff8000101f2528:       stp     x19, x20, [x29, #16]
    0.00 :   ffff8000101f252c:       mov     x19, x0
    0.00 :   ffff8000101f2530:       str     x22, [x29, #40]
         :                      struct per_cpu_pageset *p = per_cpu_ptr(zone->pageset, cpu);
    0.00 :   ffff8000101f2534:       adrp    x22, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff8000101f2538:       add     x22, x22, #0x8e8
    0.00 :   ffff8000101f253c:       nop
         :                      for_each_populated_zone(zone) {
    0.00 :   ffff8000101f2540:       ldr     x3, [x19, #120]
         :                      #endif
         :
         :                      /*
         :                      * The fast way of checking if there are any vmstat diffs.
         :                      */
         :                      if (memchr_inv(p->vm_stat_diff, 0, NR_VM_ZONE_STAT_ITEMS *
    0.00 :   ffff8000101f2544:       mov     x2, #0xc                        // #12
    0.00 :   ffff8000101f2548:       mov     w1, #0x0                        // #0
         :                      for_each_populated_zone(zone) {
    0.00 :   ffff8000101f254c:       cbz     x3, ffff8000101f257c <need_update+0x6c>
         :                      struct per_cpu_pageset *p = per_cpu_ptr(zone->pageset, cpu);
    0.00 :   ffff8000101f2550:       ldr     x20, [x22, x21, lsl #3]
   31.19 :   ffff8000101f2554:       ldr     x3, [x19, #88]
    0.00 :   ffff8000101f2558:       add     x20, x3, x20
         :                      if (memchr_inv(p->vm_stat_diff, 0, NR_VM_ZONE_STAT_ITEMS *
    0.00 :   ffff8000101f255c:       add     x0, x20, #0x4f
    0.00 :   ffff8000101f2560:       bl      ffff800010ca29e8 <memchr_inv>
    0.00 :   ffff8000101f2564:       cbnz    x0, ffff8000101f25a4 <need_update+0x94>
         :                      sizeof(p->vm_stat_diff[0])))
         :                      return true;
         :                      #ifdef CONFIG_NUMA
         :                      if (memchr_inv(p->vm_numa_stat_diff, 0, NR_VM_NUMA_STAT_ITEMS *
    0.00 :   ffff8000101f2568:       mov     x2, #0xc                        // #12
    0.00 :   ffff8000101f256c:       mov     w1, #0x0                        // #0
    0.00 :   ffff8000101f2570:       add     x0, x20, #0x42
    0.00 :   ffff8000101f2574:       bl      ffff800010ca29e8 <memchr_inv>
    0.00 :   ffff8000101f2578:       cbnz    x0, ffff8000101f25a4 <need_update+0x94>
         :                      for_each_populated_zone(zone) {
    0.00 :   ffff8000101f257c:       mov     x0, x19
    0.00 :   ffff8000101f2580:       bl      ffff8000101f21d8 <next_zone>
    0.00 :   ffff8000101f2584:       mov     x19, x0
    0.00 :   ffff8000101f2588:       cbnz    x0, ffff8000101f2540 <need_update+0x30>
    0.00 :   ffff8000101f258c:       ldp     x19, x20, [x29, #16]
    0.00 :   ffff8000101f2590:       ldr     x22, [x29, #40]
         :                      sizeof(p->vm_numa_stat_diff[0])))
         :                      return true;
         :                      #endif
         :                      }
         :                      return false;
    0.00 :   ffff8000101f2594:       mov     w0, #0x0                        // #0
         :                      }
    0.00 :   ffff8000101f2598:       ldr     x21, [sp, #32]
    0.00 :   ffff8000101f259c:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000101f25a0:       ret
         :                      return true;
    0.00 :   ffff8000101f25a4:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000101f25a8:       ldr     x22, [x29, #40]
    0.00 :   ffff8000101f25ac:       ldp     x19, x20, [x29, #16]
         :                      }
    0.00 :   ffff8000101f25b0:       ldr     x21, [sp, #32]
    0.00 :   ffff8000101f25b4:       ldp     x29, x30, [sp], #48
    0.00 :   ffff8000101f25b8:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (2 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001011b9d8 <update_numa_stats>:
         :                      update_numa_stats():
         :
         :                      /*
         :                      * XXX borrowed from update_sg_lb_stats
         :                      */
         :                      static void update_numa_stats(struct numa_stats *ns, int nid)
         :                      {
    0.00 :   ffff80001011b9d8:       stp     x29, x30, [sp, #-64]!
         :                      cpumask_of_node():
         :                      const struct cpumask *cpumask_of_node(int node);
         :                      #else
         :                      /* Returns a pointer to the cpumask of CPUs on Node 'node'. */
         :                      static inline const struct cpumask *cpumask_of_node(int node)
         :                      {
         :                      return node_to_cpumask_map[node];
    0.00 :   ffff80001011b9dc:       sbfiz   x1, x1, #5, #32
         :                      update_numa_stats():
    0.00 :   ffff80001011b9e0:       mov     x29, sp
    0.00 :   ffff80001011b9e4:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001011b9e8:       mov     x19, x0
    0.00 :   ffff80001011b9ec:       stp     x21, x22, [sp, #32]
         :                      cpumask_of_node():
    0.00 :   ffff80001011b9f0:       adrp    x0, ffff800011a7c000 <cpu_release_addr+0x748>
         :                      update_numa_stats():
    0.00 :   ffff80001011b9f4:       str     x23, [sp, #48]
         :                      cpumask_of_node():
    0.00 :   ffff80001011b9f8:       add     x0, x0, #0x308
         :                      update_numa_stats():
         :                      int cpu;
         :
         :                      memset(ns, 0, sizeof(*ns));
         :                      for_each_cpu(cpu, cpumask_of_node(nid)) {
         :                      struct rq *rq = cpu_rq(cpu);
    0.00 :   ffff80001011b9fc:       adrp    x20, ffff8000114d5000 <tegra_to+0x180>
    0.00 :   ffff80001011ba00:       adrp    x22, ffff800011899000 <page_wait_table+0x1500>
         :                      cpumask_of_node():
    0.00 :   ffff80001011ba04:       add     x21, x1, x0
    0.00 :   ffff80001011ba08:       adrp    x23, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      update_numa_stats():
         :                      for_each_cpu(cpu, cpumask_of_node(nid)) {
    0.00 :   ffff80001011ba0c:       mov     w0, #0xffffffff                 // #-1
         :                      struct rq *rq = cpu_rq(cpu);
    0.00 :   ffff80001011ba10:       add     x20, x20, #0xd80
    0.00 :   ffff80001011ba14:       add     x22, x22, #0x8e8
         :                      memset(ns, 0, sizeof(*ns));
    0.00 :   ffff80001011ba18:       stp     xzr, xzr, [x19]
         :                      for_each_cpu(cpu, cpumask_of_node(nid)) {
    0.00 :   ffff80001011ba1c:       add     x23, x23, #0x2b4
    0.00 :   ffff80001011ba20:       b       ffff80001011ba50 <update_numa_stats+0x78>
         :                      cpu_runnable_load():
         :                      return cfs_rq_runnable_load_avg(&rq->cfs);
    0.00 :   ffff80001011ba24:       ldr     x4, [x22, x5, lsl #3]
    0.00 :   ffff80001011ba28:       add     x4, x1, x4
         :                      update_numa_stats():
         :
         :                      ns->load += cpu_runnable_load(rq);
         :                      ns->compute_capacity += capacity_of(cpu);
    0.00 :   ffff80001011ba2c:       ldp     x3, x2, [x19]
         :                      ns->load += cpu_runnable_load(rq);
    0.00 :   ffff80001011ba30:       ldr     x4, [x4, #296]
    0.00 :   ffff80001011ba34:       add     x3, x3, x4
  100.00 :   ffff80001011ba38:       str     x3, [x19]
         :                      capacity_of():
         :                      return load;
         :                      }
         :
         :                      static unsigned long capacity_of(int cpu)
         :                      {
         :                      return cpu_rq(cpu)->cpu_capacity;
    0.00 :   ffff80001011ba3c:       ldr     x3, [x22, x5, lsl #3]
    0.00 :   ffff80001011ba40:       add     x1, x3, x1
         :                      update_numa_stats():
         :                      ns->compute_capacity += capacity_of(cpu);
    0.00 :   ffff80001011ba44:       ldr     x1, [x1, #2480]
    0.00 :   ffff80001011ba48:       add     x1, x2, x1
    0.00 :   ffff80001011ba4c:       str     x1, [x19, #8]
         :                      for_each_cpu(cpu, cpumask_of_node(nid)) {
    0.00 :   ffff80001011ba50:       mov     x1, x21
    0.00 :   ffff80001011ba54:       bl      ffff800010c93a58 <cpumask_next>
    0.00 :   ffff80001011ba58:       ldr     w2, [x23]
         :                      struct rq *rq = cpu_rq(cpu);
    0.00 :   ffff80001011ba5c:       mov     x1, x20
    0.00 :   ffff80001011ba60:       sxtw    x5, w0
         :                      for_each_cpu(cpu, cpumask_of_node(nid)) {
    0.00 :   ffff80001011ba64:       cmp     w0, w2
    0.00 :   ffff80001011ba68:       b.cc    ffff80001011ba24 <update_numa_stats+0x4c>  // b.lo, b.ul, b.last
         :                      }
    0.00 :   ffff80001011ba6c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001011ba70:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff80001011ba74:       ldr     x23, [sp, #48]
    0.00 :   ffff80001011ba78:       ldp     x29, x30, [sp], #64
    0.00 :   ffff80001011ba7c:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (2 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101e00c0 <mark_page_accessed>:
         :                      mark_page_accessed():
         :                      *
         :                      * When a newly allocated page is not yet visible, so safe for non-atomic ops,
         :                      * __SetPageReferenced(page) may be substituted for mark_page_accessed(page).
         :                      */
         :                      void mark_page_accessed(struct page *page)
         :                      {
   50.27 :   ffff8000101e00c0:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff8000101e00c4:       mov     x29, sp
    0.00 :   ffff8000101e00c8:       str     x19, [sp, #16]
    0.00 :   ffff8000101e00cc:       mov     x19, x0
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
   49.73 :   ffff8000101e00d0:       ldr     x0, [x0, #8]
         :                      compound_head():
         :                      static inline struct page *compound_head(struct page *page)
         :                      {
         :                      unsigned long head = READ_ONCE(page->compound_head);
         :
         :                      if (unlikely(head & 1))
         :                      return (struct page *) (head - 1);
    0.00 :   ffff8000101e00d4:       sub     x1, x0, #0x1
    0.00 :   ffff8000101e00d8:       tst     x0, #0x1
    0.00 :   ffff8000101e00dc:       csel    x19, x1, x19, ne  // ne = any
         :                      __read_once_size():
    0.00 :   ffff8000101e00e0:       ldr     x1, [x19, #8]
         :                      compound_head():
    0.00 :   ffff8000101e00e4:       sub     x0, x1, #0x1
    0.00 :   ffff8000101e00e8:       tst     x1, #0x1
    0.00 :   ffff8000101e00ec:       csel    x0, x0, x19, ne  // ne = any
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff8000101e00f0:       ldr     x0, [x0]
         :                      mark_page_accessed():
         :                      page = compound_head(page);
         :
         :                      if (!PageReferenced(page)) {
    0.00 :   ffff8000101e00f4:       tst     w0, #0x2
    0.00 :   ffff8000101e00f8:       b.ne    ffff8000101e0128 <mark_page_accessed+0x68>  // b.any
         :                      __read_once_size():
    0.00 :   ffff8000101e00fc:       ldr     x0, [x19, #8]
         :                      compound_head():
    0.00 :   ffff8000101e0100:       sub     x1, x0, #0x1
    0.00 :   ffff8000101e0104:       tst     x0, #0x1
    0.00 :   ffff8000101e0108:       csel    x19, x1, x19, ne  // ne = any
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000101e010c:       b       ffff8000101e01cc <mark_page_accessed+0x10c>
    0.00 :   ffff8000101e0110:       b       ffff8000101e01cc <mark_page_accessed+0x10c>
         :                      __lse_atomic64_or():
         :                      : [i] "+r" (i), [v] "+Q" (v->counter)                           \
         :                      : "r" (v));                                                     \
         :                      }
         :
         :                      ATOMIC64_OP(andnot, stclr)
         :                      ATOMIC64_OP(or, stset)
    0.00 :   ffff8000101e0114:       mov     x0, #0x2                        // #2
    0.00 :   ffff8000101e0118:       stset   x0, [x19]
         :                      mark_page_accessed():
         :                      if (page_is_file_cache(page))
         :                      workingset_activation(page);
         :                      }
         :                      if (page_is_idle(page))
         :                      clear_page_idle(page);
         :                      }
    0.00 :   ffff8000101e011c:       ldr     x19, [sp, #16]
    0.00 :   ffff8000101e0120:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000101e0124:       ret
         :                      __read_once_size():
    0.00 :   ffff8000101e0128:       ldr     x1, [x19, #8]
         :                      compound_head():
    0.00 :   ffff8000101e012c:       sub     x0, x1, #0x1
    0.00 :   ffff8000101e0130:       tst     x1, #0x1
    0.00 :   ffff8000101e0134:       csel    x0, x0, x19, ne  // ne = any
         :                      test_bit():
    0.00 :   ffff8000101e0138:       ldr     x0, [x0]
         :                      mark_page_accessed():
         :                      } else if (PageUnevictable(page)) {
    0.00 :   ffff8000101e013c:       tst     w0, #0x100000
    0.00 :   ffff8000101e0140:       b.ne    ffff8000101e011c <mark_page_accessed+0x5c>  // b.any
         :                      __read_once_size():
    0.00 :   ffff8000101e0144:       ldr     x1, [x19, #8]
         :                      compound_head():
    0.00 :   ffff8000101e0148:       sub     x0, x1, #0x1
    0.00 :   ffff8000101e014c:       tst     x1, #0x1
    0.00 :   ffff8000101e0150:       csel    x0, x0, x19, ne  // ne = any
         :                      test_bit():
    0.00 :   ffff8000101e0154:       ldr     x0, [x0]
         :                      mark_page_accessed():
         :                      } else if (!PageActive(page)) {
    0.00 :   ffff8000101e0158:       tst     w0, #0x20
    0.00 :   ffff8000101e015c:       b.ne    ffff8000101e011c <mark_page_accessed+0x5c>  // b.any
         :                      __read_once_size():
    0.00 :   ffff8000101e0160:       ldr     x1, [x19, #8]
         :                      compound_head():
    0.00 :   ffff8000101e0164:       sub     x0, x1, #0x1
    0.00 :   ffff8000101e0168:       tst     x1, #0x1
    0.00 :   ffff8000101e016c:       csel    x0, x0, x19, ne  // ne = any
         :                      test_bit():
    0.00 :   ffff8000101e0170:       ldr     x0, [x0]
         :                      mark_page_accessed():
         :                      if (PageLRU(page))
    0.00 :   ffff8000101e0174:       tst     w0, #0x10
    0.00 :   ffff8000101e0178:       b.eq    ffff8000101e01e8 <mark_page_accessed+0x128>  // b.none
         :                      activate_page(page);
    0.00 :   ffff8000101e017c:       mov     x0, x19
    0.00 :   ffff8000101e0180:       bl      ffff8000101dff88 <activate_page>
         :                      __read_once_size():
    0.00 :   ffff8000101e0184:       ldr     x1, [x19, #8]
         :                      compound_head():
    0.00 :   ffff8000101e0188:       sub     x0, x1, #0x1
    0.00 :   ffff8000101e018c:       tst     x1, #0x1
    0.00 :   ffff8000101e0190:       csel    x0, x0, x19, ne  // ne = any
         :                      arch_static_branch_jump():
    0.00 :   ffff8000101e0194:       b       ffff8000101e01dc <mark_page_accessed+0x11c>
    0.00 :   ffff8000101e0198:       b       ffff8000101e01dc <mark_page_accessed+0x11c>
         :                      __lse_atomic64_andnot():
         :                      ATOMIC64_OP(andnot, stclr)
    0.00 :   ffff8000101e019c:       mov     x1, #0x2                        // #2
    0.00 :   ffff8000101e01a0:       stclr   x1, [x0]
         :                      __read_once_size():
    0.00 :   ffff8000101e01a4:       ldr     x1, [x19, #8]
         :                      compound_head():
    0.00 :   ffff8000101e01a8:       sub     x0, x1, #0x1
    0.00 :   ffff8000101e01ac:       tst     x1, #0x1
    0.00 :   ffff8000101e01b0:       csel    x0, x0, x19, ne  // ne = any
         :                      test_bit():
    0.00 :   ffff8000101e01b4:       ldr     x0, [x0]
         :                      mark_page_accessed():
         :                      if (page_is_file_cache(page))
    0.00 :   ffff8000101e01b8:       tst     w0, #0x80000
    0.00 :   ffff8000101e01bc:       b.ne    ffff8000101e011c <mark_page_accessed+0x5c>  // b.any
         :                      workingset_activation(page);
    0.00 :   ffff8000101e01c0:       mov     x0, x19
    0.00 :   ffff8000101e01c4:       bl      ffff8000102031d0 <workingset_activation>
         :                      }
    0.00 :   ffff8000101e01c8:       b       ffff8000101e011c <mark_page_accessed+0x5c>
         :                      __ll_sc_atomic64_or():
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(and, and, L)
         :                      ATOMIC64_OPS(or, orr, L)
    0.00 :   ffff8000101e01cc:       b       ffff8000101e114c <pagevec_remove_exceptionals+0x22c>
         :                      mark_page_accessed():
    0.00 :   ffff8000101e01d0:       ldr     x19, [sp, #16]
    0.00 :   ffff8000101e01d4:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000101e01d8:       ret
         :                      __ll_sc_atomic64_andnot():
         :                      /*
         :                      * GAS converts the mysterious and undocumented BIC (immediate) alias to
         :                      * an AND (immediate) instruction with the immediate inverted. We don't
         :                      * have a constraint for this, so fall back to register.
         :                      */
         :                      ATOMIC64_OPS(andnot, bic, )
    0.00 :   ffff8000101e01dc:       mov     x1, #0x2                        // #2
    0.00 :   ffff8000101e01e0:       b       ffff8000101e1164 <pagevec_remove_exceptionals+0x244>
    0.00 :   ffff8000101e01e4:       b       ffff8000101e01a4 <mark_page_accessed+0xe4>
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff8000101e01e8:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.00 :   ffff8000101e01ec:       ldr     w0, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff8000101e01f0:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000101e01f4:       str     w0, [x1, #16]
         :                      __lru_cache_activate_page():
         :                      struct pagevec *pvec = &get_cpu_var(lru_add_pvec);
    0.00 :   ffff8000101e01f8:       adrp    x1, ffff8000114d3000 <pmu_sb_events>
    0.00 :   ffff8000101e01fc:       add     x1, x1, #0x598
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000101e0200:       mrs     x0, tpidr_el1
         :                      pagevec_count():
         :                      pvec->nr = 0;
         :                      }
         :
         :                      static inline unsigned pagevec_count(struct pagevec *pvec)
         :                      {
         :                      return pvec->nr;
    0.00 :   ffff8000101e0204:       ldrb    w2, [x1, x0]
         :                      __lru_cache_activate_page():
    0.00 :   ffff8000101e0208:       add     x1, x1, x0
         :                      for (i = pagevec_count(pvec) - 1; i >= 0; i--) {
    0.00 :   ffff8000101e020c:       sub     w0, w2, #0x1
    0.00 :   ffff8000101e0210:       cbnz    w2, ffff8000101e0224 <mark_page_accessed+0x164>
    0.00 :   ffff8000101e0214:       b       ffff8000101e0258 <mark_page_accessed+0x198>
    0.00 :   ffff8000101e0218:       sub     w0, w0, #0x1
    0.00 :   ffff8000101e021c:       cmn     w0, #0x1
    0.00 :   ffff8000101e0220:       b.eq    ffff8000101e0258 <mark_page_accessed+0x198>  // b.none
         :                      struct page *pagevec_page = pvec->pages[i];
    0.00 :   ffff8000101e0224:       add     x2, x1, w0, sxtw #3
         :                      if (pagevec_page == page) {
    0.00 :   ffff8000101e0228:       ldr     x2, [x2, #8]
    0.00 :   ffff8000101e022c:       cmp     x19, x2
    0.00 :   ffff8000101e0230:       b.ne    ffff8000101e0218 <mark_page_accessed+0x158>  // b.any
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000101e0234:       ldr     x1, [x19, #8]
         :                      compound_head():
    0.00 :   ffff8000101e0238:       sub     x0, x1, #0x1
    0.00 :   ffff8000101e023c:       tst     x1, #0x1
    0.00 :   ffff8000101e0240:       csel    x0, x0, x19, ne  // ne = any
         :                      arch_static_branch_jump():
    0.00 :   ffff8000101e0244:       b       ffff8000101e0284 <mark_page_accessed+0x1c4>
    0.00 :   ffff8000101e0248:       b       ffff8000101e0284 <mark_page_accessed+0x1c4>
         :                      __lse_atomic64_or():
         :                      ATOMIC64_OP(or, stset)
    0.00 :   ffff8000101e024c:       mov     x1, #0x20                       // #32
    0.00 :   ffff8000101e0250:       stset   x1, [x0]
    0.00 :   ffff8000101e0254:       nop
         :                      get_current():
    0.00 :   ffff8000101e0258:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.00 :   ffff8000101e025c:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000101e0260:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff8000101e0264:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000101e0268:       cbnz    x0, ffff8000101e0274 <mark_page_accessed+0x1b4>
         :                      __lru_cache_activate_page():
         :                      put_cpu_var(lru_add_pvec);
    0.00 :   ffff8000101e026c:       bl      ffff800010cad640 <preempt_schedule>
    0.00 :   ffff8000101e0270:       b       ffff8000101e0184 <mark_page_accessed+0xc4>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000101e0274:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000101e0278:       cbnz    x0, ffff8000101e0184 <mark_page_accessed+0xc4>
         :                      __lru_cache_activate_page():
    0.00 :   ffff8000101e027c:       bl      ffff800010cad640 <preempt_schedule>
    0.00 :   ffff8000101e0280:       b       ffff8000101e0184 <mark_page_accessed+0xc4>
         :                      __ll_sc_atomic64_or():
         :                      ATOMIC64_OPS(or, orr, L)
    0.00 :   ffff8000101e0284:       b       ffff8000101e117c <pagevec_remove_exceptionals+0x25c>
    0.00 :   ffff8000101e0288:       b       ffff8000101e0258 <mark_page_accessed+0x198>
 Percent |	Source code & Disassembly of vmlinux for cycles (3 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101653f0 <__msecs_to_jiffies>:
         :                      _msecs_to_jiffies():
         :                      * multiple of HZ, divide with the factor between them, but round
         :                      * upwards:
         :                      */
         :                      static inline unsigned long _msecs_to_jiffies(const unsigned int m)
         :                      {
         :                      return (m + (MSEC_PER_SEC / HZ) - 1) / (MSEC_PER_SEC / HZ);
   63.09 :   ffff8000101653f0:       cmp     w0, #0x0
    0.00 :   ffff8000101653f4:       mov     w0, w0
    0.00 :   ffff8000101653f8:       add     x0, x0, #0x3
    0.00 :   ffff8000101653fc:       mov     x1, #0x3ffffffffffffffe         // #4611686018427387902
   36.91 :   ffff800010165400:       asr     x0, x0, #2
         :                      __msecs_to_jiffies():
         :                      * Negative value, means infinite timeout:
         :                      */
         :                      if ((int)m < 0)
         :                      return MAX_JIFFY_OFFSET;
         :                      return _msecs_to_jiffies(m);
         :                      }
    0.00 :   ffff800010165404:       csel    x0, x0, x1, ge  // ge = tcont
    0.00 :   ffff800010165408:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (2 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010cad6c8 <schedule_idle>:
         :                      schedule_idle():
         :                      *
         :                      * schedule_idle() is similar to schedule_preempt_disable() except that it
         :                      * never enables preemption because it does not call sched_submit_work().
         :                      */
         :                      void __sched schedule_idle(void)
         :                      {
   50.86 :   ffff800010cad6c8:       stp     x29, x30, [sp, #-16]!
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800010cad6cc:       mrs     x0, sp_el0
         :                      schedule_idle():
    0.00 :   ffff800010cad6d0:       mov     x29, sp
         :                      * regardless because that function is a nop when the task is in a
         :                      * TASK_RUNNING state, make sure this isn't used someplace that the
         :                      * current task can be in any other state. Note, idle is always in the
         :                      * TASK_RUNNING state.
         :                      */
         :                      WARN_ON_ONCE(current->state);
    0.00 :   ffff800010cad6d4:       ldr     x0, [x0, #24]
    0.00 :   ffff800010cad6d8:       cbnz    x0, ffff800010cad6f8 <schedule_idle+0x30>
         :                      do {
         :                      __schedule(false);
    0.00 :   ffff800010cad6dc:       mov     w0, #0x0                        // #0
    0.00 :   ffff800010cad6e0:       bl      ffff800010caccd8 <__schedule>
         :                      get_current():
    0.00 :   ffff800010cad6e4:       mrs     x0, sp_el0
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff800010cad6e8:       ldr     x0, [x0]
         :                      schedule_idle():
         :                      } while (need_resched());
    0.00 :   ffff800010cad6ec:       tbnz    w0, #1, ffff800010cad6dc <schedule_idle+0x14>
         :                      }
   49.14 :   ffff800010cad6f0:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010cad6f4:       ret
         :                      WARN_ON_ONCE(current->state);
    0.00 :   ffff800010cad6f8:       brk     #0x800
    0.00 :   ffff800010cad6fc:       b       ffff800010cad6dc <schedule_idle+0x14>
 Percent |	Source code & Disassembly of vmlinux for cycles (2 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010121fc8 <task_numa_find_cpu>:
         :                      task_numa_find_cpu():
         :                      rcu_read_unlock();
         :                      }
         :
         :                      static void task_numa_find_cpu(struct task_numa_env *env,
         :                      long taskimp, long groupimp)
         :                      {
    0.00 :   ffff800010121fc8:       stp     x29, x30, [sp, #-160]!
    0.00 :   ffff800010121fcc:       mov     x29, sp
    0.00 :   ffff800010121fd0:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010121fd4:       adrp    x20, ffff800011a7c000 <cpu_release_addr+0x748>
    0.00 :   ffff800010121fd8:       stp     x21, x22, [sp, #32]
         :                      cpumask_of_node():
         :                      const struct cpumask *cpumask_of_node(int node);
         :                      #else
         :                      /* Returns a pointer to the cpumask of CPUs on Node 'node'. */
         :                      static inline const struct cpumask *cpumask_of_node(int node)
         :                      {
         :                      return node_to_cpumask_map[node];
    0.00 :   ffff800010121fdc:       add     x20, x20, #0x308
         :                      task_numa_find_cpu():
    0.00 :   ffff800010121fe0:       stp     x23, x24, [sp, #48]
         :                      task_numa_compare():
         :                      struct rq *dst_rq = cpu_rq(env->dst_cpu);
    0.00 :   ffff800010121fe4:       adrp    x24, ffff8000114d5000 <tegra_to+0x180>
         :                      task_numa_find_cpu():
         :                      {
    0.00 :   ffff800010121fe8:       stp     x25, x26, [sp, #64]
    0.00 :   ffff800010121fec:       mov     x25, x1
    0.00 :   ffff800010121ff0:       stp     x27, x28, [sp, #80]
    0.00 :   ffff800010121ff4:       mov     x26, x2
    0.00 :   ffff800010121ff8:       mov     x28, x0
         :                      task_numa_compare():
         :                      struct rq *dst_rq = cpu_rq(env->dst_cpu);
    0.00 :   ffff800010121ffc:       adrp    x23, ffff800011899000 <page_wait_table+0x1500>
         :                      task_numa_find_cpu():
         :                      long src_load, dst_load, load;
         :                      bool maymove = false;
         :                      int cpu;
         :
         :                      load = task_h_load(env->p);
    0.00 :   ffff800010122000:       ldr     x7, [x0]
         :                      task_numa_compare():
         :                      struct rq *dst_rq = cpu_rq(env->dst_cpu);
    0.00 :   ffff800010122004:       add     x24, x24, #0xd80
    0.00 :   ffff800010122008:       add     x23, x23, #0x8e8
         :                      task_numa_find_cpu():
         :                      * If the improvement from just moving env->p direction is better
         :                      * than swapping tasks around, check if a move is possible.
         :                      */
         :                      maymove = !load_too_imbalanced(src_load, dst_load, env);
         :
         :                      for_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {
    0.00 :   ffff80001012200c:       mov     w27, #0xffffffff                // #-1
    0.00 :   ffff800010122010:       adrp    x22, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      task_cfs_rq():
         :                      return p->se.cfs_rq;
    0.00 :   ffff800010122014:       ldr     x6, [x7, #320]
         :                      task_h_load():
         :
         :                      static unsigned long task_h_load(struct task_struct *p)
         :                      {
         :                      struct cfs_rq *cfs_rq = task_cfs_rq(p);
         :
         :                      update_cfs_rq_h_load(cfs_rq);
    0.00 :   ffff800010122018:       mov     x0, x6
    0.00 :   ffff80001012201c:       bl      ffff80001011b4a0 <update_cfs_rq_h_load>
         :                      return div64_ul(p->se.avg.load_avg * cfs_rq->h_load,
    0.00 :   ffff800010122020:       ldr     x19, [x6, #280]
    0.00 :   ffff800010122024:       ldr     x2, [x7, #416]
    0.00 :   ffff800010122028:       ldr     x3, [x6, #160]
         :                      load_too_imbalanced():
         :                      src_capacity = env->src_stats.compute_capacity;
    0.00 :   ffff80001012202c:       ldp     x0, x7, [x28, #24]
         :                      task_h_load():
         :                      return div64_ul(p->se.avg.load_avg * cfs_rq->h_load,
    0.00 :   ffff800010122030:       add     x3, x3, #0x1
    0.00 :   ffff800010122034:       mul     x2, x2, x19
         :                      load_too_imbalanced():
         :                      dst_capacity = env->dst_stats.compute_capacity;
    0.00 :   ffff800010122038:       ldp     x1, x6, [x28, #40]
         :                      div64_u64():
         :                      *
         :                      * Return: dividend / divisor
         :                      */
         :                      static inline u64 div64_u64(u64 dividend, u64 divisor)
         :                      {
         :                      return dividend / divisor;
    0.00 :   ffff80001012203c:       udiv    x2, x2, x3
         :                      load_too_imbalanced():
         :                      old_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);
    0.00 :   ffff800010122040:       mul     x21, x1, x7
    0.00 :   ffff800010122044:       msub    x21, x0, x6, x21
         :                      task_numa_find_cpu():
         :                      dst_load = env->dst_stats.load + load;
    0.00 :   ffff800010122048:       add     x19, x1, x2
         :                      src_load = env->src_stats.load - load;
    0.00 :   ffff80001012204c:       sub     x0, x0, x2
         :                      load_too_imbalanced():
         :                      imb = abs(dst_load * src_capacity - src_load * dst_capacity);
    0.00 :   ffff800010122050:       mul     x19, x19, x7
    0.00 :   ffff800010122054:       msub    x19, x0, x6, x19
    0.00 :   ffff800010122058:       cmp     x19, #0x0
    0.00 :   ffff80001012205c:       cneg    x0, x19, lt  // lt = tstop
         :                      old_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);
    0.00 :   ffff800010122060:       cmp     x21, #0x0
    0.00 :   ffff800010122064:       cneg    x21, x21, lt  // lt = tstop
         :                      imb = abs(dst_load * src_capacity - src_load * dst_capacity);
    0.00 :   ffff800010122068:       str     x0, [x29, #128]
         :                      task_numa_find_cpu():
         :                      maymove = !load_too_imbalanced(src_load, dst_load, env);
    0.00 :   ffff80001012206c:       cmp     x0, x21
    0.00 :   ffff800010122070:       cset    w0, le
    0.00 :   ffff800010122074:       str     w0, [x29, #140]
         :                      cpumask_of_node():
    0.00 :   ffff800010122078:       ldrsw   x1, [x28, #20]
         :                      task_numa_find_cpu():
         :                      for_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {
    0.00 :   ffff80001012207c:       mov     w0, w27
    0.00 :   ffff800010122080:       add     x1, x20, x1, lsl #5
    0.00 :   ffff800010122084:       bl      ffff800010c93a58 <cpumask_next>
    0.00 :   ffff800010122088:       mov     w27, w0
    0.00 :   ffff80001012208c:       ldr     w0, [x22, #692]
    0.00 :   ffff800010122090:       cmp     w27, w0
    0.00 :   ffff800010122094:       b.cs    ffff80001012226c <task_numa_find_cpu+0x2a4>  // b.hs, b.nlast
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff800010122098:       add     w0, w27, #0x3f
    0.00 :   ffff80001012209c:       cmp     w27, #0x0
   50.09 :   ffff8000101220a0:       csel    w0, w0, w27, lt  // lt = tstop
         :                      task_numa_find_cpu():
         :                      if (!cpumask_test_cpu(cpu, env->p->cpus_ptr))
    0.00 :   ffff8000101220a4:       ldr     x1, [x28]
         :                      test_bit():
    0.00 :   ffff8000101220a8:       asr     w0, w0, #6
    0.00 :   ffff8000101220ac:       ldr     x2, [x1, #736]
    0.00 :   ffff8000101220b0:       sxtw    x0, w0
    0.00 :   ffff8000101220b4:       ldr     x0, [x2, x0, lsl #3]
    0.00 :   ffff8000101220b8:       lsr     x0, x0, x27
         :                      task_numa_find_cpu():
    0.00 :   ffff8000101220bc:       tbz     w0, #0, ffff800010122078 <task_numa_find_cpu+0xb0>
         :                      env->dst_cpu = cpu;
    0.00 :   ffff8000101220c0:       str     w27, [x28, #16]
         :                      task_numa_compare():
         :                      struct rq *dst_rq = cpu_rq(env->dst_cpu);
    0.00 :   ffff8000101220c4:       mov     x0, x24
    0.00 :   ffff8000101220c8:       ldr     x3, [x23, w27, sxtw #3]
         :                      deref_curr_numa_group():
         :                      return rcu_dereference_protected(p->numa_group, p == current);
    0.00 :   ffff8000101220cc:       ldr     x2, [x1, #2248]
         :                      task_numa_compare():
         :                      struct rq *dst_rq = cpu_rq(env->dst_cpu);
    0.00 :   ffff8000101220d0:       add     x0, x0, x3
         :                      long imp = p_ng ? groupimp : taskimp;
    0.00 :   ffff8000101220d4:       cmp     x2, #0x0
    0.00 :   ffff8000101220d8:       csel    x19, x25, x26, eq  // eq = none
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000101220dc:       ldr     w1, [x0, #16]
         :                      task_numa_compare():
         :                      if (READ_ONCE(dst_rq->numa_migrate_on))
    0.00 :   ffff8000101220e0:       cbnz    w1, ffff800010122078 <task_numa_find_cpu+0xb0>
    0.00 :   ffff8000101220e4:       stp     x2, x0, [x29, #144]
         :                      int dist = env->dist;
    0.00 :   ffff8000101220e8:       ldr     w0, [x28, #60]
    0.00 :   ffff8000101220ec:       str     w0, [x29, #136]
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff8000101220f0:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
    0.00 :   ffff8000101220f4:       ldr     x0, [x29, #152]
    0.00 :   ffff8000101220f8:       ldr     x6, [x0, #2352]
         :                      task_numa_compare():
         :                      if (cur && ((cur->flags & PF_EXITING) || is_idle_task(cur)))
    0.00 :   ffff8000101220fc:       cbz     x6, ffff800010122470 <task_numa_find_cpu+0x4a8>
    0.00 :   ffff800010122100:       ldr     w0, [x6, #44]
         :                      cur = rcu_dereference(dst_rq->curr);
    0.00 :   ffff800010122104:       mov     x8, x6
    0.00 :   ffff800010122108:       ldr     x1, [x28]
         :                      if (cur && ((cur->flags & PF_EXITING) || is_idle_task(cur)))
    0.00 :   ffff80001012210c:       ldr     x2, [x29, #144]
    0.00 :   ffff800010122110:       tbz     w0, #2, ffff800010122288 <task_numa_find_cpu+0x2c0>
         :                      if (cur == env->p)
    0.00 :   ffff800010122114:       cbz     x1, ffff800010122248 <task_numa_find_cpu+0x280>
         :                      if (maymove && moveimp >= env->best_imp)
    0.00 :   ffff800010122118:       ldr     x0, [x29, #128]
    0.00 :   ffff80001012211c:       cmp     x0, x21
    0.00 :   ffff800010122120:       b.gt    ffff800010122248 <task_numa_find_cpu+0x280>
    0.00 :   ffff800010122124:       ldr     x0, [x28, #72]
    0.00 :   ffff800010122128:       cmp     x0, x19
    0.00 :   ffff80001012212c:       b.gt    ffff800010122248 <task_numa_find_cpu+0x280>
         :                      arch_local_irq_disable():
         :                      u32 pmr = read_sysreg_s(SYS_ICC_PMR_EL1);
         :
         :                      WARN_ON_ONCE(pmr != GIC_PRIO_IRQON && pmr != GIC_PRIO_IRQOFF);
         :                      }
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010122130:       mov     x0, #0x60                       // #96
    0.00 :   ffff800010122134:       msr     daifset, #0x2
         :                      task_numa_compare():
         :                      env->dst_cpu = select_idle_sibling(env->p, env->src_cpu,
    0.00 :   ffff800010122138:       ldr     w1, [x28, #8]
    0.00 :   ffff80001012213c:       ldr     w2, [x28, #16]
    0.00 :   ffff800010122140:       ldr     x0, [x28]
    0.00 :   ffff800010122144:       bl      ffff80001011ce68 <select_idle_sibling>
    0.00 :   ffff800010122148:       str     w0, [x28, #16]
         :                      arch_local_irq_enable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001012214c:       mov     x1, #0xe0                       // #224
    0.00 :   ffff800010122150:       msr     daifclr, #0x2
    0.00 :   ffff800010122154:       mov     x8, #0x0                        // #0
         :                      task_numa_assign():
         :                      struct rq *rq = cpu_rq(env->dst_cpu);
    0.00 :   ffff800010122158:       ldrsw   x0, [x28, #16]
    0.00 :   ffff80001012215c:       mov     x1, x24
         :                      __xchg_case_mb_32():
         :                      __XCHG_CASE(w, h, rel_, 16,        ,    ,  ,  , l, "memory")
         :                      __XCHG_CASE(w,  , rel_, 32,        ,    ,  ,  , l, "memory")
         :                      __XCHG_CASE( ,  , rel_, 64,        ,    ,  ,  , l, "memory")
         :                      __XCHG_CASE(w, b,  mb_,  8, dmb ish, nop,  , a, l, "memory")
         :                      __XCHG_CASE(w, h,  mb_, 16, dmb ish, nop,  , a, l, "memory")
         :                      __XCHG_CASE(w,  ,  mb_, 32, dmb ish, nop,  , a, l, "memory")
    0.00 :   ffff800010122160:       mov     w2, #0x1                        // #1
         :                      task_numa_assign():
    0.00 :   ffff800010122164:       ldr     x0, [x23, x0, lsl #3]
    0.00 :   ffff800010122168:       add     x0, x1, x0
         :                      __xchg_case_mb_32():
    0.00 :   ffff80001012216c:       add     x0, x0, #0x10
    0.00 :   ffff800010122170:       prfm    pstl1strm, [x0]
    0.00 :   ffff800010122174:       ldxr    w3, [x0]
    0.00 :   ffff800010122178:       stlxr   w6, w2, [x0]
    0.00 :   ffff80001012217c:       cbnz    w6, ffff800010122174 <task_numa_find_cpu+0x1ac>
    0.00 :   ffff800010122180:       dmb     ish
         :                      task_numa_assign():
         :                      if (xchg(&rq->numa_migrate_on, 1))
    0.00 :   ffff800010122184:       cbnz    w3, ffff800010122248 <task_numa_find_cpu+0x280>
         :                      if (env->best_cpu != -1) {
    0.00 :   ffff800010122188:       ldr     w0, [x28, #80]
    0.00 :   ffff80001012218c:       cmn     w0, #0x1
    0.00 :   ffff800010122190:       b.eq    ffff8000101221a0 <task_numa_find_cpu+0x1d8>  // b.none
         :                      rq = cpu_rq(env->best_cpu);
    0.00 :   ffff800010122194:       ldr     x0, [x23, w0, sxtw #3]
    0.00 :   ffff800010122198:       add     x1, x1, x0
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001012219c:       str     wzr, [x1, #16]
         :                      task_numa_assign():
         :                      if (env->best_task)
    0.00 :   ffff8000101221a0:       ldr     x0, [x28, #64]
    0.00 :   ffff8000101221a4:       cbz     x0, ffff8000101221e8 <task_numa_find_cpu+0x220>
         :                      put_task_struct():
         :
         :                      extern void __put_task_struct(struct task_struct *t);
         :
         :                      static inline void put_task_struct(struct task_struct *t)
         :                      {
         :                      if (refcount_dec_and_test(&t->usage))
    0.00 :   ffff8000101221a8:       add     x3, x0, #0x28
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000101221ac:       b       ffff80001012261c <task_numa_find_cpu+0x654>
    0.00 :   ffff8000101221b0:       b       ffff80001012261c <task_numa_find_cpu+0x654>
         :                      __lse_atomic_fetch_sub_release():
         :                      return i;                                                       \
         :                      }
         :
         :                      ATOMIC_FETCH_OP_SUB(_relaxed,   )
         :                      ATOMIC_FETCH_OP_SUB(_acquire,  a, "memory")
         :                      ATOMIC_FETCH_OP_SUB(_release,  l, "memory")
    0.00 :   ffff8000101221b4:       mov     w1, #0x1                        // #1
    0.00 :   ffff8000101221b8:       neg     w1, w1
    0.00 :   ffff8000101221bc:       ldaddl  w1, w1, [x3]
         :                      refcount_sub_and_test():
         :                      */
         :                      static inline __must_check bool refcount_sub_and_test(int i, refcount_t *r)
         :                      {
         :                      int old = atomic_fetch_sub_release(i, &r->refs);
         :
         :                      if (old == i) {
    0.00 :   ffff8000101221c0:       cmp     w1, #0x1
    0.00 :   ffff8000101221c4:       b.eq    ffff800010122638 <task_numa_find_cpu+0x670>  // b.none
         :                      smp_acquire__after_ctrl_dep();
         :                      return true;
         :                      }
         :
         :                      if (unlikely(old < 0 || old - i < 0))
    0.00 :   ffff8000101221c8:       cmp     w1, #0x0
    0.00 :   ffff8000101221cc:       b.gt    ffff8000101221e8 <task_numa_find_cpu+0x220>
    0.00 :   ffff8000101221d0:       str     x8, [x29, #152]
         :                      refcount_warn_saturate(r, REFCOUNT_SUB_UAF);
    0.00 :   ffff8000101221d4:       mov     w1, #0x3                        // #3
    0.00 :   ffff8000101221d8:       mov     x0, x3
    0.00 :   ffff8000101221dc:       bl      ffff80001048ba28 <refcount_warn_saturate>
    0.00 :   ffff8000101221e0:       ldr     x8, [x29, #152]
    0.00 :   ffff8000101221e4:       nop
         :                      task_numa_assign():
         :                      if (p)
    0.00 :   ffff8000101221e8:       cbz     x8, ffff800010122238 <task_numa_find_cpu+0x270>
    0.00 :   ffff8000101221ec:       str     x8, [x29, #152]
         :                      arch_atomic_fetch_add_relaxed():
         :                      ATOMIC_FETCH_OP(        , op)
         :
         :                      ATOMIC_FETCH_OPS(atomic_fetch_andnot)
         :                      ATOMIC_FETCH_OPS(atomic_fetch_or)
         :                      ATOMIC_FETCH_OPS(atomic_fetch_xor)
         :                      ATOMIC_FETCH_OPS(atomic_fetch_add)
    0.00 :   ffff8000101221f0:       bl      ffff80001011b7a8 <system_uses_lse_atomics>
         :                      get_task_struct():
         :                      refcount_inc(&t->usage);
    0.00 :   ffff8000101221f4:       ldr     x8, [x29, #152]
         :                      arch_atomic_fetch_add_relaxed():
    0.00 :   ffff8000101221f8:       tst     w0, #0xff
         :                      get_task_struct():
    0.00 :   ffff8000101221fc:       add     x2, x8, #0x28
         :                      arch_atomic_fetch_add_relaxed():
    0.00 :   ffff800010122200:       b.ne    ffff80001012262c <task_numa_find_cpu+0x664>  // b.any
         :                      __ll_sc_atomic_fetch_add_relaxed():
         :                      ATOMIC_FETCH_OP (        , dmb ish,  , l, "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
    0.00 :   ffff800010122204:       add     x4, x8, #0x28
    0.00 :   ffff800010122208:       b       ffff800010127534 <sched_group_set_shares+0x64c>
         :                      refcount_add():
         :                      if (unlikely(!old))
    0.00 :   ffff80001012220c:       cmp     w0, #0x0
    0.00 :   ffff800010122210:       b.eq    ffff80001012275c <task_numa_find_cpu+0x794>  // b.none
         :                      else if (unlikely(old < 0 || old + i < 0))
    0.00 :   ffff800010122214:       b.lt    ffff800010122220 <task_numa_find_cpu+0x258>  // b.tstop
    0.00 :   ffff800010122218:       cmn     w0, #0x1
    0.00 :   ffff80001012221c:       b.pl    ffff800010122238 <task_numa_find_cpu+0x270>  // b.nfrst
    0.00 :   ffff800010122220:       str     x8, [x29, #152]
         :                      refcount_warn_saturate(r, REFCOUNT_ADD_OVF);
    0.00 :   ffff800010122224:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010122228:       mov     x0, x2
    0.00 :   ffff80001012222c:       bl      ffff80001048ba28 <refcount_warn_saturate>
    0.00 :   ffff800010122230:       ldr     x8, [x29, #152]
    0.00 :   ffff800010122234:       nop
         :                      task_numa_assign():
         :                      env->best_cpu = env->dst_cpu;
    0.00 :   ffff800010122238:       ldr     w0, [x28, #16]
         :                      env->best_imp = imp;
    0.00 :   ffff80001012223c:       stp     x8, x19, [x28, #64]
         :                      env->best_cpu = env->dst_cpu;
    0.00 :   ffff800010122240:       str     w0, [x28, #80]
    0.00 :   ffff800010122244:       nop
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.00 :   ffff800010122248:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      cpumask_of_node():
    0.00 :   ffff80001012224c:       ldrsw   x1, [x28, #20]
         :                      task_numa_find_cpu():
         :                      for_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {
    0.00 :   ffff800010122250:       mov     w0, w27
    0.00 :   ffff800010122254:       add     x1, x20, x1, lsl #5
    0.00 :   ffff800010122258:       bl      ffff800010c93a58 <cpumask_next>
    0.00 :   ffff80001012225c:       mov     w27, w0
    0.00 :   ffff800010122260:       ldr     w0, [x22, #692]
    0.00 :   ffff800010122264:       cmp     w27, w0
    0.00 :   ffff800010122268:       b.cc    ffff800010122098 <task_numa_find_cpu+0xd0>  // b.lo, b.ul, b.last
         :                      }
    0.00 :   ffff80001012226c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010122270:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010122274:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff800010122278:       ldp     x25, x26, [sp, #64]
    0.00 :   ffff80001012227c:       ldp     x27, x28, [sp, #80]
    0.00 :   ffff800010122280:       ldp     x29, x30, [sp], #160
    0.00 :   ffff800010122284:       ret
         :                      task_numa_compare():
         :                      if (cur && ((cur->flags & PF_EXITING) || is_idle_task(cur)))
    0.00 :   ffff800010122288:       tbnz    w0, #1, ffff800010122114 <task_numa_find_cpu+0x14c>
         :                      if (cur == env->p)
    0.00 :   ffff80001012228c:       cmp     x6, x1
    0.00 :   ffff800010122290:       b.eq    ffff800010122248 <task_numa_find_cpu+0x280>  // b.none
         :                      if (!cpumask_test_cpu(env->src_cpu, cur->cpus_ptr))
    0.00 :   ffff800010122294:       ldr     w1, [x28, #8]
         :                      test_bit():
    0.00 :   ffff800010122298:       ldr     x3, [x6, #736]
    0.00 :   ffff80001012229c:       add     w0, w1, #0x3f
   49.91 :   ffff8000101222a0:       cmp     w1, #0x0
    0.00 :   ffff8000101222a4:       csel    w0, w0, w1, lt  // lt = tstop
    0.00 :   ffff8000101222a8:       asr     w0, w0, #6
    0.00 :   ffff8000101222ac:       sxtw    x0, w0
    0.00 :   ffff8000101222b0:       ldr     x0, [x3, x0, lsl #3]
    0.00 :   ffff8000101222b4:       lsr     x1, x0, x1
         :                      task_numa_compare():
    0.00 :   ffff8000101222b8:       tbz     w1, #0, ffff800010122248 <task_numa_find_cpu+0x280>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000101222bc:       ldr     x10, [x6, #2248]
    0.00 :   ffff8000101222c0:       ldr     w1, [x28, #12]
         :                      task_numa_compare():
         :                      if (cur_ng == p_ng) {
    0.00 :   ffff8000101222c4:       cmp     x2, x10
    0.00 :   ffff8000101222c8:       b.eq    ffff800010122534 <task_numa_find_cpu+0x56c>  // b.none
         :                      if (cur_ng && p_ng)
    0.00 :   ffff8000101222cc:       cmp     x10, #0x0
    0.00 :   ffff8000101222d0:       mov     x9, x19
    0.00 :   ffff8000101222d4:       ccmp    x2, #0x0, #0x4, ne  // ne = any
    0.00 :   ffff8000101222d8:       b.eq    ffff800010122480 <task_numa_find_cpu+0x4b8>  // b.none
         :                      __read_once_size():
    0.00 :   ffff8000101222dc:       ldr     x0, [x6, #2248]
         :                      group_weight():
         :                      if (!ng)
    0.00 :   ffff8000101222e0:       cbz     x0, ffff800010122338 <task_numa_find_cpu+0x370>
         :                      total_faults = ng->total_faults;
    0.00 :   ffff8000101222e4:       ldr     x11, [x0, #40]
         :                      if (!total_faults)
    0.00 :   ffff8000101222e8:       cbz     x11, ffff800010122338 <task_numa_find_cpu+0x370>
         :                      __read_once_size():
    0.00 :   ffff8000101222ec:       ldr     x0, [x6, #2248]
         :                      group_faults():
         :                      return 0;
    0.00 :   ffff8000101222f0:       mov     x10, #0x0                       // #0
         :                      if (!ng)
    0.00 :   ffff8000101222f4:       cbz     x0, ffff800010122314 <task_numa_find_cpu+0x34c>
         :                      task_faults_idx():
         :                      return NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;
    0.00 :   ffff8000101222f8:       lsl     w2, w1, #1
    0.00 :   ffff8000101222fc:       add     w3, w2, #0x1
         :                      group_faults():
         :                      return ng->faults[task_faults_idx(NUMA_MEM, nid, 0)] +
    0.00 :   ffff800010122300:       add     x2, x0, w2, sxtw #3
         :                      ng->faults[task_faults_idx(NUMA_MEM, nid, 1)];
    0.00 :   ffff800010122304:       add     x0, x0, w3, sxtw #3
         :                      return ng->faults[task_faults_idx(NUMA_MEM, nid, 0)] +
    0.00 :   ffff800010122308:       ldr     x10, [x2, #64]
    0.00 :   ffff80001012230c:       ldr     x9, [x0, #64]
    0.00 :   ffff800010122310:       add     x10, x9, x10
         :                      score_nearby_nodes():
         :                      if (sched_numa_topology_type == NUMA_DIRECT)
    0.00 :   ffff800010122314:       adrp    x0, ffff800011a80000 <def_rt_bandwidth>
    0.00 :   ffff800010122318:       ldr     w0, [x0, #240]
    0.00 :   ffff80001012231c:       cbnz    w0, ffff8000101226a8 <task_numa_find_cpu+0x6e0>
         :                      group_weight():
         :                      return 1000 * faults / total_faults;
    0.00 :   ffff800010122320:       lsl     x9, x10, #5
    0.00 :   ffff800010122324:       sub     x9, x9, x10
    0.00 :   ffff800010122328:       add     x9, x10, x9, lsl #2
    0.00 :   ffff80001012232c:       lsl     x9, x9, #3
    0.00 :   ffff800010122330:       udiv    x9, x9, x11
    0.00 :   ffff800010122334:       add     x9, x19, x9
         :                      __read_once_size():
    0.00 :   ffff800010122338:       ldr     x0, [x6, #2248]
         :                      group_weight():
         :                      if (!ng)
    0.00 :   ffff80001012233c:       cbz     x0, ffff800010122398 <task_numa_find_cpu+0x3d0>
         :                      total_faults = ng->total_faults;
    0.00 :   ffff800010122340:       ldr     x11, [x0, #40]
         :                      if (!total_faults)
    0.00 :   ffff800010122344:       cbz     x11, ffff800010122398 <task_numa_find_cpu+0x3d0>
         :                      __read_once_size():
    0.00 :   ffff800010122348:       ldr     x0, [x6, #2248]
         :                      group_faults():
         :                      return 0;
    0.00 :   ffff80001012234c:       mov     x10, #0x0                       // #0
         :                      task_numa_compare():
         :                      group_weight(cur, env->dst_nid, dist);
    0.00 :   ffff800010122350:       ldr     w1, [x28, #20]
         :                      group_faults():
         :                      if (!ng)
    0.00 :   ffff800010122354:       cbz     x0, ffff800010122374 <task_numa_find_cpu+0x3ac>
         :                      task_faults_idx():
         :                      return NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;
    0.00 :   ffff800010122358:       lsl     w2, w1, #1
    0.00 :   ffff80001012235c:       add     w3, w2, #0x1
         :                      group_faults():
         :                      return ng->faults[task_faults_idx(NUMA_MEM, nid, 0)] +
    0.00 :   ffff800010122360:       add     x2, x0, w2, sxtw #3
         :                      ng->faults[task_faults_idx(NUMA_MEM, nid, 1)];
    0.00 :   ffff800010122364:       add     x0, x0, w3, sxtw #3
         :                      return ng->faults[task_faults_idx(NUMA_MEM, nid, 0)] +
    0.00 :   ffff800010122368:       ldr     x2, [x2, #64]
    0.00 :   ffff80001012236c:       ldr     x0, [x0, #64]
    0.00 :   ffff800010122370:       add     x10, x0, x2
         :                      score_nearby_nodes():
         :                      if (sched_numa_topology_type == NUMA_DIRECT)
    0.00 :   ffff800010122374:       adrp    x0, ffff800011a80000 <def_rt_bandwidth>
    0.00 :   ffff800010122378:       ldr     w0, [x0, #240]
    0.00 :   ffff80001012237c:       cbnz    w0, ffff8000101226d0 <task_numa_find_cpu+0x708>
         :                      group_weight():
         :                      return 1000 * faults / total_faults;
    0.00 :   ffff800010122380:       lsl     x0, x10, #5
    0.00 :   ffff800010122384:       sub     x0, x0, x10
    0.00 :   ffff800010122388:       add     x0, x10, x0, lsl #2
    0.00 :   ffff80001012238c:       lsl     x0, x0, #3
    0.00 :   ffff800010122390:       udiv    x0, x0, x11
    0.00 :   ffff800010122394:       sub     x9, x9, x0
    0.00 :   ffff800010122398:       cmp     x19, x9
    0.00 :   ffff80001012239c:       ldr     w1, [x29, #140]
    0.00 :   ffff8000101223a0:       cset    w0, gt
    0.00 :   ffff8000101223a4:       and     w0, w1, w0
         :                      task_numa_compare():
         :                      if (maymove && moveimp > imp && moveimp > env->best_imp) {
    0.00 :   ffff8000101223a8:       cbz     w0, ffff8000101223b8 <task_numa_find_cpu+0x3f0>
    0.00 :   ffff8000101223ac:       ldr     x0, [x28, #72]
    0.00 :   ffff8000101223b0:       cmp     x19, x0
    0.00 :   ffff8000101223b4:       b.gt    ffff800010122130 <task_numa_find_cpu+0x168>
         :                      if (imp < SMALLIMP || imp <= env->best_imp + SMALLIMP / 2)
    0.00 :   ffff8000101223b8:       cmp     x9, #0x1d
    0.00 :   ffff8000101223bc:       b.le    ffff800010122248 <task_numa_find_cpu+0x280>
    0.00 :   ffff8000101223c0:       ldr     x0, [x28, #72]
    0.00 :   ffff8000101223c4:       add     x0, x0, #0xf
    0.00 :   ffff8000101223c8:       cmp     x0, x9
    0.00 :   ffff8000101223cc:       b.ge    ffff800010122248 <task_numa_find_cpu+0x280>  // b.tcont
         :                      load = task_h_load(env->p) - task_h_load(cur);
    0.00 :   ffff8000101223d0:       ldr     x7, [x28]
    0.00 :   ffff8000101223d4:       stp     x8, x6, [x29, #144]
         :                      task_cfs_rq():
         :                      return p->se.cfs_rq;
    0.00 :   ffff8000101223d8:       ldr     x10, [x7, #320]
         :                      task_h_load():
         :                      update_cfs_rq_h_load(cfs_rq);
    0.00 :   ffff8000101223dc:       mov     x0, x10
    0.00 :   ffff8000101223e0:       bl      ffff80001011b4a0 <update_cfs_rq_h_load>
         :                      return div64_ul(p->se.avg.load_avg * cfs_rq->h_load,
    0.00 :   ffff8000101223e4:       ldr     x2, [x10, #280]
    0.00 :   ffff8000101223e8:       ldr     x7, [x7, #416]
    0.00 :   ffff8000101223ec:       ldr     x1, [x10, #160]
         :                      task_cfs_rq():
         :                      return p->se.cfs_rq;
    0.00 :   ffff8000101223f0:       ldr     x6, [x29, #152]
         :                      task_h_load():
         :                      return div64_ul(p->se.avg.load_avg * cfs_rq->h_load,
    0.00 :   ffff8000101223f4:       add     x1, x1, #0x1
    0.00 :   ffff8000101223f8:       mul     x7, x7, x2
         :                      task_cfs_rq():
         :                      return p->se.cfs_rq;
    0.00 :   ffff8000101223fc:       ldr     x10, [x6, #320]
         :                      task_h_load():
         :                      update_cfs_rq_h_load(cfs_rq);
    0.00 :   ffff800010122400:       mov     x0, x10
         :                      div64_u64():
    0.00 :   ffff800010122404:       udiv    x7, x7, x1
         :                      task_h_load():
    0.00 :   ffff800010122408:       bl      ffff80001011b4a0 <update_cfs_rq_h_load>
         :                      return div64_ul(p->se.avg.load_avg * cfs_rq->h_load,
    0.00 :   ffff80001012240c:       ldr     x2, [x10, #280]
    0.00 :   ffff800010122410:       ldp     x8, x6, [x29, #144]
    0.00 :   ffff800010122414:       ldr     x1, [x10, #160]
    0.00 :   ffff800010122418:       ldr     x0, [x6, #416]
    0.00 :   ffff80001012241c:       add     x1, x1, #0x1
    0.00 :   ffff800010122420:       mul     x0, x0, x2
         :                      div64_u64():
    0.00 :   ffff800010122424:       udiv    x0, x0, x1
         :                      task_numa_compare():
         :                      if (!load)
    0.00 :   ffff800010122428:       subs    x7, x7, x0
    0.00 :   ffff80001012242c:       b.eq    ffff800010122468 <task_numa_find_cpu+0x4a0>  // b.none
         :                      load_too_imbalanced():
         :                      dst_capacity = env->dst_stats.compute_capacity;
    0.00 :   ffff800010122430:       ldp     x0, x6, [x28, #40]
         :                      src_capacity = env->src_stats.compute_capacity;
    0.00 :   ffff800010122434:       ldp     x1, x2, [x28, #24]
         :                      task_numa_compare():
         :                      dst_load = env->dst_stats.load + load;
    0.00 :   ffff800010122438:       add     x3, x7, x0
         :                      src_load = env->src_stats.load - load;
    0.00 :   ffff80001012243c:       sub     x7, x1, x7
         :                      load_too_imbalanced():
         :                      imb = abs(dst_load * src_capacity - src_load * dst_capacity);
    0.00 :   ffff800010122440:       mul     x3, x3, x2
         :                      old_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);
    0.00 :   ffff800010122444:       mul     x2, x0, x2
         :                      imb = abs(dst_load * src_capacity - src_load * dst_capacity);
    0.00 :   ffff800010122448:       msub    x0, x7, x6, x3
         :                      old_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);
    0.00 :   ffff80001012244c:       msub    x1, x1, x6, x2
         :                      imb = abs(dst_load * src_capacity - src_load * dst_capacity);
    0.00 :   ffff800010122450:       cmp     x0, #0x0
    0.00 :   ffff800010122454:       cneg    x0, x0, lt  // lt = tstop
         :                      old_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);
    0.00 :   ffff800010122458:       cmp     x1, #0x0
    0.00 :   ffff80001012245c:       cneg    x1, x1, lt  // lt = tstop
         :                      task_numa_compare():
         :                      if (load_too_imbalanced(src_load, dst_load, env))
    0.00 :   ffff800010122460:       cmp     x0, x1
    0.00 :   ffff800010122464:       b.gt    ffff800010122248 <task_numa_find_cpu+0x280>
         :                      if (!load)
    0.00 :   ffff800010122468:       mov     x19, x9
    0.00 :   ffff80001012246c:       b       ffff800010122158 <task_numa_find_cpu+0x190>
         :                      if (cur == env->p)
    0.00 :   ffff800010122470:       ldr     x0, [x28]
    0.00 :   ffff800010122474:       cbnz    x0, ffff800010122118 <task_numa_find_cpu+0x150>
         :                      rcu_read_unlock():
    0.00 :   ffff800010122478:       bl      ffff80001015eee0 <__rcu_read_unlock>
    0.00 :   ffff80001012247c:       b       ffff80001012224c <task_numa_find_cpu+0x284>
         :                      task_weight():
         :                      if (!p->numa_faults)
    0.00 :   ffff800010122480:       ldr     x2, [x6, #2256]
    0.00 :   ffff800010122484:       cbz     x2, ffff8000101223b8 <task_numa_find_cpu+0x3f0>
         :                      total_faults = p->total_numa_faults;
    0.00 :   ffff800010122488:       ldr     x11, [x6, #2264]
         :                      if (!total_faults)
    0.00 :   ffff80001012248c:       cbz     x11, ffff800010122398 <task_numa_find_cpu+0x3d0>
         :                      task_faults_idx():
         :                      return NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;
    0.00 :   ffff800010122490:       lsl     w0, w1, #1
         :                      score_nearby_nodes():
         :                      if (sched_numa_topology_type == NUMA_DIRECT)
    0.00 :   ffff800010122494:       adrp    x12, ffff800011a80000 <def_rt_bandwidth>
         :                      task_faults_idx():
         :                      return NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;
    0.00 :   ffff800010122498:       add     w3, w0, #0x1
         :                      task_faults():
         :                      return p->numa_faults[task_faults_idx(NUMA_MEM, nid, 0)] +
    0.00 :   ffff80001012249c:       ldr     x0, [x2, w0, sxtw #3]
    0.00 :   ffff8000101224a0:       ldr     x9, [x2, w3, sxtw #3]
    0.00 :   ffff8000101224a4:       add     x10, x9, x0
         :                      score_nearby_nodes():
         :                      if (sched_numa_topology_type == NUMA_DIRECT)
    0.00 :   ffff8000101224a8:       ldr     w0, [x12, #240]
    0.00 :   ffff8000101224ac:       cbnz    w0, ffff800010122650 <task_numa_find_cpu+0x688>
         :                      task_weight():
         :                      return 1000 * faults / total_faults;
    0.00 :   ffff8000101224b0:       lsl     x9, x10, #5
         :                      task_numa_compare():
         :                      task_weight(cur, env->dst_nid, dist);
    0.00 :   ffff8000101224b4:       ldr     w1, [x28, #20]
         :                      task_weight():
         :                      return 1000 * faults / total_faults;
    0.00 :   ffff8000101224b8:       sub     x9, x9, x10
    0.00 :   ffff8000101224bc:       add     x9, x10, x9, lsl #2
    0.00 :   ffff8000101224c0:       lsl     x9, x9, #3
    0.00 :   ffff8000101224c4:       udiv    x9, x9, x11
    0.00 :   ffff8000101224c8:       add     x9, x19, x9
         :                      if (!total_faults)
    0.00 :   ffff8000101224cc:       cbz     x11, ffff800010122398 <task_numa_find_cpu+0x3d0>
         :                      task_faults_idx():
         :                      return NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;
    0.00 :   ffff8000101224d0:       lsl     w0, w1, #1
         :                      score_nearby_nodes():
         :                      if (sched_numa_topology_type == NUMA_DIRECT)
    0.00 :   ffff8000101224d4:       ldr     w3, [x12, #240]
         :                      task_faults_idx():
         :                      return NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;
    0.00 :   ffff8000101224d8:       add     w12, w0, #0x1
         :                      task_faults():
         :                      return p->numa_faults[task_faults_idx(NUMA_MEM, nid, 0)] +
    0.00 :   ffff8000101224dc:       ldr     x10, [x2, w0, sxtw #3]
    0.00 :   ffff8000101224e0:       ldr     x0, [x2, w12, sxtw #3]
    0.00 :   ffff8000101224e4:       add     x10, x0, x10
         :                      score_nearby_nodes():
         :                      if (sched_numa_topology_type == NUMA_DIRECT)
    0.00 :   ffff8000101224e8:       cbz     w3, ffff800010122518 <task_numa_find_cpu+0x550>
    0.00 :   ffff8000101224ec:       ldr     w2, [x29, #136]
    0.00 :   ffff8000101224f0:       mov     x0, x6
    0.00 :   ffff8000101224f4:       stp     x11, x9, [x29, #104]
    0.00 :   ffff8000101224f8:       mov     w3, #0x1                        // #1
    0.00 :   ffff8000101224fc:       str     x10, [x29, #120]
    0.00 :   ffff800010122500:       stp     x8, x6, [x29, #144]
    0.00 :   ffff800010122504:       bl      ffff80001011c640 <score_nearby_nodes.part.115>
    0.00 :   ffff800010122508:       ldr     x10, [x29, #120]
    0.00 :   ffff80001012250c:       ldp     x11, x9, [x29, #104]
    0.00 :   ffff800010122510:       add     x10, x10, x0
    0.00 :   ffff800010122514:       ldp     x8, x6, [x29, #144]
         :                      task_weight():
         :                      return 1000 * faults / total_faults;
    0.00 :   ffff800010122518:       lsl     x0, x10, #5
    0.00 :   ffff80001012251c:       sub     x0, x0, x10
    0.00 :   ffff800010122520:       add     x0, x10, x0, lsl #2
    0.00 :   ffff800010122524:       lsl     x0, x0, #3
    0.00 :   ffff800010122528:       udiv    x11, x0, x11
    0.00 :   ffff80001012252c:       sub     x9, x9, x11
    0.00 :   ffff800010122530:       b       ffff800010122398 <task_numa_find_cpu+0x3d0>
         :                      if (!p->numa_faults)
    0.00 :   ffff800010122534:       ldr     x2, [x6, #2256]
    0.00 :   ffff800010122538:       mov     x9, x25
         :                      return 0;
    0.00 :   ffff80001012253c:       mov     x0, #0x0                        // #0
         :                      if (!p->numa_faults)
    0.00 :   ffff800010122540:       cbz     x2, ffff8000101225f0 <task_numa_find_cpu+0x628>
         :                      total_faults = p->total_numa_faults;
    0.00 :   ffff800010122544:       ldr     x11, [x6, #2264]
    0.00 :   ffff800010122548:       mov     x0, x11
         :                      if (!total_faults)
    0.00 :   ffff80001012254c:       cbz     x11, ffff8000101225f0 <task_numa_find_cpu+0x628>
         :                      task_faults_idx():
         :                      return NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;
    0.00 :   ffff800010122550:       lsl     w0, w1, #1
         :                      score_nearby_nodes():
         :                      if (sched_numa_topology_type == NUMA_DIRECT)
    0.00 :   ffff800010122554:       adrp    x12, ffff800011a80000 <def_rt_bandwidth>
         :                      task_faults_idx():
         :                      return NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;
    0.00 :   ffff800010122558:       add     w3, w0, #0x1
         :                      task_faults():
         :                      return p->numa_faults[task_faults_idx(NUMA_MEM, nid, 0)] +
    0.00 :   ffff80001012255c:       ldr     x0, [x2, w0, sxtw #3]
    0.00 :   ffff800010122560:       ldr     x9, [x2, w3, sxtw #3]
    0.00 :   ffff800010122564:       add     x9, x9, x0
         :                      score_nearby_nodes():
         :                      if (sched_numa_topology_type == NUMA_DIRECT)
    0.00 :   ffff800010122568:       ldr     w0, [x12, #240]
    0.00 :   ffff80001012256c:       cbnz    w0, ffff800010122700 <task_numa_find_cpu+0x738>
         :                      task_weight():
         :                      return 1000 * faults / total_faults;
    0.00 :   ffff800010122570:       lsl     x0, x9, #5
         :                      task_numa_compare():
         :                      task_weight(cur, env->dst_nid, dist);
    0.00 :   ffff800010122574:       ldr     w1, [x28, #20]
         :                      task_weight():
         :                      return 1000 * faults / total_faults;
    0.00 :   ffff800010122578:       sub     x0, x0, x9
    0.00 :   ffff80001012257c:       add     x9, x9, x0, lsl #2
    0.00 :   ffff800010122580:       lsl     x9, x9, #3
    0.00 :   ffff800010122584:       udiv    x9, x9, x11
         :                      task_numa_compare():
         :                      imp = taskimp + task_weight(cur, env->src_nid, dist) -
    0.00 :   ffff800010122588:       add     x9, x25, x9
    0.00 :   ffff80001012258c:       mov     x0, x11
         :                      task_weight():
         :                      if (!total_faults)
    0.00 :   ffff800010122590:       cbz     x11, ffff8000101225f0 <task_numa_find_cpu+0x628>
         :                      task_faults_idx():
         :                      return NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;
    0.00 :   ffff800010122594:       lsl     w0, w1, #1
         :                      score_nearby_nodes():
         :                      if (sched_numa_topology_type == NUMA_DIRECT)
    0.00 :   ffff800010122598:       ldr     w3, [x12, #240]
         :                      task_faults_idx():
         :                      return NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;
    0.00 :   ffff80001012259c:       add     w13, w0, #0x1
         :                      task_faults():
         :                      return p->numa_faults[task_faults_idx(NUMA_MEM, nid, 0)] +
    0.00 :   ffff8000101225a0:       ldr     x12, [x2, w0, sxtw #3]
    0.00 :   ffff8000101225a4:       ldr     x0, [x2, w13, sxtw #3]
    0.00 :   ffff8000101225a8:       add     x12, x0, x12
         :                      score_nearby_nodes():
         :                      if (sched_numa_topology_type == NUMA_DIRECT)
    0.00 :   ffff8000101225ac:       cbz     w3, ffff8000101225dc <task_numa_find_cpu+0x614>
    0.00 :   ffff8000101225b0:       ldr     w2, [x29, #136]
    0.00 :   ffff8000101225b4:       mov     x0, x6
    0.00 :   ffff8000101225b8:       stp     x11, x9, [x29, #96]
    0.00 :   ffff8000101225bc:       mov     w3, #0x1                        // #1
    0.00 :   ffff8000101225c0:       stp     x12, x10, [x29, #112]
    0.00 :   ffff8000101225c4:       stp     x8, x6, [x29, #144]
    0.00 :   ffff8000101225c8:       bl      ffff80001011c640 <score_nearby_nodes.part.115>
    0.00 :   ffff8000101225cc:       ldp     x12, x10, [x29, #112]
    0.00 :   ffff8000101225d0:       ldp     x11, x9, [x29, #96]
    0.00 :   ffff8000101225d4:       add     x12, x12, x0
    0.00 :   ffff8000101225d8:       ldp     x8, x6, [x29, #144]
         :                      task_weight():
         :                      return 1000 * faults / total_faults;
    0.00 :   ffff8000101225dc:       lsl     x0, x12, #5
    0.00 :   ffff8000101225e0:       sub     x0, x0, x12
    0.00 :   ffff8000101225e4:       add     x0, x12, x0, lsl #2
    0.00 :   ffff8000101225e8:       lsl     x0, x0, #3
    0.00 :   ffff8000101225ec:       udiv    x0, x0, x11
         :                      task_numa_compare():
         :                      imp = taskimp + task_weight(cur, env->src_nid, dist) -
    0.00 :   ffff8000101225f0:       sub     x9, x9, x0
         :                      if (cur_ng)
    0.00 :   ffff8000101225f4:       cbz     x10, ffff800010122398 <task_numa_find_cpu+0x3d0>
         :                      imp -= imp / 16;
    0.00 :   ffff8000101225f8:       cmp     x9, #0x0
    0.00 :   ffff8000101225fc:       add     x0, x9, #0xf
    0.00 :   ffff800010122600:       csel    x0, x0, x9, lt  // lt = tstop
    0.00 :   ffff800010122604:       ldr     w1, [x29, #140]
    0.00 :   ffff800010122608:       sub     x9, x9, x0, asr #4
    0.00 :   ffff80001012260c:       cmp     x19, x9
    0.00 :   ffff800010122610:       cset    w0, gt
    0.00 :   ffff800010122614:       and     w0, w1, w0
    0.00 :   ffff800010122618:       b       ffff8000101223a8 <task_numa_find_cpu+0x3e0>
         :                      __ll_sc_atomic_fetch_sub_release():
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff80001012261c:       mov     w2, #0x1                        // #1
    0.00 :   ffff800010122620:       add     x4, x0, #0x28
    0.00 :   ffff800010122624:       b       ffff80001012754c <sched_group_set_shares+0x664>
    0.00 :   ffff800010122628:       b       ffff8000101221c0 <task_numa_find_cpu+0x1f8>
         :                      __lse_atomic_fetch_add_relaxed():
         :                      ATOMIC_FETCH_OPS(add, ldadd)
    0.00 :   ffff80001012262c:       mov     w0, #0x1                        // #1
    0.00 :   ffff800010122630:       ldadd   w0, w0, [x2]
    0.00 :   ffff800010122634:       b       ffff80001012220c <task_numa_find_cpu+0x244>
    0.00 :   ffff800010122638:       str     x8, [x29, #152]
         :                      refcount_sub_and_test():
         :                      smp_acquire__after_ctrl_dep();
    0.00 :   ffff80001012263c:       dmb     ishld
         :                      put_task_struct():
         :                      __put_task_struct(t);
    0.00 :   ffff800010122640:       bl      ffff8000100e1e18 <__put_task_struct>
    0.00 :   ffff800010122644:       ldr     x8, [x29, #152]
         :                      task_numa_assign():
         :                      if (p)
    0.00 :   ffff800010122648:       cbnz    x8, ffff8000101221ec <task_numa_find_cpu+0x224>
    0.00 :   ffff80001012264c:       b       ffff800010122238 <task_numa_find_cpu+0x270>
         :                      score_nearby_nodes():
    0.00 :   ffff800010122650:       ldr     w2, [x29, #136]
    0.00 :   ffff800010122654:       mov     x0, x6
    0.00 :   ffff800010122658:       stp     x12, x11, [x29, #104]
    0.00 :   ffff80001012265c:       mov     w3, #0x1                        // #1
    0.00 :   ffff800010122660:       str     x10, [x29, #120]
    0.00 :   ffff800010122664:       stp     x8, x6, [x29, #144]
    0.00 :   ffff800010122668:       bl      ffff80001011c640 <score_nearby_nodes.part.115>
         :                      task_weight():
         :                      return 1000 * faults / total_faults;
    0.00 :   ffff80001012266c:       ldp     x11, x10, [x29, #112]
    0.00 :   ffff800010122670:       ldp     x8, x6, [x29, #144]
    0.00 :   ffff800010122674:       add     x0, x10, x0
         :                      task_numa_compare():
         :                      task_weight(cur, env->dst_nid, dist);
    0.00 :   ffff800010122678:       ldr     w1, [x28, #20]
         :                      task_weight():
         :                      return 1000 * faults / total_faults;
    0.00 :   ffff80001012267c:       lsl     x9, x0, #5
    0.00 :   ffff800010122680:       sub     x9, x9, x0
    0.00 :   ffff800010122684:       ldr     x2, [x6, #2256]
    0.00 :   ffff800010122688:       add     x9, x0, x9, lsl #2
    0.00 :   ffff80001012268c:       lsl     x9, x9, #3
    0.00 :   ffff800010122690:       udiv    x9, x9, x11
    0.00 :   ffff800010122694:       add     x9, x19, x9
         :                      if (!p->numa_faults)
    0.00 :   ffff800010122698:       cbz     x2, ffff800010122398 <task_numa_find_cpu+0x3d0>
    0.00 :   ffff80001012269c:       ldr     x11, [x6, #2264]
    0.00 :   ffff8000101226a0:       ldr     x12, [x29, #104]
    0.00 :   ffff8000101226a4:       b       ffff8000101224cc <task_numa_find_cpu+0x504>
         :                      score_nearby_nodes():
    0.00 :   ffff8000101226a8:       ldr     w2, [x29, #136]
    0.00 :   ffff8000101226ac:       mov     x0, x6
    0.00 :   ffff8000101226b0:       stp     x10, x11, [x29, #112]
    0.00 :   ffff8000101226b4:       mov     w3, #0x0                        // #0
    0.00 :   ffff8000101226b8:       stp     x8, x6, [x29, #144]
    0.00 :   ffff8000101226bc:       bl      ffff80001011c640 <score_nearby_nodes.part.115>
    0.00 :   ffff8000101226c0:       ldp     x10, x11, [x29, #112]
    0.00 :   ffff8000101226c4:       ldp     x8, x6, [x29, #144]
    0.00 :   ffff8000101226c8:       add     x10, x10, x0
    0.00 :   ffff8000101226cc:       b       ffff800010122320 <task_numa_find_cpu+0x358>
    0.00 :   ffff8000101226d0:       ldr     w2, [x29, #136]
    0.00 :   ffff8000101226d4:       mov     x0, x6
    0.00 :   ffff8000101226d8:       stp     x9, x10, [x29, #104]
    0.00 :   ffff8000101226dc:       mov     w3, #0x0                        // #0
    0.00 :   ffff8000101226e0:       str     x11, [x29, #120]
    0.00 :   ffff8000101226e4:       stp     x8, x6, [x29, #144]
    0.00 :   ffff8000101226e8:       bl      ffff80001011c640 <score_nearby_nodes.part.115>
    0.00 :   ffff8000101226ec:       ldp     x9, x10, [x29, #104]
    0.00 :   ffff8000101226f0:       ldr     x11, [x29, #120]
    0.00 :   ffff8000101226f4:       ldp     x8, x6, [x29, #144]
    0.00 :   ffff8000101226f8:       add     x10, x10, x0
    0.00 :   ffff8000101226fc:       b       ffff800010122380 <task_numa_find_cpu+0x3b8>
    0.00 :   ffff800010122700:       ldr     w2, [x29, #136]
    0.00 :   ffff800010122704:       mov     x0, x6
    0.00 :   ffff800010122708:       stp     x12, x11, [x29, #96]
    0.00 :   ffff80001012270c:       mov     w3, #0x1                        // #1
    0.00 :   ffff800010122710:       stp     x9, x10, [x29, #112]
    0.00 :   ffff800010122714:       stp     x8, x6, [x29, #144]
    0.00 :   ffff800010122718:       bl      ffff80001011c640 <score_nearby_nodes.part.115>
         :                      task_weight():
         :                      return 1000 * faults / total_faults;
    0.00 :   ffff80001012271c:       ldp     x11, x9, [x29, #104]
    0.00 :   ffff800010122720:       ldp     x8, x6, [x29, #144]
    0.00 :   ffff800010122724:       add     x0, x9, x0
         :                      task_numa_compare():
         :                      task_weight(cur, env->dst_nid, dist);
    0.00 :   ffff800010122728:       ldr     w1, [x28, #20]
         :                      task_weight():
         :                      return 1000 * faults / total_faults;
    0.00 :   ffff80001012272c:       lsl     x9, x0, #5
         :                      if (!p->numa_faults)
    0.00 :   ffff800010122730:       ldr     x10, [x29, #120]
         :                      return 1000 * faults / total_faults;
    0.00 :   ffff800010122734:       sub     x9, x9, x0
    0.00 :   ffff800010122738:       ldr     x2, [x6, #2256]
    0.00 :   ffff80001012273c:       add     x9, x0, x9, lsl #2
    0.00 :   ffff800010122740:       lsl     x9, x9, #3
    0.00 :   ffff800010122744:       udiv    x9, x9, x11
         :                      task_numa_compare():
         :                      imp = taskimp + task_weight(cur, env->src_nid, dist) -
    0.00 :   ffff800010122748:       add     x9, x25, x9
         :                      task_weight():
         :                      if (!p->numa_faults)
    0.00 :   ffff80001012274c:       cbz     x2, ffff800010122774 <task_numa_find_cpu+0x7ac>
    0.00 :   ffff800010122750:       ldr     x11, [x6, #2264]
    0.00 :   ffff800010122754:       ldr     x12, [x29, #96]
    0.00 :   ffff800010122758:       b       ffff80001012258c <task_numa_find_cpu+0x5c4>
    0.00 :   ffff80001012275c:       str     x8, [x29, #152]
         :                      refcount_add():
         :                      refcount_warn_saturate(r, REFCOUNT_ADD_UAF);
    0.00 :   ffff800010122760:       mov     w1, #0x2                        // #2
    0.00 :   ffff800010122764:       mov     x0, x2
    0.00 :   ffff800010122768:       bl      ffff80001048ba28 <refcount_warn_saturate>
    0.00 :   ffff80001012276c:       ldr     x8, [x29, #152]
    0.00 :   ffff800010122770:       b       ffff800010122238 <task_numa_find_cpu+0x270>
         :                      task_weight():
         :                      return 0;
    0.00 :   ffff800010122774:       mov     x0, #0x0                        // #0
    0.00 :   ffff800010122778:       b       ffff8000101225f0 <task_numa_find_cpu+0x628>
 Percent |	Source code & Disassembly of vmlinux for cycles (1 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001041dba0 <shm_get_policy>:
         :                      shm_get_policy():
         :
         :                      static struct mempolicy *shm_get_policy(struct vm_area_struct *vma,
         :                      unsigned long addr)
         :                      {
         :                      struct file *file = vma->vm_file;
         :                      struct shm_file_data *sfd = shm_file_data(file);
  100.00 :   ffff80001041dba0:       ldr     x2, [x0, #160]
         :                      struct mempolicy *pol = NULL;
         :
         :                      if (sfd->vm_ops->get_policy)
    0.00 :   ffff80001041dba4:       ldr     x2, [x2, #200]
    0.00 :   ffff80001041dba8:       ldr     x2, [x2, #24]
    0.00 :   ffff80001041dbac:       ldr     x2, [x2, #104]
    0.00 :   ffff80001041dbb0:       cbz     x2, ffff80001041dbc8 <shm_get_policy+0x28>
         :                      {
    0.00 :   ffff80001041dbb4:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff80001041dbb8:       mov     x29, sp
         :                      pol = sfd->vm_ops->get_policy(vma, addr);
    0.00 :   ffff80001041dbbc:       blr     x2
         :                      else if (vma->vm_policy)
         :                      pol = vma->vm_policy;
         :
         :                      return pol;
         :                      }
    0.00 :   ffff80001041dbc0:       ldp     x29, x30, [sp], #16
    0.00 :   ffff80001041dbc4:       ret
         :                      else if (vma->vm_policy)
    0.00 :   ffff80001041dbc8:       ldr     x0, [x0, #184]
         :                      }
    0.00 :   ffff80001041dbcc:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (1 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001020ac40 <__handle_mm_fault>:
         :                      __handle_mm_fault():
         :                      * The mmap_sem may have been released depending on flags and our
         :                      * return value.  See filemap_fault() and __lock_page_or_retry().
         :                      */
         :                      static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
         :                      unsigned long address, unsigned int flags)
         :                      {
    0.00 :   ffff80001020ac40:       stp     x29, x30, [sp, #-208]!
    0.00 :   ffff80001020ac44:       mov     x29, sp
    0.00 :   ffff80001020ac48:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001020ac4c:       mov     x19, x0
    0.00 :   ffff80001020ac50:       stp     x21, x22, [sp, #32]
    0.00 :   ffff80001020ac54:       adrp    x21, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff80001020ac58:       stp     x23, x24, [sp, #48]
    0.00 :   ffff80001020ac5c:       add     x0, x21, #0x8c8
    0.00 :   ffff80001020ac60:       str     x25, [sp, #64]
    0.00 :   ffff80001020ac64:       mov     w24, w2
         :                      struct vm_fault vmf = {
    0.00 :   ffff80001020ac68:       str     w2, [x29, #104]
         :                      {
    0.00 :   ffff80001020ac6c:       mov     x20, x1
    0.00 :   ffff80001020ac70:       ldr     x2, [x0]
    0.00 :   ffff80001020ac74:       str     x2, [x29, #200]
    0.00 :   ffff80001020ac78:       mov     x2, #0x0                        // #0
         :                      struct vm_fault vmf = {
    0.00 :   ffff80001020ac7c:       add     x0, x29, #0x6c
         :                      .vma = vma,
         :                      .address = address & PAGE_MASK,
         :                      .flags = flags,
         :                      .pgoff = linear_page_index(vma, address),
         :                      .gfp_mask = __get_fault_gfp_mask(vma),
    0.00 :   ffff80001020ac80:       ldr     x2, [x19, #160]
         :                      struct vm_fault vmf = {
    0.00 :   ffff80001020ac84:       str     x19, [x29, #96]
    0.00 :   ffff80001020ac88:       stp     xzr, xzr, [x0]
    0.00 :   ffff80001020ac8c:       add     x0, x29, #0x7c
    0.00 :   ffff80001020ac90:       stp     xzr, xzr, [x0]
    0.00 :   ffff80001020ac94:       add     x0, x29, #0x8c
    0.00 :   ffff80001020ac98:       stp     xzr, xzr, [x0]
    0.00 :   ffff80001020ac9c:       add     x0, x29, #0x9c
    0.00 :   ffff80001020aca0:       stp     xzr, xzr, [x0]
    0.00 :   ffff80001020aca4:       add     x0, x29, #0xac
    0.00 :   ffff80001020aca8:       stp     xzr, xzr, [x0]
    0.00 :   ffff80001020acac:       mov     w0, #0xcc0                      // #3264
    0.00 :   ffff80001020acb0:       stur    xzr, [x29, #188]
    0.00 :   ffff80001020acb4:       str     wzr, [x29, #196]
         :                      __get_fault_gfp_mask():
         :                      if (vm_file)
    0.00 :   ffff80001020acb8:       cbz     x2, ffff80001020acc8 <__handle_mm_fault+0x88>
         :                      return mapping_gfp_mask(vm_file->f_mapping) | __GFP_FS | __GFP_IO;
    0.00 :   ffff80001020acbc:       ldr     x0, [x2, #240]
    0.00 :   ffff80001020acc0:       ldr     w0, [x0, #24]
    0.00 :   ffff80001020acc4:       orr     w0, w0, #0xc0
         :                      is_vm_hugetlb_page():
         :
         :                      #include <linux/mm.h>
         :
         :                      static inline bool is_vm_hugetlb_page(struct vm_area_struct *vma)
         :                      {
         :                      return !!(vma->vm_flags & VM_HUGETLB);
    0.00 :   ffff80001020acc8:       ldr     x1, [x19, #80]
         :                      __handle_mm_fault():
         :                      struct vm_fault vmf = {
    0.00 :   ffff80001020accc:       str     w0, [x29, #108]
         :                      linear_page_index():
         :
         :                      static inline pgoff_t linear_page_index(struct vm_area_struct *vma,
         :                      unsigned long address)
         :                      {
         :                      pgoff_t pgoff;
         :                      if (unlikely(is_vm_hugetlb_page(vma)))
    0.00 :   ffff80001020acd0:       tbnz    w1, #22, ffff80001020afa0 <__handle_mm_fault+0x360>
         :                      return linear_hugepage_index(vma, address);
         :                      pgoff = (address - vma->vm_start) >> PAGE_SHIFT;
    0.00 :   ffff80001020acd4:       ldr     x0, [x19]
         :                      pgoff += vma->vm_pgoff;
    0.00 :   ffff80001020acd8:       ldr     x1, [x19, #152]
         :                      pgoff = (address - vma->vm_start) >> PAGE_SHIFT;
    0.00 :   ffff80001020acdc:       sub     x0, x20, x0
         :                      pgoff += vma->vm_pgoff;
    0.00 :   ffff80001020ace0:       add     x0, x1, x0, lsr #12
         :                      __handle_mm_fault():
         :                      };
         :                      unsigned int dirty = flags & FAULT_FLAG_WRITE;
         :                      struct mm_struct *mm = vma->vm_mm;
    0.00 :   ffff80001020ace4:       ldr     x25, [x19, #64]
         :                      .address = address & PAGE_MASK,
    0.00 :   ffff80001020ace8:       and     x1, x20, #0xfffffffffffff000
         :                      pgd_t *pgd;
         :                      p4d_t *p4d;
         :                      vm_fault_t ret;
         :
         :                      pgd = pgd_offset(mm, address);
    0.00 :   ffff80001020acec:       ubfx    x22, x20, #39, #9
         :                      struct vm_fault vmf = {
    0.00 :   ffff80001020acf0:       stp     x0, x1, [x29, #112]
         :                      pgd = pgd_offset(mm, address);
    0.00 :   ffff80001020acf4:       ldr     x23, [x25, #64]
         :                      p4d = p4d_alloc(mm, pgd, address);
         :                      if (!p4d)
    0.00 :   ffff80001020acf8:       adds    x1, x23, x22, lsl #3
    0.00 :   ffff80001020acfc:       b.eq    ffff80001020adcc <__handle_mm_fault+0x18c>  // b.none
         :                      return VM_FAULT_OOM;
         :
         :                      vmf.pud = pud_alloc(mm, p4d, address);
    0.00 :   ffff80001020ad00:       ldr     x0, [x23, x22, lsl #3]
    0.00 :   ffff80001020ad04:       cbz     x0, ffff80001020afb0 <__handle_mm_fault+0x370>
    0.00 :   ffff80001020ad08:       str     x26, [x29, #72]
    0.00 :   ffff80001020ad0c:       adrp    x26, ffff8000112ae000 <cpu_ops+0x248>
    0.00 :   ffff80001020ad10:       ubfx    x0, x20, #30, #9
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001020ad14:       ldr     x1, [x23, x22, lsl #3]
         :                      __handle_mm_fault():
    0.00 :   ffff80001020ad18:       ldr     x2, [x26, #1872]
    0.00 :   ffff80001020ad1c:       lsl     x0, x0, #3
         :                      pgd_page_paddr():
         :                      set_pgd(pgdp, __pgd(0));
         :                      }
         :
         :                      static inline phys_addr_t pgd_page_paddr(pgd_t pgd)
         :                      {
         :                      return __pgd_to_phys(pgd);
    0.00 :   ffff80001020ad20:       and     x1, x1, #0xfffffffff000
         :                      __handle_mm_fault():
    0.00 :   ffff80001020ad24:       sub     x0, x0, x2
    0.00 :   ffff80001020ad28:       add     x2, x0, x1
    0.00 :   ffff80001020ad2c:       str     x2, [x29, #136]
         :                      if (!vmf.pud)
    0.00 :   ffff80001020ad30:       cbz     x2, ffff80001020adc8 <__handle_mm_fault+0x188>
         :                      return VM_FAULT_OOM;
         :                      retry_pud:
         :                      if (pud_none(*vmf.pud) && __transparent_hugepage_enabled(vma)) {
    0.00 :   ffff80001020ad34:       ldr     x0, [x0, x1]
    0.00 :   ffff80001020ad38:       cbz     x0, ffff80001020ae00 <__handle_mm_fault+0x1c0>
         :                      return 0;
         :                      }
         :                      }
         :                      }
         :
         :                      vmf.pmd = pmd_alloc(mm, vmf.pud, address);
    0.00 :   ffff80001020ad3c:       ldr     x22, [x29, #136]
         :                      pmd_alloc():
         :                      #endif /* !__ARCH_HAS_5LEVEL_HACK */
         :
         :                      static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
         :                      {
         :                      return (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?
         :                      NULL: pmd_offset(pud, address);
    0.00 :   ffff80001020ad40:       ldr     x0, [x22]
    0.00 :   ffff80001020ad44:       cbz     x0, ffff80001020adb0 <__handle_mm_fault+0x170>
         :                      __read_once_size():
    0.00 :   ffff80001020ad48:       ldr     x0, [x22]
         :                      pmd_alloc():
    0.00 :   ffff80001020ad4c:       ubfx    x20, x20, #21, #9
    0.00 :   ffff80001020ad50:       ldr     x1, [x26, #1872]
         :                      pud_page_paddr():
         :                      return __pud_to_phys(pud);
    0.00 :   ffff80001020ad54:       and     x0, x0, #0xfffffffff000
         :                      pmd_alloc():
    0.00 :   ffff80001020ad58:       sub     x0, x0, x1
    0.00 :   ffff80001020ad5c:       add     x1, x0, x20, lsl #3
         :                      __handle_mm_fault():
    0.00 :   ffff80001020ad60:       str     x1, [x29, #128]
         :                      if (!vmf.pmd)
    0.00 :   ffff80001020ad64:       cbz     x1, ffff80001020adc8 <__handle_mm_fault+0x188>
         :
         :                      /* Huge pud page fault raced with pmd_alloc? */
         :                      if (pud_trans_unstable(vmf.pud))
         :                      goto retry_pud;
         :
         :                      if (pmd_none(*vmf.pmd) && __transparent_hugepage_enabled(vma)) {
    0.00 :   ffff80001020ad68:       ldr     x1, [x0, x20, lsl #3]
    0.00 :   ffff80001020ad6c:       cbz     x1, ffff80001020ae70 <__handle_mm_fault+0x230>
         :                      is_swap_pmd():
         :                      extern spinlock_t *__pud_trans_huge_lock(pud_t *pud,
         :                      struct vm_area_struct *vma);
         :
         :                      static inline int is_swap_pmd(pmd_t pmd)
         :                      {
         :                      return !pmd_none(pmd) && !pmd_present(pmd);
    0.00 :   ffff80001020ad70:       cbz     x1, ffff80001020ae78 <__handle_mm_fault+0x238>
    0.00 :   ffff80001020ad74:       and     x0, x1, #0x7ffffffffffffff
    0.00 :   ffff80001020ad78:       and     x0, x0, #0xfc00000000000001
    0.00 :   ffff80001020ad7c:       cbz     x0, ffff80001020af94 <__handle_mm_fault+0x354>
         :                      __handle_mm_fault():
         :                      !is_pmd_migration_entry(orig_pmd));
         :                      if (is_pmd_migration_entry(orig_pmd))
         :                      pmd_migration_entry_wait(mm, vmf.pmd);
         :                      return 0;
         :                      }
         :                      if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
    0.00 :   ffff80001020ad80:       tbz     w1, #1, ffff80001020ad88 <__handle_mm_fault+0x148>
    0.00 :   ffff80001020ad84:       tbz     x1, #57, ffff80001020ae78 <__handle_mm_fault+0x238>
         :                      if (pmd_protnone(orig_pmd) && vma_is_accessible(vma))
    0.00 :   ffff80001020ad88:       mov     x2, #0x400000000000000          // #288230376151711744
    0.00 :   ffff80001020ad8c:       cmp     x0, x2
    0.00 :   ffff80001020ad90:       b.eq    ffff80001020af74 <__handle_mm_fault+0x334>  // b.none
         :                      return do_huge_pmd_numa_page(&vmf, orig_pmd);
         :
         :                      if (dirty && !pmd_write(orig_pmd)) {
    0.00 :   ffff80001020ad94:       tbz     w24, #0, ffff80001020ad9c <__handle_mm_fault+0x15c>
    0.00 :   ffff80001020ad98:       tbz     x1, #51, ffff80001020b02c <__handle_mm_fault+0x3ec>
         :                      ret = wp_huge_pmd(&vmf, orig_pmd);
         :                      if (!(ret & VM_FAULT_FALLBACK))
         :                      return ret;
         :                      } else {
         :                      huge_pmd_set_accessed(&vmf, orig_pmd);
    0.00 :   ffff80001020ad9c:       add     x0, x29, #0x60
         :                      return 0;
    0.00 :   ffff80001020ada0:       mov     w22, #0x0                       // #0
         :                      huge_pmd_set_accessed(&vmf, orig_pmd);
    0.00 :   ffff80001020ada4:       bl      ffff80001025b030 <huge_pmd_set_accessed>
         :                      return 0;
    0.00 :   ffff80001020ada8:       ldr     x26, [x29, #72]
    0.00 :   ffff80001020adac:       b       ffff80001020add0 <__handle_mm_fault+0x190>
         :                      pmd_alloc():
         :                      return (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?
    0.00 :   ffff80001020adb0:       mov     x2, x20
    0.00 :   ffff80001020adb4:       mov     x1, x22
    0.00 :   ffff80001020adb8:       mov     x0, x25
    0.00 :   ffff80001020adbc:       bl      ffff80001020aae8 <__pmd_alloc>
    0.00 :   ffff80001020adc0:       cbz     w0, ffff80001020ad48 <__handle_mm_fault+0x108>
    0.00 :   ffff80001020adc4:       nop
    0.00 :   ffff80001020adc8:       ldr     x26, [x29, #72]
         :                      __handle_mm_fault():
         :                      return VM_FAULT_OOM;
    0.00 :   ffff80001020adcc:       mov     w22, #0x1                       // #1
         :                      }
         :                      }
         :                      }
         :
         :                      return handle_pte_fault(&vmf);
         :                      }
    0.00 :   ffff80001020add0:       add     x21, x21, #0x8c8
    0.00 :   ffff80001020add4:       mov     w0, w22
    0.00 :   ffff80001020add8:       ldr     x2, [x29, #200]
    0.00 :   ffff80001020addc:       ldr     x1, [x21]
    0.00 :   ffff80001020ade0:       eor     x1, x2, x1
    0.00 :   ffff80001020ade4:       cbnz    x1, ffff80001020b8d4 <__handle_mm_fault+0xc94>
    0.00 :   ffff80001020ade8:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001020adec:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff80001020adf0:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff80001020adf4:       ldr     x25, [sp, #64]
    0.00 :   ffff80001020adf8:       ldp     x29, x30, [sp], #208
    0.00 :   ffff80001020adfc:       ret
         :                      __transparent_hugepage_enabled():
         :                      if (vma->vm_flags & VM_NOHUGEPAGE)
    0.00 :   ffff80001020ae00:       ldr     x0, [x19, #80]
    0.00 :   ffff80001020ae04:       tbnz    w0, #30, ffff80001020ad3c <__handle_mm_fault+0xfc>
         :                      if (is_vma_temporary_stack(vma))
    0.00 :   ffff80001020ae08:       mov     x0, x19
    0.00 :   ffff80001020ae0c:       bl      ffff80001021b858 <is_vma_temporary_stack>
    0.00 :   ffff80001020ae10:       tst     w0, #0xff
    0.00 :   ffff80001020ae14:       b.ne    ffff80001020ad3c <__handle_mm_fault+0xfc>  // b.any
         :                      if (test_bit(MMF_DISABLE_THP, &vma->vm_mm->flags))
    0.00 :   ffff80001020ae18:       ldr     x0, [x19, #64]
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff80001020ae1c:       ldr     x0, [x0, #760]
         :                      __transparent_hugepage_enabled():
    0.00 :   ffff80001020ae20:       tst     w0, #0x1000000
    0.00 :   ffff80001020ae24:       b.ne    ffff80001020ad3c <__handle_mm_fault+0xfc>  // b.any
         :                      if (transparent_hugepage_flags & (1 << TRANSPARENT_HUGEPAGE_FLAG))
    0.00 :   ffff80001020ae28:       adrp    x0, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff80001020ae2c:       ldr     x0, [x0, #1440]
    0.00 :   ffff80001020ae30:       tbnz    w0, #0, ffff80001020ae40 <__handle_mm_fault+0x200>
         :                      if (transparent_hugepage_flags &
    0.00 :   ffff80001020ae34:       tbz     w0, #1, ffff80001020ad3c <__handle_mm_fault+0xfc>
         :                      return !!(vma->vm_flags & VM_HUGEPAGE);
    0.00 :   ffff80001020ae38:       ldr     x0, [x19, #80]
         :                      __handle_mm_fault():
         :                      if (pud_none(*vmf.pud) && __transparent_hugepage_enabled(vma)) {
    0.00 :   ffff80001020ae3c:       tbz     w0, #29, ffff80001020ad3c <__handle_mm_fault+0xfc>
         :                      create_huge_pud():
         :                      if (vma_is_anonymous(vmf->vma))
    0.00 :   ffff80001020ae40:       ldr     x0, [x29, #96]
    0.00 :   ffff80001020ae44:       ldr     x0, [x0, #144]
    0.00 :   ffff80001020ae48:       cbz     x0, ffff80001020ad3c <__handle_mm_fault+0xfc>
         :                      if (vmf->vma->vm_ops->huge_fault)
    0.00 :   ffff80001020ae4c:       ldr     x2, [x0, #40]
    0.00 :   ffff80001020ae50:       cbz     x2, ffff80001020ad3c <__handle_mm_fault+0xfc>
         :                      return vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PUD);
    0.00 :   ffff80001020ae54:       mov     w1, #0x2                        // #2
    0.00 :   ffff80001020ae58:       add     x0, x29, #0x60
    0.00 :   ffff80001020ae5c:       blr     x2
    0.00 :   ffff80001020ae60:       mov     w22, w0
         :                      __handle_mm_fault():
         :                      if (!(ret & VM_FAULT_FALLBACK))
    0.00 :   ffff80001020ae64:       tbnz    w22, #11, ffff80001020ad3c <__handle_mm_fault+0xfc>
    0.00 :   ffff80001020ae68:       ldr     x26, [x29, #72]
    0.00 :   ffff80001020ae6c:       b       ffff80001020add0 <__handle_mm_fault+0x190>
         :                      __transparent_hugepage_enabled():
         :                      if (vma->vm_flags & VM_NOHUGEPAGE)
    0.00 :   ffff80001020ae70:       ldr     x0, [x19, #80]
    0.00 :   ffff80001020ae74:       tbz     w0, #30, ffff80001020afc4 <__handle_mm_fault+0x384>
         :                      handle_pte_fault():
         :                      if (unlikely(pmd_none(*vmf->pmd))) {
    0.00 :   ffff80001020ae78:       ldr     x0, [x29, #128]
    0.00 :   ffff80001020ae7c:       ldr     x0, [x0]
    0.00 :   ffff80001020ae80:       cbz     x0, ffff80001020b064 <__handle_mm_fault+0x424>
         :                      pmd_devmap_trans_unstable():
         :                      return pmd_devmap(*pmd) || pmd_trans_unstable(pmd);
    0.00 :   ffff80001020ae84:       and     x19, x0, #0x200000000000000
    0.00 :   ffff80001020ae88:       tbnz    x0, #57, ffff80001020af94 <__handle_mm_fault+0x354>
         :                      pmd_none_or_trans_huge_or_clear_bad():
         :                      *
         :                      * pmd_none() is preseved for future condition checks on pmd migration
         :                      * entries and not confusing with this function name, although it is
         :                      * redundant with !pmd_present().
         :                      */
         :                      if (pmd_none(pmdval) || pmd_trans_huge(pmdval) ||
    0.00 :   ffff80001020ae8c:       tbz     w0, #1, ffff80001020af94 <__handle_mm_fault+0x354>
         :                      handle_pte_fault():
         :                      vmf->pte = pte_offset_map(vmf->pmd, vmf->address);
    0.00 :   ffff80001020ae90:       ldp     x1, x0, [x29, #120]
    0.00 :   ffff80001020ae94:       ldr     x2, [x26, #1872]
         :                      __read_once_size():
    0.00 :   ffff80001020ae98:       ldr     x0, [x0]
         :                      handle_pte_fault():
    0.00 :   ffff80001020ae9c:       ubfx    x1, x1, #12, #9
         :                      pmd_page_paddr():
         :                      return __pmd_to_phys(pmd);
    0.00 :   ffff80001020aea0:       and     x0, x0, #0xfffffffff000
         :                      handle_pte_fault():
    0.00 :   ffff80001020aea4:       sub     x0, x0, x2
    0.00 :   ffff80001020aea8:       add     x2, x0, x1, lsl #3
    0.00 :   ffff80001020aeac:       str     x2, [x29, #176]
         :                      vmf->orig_pte = *vmf->pte;
    0.00 :   ffff80001020aeb0:       ldr     x0, [x0, x1, lsl #3]
    0.00 :   ffff80001020aeb4:       str     x0, [x29, #144]
         :                      if (pte_none(vmf->orig_pte)) {
    0.00 :   ffff80001020aeb8:       ldr     x1, [x29, #144]
    0.00 :   ffff80001020aebc:       cbz     x1, ffff80001020b064 <__handle_mm_fault+0x424>
         :                      if (!vmf->pte) {
    0.00 :   ffff80001020aec0:       ldr     x0, [x29, #176]
    0.00 :   ffff80001020aec4:       cbz     x0, ffff80001020b068 <__handle_mm_fault+0x428>
         :                      if (!pte_present(vmf->orig_pte))
    0.00 :   ffff80001020aec8:       and     x0, x1, #0x7ffffffffffffff
    0.00 :   ffff80001020aecc:       and     x0, x0, #0xfc00000000000001
    0.00 :   ffff80001020aed0:       cbz     x0, ffff80001020b134 <__handle_mm_fault+0x4f4>
    0.00 :   ffff80001020aed4:       ldr     x3, [x29, #128]
    0.00 :   ffff80001020aed8:       adrp    x2, ffff8000112ae000 <cpu_ops+0x248>
         :                      if (pte_protnone(vmf->orig_pte) && vma_is_accessible(vmf->vma))
    0.00 :   ffff80001020aedc:       mov     x4, #0x400000000000000          // #288230376151711744
    0.00 :   ffff80001020aee0:       cmp     x0, x4
    0.00 :   ffff80001020aee4:       ldr     x2, [x2, #1880]
    0.00 :   ffff80001020aee8:       ldr     x0, [x3]
    0.00 :   ffff80001020aeec:       ubfx    x0, x0, #12, #36
    0.00 :   ffff80001020aef0:       add     x0, x2, x0, lsl #6
    0.00 :   ffff80001020aef4:       add     x0, x0, #0x28
    0.00 :   ffff80001020aef8:       b.ne    ffff80001020af0c <__handle_mm_fault+0x2cc>  // b.any
    0.00 :   ffff80001020aefc:       ldr     x22, [x29, #96]
         :                      vma_is_accessible():
         :                      return vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE);
    0.00 :   ffff80001020af00:       ldr     x2, [x22, #80]
         :                      handle_pte_fault():
         :                      if (pte_protnone(vmf->orig_pte) && vma_is_accessible(vmf->vma))
    0.00 :   ffff80001020af04:       tst     x2, #0x7
    0.00 :   ffff80001020af08:       b.ne    ffff80001020b36c <__handle_mm_fault+0x72c>  // b.any
         :                      vmf->ptl = pte_lockptr(vmf->vma->vm_mm, vmf->pmd);
    0.00 :   ffff80001020af0c:       str     x0, [x29, #184]
         :                      spin_lock():
         :                      raw_spin_lock_init(&(_lock)->rlock);            \
         :                      } while (0)
         :
         :                      static __always_inline void spin_lock(spinlock_t *lock)
         :                      {
         :                      raw_spin_lock(&lock->rlock);
    0.00 :   ffff80001020af10:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      handle_pte_fault():
         :                      if (unlikely(!pte_same(*vmf->pte, entry)))
    0.00 :   ffff80001020af14:       ldr     x2, [x29, #176]
         :                      entry = vmf->orig_pte;
    0.00 :   ffff80001020af18:       ldr     x3, [x29, #144]
         :                      if (unlikely(!pte_same(*vmf->pte, entry)))
    0.00 :   ffff80001020af1c:       ldr     x0, [x2]
    0.00 :   ffff80001020af20:       cmp     x3, x0
    0.00 :   ffff80001020af24:       b.ne    ffff80001020af60 <__handle_mm_fault+0x320>  // b.any
         :                      if (vmf->flags & FAULT_FLAG_WRITE) {
    0.00 :   ffff80001020af28:       ldr     w0, [x29, #104]
    0.00 :   ffff80001020af2c:       and     w4, w0, #0x1
    0.00 :   ffff80001020af30:       tbz     w0, #0, ffff80001020af44 <__handle_mm_fault+0x304>
         :                      clear_pte_bit():
         :                      pte_val(pte) &= ~pgprot_val(prot);
    0.00 :   ffff80001020af34:       and     x1, x3, #0xffffffffffffff7f
         :                      handle_pte_fault():
         :                      if (!pte_write(entry))
    0.00 :   ffff80001020af38:       and     x0, x3, #0x8000000000000
         :                      clear_pte_bit():
    0.00 :   ffff80001020af3c:       orr     x3, x1, #0x80000000000000
         :                      handle_pte_fault():
    0.00 :   ffff80001020af40:       cbz     x0, ffff80001020b548 <__handle_mm_fault+0x908>
         :                      if (ptep_set_access_flags(vmf->vma, vmf->address, vmf->pte, entry,
    0.00 :   ffff80001020af44:       ldr     x0, [x29, #96]
    0.00 :   ffff80001020af48:       orr     x3, x3, #0x400
    0.00 :   ffff80001020af4c:       ldr     x1, [x29, #120]
    0.00 :   ffff80001020af50:       bl      ffff8000100a1af0 <ptep_set_access_flags>
    0.00 :   ffff80001020af54:       cbnz    w0, ffff80001020af60 <__handle_mm_fault+0x320>
         :                      if (vmf->flags & FAULT_FLAG_WRITE)
    0.00 :   ffff80001020af58:       ldr     w0, [x29, #104]
    0.00 :   ffff80001020af5c:       tbnz    w0, #0, ffff80001020b640 <__handle_mm_fault+0xa00>
         :                      spin_unlock():
         :                      raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
         :                      } while (0)
         :
         :                      static __always_inline void spin_unlock(spinlock_t *lock)
         :                      {
         :                      raw_spin_unlock(&lock->rlock);
    0.00 :   ffff80001020af60:       ldr     x0, [x29, #184]
         :                      handle_pte_fault():
         :                      return 0;
    0.00 :   ffff80001020af64:       mov     w22, #0x0                       // #0
         :                      spin_unlock():
    0.00 :   ffff80001020af68:       bl      ffff800010cb2408 <_raw_spin_unlock>
    0.00 :   ffff80001020af6c:       ldr     x26, [x29, #72]
    0.00 :   ffff80001020af70:       b       ffff80001020add0 <__handle_mm_fault+0x190>
         :                      vma_is_accessible():
         :                      return vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE);
    0.00 :   ffff80001020af74:       ldr     x0, [x19, #80]
         :                      __handle_mm_fault():
         :                      if (pmd_protnone(orig_pmd) && vma_is_accessible(vma))
    0.00 :   ffff80001020af78:       tst     x0, #0x7
    0.00 :   ffff80001020af7c:       b.eq    ffff80001020ad94 <__handle_mm_fault+0x154>  // b.none
         :                      return do_huge_pmd_numa_page(&vmf, orig_pmd);
    0.00 :   ffff80001020af80:       add     x0, x29, #0x60
    0.00 :   ffff80001020af84:       bl      ffff80001025b368 <do_huge_pmd_numa_page>
    0.00 :   ffff80001020af88:       ldr     x26, [x29, #72]
    0.00 :   ffff80001020af8c:       mov     w22, w0
    0.00 :   ffff80001020af90:       b       ffff80001020add0 <__handle_mm_fault+0x190>
         :                      return 0;
    0.00 :   ffff80001020af94:       mov     w22, #0x0                       // #0
    0.00 :   ffff80001020af98:       ldr     x26, [x29, #72]
    0.00 :   ffff80001020af9c:       b       ffff80001020add0 <__handle_mm_fault+0x190>
         :                      linear_page_index():
         :                      return linear_hugepage_index(vma, address);
    0.00 :   ffff80001020afa0:       mov     x1, x20
    0.00 :   ffff80001020afa4:       mov     x0, x19
    0.00 :   ffff80001020afa8:       bl      ffff8000102388e8 <linear_hugepage_index>
    0.00 :   ffff80001020afac:       b       ffff80001020ace4 <__handle_mm_fault+0xa4>
         :                      __handle_mm_fault():
         :                      vmf.pud = pud_alloc(mm, p4d, address);
    0.00 :   ffff80001020afb0:       mov     x2, x20
    0.00 :   ffff80001020afb4:       mov     x0, x25
    0.00 :   ffff80001020afb8:       bl      ffff80001020a9a0 <__pud_alloc>
    0.00 :   ffff80001020afbc:       cbz     w0, ffff80001020ad08 <__handle_mm_fault+0xc8>
    0.00 :   ffff80001020afc0:       b       ffff80001020adcc <__handle_mm_fault+0x18c>
         :                      __transparent_hugepage_enabled():
         :                      if (is_vma_temporary_stack(vma))
    0.00 :   ffff80001020afc4:       mov     x0, x19
    0.00 :   ffff80001020afc8:       bl      ffff80001021b858 <is_vma_temporary_stack>
    0.00 :   ffff80001020afcc:       tst     w0, #0xff
    0.00 :   ffff80001020afd0:       b.ne    ffff80001020b054 <__handle_mm_fault+0x414>  // b.any
         :                      if (test_bit(MMF_DISABLE_THP, &vma->vm_mm->flags))
    0.00 :   ffff80001020afd4:       ldr     x0, [x19, #64]
         :                      test_bit():
    0.00 :   ffff80001020afd8:       ldr     x0, [x0, #760]
         :                      __transparent_hugepage_enabled():
    0.00 :   ffff80001020afdc:       tst     w0, #0x1000000
    0.00 :   ffff80001020afe0:       b.ne    ffff80001020b054 <__handle_mm_fault+0x414>  // b.any
         :                      if (transparent_hugepage_flags & (1 << TRANSPARENT_HUGEPAGE_FLAG))
    0.00 :   ffff80001020afe4:       adrp    x0, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff80001020afe8:       ldr     x0, [x0, #1440]
    0.00 :   ffff80001020afec:       tbnz    w0, #0, ffff80001020affc <__handle_mm_fault+0x3bc>
         :                      if (transparent_hugepage_flags &
    0.00 :   ffff80001020aff0:       tbz     w0, #1, ffff80001020b054 <__handle_mm_fault+0x414>
         :                      return !!(vma->vm_flags & VM_HUGEPAGE);
    0.00 :   ffff80001020aff4:       ldr     x0, [x19, #80]
         :                      __handle_mm_fault():
         :                      if (pmd_none(*vmf.pmd) && __transparent_hugepage_enabled(vma)) {
    0.00 :   ffff80001020aff8:       tbz     w0, #29, ffff80001020b054 <__handle_mm_fault+0x414>
         :                      create_huge_pmd():
         :                      if (vma_is_anonymous(vmf->vma))
    0.00 :   ffff80001020affc:       ldr     x0, [x29, #96]
    0.00 :   ffff80001020b000:       ldr     x0, [x0, #144]
    0.00 :   ffff80001020b004:       cbz     x0, ffff80001020b15c <__handle_mm_fault+0x51c>
         :                      if (vmf->vma->vm_ops->huge_fault)
    0.00 :   ffff80001020b008:       ldr     x2, [x0, #40]
    0.00 :   ffff80001020b00c:       cbz     x2, ffff80001020ae78 <__handle_mm_fault+0x238>
         :                      wp_huge_pmd():
         :                      return vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PMD);
    0.00 :   ffff80001020b010:       mov     w1, #0x1                        // #1
    0.00 :   ffff80001020b014:       add     x0, x29, #0x60
    0.00 :   ffff80001020b018:       blr     x2
    0.00 :   ffff80001020b01c:       mov     w22, w0
         :                      __handle_mm_fault():
         :                      if (!(ret & VM_FAULT_FALLBACK))
    0.00 :   ffff80001020b020:       tbnz    w22, #11, ffff80001020ae78 <__handle_mm_fault+0x238>
    0.00 :   ffff80001020b024:       ldr     x26, [x29, #72]
    0.00 :   ffff80001020b028:       b       ffff80001020add0 <__handle_mm_fault+0x190>
         :                      wp_huge_pmd():
         :                      if (vma_is_anonymous(vmf->vma))
    0.00 :   ffff80001020b02c:       ldr     x0, [x29, #96]
    0.00 :   ffff80001020b030:       ldr     x2, [x0, #144]
    0.00 :   ffff80001020b034:       cbz     x2, ffff80001020b148 <__handle_mm_fault+0x508>
         :                      if (vmf->vma->vm_ops->huge_fault)
    0.00 :   ffff80001020b038:       ldr     x2, [x2, #40]
    0.00 :   ffff80001020b03c:       cbnz    x2, ffff80001020b010 <__handle_mm_fault+0x3d0>
         :                      __split_huge_pmd(vmf->vma, vmf->pmd, vmf->address, false, NULL);
    0.00 :   ffff80001020b040:       ldp     x2, x1, [x29, #120]
    0.00 :   ffff80001020b044:       mov     x4, #0x0                        // #0
    0.00 :   ffff80001020b048:       mov     w3, #0x0                        // #0
    0.00 :   ffff80001020b04c:       bl      ffff80001025c228 <__split_huge_pmd>
    0.00 :   ffff80001020b050:       b       ffff80001020ae78 <__handle_mm_fault+0x238>
    0.00 :   ffff80001020b054:       ldr     x0, [x29, #128]
    0.00 :   ffff80001020b058:       ldr     x1, [x0]
         :                      is_swap_pmd():
         :                      return !pmd_none(pmd) && !pmd_present(pmd);
    0.00 :   ffff80001020b05c:       cbnz    x1, ffff80001020ad74 <__handle_mm_fault+0x134>
    0.00 :   ffff80001020b060:       b       ffff80001020ae78 <__handle_mm_fault+0x238>
         :                      handle_pte_fault():
         :                      vmf->pte = NULL;
    0.00 :   ffff80001020b064:       str     xzr, [x29, #176]
         :                      if (vma_is_anonymous(vmf->vma))
    0.00 :   ffff80001020b068:       ldr     x19, [x29, #96]
    0.00 :   ffff80001020b06c:       ldr     x0, [x19, #144]
    0.00 :   ffff80001020b070:       cbz     x0, ffff80001020b170 <__handle_mm_fault+0x530>
         :                      do_fault():
         :                      if (!vma->vm_ops->fault) {
    0.00 :   ffff80001020b074:       ldr     x1, [x0, #32]
    0.00 :   ffff80001020b078:       cbz     x1, ffff80001020b55c <__handle_mm_fault+0x91c>
         :                      } else if (!(vmf->flags & FAULT_FLAG_WRITE))
    0.00 :   ffff80001020b07c:       ldr     w1, [x29, #104]
    0.00 :   ffff80001020b080:       tbz     w1, #0, ffff80001020b678 <__handle_mm_fault+0xa38>
         :                      else if (!(vma->vm_flags & VM_SHARED))
    0.00 :   ffff80001020b084:       ldr     x0, [x19, #80]
    0.00 :   ffff80001020b088:       tbnz    w0, #3, ffff80001020b5cc <__handle_mm_fault+0x98c>
         :                      anon_vma_prepare():
         :                      int anon_vma_clone(struct vm_area_struct *, struct vm_area_struct *);
         :                      int anon_vma_fork(struct vm_area_struct *, struct vm_area_struct *);
         :
         :                      static inline int anon_vma_prepare(struct vm_area_struct *vma)
         :                      {
         :                      if (likely(vma->anon_vma))
    0.00 :   ffff80001020b08c:       ldr     x0, [x19, #136]
    0.00 :   ffff80001020b090:       cbz     x0, ffff80001020b7f8 <__handle_mm_fault+0xbb8>
         :                      numa_node_id():
         :
         :                      #ifndef numa_node_id
         :                      /* Returns the number of the current Node. */
         :                      static inline int numa_node_id(void)
         :                      {
         :                      return raw_cpu_read(numa_node);
    0.00 :   ffff80001020b094:       adrp    x1, ffff8000114d3000 <pmu_sb_events>
    0.00 :   ffff80001020b098:       add     x1, x1, #0xc58
    0.00 :   ffff80001020b09c:       bl      ffff800010206350 <__my_cpu_offset>
         :                      do_cow_fault():
         :                      vmf->cow_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, vmf->address);
    0.00 :   ffff80001020b0a0:       ldr     x3, [x29, #120]
    0.00 :   ffff80001020b0a4:       mov     w5, #0x0                        // #0
    0.00 :   ffff80001020b0a8:       ldr     w4, [x1, x0]
    0.00 :   ffff80001020b0ac:       mov     w0, #0xcca                      // #3274
    0.00 :   ffff80001020b0b0:       mov     x2, x19
    0.00 :   ffff80001020b0b4:       mov     w1, #0x0                        // #0
    0.00 :   ffff80001020b0b8:       movk    w0, #0x10, lsl #16
    0.00 :   ffff80001020b0bc:       bl      ffff800010243150 <alloc_pages_vma>
    0.00 :   ffff80001020b0c0:       str     x0, [x29, #152]
         :                      if (!vmf->cow_page)
    0.00 :   ffff80001020b0c4:       cbz     x0, ffff80001020b804 <__handle_mm_fault+0xbc4>
         :                      if (mem_cgroup_try_charge_delay(vmf->cow_page, vma->vm_mm, GFP_KERNEL,
    0.00 :   ffff80001020b0c8:       ldr     x1, [x19, #64]
    0.00 :   ffff80001020b0cc:       add     x23, x29, #0x60
    0.00 :   ffff80001020b0d0:       mov     w4, #0x0                        // #0
    0.00 :   ffff80001020b0d4:       add     x3, x23, #0x40
    0.00 :   ffff80001020b0d8:       mov     w2, #0xcc0                      // #3264
    0.00 :   ffff80001020b0dc:       bl      ffff80001026cb48 <mem_cgroup_try_charge_delay>
    0.00 :   ffff80001020b0e0:       cbnz    w0, ffff80001020b8a0 <__handle_mm_fault+0xc60>
         :                      ret = __do_fault(vmf);
    0.00 :   ffff80001020b0e4:       mov     x0, x23
    0.00 :   ffff80001020b0e8:       bl      ffff800010206d38 <__do_fault>
    0.00 :   ffff80001020b0ec:       mov     w22, w0
         :                      if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
    0.00 :   ffff80001020b0f0:       mov     w0, #0xd73                      // #3443
    0.00 :   ffff80001020b0f4:       tst     w22, w0
    0.00 :   ffff80001020b0f8:       b.ne    ffff80001020b9e0 <__handle_mm_fault+0xda0>  // b.any
         :                      if (ret & VM_FAULT_DONE_COW)
    0.00 :   ffff80001020b0fc:       tbz     w22, #12, ffff80001020b8f0 <__handle_mm_fault+0xcb0>
         :                      do_fault():
         :                      if (vmf->prealloc_pte) {
    0.00 :   ffff80001020b100:       ldr     x19, [x29, #192]
    0.00 :   ffff80001020b104:       cbz     x19, ffff80001020ae68 <__handle_mm_fault+0x228>
         :                      __ClearPageTable():
         :                      PAGE_TYPE_OPS(Kmemcg, kmemcg)
         :
         :                      /*
         :                      * Marks pages in use as page tables.
         :                      */
         :                      PAGE_TYPE_OPS(Table, table)
    0.00 :   ffff80001020b108:       ldr     w2, [x19, #48]
         :                      pgtable_pte_page_dtor():
         :
         :                      static inline void pgtable_pte_page_dtor(struct page *page)
         :                      {
         :                      ptlock_free(page);
         :                      __ClearPageTable(page);
         :                      dec_zone_page_state(page, NR_PAGETABLE);
    0.00 :   ffff80001020b10c:       mov     x0, x19
    0.00 :   ffff80001020b110:       mov     w1, #0x8                        // #8
         :                      __ClearPageTable():
    0.00 :   ffff80001020b114:       orr     w2, w2, #0x400
    0.00 :   ffff80001020b118:       str     w2, [x19, #48]
         :                      pgtable_pte_page_dtor():
    0.00 :   ffff80001020b11c:       bl      ffff8000101f41e0 <dec_zone_page_state>
         :                      pte_free():
         :                      * @pte_page: the `struct page` representing the page table
         :                      */
         :                      static inline void pte_free(struct mm_struct *mm, struct page *pte_page)
         :                      {
         :                      pgtable_pte_page_dtor(pte_page);
         :                      __free_page(pte_page);
    0.00 :   ffff80001020b120:       mov     w1, #0x0                        // #0
    0.00 :   ffff80001020b124:       mov     x0, x19
    0.00 :   ffff80001020b128:       bl      ffff8000102265b8 <__free_pages>
    0.00 :   ffff80001020b12c:       ldr     x26, [x29, #72]
    0.00 :   ffff80001020b130:       b       ffff80001020add0 <__handle_mm_fault+0x190>
         :                      handle_pte_fault():
         :                      return do_swap_page(vmf);
    0.00 :   ffff80001020b134:       add     x0, x29, #0x60
    0.00 :   ffff80001020b138:       bl      ffff800010209b18 <do_swap_page>
    0.00 :   ffff80001020b13c:       ldr     x26, [x29, #72]
    0.00 :   ffff80001020b140:       mov     w22, w0
    0.00 :   ffff80001020b144:       b       ffff80001020add0 <__handle_mm_fault+0x190>
         :                      wp_huge_pmd():
         :                      return do_huge_pmd_wp_page(vmf, orig_pmd);
    0.00 :   ffff80001020b148:       add     x0, x29, #0x60
    0.00 :   ffff80001020b14c:       bl      ffff80001025ca60 <do_huge_pmd_wp_page>
    0.00 :   ffff80001020b150:       mov     w22, w0
         :                      __handle_mm_fault():
         :                      if (!(ret & VM_FAULT_FALLBACK))
    0.00 :   ffff80001020b154:       tbnz    w22, #11, ffff80001020ae78 <__handle_mm_fault+0x238>
    0.00 :   ffff80001020b158:       b       ffff80001020b024 <__handle_mm_fault+0x3e4>
         :                      create_huge_pmd():
         :                      return do_huge_pmd_anonymous_page(vmf);
    0.00 :   ffff80001020b15c:       add     x0, x29, #0x60
    0.00 :   ffff80001020b160:       bl      ffff80001025a528 <do_huge_pmd_anonymous_page>
    0.00 :   ffff80001020b164:       mov     w22, w0
         :                      __handle_mm_fault():
         :                      if (!(ret & VM_FAULT_FALLBACK))
    0.00 :   ffff80001020b168:       tbnz    w22, #11, ffff80001020ae78 <__handle_mm_fault+0x238>
    0.00 :   ffff80001020b16c:       b       ffff80001020b024 <__handle_mm_fault+0x3e4>
         :                      do_anonymous_page():
         :                      if (vma->vm_flags & VM_SHARED)
    0.00 :   ffff80001020b170:       ldr     x0, [x19, #80]
         :                      return VM_FAULT_SIGBUS;
    0.00 :   ffff80001020b174:       mov     w22, #0x2                       // #2
         :                      if (vma->vm_flags & VM_SHARED)
    0.00 :   ffff80001020b178:       tbnz    w0, #3, ffff80001020ae68 <__handle_mm_fault+0x228>
         :                      if (pte_alloc(vma->vm_mm, vmf->pmd))
    0.00 :   ffff80001020b17c:       ldr     x1, [x29, #128]
    0.00 :   ffff80001020b180:       ldr     x0, [x1]
    0.00 :   ffff80001020b184:       cbz     x0, ffff80001020ba54 <__handle_mm_fault+0xe14>
         :                      return 0;
    0.00 :   ffff80001020b188:       mov     w22, #0x0                       // #0
         :                      pmd_none_or_trans_huge_or_clear_bad():
    0.00 :   ffff80001020b18c:       cbz     x0, ffff80001020ae68 <__handle_mm_fault+0x228>
    0.00 :   ffff80001020b190:       tbz     w0, #1, ffff80001020ae68 <__handle_mm_fault+0x228>
         :                      do_anonymous_page():
         :                      if (!(vmf->flags & FAULT_FLAG_WRITE) &&
    0.00 :   ffff80001020b194:       ldr     w0, [x29, #104]
    0.00 :   ffff80001020b198:       tbz     w0, #0, ffff80001020b80c <__handle_mm_fault+0xbcc>
         :                      anon_vma_prepare():
    0.00 :   ffff80001020b19c:       ldr     x0, [x19, #136]
    0.00 :   ffff80001020b1a0:       cbz     x0, ffff80001020b888 <__handle_mm_fault+0xc48>
         :                      numa_node_id():
    0.00 :   ffff80001020b1a4:       adrp    x1, ffff8000114d3000 <pmu_sb_events>
    0.00 :   ffff80001020b1a8:       add     x1, x1, #0xc58
    0.00 :   ffff80001020b1ac:       bl      ffff800010206350 <__my_cpu_offset>
         :                      do_anonymous_page():
         :                      page = alloc_zeroed_user_highpage_movable(vma, vmf->address);
    0.00 :   ffff80001020b1b0:       ldr     x22, [x29, #120]
         :                      __alloc_zeroed_user_highpage():
         :                      static inline struct page *
         :                      __alloc_zeroed_user_highpage(gfp_t movableflags,
         :                      struct vm_area_struct *vma,
         :                      unsigned long vaddr)
         :                      {
         :                      struct page *page = alloc_page_vma(GFP_HIGHUSER | movableflags,
    0.00 :   ffff80001020b1b4:       ldr     w4, [x1, x0]
    0.00 :   ffff80001020b1b8:       mov     w0, #0xcca                      // #3274
    0.00 :   ffff80001020b1bc:       mov     w5, #0x0                        // #0
    0.00 :   ffff80001020b1c0:       mov     x2, x19
    0.00 :   ffff80001020b1c4:       mov     w1, #0x0                        // #0
    0.00 :   ffff80001020b1c8:       movk    w0, #0x10, lsl #16
    0.00 :   ffff80001020b1cc:       mov     x3, x22
    0.00 :   ffff80001020b1d0:       bl      ffff800010243150 <alloc_pages_vma>
    0.00 :   ffff80001020b1d4:       mov     x23, x0
         :                      vma, vaddr);
         :
         :                      if (page)
    0.00 :   ffff80001020b1d8:       cbz     x0, ffff80001020b894 <__handle_mm_fault+0xc54>
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80001020b1dc:       mrs     x20, sp_el0
         :                      __read_once_size():
    0.00 :   ffff80001020b1e0:       ldr     w0, [x20, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff80001020b1e4:       add     w0, w0, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001020b1e8:       str     w0, [x20, #16]
         :                      pagefault_disabled_inc():
         :                      }
         :                      #endif
         :
         :                      static __always_inline void pagefault_disabled_inc(void)
         :                      {
         :                      current->pagefault_disabled++;
    0.00 :   ffff80001020b1ec:       ldr     w0, [x20, #2448]
    0.00 :   ffff80001020b1f0:       add     w0, w0, #0x1
    0.00 :   ffff80001020b1f4:       str     w0, [x20, #2448]
         :                      lowmem_page_address():
         :                      return page_to_virt(page);
    0.00 :   ffff80001020b1f8:       mov     x0, #0x200000                   // #2097152
    0.00 :   ffff80001020b1fc:       mov     x2, #0xffff000000000000         // #-281474976710656
    0.00 :   ffff80001020b200:       movk    x0, #0x200, lsl #32
    0.00 :   ffff80001020b204:       add     x0, x23, x0
         :                      clear_user_highpage():
         :                      clear_user_page(addr, vaddr, page);
    0.00 :   ffff80001020b208:       mov     x1, x22
         :                      lowmem_page_address():
    0.00 :   ffff80001020b20c:       lsr     x0, x0, #6
         :                      clear_user_highpage():
    0.00 :   ffff80001020b210:       add     x0, x2, x0, lsl #12
    0.00 :   ffff80001020b214:       bl      ffff8000100a25f0 <__cpu_clear_user_page>
         :                      pagefault_disabled_dec():
         :                      }
         :
         :                      static __always_inline void pagefault_disabled_dec(void)
         :                      {
         :                      current->pagefault_disabled--;
    0.00 :   ffff80001020b218:       ldr     w0, [x20, #2448]
    0.00 :   ffff80001020b21c:       sub     w0, w0, #0x1
    0.00 :   ffff80001020b220:       str     w0, [x20, #2448]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001020b224:       ldr     x0, [x20, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff80001020b228:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001020b22c:       str     w0, [x20, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff80001020b230:       cbz     x0, ffff80001020b364 <__handle_mm_fault+0x724>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001020b234:       ldr     x0, [x20, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff80001020b238:       cbz     x0, ffff80001020b364 <__handle_mm_fault+0x724>
         :                      do_anonymous_page():
         :                      if (mem_cgroup_try_charge_delay(page, vma->vm_mm, GFP_KERNEL, &memcg,
    0.00 :   ffff80001020b23c:       ldr     x1, [x19, #64]
    0.00 :   ffff80001020b240:       mov     w4, #0x0                        // #0
    0.00 :   ffff80001020b244:       add     x3, x29, #0x58
    0.00 :   ffff80001020b248:       mov     w2, #0xcc0                      // #3264
    0.00 :   ffff80001020b24c:       mov     x0, x23
    0.00 :   ffff80001020b250:       bl      ffff80001026cb48 <mem_cgroup_try_charge_delay>
    0.00 :   ffff80001020b254:       cbnz    w0, ffff80001020ba18 <__handle_mm_fault+0xdd8>
         :                      __SetPageUptodate():
         :                      smp_wmb();
    0.00 :   ffff80001020b258:       dmb     ishst
         :                      __set_bit():
         :                      *p  |= mask;
    0.00 :   ffff80001020b25c:       ldr     x0, [x23]
         :                      do_anonymous_page():
         :                      entry = mk_pte(page, vma->vm_page_prot);
    0.00 :   ffff80001020b260:       adrp    x1, ffff8000112ae000 <cpu_ops+0x248>
         :                      __set_bit():
    0.00 :   ffff80001020b264:       orr     x0, x0, #0x4
    0.00 :   ffff80001020b268:       str     x0, [x23]
         :                      do_anonymous_page():
    0.00 :   ffff80001020b26c:       ldr     x1, [x1, #1880]
         :                      if (vma->vm_flags & VM_WRITE)
    0.00 :   ffff80001020b270:       ldp     x2, x0, [x19, #72]
         :                      entry = mk_pte(page, vma->vm_page_prot);
    0.00 :   ffff80001020b274:       sub     x20, x23, x1
    0.00 :   ffff80001020b278:       asr     x20, x20, #6
    0.00 :   ffff80001020b27c:       orr     x20, x2, x20, lsl #12
         :                      if (vma->vm_flags & VM_WRITE)
    0.00 :   ffff80001020b280:       tbz     w0, #1, ffff80001020b2a0 <__handle_mm_fault+0x660>
         :                      set_pte_bit():
         :                      pte_val(pte) |= pgprot_val(prot);
    0.00 :   ffff80001020b284:       and     x0, x20, #0xffffffffffffff7f
    0.00 :   ffff80001020b288:       orr     x2, x20, #0x80000000000000
    0.00 :   ffff80001020b28c:       tst     x20, #0x8000000000000
    0.00 :   ffff80001020b290:       orr     x20, x0, #0x80000000000000
    0.00 :   ffff80001020b294:       csel    x20, x20, x2, ne  // ne = any
         :                      clear_pte_bit():
         :                      pte_val(pte) &= ~pgprot_val(prot);
    0.00 :   ffff80001020b298:       and     x20, x20, #0xffffffffffffff7f
    0.00 :   ffff80001020b29c:       orr     x20, x20, #0x8000000000000
         :                      do_anonymous_page():
         :                      vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,
    0.00 :   ffff80001020b2a0:       ldp     x24, x2, [x29, #120]
    0.00 :   ffff80001020b2a4:       ldr     x3, [x26, #1872]
         :                      pte_lockptr():
         :                      return ptlock_ptr(pmd_page(*pmd));
    0.00 :   ffff80001020b2a8:       ldr     x0, [x2]
         :                      do_anonymous_page():
    0.00 :   ffff80001020b2ac:       ubfx    x24, x24, #12, #9
         :                      __read_once_size():
    0.00 :   ffff80001020b2b0:       ldr     x22, [x2]
         :                      do_anonymous_page():
    0.00 :   ffff80001020b2b4:       lsl     x24, x24, #3
         :                      pte_lockptr():
    0.00 :   ffff80001020b2b8:       ubfx    x0, x0, #12, #36
         :                      pmd_page_paddr():
         :                      return __pmd_to_phys(pmd);
    0.00 :   ffff80001020b2bc:       and     x22, x22, #0xfffffffff000
         :                      do_anonymous_page():
    0.00 :   ffff80001020b2c0:       sub     x22, x22, x3
         :                      pte_lockptr():
    0.00 :   ffff80001020b2c4:       add     x0, x1, x0, lsl #6
         :                      do_anonymous_page():
    0.00 :   ffff80001020b2c8:       add     x25, x24, x22
         :                      ptlock_ptr():
         :                      return &page->ptl;
    0.00 :   ffff80001020b2cc:       add     x0, x0, #0x28
         :                      do_anonymous_page():
    0.00 :   ffff80001020b2d0:       str     x0, [x29, #184]
         :                      spin_lock():
         :                      raw_spin_lock(&lock->rlock);
    0.00 :   ffff80001020b2d4:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      do_anonymous_page():
    0.00 :   ffff80001020b2d8:       str     x25, [x29, #176]
         :                      if (!pte_none(*vmf->pte))
    0.00 :   ffff80001020b2dc:       ldr     x0, [x24, x22]
    0.00 :   ffff80001020b2e0:       cbnz    x0, ffff80001020ba2c <__handle_mm_fault+0xdec>
         :                      ret = check_stable_address_space(vma->vm_mm);
    0.00 :   ffff80001020b2e4:       ldr     x0, [x19, #64]
         :                      test_bit():
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff80001020b2e8:       ldr     x1, [x0, #760]
         :                      check_stable_address_space():
         :                      *
         :                      * Return 0 when the PF is safe VM_FAULT_SIGBUS otherwise.
         :                      */
         :                      static inline vm_fault_t check_stable_address_space(struct mm_struct *mm)
         :                      {
         :                      if (unlikely(test_bit(MMF_UNSTABLE, &mm->flags)))
    0.00 :   ffff80001020b2ec:       tst     w1, #0x400000
    0.00 :   ffff80001020b2f0:       b.ne    ffff80001020bb08 <__handle_mm_fault+0xec8>  // b.any
         :                      get_current():
    0.00 :   ffff80001020b2f4:       mrs     x1, sp_el0
         :                      add_mm_counter_fast():
         :                      if (likely(task->mm == mm))
    0.00 :   ffff80001020b2f8:       ldr     x2, [x1, #952]
    0.00 :   ffff80001020b2fc:       cmp     x0, x2
    0.00 :   ffff80001020b300:       b.ne    ffff80001020baec <__handle_mm_fault+0xeac>  // b.any
         :                      task->rss_stat.count[member] += val;
    0.00 :   ffff80001020b304:       ldr     w0, [x1, #1016]
    0.00 :   ffff80001020b308:       add     w0, w0, #0x1
    0.00 :   ffff80001020b30c:       str     w0, [x1, #1016]
         :                      do_anonymous_page():
         :                      page_add_new_anon_rmap(page, vma, vmf->address, false);
    0.00 :   ffff80001020b310:       ldr     x2, [x29, #120]
    0.00 :   ffff80001020b314:       mov     w3, #0x0                        // #0
    0.00 :   ffff80001020b318:       mov     x1, x19
    0.00 :   ffff80001020b31c:       mov     x0, x23
    0.00 :   ffff80001020b320:       bl      ffff80001021a658 <page_add_new_anon_rmap>
         :                      mem_cgroup_commit_charge(page, memcg, false, false);
    0.00 :   ffff80001020b324:       ldr     x1, [x29, #88]
    0.00 :   ffff80001020b328:       mov     w3, #0x0                        // #0
    0.00 :   ffff80001020b32c:       mov     w2, #0x0                        // #0
    0.00 :   ffff80001020b330:       mov     x0, x23
    0.00 :   ffff80001020b334:       bl      ffff80001026d550 <mem_cgroup_commit_charge>
         :                      lru_cache_add_active_or_unevictable(page, vma);
    0.00 :   ffff80001020b338:       mov     x1, x19
    0.00 :   ffff80001020b33c:       mov     x0, x23
    0.00 :   ffff80001020b340:       bl      ffff8000101e0328 <lru_cache_add_active_or_unevictable>
         :                      set_pte_at(vma->vm_mm, vmf->address, vmf->pte, entry);
    0.00 :   ffff80001020b344:       ldr     x0, [x29, #176]
    0.00 :   ffff80001020b348:       mov     x1, x20
    0.00 :   ffff80001020b34c:       mov     w22, #0x0                       // #0
    0.00 :   ffff80001020b350:       bl      ffff80001020e5a0 <set_pte_at.isra.101>
         :                      spin_unlock():
         :                      raw_spin_unlock(&lock->rlock);
    0.00 :   ffff80001020b354:       ldr     x0, [x29, #184]
    0.00 :   ffff80001020b358:       bl      ffff800010cb2408 <_raw_spin_unlock>
    0.00 :   ffff80001020b35c:       ldr     x26, [x29, #72]
    0.00 :   ffff80001020b360:       b       ffff80001020add0 <__handle_mm_fault+0x190>
         :                      __kunmap_atomic():
         :                      preempt_enable();
    0.00 :   ffff80001020b364:       bl      ffff800010cad640 <preempt_schedule>
    0.00 :   ffff80001020b368:       b       ffff80001020b23c <__handle_mm_fault+0x5fc>
         :                      do_numa_page():
         :                      bool was_writable = pte_savedwrite(vmf->orig_pte);
    0.00 :   ffff80001020b36c:       and     x20, x1, #0x8000000000000
         :                      vmf->ptl = pte_lockptr(vma->vm_mm, vmf->pmd);
    0.00 :   ffff80001020b370:       str     x0, [x29, #184]
         :                      spin_lock():
         :                      raw_spin_lock(&lock->rlock);
    0.00 :   ffff80001020b374:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      do_numa_page():
         :                      if (unlikely(!pte_same(*vmf->pte, vmf->orig_pte))) {
    0.00 :   ffff80001020b378:       ldr     x0, [x29, #176]
    0.00 :   ffff80001020b37c:       ldr     x1, [x29, #144]
    0.00 :   ffff80001020b380:       ldr     x2, [x0]
    0.00 :   ffff80001020b384:       cmp     x2, x1
    0.00 :   ffff80001020b388:       b.ne    ffff80001020af60 <__handle_mm_fault+0x320>  // b.any
         :                      __xchg_case_64():
         :                      }
         :
         :                      __XCHG_CASE(w, b,     ,  8,        ,    ,  ,  ,  ,         )
         :                      __XCHG_CASE(w, h,     , 16,        ,    ,  ,  ,  ,         )
         :                      __XCHG_CASE(w,  ,     , 32,        ,    ,  ,  ,  ,         )
         :                      __XCHG_CASE( ,  ,     , 64,        ,    ,  ,  ,  ,         )
    0.00 :   ffff80001020b38c:       prfm    pstl1strm, [x0]
    0.00 :   ffff80001020b390:       ldxr    x1, [x0]
    0.00 :   ffff80001020b394:       stxr    w2, x19, [x0]
    0.00 :   ffff80001020b398:       cbnz    w2, ffff80001020b390 <__handle_mm_fault+0x750>
         :                      pte_modify():
         :                      static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
         :                      {
         :                      const pteval_t mask = PTE_USER | PTE_PXN | PTE_UXN | PTE_RDONLY |
         :                      PTE_PROT_NONE | PTE_VALID | PTE_WRITE;
         :                      /* preserve the hardware dirty information */
         :                      if (pte_hw_dirty(pte))
    0.00 :   ffff80001020b39c:       and     x3, x1, #0xfffffffffff80
    0.00 :   ffff80001020b3a0:       mov     x4, #0x8000000000000            // #2251799813685248
    0.00 :   ffff80001020b3a4:       and     x3, x3, #0xfff80000000000ff
         :                      __xchg_case_64():
    0.00 :   ffff80001020b3a8:       mov     x0, x1
         :                      pte_modify():
    0.00 :   ffff80001020b3ac:       cmp     x3, x4
    0.00 :   ffff80001020b3b0:       ldr     x2, [x22, #72]
    0.00 :   ffff80001020b3b4:       b.eq    ffff80001020b7e0 <__handle_mm_fault+0xba0>  // b.none
         :                      pte = pte_mkdirty(pte);
         :                      pte_val(pte) = (pte_val(pte) & ~mask) | (pgprot_val(newprot) & mask);
    0.00 :   ffff80001020b3b8:       mov     x3, #0xffffffffffffff3e         // #-194
    0.00 :   ffff80001020b3bc:       mov     x1, #0xc1                       // #193
    0.00 :   ffff80001020b3c0:       movk    x3, #0xfb97, lsl #48
    0.00 :   ffff80001020b3c4:       movk    x1, #0x468, lsl #48
    0.00 :   ffff80001020b3c8:       and     x0, x0, x3
    0.00 :   ffff80001020b3cc:       and     x1, x2, x1
    0.00 :   ffff80001020b3d0:       orr     x0, x0, x1
         :                      set_pte_bit():
         :                      pte_val(pte) |= pgprot_val(prot);
    0.00 :   ffff80001020b3d4:       mov     x1, #0x400                      // #1024
    0.00 :   ffff80001020b3d8:       and     x19, x0, #0xffffffffffffff7f
    0.00 :   ffff80001020b3dc:       cmp     x20, #0x0
    0.00 :   ffff80001020b3e0:       orr     x0, x0, #0x400
    0.00 :   ffff80001020b3e4:       movk    x1, #0x8, lsl #48
    0.00 :   ffff80001020b3e8:       orr     x19, x19, x1
         :                      do_numa_page():
         :                      ptep_modify_prot_commit(vma, vmf->address, vmf->pte, old_pte, pte);
    0.00 :   ffff80001020b3ec:       ldr     x20, [x29, #176]
         :                      set_pte_bit():
    0.00 :   ffff80001020b3f0:       csel    x19, x19, x0, ne  // ne = any
         :                      set_pte_at():
         :                      if (pte_present(pte) && pte_user_exec(pte) && !pte_special(pte))
    0.00 :   ffff80001020b3f4:       and     x0, x19, #0x7ffffffffffffff
    0.00 :   ffff80001020b3f8:       and     x0, x0, #0xfc00000000000001
    0.00 :   ffff80001020b3fc:       cbz     x0, ffff80001020b414 <__handle_mm_fault+0x7d4>
    0.00 :   ffff80001020b400:       mov     x0, #0x140000000000000          // #90071992547409920
    0.00 :   ffff80001020b404:       tst     x19, x0
    0.00 :   ffff80001020b408:       b.ne    ffff80001020b414 <__handle_mm_fault+0x7d4>  // b.any
         :                      __sync_icache_dcache(pte);
    0.00 :   ffff80001020b40c:       mov     x0, x19
    0.00 :   ffff80001020b410:       bl      ffff8000100a2690 <__sync_icache_dcache>
         :                      __write_once_size():
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff80001020b414:       str     x19, [x20]
         :                      set_pte():
         :                      if (pte_valid_not_user(pte)) {
    0.00 :   ffff80001020b418:       mov     x0, #0x41                       // #65
    0.00 :   ffff80001020b41c:       and     x0, x19, x0
    0.00 :   ffff80001020b420:       cmp     x0, #0x1
    0.00 :   ffff80001020b424:       b.ne    ffff80001020b430 <__handle_mm_fault+0x7f0>  // b.any
         :                      dsb(ishst);
    0.00 :   ffff80001020b428:       dsb     ishst
         :                      isb();
    0.00 :   ffff80001020b42c:       isb
         :                      do_numa_page():
         :                      page = vm_normal_page(vma, vmf->address, pte);
    0.00 :   ffff80001020b430:       ldr     x1, [x29, #120]
    0.00 :   ffff80001020b434:       mov     x2, x19
    0.00 :   ffff80001020b438:       mov     x0, x22
    0.00 :   ffff80001020b43c:       bl      ffff800010208180 <vm_normal_page>
    0.00 :   ffff80001020b440:       mov     x20, x0
         :                      if (!page) {
    0.00 :   ffff80001020b444:       cbz     x0, ffff80001020af60 <__handle_mm_fault+0x320>
         :                      test_bit():
    0.00 :   ffff80001020b448:       ldr     x0, [x0]
         :                      PageCompound():
         :                      return test_bit(PG_head, &page->flags) || PageTail(page);
    0.00 :   ffff80001020b44c:       tst     w0, #0x10000
    0.00 :   ffff80001020b450:       b.ne    ffff80001020af60 <__handle_mm_fault+0x320>  // b.any
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
  100.00 :   ffff80001020b454:       ldr     x0, [x20, #8]
         :                      PageCompound():
    0.00 :   ffff80001020b458:       tbnz    w0, #0, ffff80001020af60 <__handle_mm_fault+0x320>
         :                      test_bit():
    0.00 :   ffff80001020b45c:       ldr     x0, [x20]
         :                      do_numa_page():
         :                      flags |= TNF_NO_GROUP;
    0.00 :   ffff80001020b460:       tst     x19, #0x8000000000000
    0.00 :   ffff80001020b464:       mov     w19, #0x2                       // #2
    0.00 :   ffff80001020b468:       csel    w19, wzr, w19, ne  // ne = any
         :                      PageCompound():
    0.00 :   ffff80001020b46c:       tst     w0, #0x10000
    0.00 :   ffff80001020b470:       b.eq    ffff80001020b8c0 <__handle_mm_fault+0xc80>  // b.none
         :                      page_mapcount():
         :                      return __page_mapcount(page);
    0.00 :   ffff80001020b474:       mov     x0, x20
    0.00 :   ffff80001020b478:       bl      ffff8000101f13f0 <__page_mapcount>
         :                      do_numa_page():
         :                      if (page_mapcount(page) > 1 && (vma->vm_flags & VM_SHARED))
    0.00 :   ffff80001020b47c:       cmp     w0, #0x1
    0.00 :   ffff80001020b480:       b.le    ffff80001020b490 <__handle_mm_fault+0x850>
    0.00 :   ffff80001020b484:       ldr     x0, [x22, #80]
    0.00 :   ffff80001020b488:       tbz     w0, #3, ffff80001020b490 <__handle_mm_fault+0x850>
         :                      flags |= TNF_SHARED;
    0.00 :   ffff80001020b48c:       orr     w19, w19, #0x4
         :                      __read_once_size():
    0.00 :   ffff80001020b490:       ldr     x1, [x20, #8]
         :                      do_numa_page():
         :                      last_cpupid = page_cpupid_last(page);
    0.00 :   ffff80001020b494:       ldr     x23, [x20]
         :                      compound_head():
         :                      return (struct page *) (head - 1);
    0.00 :   ffff80001020b498:       sub     x0, x1, #0x1
         :                      do_numa_page():
         :                      target_nid = numa_migrate_prep(page, vma, vmf->address, page_nid,
    0.00 :   ffff80001020b49c:       ldr     x24, [x29, #120]
         :                      compound_head():
    0.00 :   ffff80001020b4a0:       tst     x1, #0x1
         :                      page_to_nid():
         :                      return (PF_POISONED_CHECK(p)->flags >> NODES_PGSHIFT) & NODES_MASK;
    0.00 :   ffff80001020b4a4:       lsr     x26, x23, #62
         :                      compound_head():
    0.00 :   ffff80001020b4a8:       csel    x0, x0, x20, ne  // ne = any
         :                      page_cpupid_last():
         :                      return (page->flags >> LAST_CPUPID_PGSHIFT) & LAST_CPUPID_MASK;
    0.00 :   ffff80001020b4ac:       ubfx    x23, x23, #44, #16
         :                      page_to_nid():
         :                      return (PF_POISONED_CHECK(p)->flags >> NODES_PGSHIFT) & NODES_MASK;
    0.00 :   ffff80001020b4b0:       mov     w25, w26
         :                      page_ref_inc():
         :                      __page_ref_mod(page, -nr);
         :                      }
         :
         :                      static inline void page_ref_inc(struct page *page)
         :                      {
         :                      atomic_inc(&page->_refcount);
    0.00 :   ffff80001020b4b4:       add     x0, x0, #0x34
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff80001020b4b8:       b       ffff80001020b880 <__handle_mm_fault+0xc40>
    0.00 :   ffff80001020b4bc:       b       ffff80001020b880 <__handle_mm_fault+0xc40>
         :                      __lse_atomic_add():
         :                      }
         :
         :                      ATOMIC_OP(andnot, stclr)
         :                      ATOMIC_OP(or, stset)
         :                      ATOMIC_OP(xor, steor)
         :                      ATOMIC_OP(add, stadd)
    0.00 :   ffff80001020b4c0:       mov     w1, #0x1                        // #1
    0.00 :   ffff80001020b4c4:       stadd   w1, [x0]
         :                      numa_migrate_prep():
         :                      count_vm_numa_event(NUMA_HINT_FAULTS);
    0.00 :   ffff80001020b4c8:       mov     w0, #0x2a                       // #42
    0.00 :   ffff80001020b4cc:       bl      ffff80001020e5fc <count_vm_event>
         :                      numa_node_id():
    0.00 :   ffff80001020b4d0:       adrp    x1, ffff8000114d3000 <pmu_sb_events>
    0.00 :   ffff80001020b4d4:       add     x1, x1, #0xc58
    0.00 :   ffff80001020b4d8:       bl      ffff800010206350 <__my_cpu_offset>
         :                      numa_migrate_prep():
         :                      if (page_nid == numa_node_id()) {
    0.00 :   ffff80001020b4dc:       ldr     w0, [x1, x0]
    0.00 :   ffff80001020b4e0:       cmp     w26, w0
    0.00 :   ffff80001020b4e4:       b.eq    ffff80001020ba08 <__handle_mm_fault+0xdc8>  // b.none
         :                      return mpol_misplaced(page, vma, addr);
    0.00 :   ffff80001020b4e8:       mov     x2, x24
    0.00 :   ffff80001020b4ec:       mov     x1, x22
    0.00 :   ffff80001020b4f0:       mov     x0, x20
    0.00 :   ffff80001020b4f4:       bl      ffff8000102441d8 <mpol_misplaced>
    0.00 :   ffff80001020b4f8:       mov     w24, w0
         :                      spin_unlock():
         :                      raw_spin_unlock(&lock->rlock);
    0.00 :   ffff80001020b4fc:       ldr     x0, [x29, #184]
    0.00 :   ffff80001020b500:       bl      ffff800010cb2408 <_raw_spin_unlock>
         :                      do_numa_page():
         :                      if (target_nid == NUMA_NO_NODE) {
    0.00 :   ffff80001020b504:       cmn     w24, #0x1
    0.00 :   ffff80001020b508:       b.eq    ffff80001020b8e4 <__handle_mm_fault+0xca4>  // b.none
         :                      migrated = migrate_misplaced_page(page, vma, target_nid);
    0.00 :   ffff80001020b50c:       mov     w2, w24
    0.00 :   ffff80001020b510:       mov     x1, x22
    0.00 :   ffff80001020b514:       mov     x0, x20
    0.00 :   ffff80001020b518:       bl      ffff8000102580c0 <migrate_misplaced_page>
         :                      if (migrated) {
    0.00 :   ffff80001020b51c:       cbz     w0, ffff80001020b8dc <__handle_mm_fault+0xc9c>
         :                      flags |= TNF_MIGRATED;
    0.00 :   ffff80001020b520:       orr     w19, w19, #0x1
    0.00 :   ffff80001020b524:       mov     w25, w24
         :                      task_numa_fault(last_cpupid, page_nid, 1, flags);
    0.00 :   ffff80001020b528:       mov     w3, w19
    0.00 :   ffff80001020b52c:       mov     w2, #0x1                        // #1
    0.00 :   ffff80001020b530:       mov     w1, w25
    0.00 :   ffff80001020b534:       mov     w0, w23
         :                      handle_pte_fault():
         :                      return do_numa_page(vmf);
    0.00 :   ffff80001020b538:       mov     w22, #0x0                       // #0
         :                      do_numa_page():
         :                      task_numa_fault(last_cpupid, page_nid, 1, flags);
    0.00 :   ffff80001020b53c:       bl      ffff800010122bf8 <task_numa_fault>
    0.00 :   ffff80001020b540:       ldr     x26, [x29, #72]
    0.00 :   ffff80001020b544:       b       ffff80001020add0 <__handle_mm_fault+0x190>
         :                      handle_pte_fault():
         :                      return do_wp_page(vmf);
    0.00 :   ffff80001020b548:       add     x0, x29, #0x60
    0.00 :   ffff80001020b54c:       bl      ffff800010209350 <do_wp_page>
    0.00 :   ffff80001020b550:       ldr     x26, [x29, #72]
    0.00 :   ffff80001020b554:       mov     w22, w0
    0.00 :   ffff80001020b558:       b       ffff80001020add0 <__handle_mm_fault+0x190>
         :                      do_fault():
         :                      if (unlikely(!pmd_present(*vmf->pmd)))
    0.00 :   ffff80001020b55c:       ldr     x2, [x29, #128]
    0.00 :   ffff80001020b560:       ldr     x0, [x2]
    0.00 :   ffff80001020b564:       and     x1, x0, #0x7ffffffffffffff
    0.00 :   ffff80001020b568:       and     x1, x1, #0xfc00000000000001
    0.00 :   ffff80001020b56c:       cbz     x1, ffff80001020ba4c <__handle_mm_fault+0xe0c>
         :                      pte_lockptr():
         :                      return ptlock_ptr(pmd_page(*pmd));
    0.00 :   ffff80001020b570:       adrp    x1, ffff8000112ae000 <cpu_ops+0x248>
         :                      do_fault():
         :                      vmf->pte = pte_offset_map_lock(vmf->vma->vm_mm,
    0.00 :   ffff80001020b574:       ldr     x20, [x29, #120]
         :                      __read_once_size():
    0.00 :   ffff80001020b578:       ldr     x19, [x2]
         :                      pte_lockptr():
    0.00 :   ffff80001020b57c:       ubfx    x0, x0, #12, #36
         :                      do_fault():
    0.00 :   ffff80001020b580:       ldr     x2, [x26, #1872]
         :                      pte_lockptr():
    0.00 :   ffff80001020b584:       ldr     x1, [x1, #1880]
         :                      do_fault():
    0.00 :   ffff80001020b588:       ubfx    x20, x20, #12, #9
         :                      pmd_page_paddr():
         :                      return __pmd_to_phys(pmd);
    0.00 :   ffff80001020b58c:       and     x19, x19, #0xfffffffff000
         :                      do_fault():
    0.00 :   ffff80001020b590:       sub     x19, x19, x2
         :                      pte_lockptr():
    0.00 :   ffff80001020b594:       add     x0, x1, x0, lsl #6
         :                      do_fault():
    0.00 :   ffff80001020b598:       add     x22, x19, x20, lsl #3
         :                      ptlock_ptr():
         :                      return &page->ptl;
    0.00 :   ffff80001020b59c:       add     x0, x0, #0x28
         :                      do_fault():
    0.00 :   ffff80001020b5a0:       str     x0, [x29, #184]
         :                      spin_lock():
         :                      raw_spin_lock(&lock->rlock);
    0.00 :   ffff80001020b5a4:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      do_fault():
    0.00 :   ffff80001020b5a8:       str     x22, [x29, #176]
         :                      spin_unlock():
         :                      raw_spin_unlock(&lock->rlock);
    0.00 :   ffff80001020b5ac:       ldr     x0, [x29, #184]
         :                      do_fault():
         :                      ret = VM_FAULT_SIGBUS;
    0.00 :   ffff80001020b5b0:       mov     w1, #0x2                        // #2
         :                      if (unlikely(pte_none(*vmf->pte)))
    0.00 :   ffff80001020b5b4:       ldr     x2, [x19, x20, lsl #3]
         :                      ret = VM_FAULT_SIGBUS;
    0.00 :   ffff80001020b5b8:       mov     w22, #0x100                     // #256
    0.00 :   ffff80001020b5bc:       cmp     x2, #0x0
    0.00 :   ffff80001020b5c0:       csel    w22, w22, w1, ne  // ne = any
         :                      spin_unlock():
    0.00 :   ffff80001020b5c4:       bl      ffff800010cb2408 <_raw_spin_unlock>
    0.00 :   ffff80001020b5c8:       b       ffff80001020b100 <__handle_mm_fault+0x4c0>
         :                      do_shared_fault():
         :                      ret = __do_fault(vmf);
    0.00 :   ffff80001020b5cc:       add     x23, x29, #0x60
    0.00 :   ffff80001020b5d0:       mov     x0, x23
    0.00 :   ffff80001020b5d4:       bl      ffff800010206d38 <__do_fault>
    0.00 :   ffff80001020b5d8:       mov     w22, w0
         :                      if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
    0.00 :   ffff80001020b5dc:       mov     w0, #0xd73                      // #3443
    0.00 :   ffff80001020b5e0:       tst     w22, w0
    0.00 :   ffff80001020b5e4:       b.ne    ffff80001020b100 <__handle_mm_fault+0x4c0>  // b.any
         :                      if (vma->vm_ops->page_mkwrite) {
    0.00 :   ffff80001020b5e8:       ldr     x0, [x19, #144]
    0.00 :   ffff80001020b5ec:       ldr     x0, [x0, #64]
    0.00 :   ffff80001020b5f0:       cbz     x0, ffff80001020b618 <__handle_mm_fault+0x9d8>
         :                      unlock_page(vmf->page);
    0.00 :   ffff80001020b5f4:       ldr     x0, [x29, #168]
    0.00 :   ffff80001020b5f8:       bl      ffff8000101cfaf8 <unlock_page>
         :                      tmp = do_page_mkwrite(vmf);
    0.00 :   ffff80001020b5fc:       mov     x0, x23
    0.00 :   ffff80001020b600:       bl      ffff800010206ae0 <do_page_mkwrite>
    0.00 :   ffff80001020b604:       mov     w1, w0
         :                      if (unlikely(!tmp ||
    0.00 :   ffff80001020b608:       cbz     w0, ffff80001020ba6c <__handle_mm_fault+0xe2c>
    0.00 :   ffff80001020b60c:       mov     w0, #0x973                      // #2419
    0.00 :   ffff80001020b610:       tst     w1, w0
    0.00 :   ffff80001020b614:       b.ne    ffff80001020ba6c <__handle_mm_fault+0xe2c>  // b.any
         :                      ret |= finish_fault(vmf);
    0.00 :   ffff80001020b618:       mov     x0, x23
    0.00 :   ffff80001020b61c:       bl      ffff80001020a918 <finish_fault>
    0.00 :   ffff80001020b620:       orr     w22, w22, w0
         :                      if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE |
    0.00 :   ffff80001020b624:       mov     w0, #0xd73                      // #3443
    0.00 :   ffff80001020b628:       tst     w22, w0
    0.00 :   ffff80001020b62c:       b.ne    ffff80001020bacc <__handle_mm_fault+0xe8c>  // b.any
         :                      ret |= fault_dirty_shared_page(vmf);
    0.00 :   ffff80001020b630:       mov     x0, x23
    0.00 :   ffff80001020b634:       bl      ffff800010206c00 <fault_dirty_shared_page>
    0.00 :   ffff80001020b638:       orr     w22, w22, w0
    0.00 :   ffff80001020b63c:       b       ffff80001020b100 <__handle_mm_fault+0x4c0>
         :                      flush_tlb_page():
         :                      }
         :
         :                      static inline void flush_tlb_page(struct vm_area_struct *vma,
         :                      unsigned long uaddr)
         :                      {
         :                      flush_tlb_page_nosync(vma, uaddr);
    0.00 :   ffff80001020b640:       ldr     x1, [x29, #96]
         :                      flush_tlb_page_nosync():
         :                      unsigned long addr = __TLBI_VADDR(uaddr, ASID(vma->vm_mm));
    0.00 :   ffff80001020b644:       ldr     x0, [x29, #120]
    0.00 :   ffff80001020b648:       ldr     x2, [x1, #64]
    0.00 :   ffff80001020b64c:       ubfx    x1, x0, #12, #44
    0.00 :   ffff80001020b650:       ldr     x0, [x2, #736]
    0.00 :   ffff80001020b654:       orr     x0, x1, x0, lsl #48
         :                      dsb(ishst);
    0.00 :   ffff80001020b658:       dsb     ishst
         :                      __tlbi(vale1is, addr);
    0.00 :   ffff80001020b65c:       tlbi    vale1is, x0
    0.00 :   ffff80001020b660:       nop
    0.00 :   ffff80001020b664:       nop
         :                      arch_static_branch_jump():
    0.00 :   ffff80001020b668:       b       ffff80001020b7b8 <__handle_mm_fault+0xb78>
         :                      arch_static_branch():
         :                      asm_volatile_goto(
    0.00 :   ffff80001020b66c:       nop
         :                      flush_tlb_page():
         :                      dsb(ish);
    0.00 :   ffff80001020b670:       dsb     ish
    0.00 :   ffff80001020b674:       b       ffff80001020af60 <__handle_mm_fault+0x320>
         :                      do_read_fault():
         :                      if (vma->vm_ops->map_pages && fault_around_bytes >> PAGE_SHIFT > 1) {
    0.00 :   ffff80001020b678:       ldr     x0, [x0, #48]
    0.00 :   ffff80001020b67c:       cbz     x0, ffff80001020b698 <__handle_mm_fault+0xa58>
    0.00 :   ffff80001020b680:       adrp    x0, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff80001020b684:       add     x0, x0, #0x3a0
    0.00 :   ffff80001020b688:       ldr     x1, [x0, #16]
    0.00 :   ffff80001020b68c:       lsr     x1, x1, #12
    0.00 :   ffff80001020b690:       cmp     x1, #0x1
    0.00 :   ffff80001020b694:       b.hi    ffff80001020b6dc <__handle_mm_fault+0xa9c>  // b.pmore
    0.00 :   ffff80001020b698:       add     x23, x29, #0x60
         :                      ret = __do_fault(vmf);
    0.00 :   ffff80001020b69c:       mov     x0, x23
         :                      if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
    0.00 :   ffff80001020b6a0:       mov     w19, #0xd73                     // #3443
         :                      ret = __do_fault(vmf);
    0.00 :   ffff80001020b6a4:       bl      ffff800010206d38 <__do_fault>
    0.00 :   ffff80001020b6a8:       mov     w22, w0
         :                      if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
    0.00 :   ffff80001020b6ac:       tst     w0, w19
    0.00 :   ffff80001020b6b0:       b.ne    ffff80001020b100 <__handle_mm_fault+0x4c0>  // b.any
         :                      ret |= finish_fault(vmf);
    0.00 :   ffff80001020b6b4:       mov     x0, x23
    0.00 :   ffff80001020b6b8:       bl      ffff80001020a918 <finish_fault>
    0.00 :   ffff80001020b6bc:       orr     w22, w22, w0
         :                      unlock_page(vmf->page);
    0.00 :   ffff80001020b6c0:       ldr     x0, [x29, #168]
    0.00 :   ffff80001020b6c4:       bl      ffff8000101cfaf8 <unlock_page>
         :                      if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
    0.00 :   ffff80001020b6c8:       tst     w22, w19
    0.00 :   ffff80001020b6cc:       b.eq    ffff80001020b100 <__handle_mm_fault+0x4c0>  // b.none
         :                      do_shared_fault():
         :                      put_page(vmf->page);
    0.00 :   ffff80001020b6d0:       ldr     x0, [x29, #168]
    0.00 :   ffff80001020b6d4:       bl      ffff8000102069b0 <put_page>
    0.00 :   ffff80001020b6d8:       b       ffff80001020b100 <__handle_mm_fault+0x4c0>
         :                      __read_once_size():
    0.00 :   ffff80001020b6dc:       ldr     x0, [x0, #16]
         :                      do_fault_around():
         :                      unsigned long address = vmf->address, nr_pages, mask;
    0.00 :   ffff80001020b6e0:       ldp     x24, x20, [x29, #112]
         :                      nr_pages = READ_ONCE(fault_around_bytes) >> PAGE_SHIFT;
    0.00 :   ffff80001020b6e4:       lsr     x0, x0, #12
         :                      vmf->address = max(address & mask, vmf->vma->vm_start);
    0.00 :   ffff80001020b6e8:       ldr     x4, [x19]
         :                      end_pgoff = min3(end_pgoff, vma_pages(vmf->vma) + vmf->vma->vm_pgoff - 1,
    0.00 :   ffff80001020b6ec:       sub     x2, x0, #0x1
    0.00 :   ffff80001020b6f0:       ldr     x1, [x19, #152]
         :                      mask = ~(nr_pages * PAGE_SIZE - 1) & PAGE_MASK;
    0.00 :   ffff80001020b6f4:       neg     x0, x0, lsl #12
         :                      if (pmd_none(*vmf->pmd)) {
    0.00 :   ffff80001020b6f8:       ldr     x5, [x29, #128]
         :                      vmf->address = max(address & mask, vmf->vma->vm_start);
    0.00 :   ffff80001020b6fc:       and     x0, x0, x20
         :                      end_pgoff = min3(end_pgoff, vma_pages(vmf->vma) + vmf->vma->vm_pgoff - 1,
    0.00 :   ffff80001020b700:       sub     x3, x1, #0x1
         :                      vmf->address = max(address & mask, vmf->vma->vm_start);
    0.00 :   ffff80001020b704:       cmp     x0, x4
    0.00 :   ffff80001020b708:       csel    x0, x0, x4, cs  // cs = hs, nlast
    0.00 :   ffff80001020b70c:       str     x0, [x29, #120]
         :                      off = ((address - vmf->address) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    0.00 :   ffff80001020b710:       sub     x4, x20, x0
         :                      vma_pages():
         :                      return vm_end;
         :                      }
         :
         :                      static inline unsigned long vma_pages(struct vm_area_struct *vma)
         :                      {
         :                      return (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;
    0.00 :   ffff80001020b714:       ldp     x6, x1, [x19]
         :                      do_fault_around():
         :                      start_pgoff -= off;
    0.00 :   ffff80001020b718:       ubfx    x4, x4, #12, #9
         :                      end_pgoff = start_pgoff -
    0.00 :   ffff80001020b71c:       mvn     x0, x0, lsr #12
         :                      start_pgoff -= off;
    0.00 :   ffff80001020b720:       sub     x24, x24, x4
         :                      end_pgoff = start_pgoff -
    0.00 :   ffff80001020b724:       and     x0, x0, #0x1ff
         :                      end_pgoff = min3(end_pgoff, vma_pages(vmf->vma) + vmf->vma->vm_pgoff - 1,
    0.00 :   ffff80001020b728:       add     x2, x2, x24
         :                      end_pgoff = start_pgoff -
    0.00 :   ffff80001020b72c:       add     x0, x0, x24
         :                      vma_pages():
    0.00 :   ffff80001020b730:       sub     x1, x1, x6
         :                      do_fault_around():
         :                      if (pmd_none(*vmf->pmd)) {
    0.00 :   ffff80001020b734:       ldr     x4, [x5]
         :                      end_pgoff = min3(end_pgoff, vma_pages(vmf->vma) + vmf->vma->vm_pgoff - 1,
    0.00 :   ffff80001020b738:       cmp     x0, x2
    0.00 :   ffff80001020b73c:       csel    x0, x0, x2, ls  // ls = plast
    0.00 :   ffff80001020b740:       add     x1, x3, x1, lsr #12
    0.00 :   ffff80001020b744:       cmp     x1, x0
    0.00 :   ffff80001020b748:       csel    x22, x1, x0, ls  // ls = plast
         :                      if (pmd_none(*vmf->pmd)) {
    0.00 :   ffff80001020b74c:       cbz     x4, ffff80001020ba7c <__handle_mm_fault+0xe3c>
         :                      vmf->vma->vm_ops->map_pages(vmf, start_pgoff, end_pgoff);
    0.00 :   ffff80001020b750:       ldr     x3, [x19, #144]
    0.00 :   ffff80001020b754:       add     x23, x29, #0x60
    0.00 :   ffff80001020b758:       mov     x0, x23
    0.00 :   ffff80001020b75c:       mov     x2, x22
    0.00 :   ffff80001020b760:       mov     x1, x24
    0.00 :   ffff80001020b764:       ldr     x3, [x3, #48]
    0.00 :   ffff80001020b768:       blr     x3
         :                      if (pmd_trans_huge(*vmf->pmd)) {
    0.00 :   ffff80001020b76c:       ldr     x0, [x29, #128]
    0.00 :   ffff80001020b770:       ldr     x0, [x0]
    0.00 :   ffff80001020b774:       cbz     x0, ffff80001020b77c <__handle_mm_fault+0xb3c>
    0.00 :   ffff80001020b778:       tbz     w0, #1, ffff80001020babc <__handle_mm_fault+0xe7c>
         :                      if (!vmf->pte)
    0.00 :   ffff80001020b77c:       ldr     x2, [x29, #176]
    0.00 :   ffff80001020b780:       cbz     x2, ffff80001020b8b4 <__handle_mm_fault+0xc74>
         :                      vmf->pte -= (vmf->address >> PAGE_SHIFT) - (address >> PAGE_SHIFT);
    0.00 :   ffff80001020b784:       ldr     x1, [x29, #120]
    0.00 :   ffff80001020b788:       ldr     x0, [x29, #184]
    0.00 :   ffff80001020b78c:       lsr     x1, x1, #12
    0.00 :   ffff80001020b790:       sub     x1, x1, x20, lsr #12
    0.00 :   ffff80001020b794:       sub     x1, x2, x1, lsl #3
    0.00 :   ffff80001020b798:       str     x1, [x29, #176]
         :                      if (!pte_none(*vmf->pte))
    0.00 :   ffff80001020b79c:       ldr     x1, [x1]
    0.00 :   ffff80001020b7a0:       cbz     x1, ffff80001020b8b0 <__handle_mm_fault+0xc70>
         :                      spin_unlock():
    0.00 :   ffff80001020b7a4:       bl      ffff800010cb2408 <_raw_spin_unlock>
         :                      do_fault_around():
         :                      ret = VM_FAULT_NOPAGE;
    0.00 :   ffff80001020b7a8:       mov     w22, #0x100                     // #256
         :                      vmf->address = address;
    0.00 :   ffff80001020b7ac:       str     x20, [x29, #120]
         :                      vmf->pte = NULL;
    0.00 :   ffff80001020b7b0:       str     xzr, [x29, #176]
    0.00 :   ffff80001020b7b4:       b       ffff80001020b100 <__handle_mm_fault+0x4c0>
         :                      test_bit():
    0.00 :   ffff80001020b7b8:       adrp    x1, ffff800011a76000 <reset_devices>
    0.00 :   ffff80001020b7bc:       ldr     x1, [x1, #1320]
         :                      flush_tlb_page_nosync():
         :                      __tlbi_user(vale1is, addr);
    0.00 :   ffff80001020b7c0:       tst     w1, #0x800000
    0.00 :   ffff80001020b7c4:       b.eq    ffff80001020b670 <__handle_mm_fault+0xa30>  // b.none
    0.00 :   ffff80001020b7c8:       orr     x0, x0, #0x1000000000000
    0.00 :   ffff80001020b7cc:       tlbi    vale1is, x0
    0.00 :   ffff80001020b7d0:       nop
    0.00 :   ffff80001020b7d4:       nop
         :                      flush_tlb_page():
         :                      dsb(ish);
    0.00 :   ffff80001020b7d8:       dsb     ish
    0.00 :   ffff80001020b7dc:       b       ffff80001020af60 <__handle_mm_fault+0x320>
         :                      set_pte_bit():
         :                      pte_val(pte) |= pgprot_val(prot);
    0.00 :   ffff80001020b7e0:       and     x0, x1, #0xffffffffffffff7f
    0.00 :   ffff80001020b7e4:       orr     x3, x1, #0x80000000000000
    0.00 :   ffff80001020b7e8:       orr     x0, x0, #0x80000000000000
    0.00 :   ffff80001020b7ec:       tst     x1, #0x8000000000000
    0.00 :   ffff80001020b7f0:       csel    x0, x0, x3, ne  // ne = any
    0.00 :   ffff80001020b7f4:       b       ffff80001020b3b8 <__handle_mm_fault+0x778>
         :                      anon_vma_prepare():
         :                      return 0;
         :
         :                      return __anon_vma_prepare(vma);
    0.00 :   ffff80001020b7f8:       mov     x0, x19
    0.00 :   ffff80001020b7fc:       bl      ffff80001021c188 <__anon_vma_prepare>
         :                      do_cow_fault():
         :                      if (unlikely(anon_vma_prepare(vma)))
    0.00 :   ffff80001020b800:       cbz     w0, ffff80001020b094 <__handle_mm_fault+0x454>
         :                      return VM_FAULT_OOM;
    0.00 :   ffff80001020b804:       mov     w22, #0x1                       // #1
    0.00 :   ffff80001020b808:       b       ffff80001020b100 <__handle_mm_fault+0x4c0>
         :                      do_anonymous_page():
         :                      vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,
    0.00 :   ffff80001020b80c:       ldp     x23, x1, [x29, #120]
         :                      pte_lockptr():
         :                      return ptlock_ptr(pmd_page(*pmd));
    0.00 :   ffff80001020b810:       adrp    x0, ffff8000112ae000 <cpu_ops+0x248>
         :                      do_anonymous_page():
    0.00 :   ffff80001020b814:       ldr     x3, [x26, #1872]
         :                      my_zero_pfn():
         :                      return zero_pfn;
    0.00 :   ffff80001020b818:       adrp    x4, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      pte_lockptr():
    0.00 :   ffff80001020b81c:       ldr     x2, [x0, #1880]
    0.00 :   ffff80001020b820:       ldr     x0, [x1]
         :                      do_anonymous_page():
    0.00 :   ffff80001020b824:       ubfx    x23, x23, #12, #9
         :                      __read_once_size():
    0.00 :   ffff80001020b828:       ldr     x20, [x1]
         :                      do_anonymous_page():
    0.00 :   ffff80001020b82c:       lsl     x23, x23, #3
         :                      entry = pte_mkspecial(pfn_pte(my_zero_pfn(vmf->address),
    0.00 :   ffff80001020b830:       ldr     x25, [x19, #72]
         :                      pte_lockptr():
    0.00 :   ffff80001020b834:       ubfx    x0, x0, #12, #36
         :                      pmd_page_paddr():
         :                      return __pmd_to_phys(pmd);
    0.00 :   ffff80001020b838:       and     x20, x20, #0xfffffffff000
         :                      do_anonymous_page():
         :                      vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,
    0.00 :   ffff80001020b83c:       sub     x20, x20, x3
         :                      my_zero_pfn():
    0.00 :   ffff80001020b840:       ldr     x24, [x4, #936]
         :                      pte_lockptr():
    0.00 :   ffff80001020b844:       add     x0, x2, x0, lsl #6
         :                      do_anonymous_page():
    0.00 :   ffff80001020b848:       add     x26, x23, x20
         :                      ptlock_ptr():
         :                      return &page->ptl;
    0.00 :   ffff80001020b84c:       add     x0, x0, #0x28
         :                      do_anonymous_page():
    0.00 :   ffff80001020b850:       str     x0, [x29, #184]
         :                      spin_lock():
         :                      raw_spin_lock(&lock->rlock);
    0.00 :   ffff80001020b854:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      do_anonymous_page():
    0.00 :   ffff80001020b858:       str     x26, [x29, #176]
         :                      if (!pte_none(*vmf->pte))
    0.00 :   ffff80001020b85c:       ldr     x0, [x23, x20]
    0.00 :   ffff80001020b860:       cbnz    x0, ffff80001020b354 <__handle_mm_fault+0x714>
         :                      ret = check_stable_address_space(vma->vm_mm);
    0.00 :   ffff80001020b864:       ldr     x0, [x19, #64]
         :                      test_bit():
    0.00 :   ffff80001020b868:       ldr     x0, [x0, #760]
         :                      check_stable_address_space():
    0.00 :   ffff80001020b86c:       tst     w0, #0x400000
    0.00 :   ffff80001020b870:       b.ne    ffff80001020bae4 <__handle_mm_fault+0xea4>  // b.any
         :                      do_anonymous_page():
         :                      entry = pte_mkspecial(pfn_pte(my_zero_pfn(vmf->address),
    0.00 :   ffff80001020b874:       orr     x20, x25, x24, lsl #12
         :                      set_pte_bit():
         :                      pte_val(pte) |= pgprot_val(prot);
    0.00 :   ffff80001020b878:       orr     x20, x20, #0x100000000000000
    0.00 :   ffff80001020b87c:       b       ffff80001020b344 <__handle_mm_fault+0x704>
         :                      __ll_sc_atomic_add():
         :                      ATOMIC_FETCH_OP (        , dmb ish,  , l, "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
    0.00 :   ffff80001020b880:       b       ffff80001020e4b4 <copy_huge_page_from_user+0x5cc>
    0.00 :   ffff80001020b884:       b       ffff80001020b4c8 <__handle_mm_fault+0x888>
         :                      anon_vma_prepare():
    0.00 :   ffff80001020b888:       mov     x0, x19
    0.00 :   ffff80001020b88c:       bl      ffff80001021c188 <__anon_vma_prepare>
         :                      do_anonymous_page():
         :                      if (unlikely(anon_vma_prepare(vma)))
    0.00 :   ffff80001020b890:       cbz     w0, ffff80001020b1a4 <__handle_mm_fault+0x564>
         :                      return VM_FAULT_OOM;
    0.00 :   ffff80001020b894:       mov     w22, #0x1                       // #1
    0.00 :   ffff80001020b898:       ldr     x26, [x29, #72]
    0.00 :   ffff80001020b89c:       b       ffff80001020add0 <__handle_mm_fault+0x190>
         :                      do_cow_fault():
         :                      put_page(vmf->cow_page);
    0.00 :   ffff80001020b8a0:       ldr     x0, [x29, #152]
         :                      return VM_FAULT_OOM;
    0.00 :   ffff80001020b8a4:       mov     w22, #0x1                       // #1
         :                      put_page(vmf->cow_page);
    0.00 :   ffff80001020b8a8:       bl      ffff8000102069b0 <put_page>
    0.00 :   ffff80001020b8ac:       b       ffff80001020b100 <__handle_mm_fault+0x4c0>
         :                      spin_unlock():
         :                      raw_spin_unlock(&lock->rlock);
    0.00 :   ffff80001020b8b0:       bl      ffff800010cb2408 <_raw_spin_unlock>
         :                      do_fault_around():
         :                      vmf->address = address;
    0.00 :   ffff80001020b8b4:       str     x20, [x29, #120]
         :                      vmf->pte = NULL;
    0.00 :   ffff80001020b8b8:       str     xzr, [x29, #176]
    0.00 :   ffff80001020b8bc:       b       ffff80001020b69c <__handle_mm_fault+0xa5c>
         :                      __read_once_size():
    0.00 :   ffff80001020b8c0:       ldr     x0, [x20, #8]
         :                      PageCompound():
         :                      return test_bit(PG_head, &page->flags) || PageTail(page);
    0.00 :   ffff80001020b8c4:       tbnz    w0, #0, ffff80001020b474 <__handle_mm_fault+0x834>
         :                      __read_once_size():
    0.00 :   ffff80001020b8c8:       ldr     w0, [x20, #48]
         :                      page_mapcount():
         :                      return atomic_read(&page->_mapcount) + 1;
    0.00 :   ffff80001020b8cc:       add     w0, w0, #0x1
    0.00 :   ffff80001020b8d0:       b       ffff80001020b47c <__handle_mm_fault+0x83c>
    0.00 :   ffff80001020b8d4:       str     x26, [x29, #72]
         :                      __handle_mm_fault():
         :                      }
    0.00 :   ffff80001020b8d8:       bl      ffff8000100e5630 <__stack_chk_fail>
         :                      do_numa_page():
         :                      flags |= TNF_MIGRATE_FAIL;
    0.00 :   ffff80001020b8dc:       orr     w19, w19, #0x10
    0.00 :   ffff80001020b8e0:       b       ffff80001020b528 <__handle_mm_fault+0x8e8>
         :                      put_page(page);
    0.00 :   ffff80001020b8e4:       mov     x0, x20
    0.00 :   ffff80001020b8e8:       bl      ffff8000102069b0 <put_page>
    0.00 :   ffff80001020b8ec:       b       ffff80001020b528 <__handle_mm_fault+0x8e8>
         :                      get_current():
    0.00 :   ffff80001020b8f0:       mrs     x19, sp_el0
         :                      __read_once_size():
    0.00 :   ffff80001020b8f4:       ldr     w3, [x19, #16]
         :                      do_cow_fault():
         :                      copy_user_highpage(vmf->cow_page, vmf->page, vmf->address, vma);
    0.00 :   ffff80001020b8f8:       ldr     x2, [x29, #120]
         :                      __preempt_count_add():
         :                      pc += val;
    0.00 :   ffff80001020b8fc:       add     w3, w3, #0x1
         :                      do_cow_fault():
    0.00 :   ffff80001020b900:       ldr     x0, [x29, #152]
    0.00 :   ffff80001020b904:       ldr     x1, [x29, #168]
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001020b908:       str     w3, [x19, #16]
         :                      pagefault_disabled_inc():
         :                      current->pagefault_disabled++;
    0.00 :   ffff80001020b90c:       ldr     w3, [x19, #2448]
    0.00 :   ffff80001020b910:       add     w3, w3, #0x1
    0.00 :   ffff80001020b914:       str     w3, [x19, #2448]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001020b918:       ldr     w3, [x19, #16]
         :                      __preempt_count_add():
    0.00 :   ffff80001020b91c:       add     w3, w3, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001020b920:       str     w3, [x19, #16]
         :                      pagefault_disabled_inc():
    0.00 :   ffff80001020b924:       ldr     w3, [x19, #2448]
    0.00 :   ffff80001020b928:       add     w3, w3, #0x1
    0.00 :   ffff80001020b92c:       str     w3, [x19, #2448]
         :                      lowmem_page_address():
         :                      return page_to_virt(page);
    0.00 :   ffff80001020b930:       mov     x3, #0x200000                   // #2097152
    0.00 :   ffff80001020b934:       movk    x3, #0x200, lsl #32
    0.00 :   ffff80001020b938:       add     x1, x1, x3
    0.00 :   ffff80001020b93c:       add     x0, x0, x3
    0.00 :   ffff80001020b940:       mov     x3, #0xffff000000000000         // #-281474976710656
    0.00 :   ffff80001020b944:       lsr     x1, x1, #6
    0.00 :   ffff80001020b948:       lsr     x0, x0, #6
         :                      copy_user_highpage():
         :                      {
         :                      char *vfrom, *vto;
         :
         :                      vfrom = kmap_atomic(from);
         :                      vto = kmap_atomic(to);
         :                      copy_user_page(vto, vfrom, vaddr, to);
    0.00 :   ffff80001020b94c:       add     x1, x3, x1, lsl #12
    0.00 :   ffff80001020b950:       add     x0, x3, x0, lsl #12
    0.00 :   ffff80001020b954:       bl      ffff8000100a25b0 <__cpu_copy_user_page>
         :                      pagefault_disabled_dec():
         :                      current->pagefault_disabled--;
    0.00 :   ffff80001020b958:       ldr     w0, [x19, #2448]
    0.00 :   ffff80001020b95c:       sub     w0, w0, #0x1
    0.00 :   ffff80001020b960:       str     w0, [x19, #2448]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001020b964:       ldr     x0, [x19, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff80001020b968:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001020b96c:       str     w0, [x19, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff80001020b970:       cbz     x0, ffff80001020ba00 <__handle_mm_fault+0xdc0>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001020b974:       ldr     x0, [x19, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff80001020b978:       cbz     x0, ffff80001020ba00 <__handle_mm_fault+0xdc0>
         :                      get_current():
    0.00 :   ffff80001020b97c:       mrs     x0, sp_el0
         :                      pagefault_disabled_dec():
    0.00 :   ffff80001020b980:       ldr     w1, [x0, #2448]
    0.00 :   ffff80001020b984:       sub     w1, w1, #0x1
    0.00 :   ffff80001020b988:       str     w1, [x0, #2448]
         :                      __read_once_size():
    0.00 :   ffff80001020b98c:       ldr     x1, [x0, #16]
         :                      __preempt_count_dec_and_test():
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff80001020b990:       sub     x1, x1, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff80001020b994:       str     w1, [x0, #16]
         :                      __preempt_count_dec_and_test():
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff80001020b998:       cbz     x1, ffff80001020b9f8 <__handle_mm_fault+0xdb8>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001020b99c:       ldr     x0, [x0, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff80001020b9a0:       cbz     x0, ffff80001020b9f8 <__handle_mm_fault+0xdb8>
         :                      do_cow_fault():
         :                      __SetPageUptodate(vmf->cow_page);
    0.00 :   ffff80001020b9a4:       ldr     x2, [x29, #152]
         :                      __SetPageUptodate():
         :                      smp_wmb();
    0.00 :   ffff80001020b9a8:       dmb     ishst
         :                      __set_bit():
         :                      *p  |= mask;
    0.00 :   ffff80001020b9ac:       ldr     x1, [x2]
         :                      do_cow_fault():
         :                      ret |= finish_fault(vmf);
    0.00 :   ffff80001020b9b0:       mov     x0, x23
         :                      __set_bit():
    0.00 :   ffff80001020b9b4:       orr     x1, x1, #0x4
    0.00 :   ffff80001020b9b8:       str     x1, [x2]
         :                      do_cow_fault():
    0.00 :   ffff80001020b9bc:       bl      ffff80001020a918 <finish_fault>
    0.00 :   ffff80001020b9c0:       orr     w22, w22, w0
         :                      unlock_page(vmf->page);
    0.00 :   ffff80001020b9c4:       ldr     x0, [x29, #168]
    0.00 :   ffff80001020b9c8:       bl      ffff8000101cfaf8 <unlock_page>
         :                      put_page(vmf->page);
    0.00 :   ffff80001020b9cc:       ldr     x0, [x29, #168]
    0.00 :   ffff80001020b9d0:       bl      ffff8000102069b0 <put_page>
         :                      if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
    0.00 :   ffff80001020b9d4:       mov     w0, #0xd73                      // #3443
    0.00 :   ffff80001020b9d8:       tst     w22, w0
    0.00 :   ffff80001020b9dc:       b.eq    ffff80001020b100 <__handle_mm_fault+0x4c0>  // b.none
         :                      mem_cgroup_cancel_charge(vmf->cow_page, vmf->memcg, false);
    0.00 :   ffff80001020b9e0:       ldp     x0, x1, [x29, #152]
    0.00 :   ffff80001020b9e4:       mov     w2, #0x0                        // #0
    0.00 :   ffff80001020b9e8:       bl      ffff80001026cb98 <mem_cgroup_cancel_charge>
         :                      put_page(vmf->cow_page);
    0.00 :   ffff80001020b9ec:       ldr     x0, [x29, #152]
    0.00 :   ffff80001020b9f0:       bl      ffff8000102069b0 <put_page>
    0.00 :   ffff80001020b9f4:       b       ffff80001020b100 <__handle_mm_fault+0x4c0>
         :                      __kunmap_atomic():
         :                      preempt_enable();
    0.00 :   ffff80001020b9f8:       bl      ffff800010cad640 <preempt_schedule>
    0.00 :   ffff80001020b9fc:       b       ffff80001020b9a4 <__handle_mm_fault+0xd64>
    0.00 :   ffff80001020ba00:       bl      ffff800010cad640 <preempt_schedule>
    0.00 :   ffff80001020ba04:       b       ffff80001020b97c <__handle_mm_fault+0xd3c>
         :                      numa_migrate_prep():
         :                      *flags |= TNF_FAULT_LOCAL;
    0.00 :   ffff80001020ba08:       orr     w19, w19, #0x8
         :                      count_vm_numa_event(NUMA_HINT_FAULTS_LOCAL);
    0.00 :   ffff80001020ba0c:       mov     w0, #0x2b                       // #43
    0.00 :   ffff80001020ba10:       bl      ffff80001020e5fc <count_vm_event>
    0.00 :   ffff80001020ba14:       b       ffff80001020b4e8 <__handle_mm_fault+0x8a8>
         :                      do_anonymous_page():
         :                      put_page(page);
    0.00 :   ffff80001020ba18:       mov     x0, x23
         :                      return VM_FAULT_OOM;
    0.00 :   ffff80001020ba1c:       mov     w22, #0x1                       // #1
         :                      put_page(page);
    0.00 :   ffff80001020ba20:       bl      ffff8000102069b0 <put_page>
    0.00 :   ffff80001020ba24:       ldr     x26, [x29, #72]
    0.00 :   ffff80001020ba28:       b       ffff80001020add0 <__handle_mm_fault+0x190>
         :                      vm_fault_t ret = 0;
    0.00 :   ffff80001020ba2c:       mov     w22, #0x0                       // #0
         :                      mem_cgroup_cancel_charge(page, memcg, false);
    0.00 :   ffff80001020ba30:       ldr     x1, [x29, #88]
    0.00 :   ffff80001020ba34:       mov     w2, #0x0                        // #0
    0.00 :   ffff80001020ba38:       mov     x0, x23
    0.00 :   ffff80001020ba3c:       bl      ffff80001026cb98 <mem_cgroup_cancel_charge>
         :                      put_page(page);
    0.00 :   ffff80001020ba40:       mov     x0, x23
    0.00 :   ffff80001020ba44:       bl      ffff8000102069b0 <put_page>
    0.00 :   ffff80001020ba48:       b       ffff80001020b354 <__handle_mm_fault+0x714>
         :                      do_fault():
         :                      ret = VM_FAULT_SIGBUS;
    0.00 :   ffff80001020ba4c:       mov     w22, #0x2                       // #2
    0.00 :   ffff80001020ba50:       b       ffff80001020b100 <__handle_mm_fault+0x4c0>
         :                      do_anonymous_page():
         :                      if (pte_alloc(vma->vm_mm, vmf->pmd))
    0.00 :   ffff80001020ba54:       ldr     x0, [x19, #64]
    0.00 :   ffff80001020ba58:       bl      ffff800010207f78 <__pte_alloc>
    0.00 :   ffff80001020ba5c:       cbnz    w0, ffff80001020b894 <__handle_mm_fault+0xc54>
    0.00 :   ffff80001020ba60:       ldr     x0, [x29, #128]
    0.00 :   ffff80001020ba64:       ldr     x0, [x0]
    0.00 :   ffff80001020ba68:       b       ffff80001020b188 <__handle_mm_fault+0x548>
         :                      do_shared_fault():
         :                      put_page(vmf->page);
    0.00 :   ffff80001020ba6c:       ldr     x0, [x29, #168]
         :                      tmp = do_page_mkwrite(vmf);
    0.00 :   ffff80001020ba70:       mov     w22, w1
         :                      put_page(vmf->page);
    0.00 :   ffff80001020ba74:       bl      ffff8000102069b0 <put_page>
    0.00 :   ffff80001020ba78:       b       ffff80001020b100 <__handle_mm_fault+0x4c0>
         :                      alloc_pages():
         :                      extern struct page *alloc_pages_current(gfp_t gfp_mask, unsigned order);
         :
         :                      static inline struct page *
         :                      alloc_pages(gfp_t gfp_mask, unsigned int order)
         :                      {
         :                      return alloc_pages_current(gfp_mask, order);
    0.00 :   ffff80001020ba7c:       mov     w0, #0xdc0                      // #3520
    0.00 :   ffff80001020ba80:       mov     w1, #0x0                        // #0
    0.00 :   ffff80001020ba84:       movk    w0, #0x40, lsl #16
    0.00 :   ffff80001020ba88:       bl      ffff800010241498 <alloc_pages_current>
    0.00 :   ffff80001020ba8c:       mov     x19, x0
         :                      __pte_alloc_one():
         :                      if (!pte)
    0.00 :   ffff80001020ba90:       cbz     x0, ffff80001020bad8 <__handle_mm_fault+0xe98>
         :                      __SetPageTable():
         :                      PAGE_TYPE_OPS(Table, table)
    0.00 :   ffff80001020ba94:       ldr     w2, [x0, #48]
         :                      pgtable_pte_page_ctor():
         :                      inc_zone_page_state(page, NR_PAGETABLE);
    0.00 :   ffff80001020ba98:       mov     w1, #0x8                        // #8
         :                      ptlock_init():
         :                      spin_lock_init(ptlock_ptr(page));
    0.00 :   ffff80001020ba9c:       str     wzr, [x0, #40]
         :                      __SetPageTable():
    0.00 :   ffff80001020baa0:       and     w2, w2, #0xfffffbff
    0.00 :   ffff80001020baa4:       str     w2, [x0, #48]
         :                      pgtable_pte_page_ctor():
         :                      inc_zone_page_state(page, NR_PAGETABLE);
    0.00 :   ffff80001020baa8:       bl      ffff8000101f3b68 <inc_zone_page_state>
         :                      do_fault_around():
         :                      vmf->prealloc_pte = pte_alloc_one(vmf->vma->vm_mm);
    0.00 :   ffff80001020baac:       str     x19, [x29, #192]
         :                      smp_wmb(); /* See comment in __pte_alloc() */
    0.00 :   ffff80001020bab0:       dmb     ishst
    0.00 :   ffff80001020bab4:       ldr     x19, [x29, #96]
    0.00 :   ffff80001020bab8:       b       ffff80001020b750 <__handle_mm_fault+0xb10>
         :                      ret = VM_FAULT_NOPAGE;
    0.00 :   ffff80001020babc:       mov     w22, #0x100                     // #256
         :                      vmf->address = address;
    0.00 :   ffff80001020bac0:       str     x20, [x29, #120]
         :                      vmf->pte = NULL;
    0.00 :   ffff80001020bac4:       str     xzr, [x29, #176]
    0.00 :   ffff80001020bac8:       b       ffff80001020b100 <__handle_mm_fault+0x4c0>
         :                      do_shared_fault():
         :                      unlock_page(vmf->page);
    0.00 :   ffff80001020bacc:       ldr     x0, [x29, #168]
    0.00 :   ffff80001020bad0:       bl      ffff8000101cfaf8 <unlock_page>
    0.00 :   ffff80001020bad4:       b       ffff80001020b6d0 <__handle_mm_fault+0xa90>
    0.00 :   ffff80001020bad8:       add     x23, x29, #0x60
         :                      do_fault_around():
         :                      vmf->prealloc_pte = pte_alloc_one(vmf->vma->vm_mm);
    0.00 :   ffff80001020badc:       str     xzr, [x29, #192]
    0.00 :   ffff80001020bae0:       b       ffff80001020b8b4 <__handle_mm_fault+0xc74>
         :                      check_stable_address_space():
         :                      return VM_FAULT_SIGBUS;
    0.00 :   ffff80001020bae4:       mov     w22, #0x2                       // #2
    0.00 :   ffff80001020bae8:       b       ffff80001020b354 <__handle_mm_fault+0x714>
         :                      add_mm_counter():
         :                      long count = atomic_long_add_return(value, &mm->rss_stat.count[member]);
    0.00 :   ffff80001020baec:       add     x2, x0, #0x2c0
         :                      arch_static_branch_jump():
         :                      asm_volatile_goto(
    0.00 :   ffff80001020baf0:       b       ffff80001020bb10 <__handle_mm_fault+0xed0>
    0.00 :   ffff80001020baf4:       b       ffff80001020bb10 <__handle_mm_fault+0xed0>
         :                      __lse_atomic64_add_return():
         :                      }
         :
         :                      ATOMIC64_OP_ADD_RETURN(_relaxed,   )
         :                      ATOMIC64_OP_ADD_RETURN(_acquire,  a, "memory")
         :                      ATOMIC64_OP_ADD_RETURN(_release,  l, "memory")
         :                      ATOMIC64_OP_ADD_RETURN(        , al, "memory")
    0.00 :   ffff80001020baf8:       mov     x1, #0x1                        // #1
    0.00 :   ffff80001020bafc:       ldaddal x1, x3, [x2]
    0.00 :   ffff80001020bb00:       add     x1, x1, x3
    0.00 :   ffff80001020bb04:       b       ffff80001020b310 <__handle_mm_fault+0x6d0>
         :                      check_stable_address_space():
    0.00 :   ffff80001020bb08:       mov     w22, #0x2                       // #2
    0.00 :   ffff80001020bb0c:       b       ffff80001020ba30 <__handle_mm_fault+0xdf0>
         :                      __ll_sc_atomic64_add_return():
         :                      ATOMIC64_FETCH_OP (, dmb ish,  , l, "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(add, add, I)
    0.00 :   ffff80001020bb10:       add     x0, x0, #0x2c0
    0.00 :   ffff80001020bb14:       b       ffff80001020e4cc <copy_huge_page_from_user+0x5e4>
    0.00 :   ffff80001020bb18:       b       ffff80001020b310 <__handle_mm_fault+0x6d0>
 Percent |	Source code & Disassembly of vmlinux for cycles (1 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101376e8 <up_read>:
         :                      up_read():
         :
         :                      /*
         :                      * release a read lock
         :                      */
         :                      void up_read(struct rw_semaphore *sem)
         :                      {
    0.00 :   ffff8000101376e8:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff8000101376ec:       mov     x29, sp
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000101376f0:       b       ffff800010137724 <up_read+0x3c>
    0.00 :   ffff8000101376f4:       b       ffff800010137724 <up_read+0x3c>
         :                      __lse_atomic64_add_return_release():
         :                      return i;                                                       \
         :                      }
         :
         :                      ATOMIC64_OP_ADD_RETURN(_relaxed,   )
         :                      ATOMIC64_OP_ADD_RETURN(_acquire,  a, "memory")
         :                      ATOMIC64_OP_ADD_RETURN(_release,  l, "memory")
    0.00 :   ffff8000101376f8:       mov     x1, #0xffffffffffffff00         // #-256
    0.00 :   ffff8000101376fc:       ldaddl  x1, x2, [x0]
  100.00 :   ffff800010137700:       add     x1, x1, x2
         :                      __up_read():
         :                      if (unlikely((tmp & (RWSEM_LOCK_MASK|RWSEM_FLAG_WAITERS)) ==
    0.00 :   ffff800010137704:       and     x1, x1, #0xffffffffffffff03
    0.00 :   ffff800010137708:       cmp     x1, #0x2
    0.00 :   ffff80001013770c:       b.ne    ffff800010137738 <up_read+0x50>  // b.any
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010137710:       ldr     x1, [x0, #8]
         :                      clear_wr_nonspinnable():
         :                      if (rwsem_test_oflags(sem, RWSEM_WR_NONSPINNABLE))
    0.00 :   ffff800010137714:       tbnz    w1, #2, ffff800010137740 <up_read+0x58>
         :                      __up_read():
         :                      rwsem_wake(sem, tmp);
    0.00 :   ffff800010137718:       bl      ffff8000101375f8 <rwsem_wake.isra.10>
         :                      up_read():
         :                      rwsem_release(&sem->dep_map, _RET_IP_);
         :                      __up_read(sem);
         :                      }
    0.00 :   ffff80001013771c:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010137720:       ret
         :                      __ll_sc_atomic64_add_return_release():
         :                      ATOMIC64_FETCH_OP (, dmb ish,  , l, "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         :                      ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC64_OPS(add, add, I)
    0.00 :   ffff800010137724:       mov     x2, #0xffffffffffffff00         // #-256
    0.00 :   ffff800010137728:       b       ffff800010137f90 <__up_read+0x1a8>
         :                      __up_read():
         :                      if (unlikely((tmp & (RWSEM_LOCK_MASK|RWSEM_FLAG_WAITERS)) ==
    0.00 :   ffff80001013772c:       and     x1, x1, #0xffffffffffffff03
    0.00 :   ffff800010137730:       cmp     x1, #0x2
    0.00 :   ffff800010137734:       b.eq    ffff800010137710 <up_read+0x28>  // b.none
         :                      up_read():
         :                      }
    0.00 :   ffff800010137738:       ldp     x29, x30, [sp], #16
    0.00 :   ffff80001013773c:       ret
         :                      clear_wr_nonspinnable():
         :                      atomic_long_andnot(RWSEM_WR_NONSPINNABLE, &sem->owner);
    0.00 :   ffff800010137740:       add     x2, x0, #0x8
         :                      arch_static_branch_jump():
    0.00 :   ffff800010137744:       b       ffff80001013775c <up_read+0x74>
    0.00 :   ffff800010137748:       b       ffff80001013775c <up_read+0x74>
         :                      __lse_atomic64_andnot():
         :                      ATOMIC64_OP(andnot, stclr)
    0.00 :   ffff80001013774c:       mov     x1, #0x4                        // #4
    0.00 :   ffff800010137750:       stclr   x1, [x2]
         :                      __up_read():
         :                      rwsem_wake(sem, tmp);
    0.00 :   ffff800010137754:       bl      ffff8000101375f8 <rwsem_wake.isra.10>
    0.00 :   ffff800010137758:       b       ffff80001013771c <up_read+0x34>
         :                      __ll_sc_atomic64_andnot():
         :                      /*
         :                      * GAS converts the mysterious and undocumented BIC (immediate) alias to
         :                      * an AND (immediate) instruction with the immediate inverted. We don't
         :                      * have a constraint for this, so fall back to register.
         :                      */
         :                      ATOMIC64_OPS(andnot, bic, )
    0.00 :   ffff80001013775c:       mov     x1, #0x4                        // #4
    0.00 :   ffff800010137760:       add     x4, x0, #0x8
    0.00 :   ffff800010137764:       b       ffff800010137fa8 <__up_read+0x1c0>
         :                      __up_read():
    0.00 :   ffff800010137768:       bl      ffff8000101375f8 <rwsem_wake.isra.10>
    0.00 :   ffff80001013776c:       b       ffff80001013771c <up_read+0x34>
 Percent |	Source code & Disassembly of vmlinux for cycles (1 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010226058 <drain_zone_pages>:
         :                      drain_zone_pages():
         :                      *
         :                      * Note that this function must be called with the thread pinned to
         :                      * a single processor.
         :                      */
         :                      void drain_zone_pages(struct zone *zone, struct per_cpu_pages *pcp)
         :                      {
    0.00 :   ffff800010226058:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff80001022605c:       mov     x29, sp
    0.00 :   ffff800010226060:       str     x19, [sp, #16]
         :                      arch_local_save_flags():
         :                      */
         :                      static inline unsigned long arch_local_save_flags(void)
         :                      {
         :                      unsigned long flags;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010226064:       mrs     x19, daif
         :                      arch_irqs_disabled_flags():
         :
         :                      static inline int arch_irqs_disabled_flags(unsigned long flags)
         :                      {
         :                      int res;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010226068:       and     w2, w19, #0x80
         :                      arch_local_irq_save():
         :
         :                      /*
         :                      * There are too many states with IRQs disabled, just keep the current
         :                      * state if interrupts are already disabled/masked.
         :                      */
         :                      if (!arch_irqs_disabled_flags(flags))
    0.00 :   ffff80001022606c:       cbnz    w2, ffff800010226078 <drain_zone_pages+0x20>
         :                      arch_local_irq_disable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010226070:       mov     x2, #0x60                       // #96
    0.00 :   ffff800010226074:       msr     daifset, #0x2
         :                      drain_zone_pages():
         :                      unsigned long flags;
         :                      int to_drain, batch;
         :
         :                      local_irq_save(flags);
         :                      batch = READ_ONCE(pcp->batch);
         :                      to_drain = min(pcp->count, batch);
    0.00 :   ffff800010226078:       ldr     w3, [x1]
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001022607c:       ldr     w2, [x1, #8]
         :                      drain_zone_pages():
    0.00 :   ffff800010226080:       cmp     w3, w2
    0.00 :   ffff800010226084:       csel    w3, w3, w2, le
         :                      if (to_drain > 0)
    0.00 :   ffff800010226088:       cmp     w3, #0x0
    0.00 :   ffff80001022608c:       b.le    ffff80001022609c <drain_zone_pages+0x44>
    0.00 :   ffff800010226090:       mov     x2, x1
         :                      free_pcppages_bulk(zone, to_drain, pcp);
    0.00 :   ffff800010226094:       mov     w1, w3
    0.00 :   ffff800010226098:       bl      ffff800010224f30 <free_pcppages_bulk>
         :                      arch_local_irq_restore():
         :                      /*
         :                      * restore saved IRQ state
         :                      */
         :                      static inline void arch_local_irq_restore(unsigned long flags)
         :                      {
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff80001022609c:       msr     daif, x19
         :                      drain_zone_pages():
         :                      local_irq_restore(flags);
         :                      }
  100.00 :   ffff8000102260a0:       ldr     x19, [sp, #16]
    0.00 :   ffff8000102260a4:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000102260a8:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (1 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001015e990 <rcu_core_si>:
         :                      rcu_core_si():
         :                      do_nocb_deferred_wakeup(rdp);
         :                      trace_rcu_utilization(TPS("End RCU core"));
         :                      }
         :
         :                      static void rcu_core_si(struct softirq_action *h)
         :                      {
    0.00 :   ffff80001015e990:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff80001015e994:       mov     x29, sp
         :                      rcu_core();
    0.00 :   ffff80001015e998:       bl      ffff80001015e4c8 <rcu_core>
         :                      }
  100.00 :   ffff80001015e99c:       ldp     x29, x30, [sp], #16
    0.00 :   ffff80001015e9a0:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (1 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000105334f8 <pci_pme_list_scan>:
         :                      pci_pme_list_scan():
         :                      return !!(dev->pme_support & (1 << state));
         :                      }
         :                      EXPORT_SYMBOL(pci_pme_capable);
         :
         :                      static void pci_pme_list_scan(struct work_struct *work)
         :                      {
    0.00 :   ffff8000105334f8:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff8000105334fc:       mov     x29, sp
    0.00 :   ffff800010533500:       stp     x20, x21, [sp, #24]
    0.00 :   ffff800010533504:       str     x22, [sp, #40]
         :                      struct pci_pme_device *pme_dev, *n;
         :
         :                      mutex_lock(&pci_pme_list_mutex);
    0.00 :   ffff800010533508:       adrp    x22, ffff800011929000 <pwm_attrs+0x18>
    0.00 :   ffff80001053350c:       add     x21, x22, #0x478
    0.00 :   ffff800010533510:       add     x0, x21, #0x88
    0.00 :   ffff800010533514:       bl      ffff800010caf310 <mutex_lock>
         :                      list_for_each_entry_safe(pme_dev, n, &pci_pme_list, list) {
    0.00 :   ffff800010533518:       ldr     x1, [x21, #168]!
    0.00 :   ffff80001053351c:       cmp     x1, x21
    0.00 :   ffff800010533520:       ldr     x20, [x1]
    0.00 :   ffff800010533524:       b.eq    ffff8000105335c0 <pci_pme_list_scan+0xc8>  // b.none
    0.00 :   ffff800010533528:       stp     x23, x24, [x29, #48]
         :                      list_del():
         :                      }
         :
         :                      static inline void list_del(struct list_head *entry)
         :                      {
         :                      __list_del_entry(entry);
         :                      entry->next = LIST_POISON1;
    0.00 :   ffff80001053352c:       mov     x24, #0x100                     // #256
         :                      entry->prev = LIST_POISON2;
    0.00 :   ffff800010533530:       mov     x23, #0x122                     // #290
         :                      entry->next = LIST_POISON1;
    0.00 :   ffff800010533534:       movk    x24, #0xdead, lsl #48
         :                      entry->prev = LIST_POISON2;
    0.00 :   ffff800010533538:       movk    x23, #0xdead, lsl #48
    0.00 :   ffff80001053353c:       str     x19, [x29, #16]
    0.00 :   ffff800010533540:       b       ffff800010533584 <pci_pme_list_scan+0x8c>
         :                      pci_pme_list_scan():
         :                      if (pme_dev->dev->pme_poll) {
         :                      struct pci_dev *bridge;
         :
         :                      bridge = pme_dev->dev->bus->self;
    0.00 :   ffff800010533544:       ldr     x1, [x19, #16]
         :                      pci_pme_wakeup():
         :                      if (pci_check_pme_status(dev)) {
    0.00 :   ffff800010533548:       mov     x0, x19
         :                      pci_pme_list_scan():
         :                      bridge = pme_dev->dev->bus->self;
    0.00 :   ffff80001053354c:       ldr     x1, [x1, #56]
         :                      /*
         :                      * If bridge is in low power state, the
         :                      * configuration space of subordinate devices
         :                      * may be not accessible
         :                      */
         :                      if (bridge && bridge->current_state != PCI_D0)
    0.00 :   ffff800010533550:       cbz     x1, ffff80001053355c <pci_pme_list_scan+0x64>
    0.00 :   ffff800010533554:       ldr     w1, [x1, #136]
    0.00 :   ffff800010533558:       cbnz    w1, ffff800010533574 <pci_pme_list_scan+0x7c>
         :                      continue;
         :                      /*
         :                      * If the device is in D3cold it should not be
         :                      * polled either.
         :                      */
         :                      if (pme_dev->dev->current_state == PCI_D3cold)
    0.00 :   ffff80001053355c:       ldr     w1, [x19, #136]
    0.00 :   ffff800010533560:       cmp     w1, #0x4
    0.00 :   ffff800010533564:       b.eq    ffff800010533574 <pci_pme_list_scan+0x7c>  // b.none
         :                      pci_pme_wakeup():
         :                      if (pci_check_pme_status(dev)) {
    0.00 :   ffff800010533568:       bl      ffff8000105333d8 <pci_check_pme_status>
    0.00 :   ffff80001053356c:       tst     w0, #0xff
    0.00 :   ffff800010533570:       b.ne    ffff800010533608 <pci_pme_list_scan+0x110>  // b.any
    0.00 :   ffff800010533574:       mov     x1, x20
         :                      pci_pme_list_scan():
         :                      list_for_each_entry_safe(pme_dev, n, &pci_pme_list, list) {
    0.00 :   ffff800010533578:       cmp     x20, x21
    0.00 :   ffff80001053357c:       ldr     x20, [x20]
    0.00 :   ffff800010533580:       b.eq    ffff8000105335b8 <pci_pme_list_scan+0xc0>  // b.none
         :                      if (pme_dev->dev->pme_poll) {
    0.00 :   ffff800010533584:       ldr     x19, [x1, #16]
    0.00 :   ffff800010533588:       ldrb    w0, [x19, #142]
    0.00 :   ffff80001053358c:       tbnz    w0, #5, ffff800010533544 <pci_pme_list_scan+0x4c>
         :                      __list_del_entry():
         :                      __list_del(entry->prev, entry->next);
    0.00 :   ffff800010533590:       ldp     x3, x2, [x1]
         :                      __list_del():
         :                      next->prev = prev;
    0.00 :   ffff800010533594:       str     x2, [x3, #8]
         :                      pci_pme_list_scan():
         :                      continue;
         :
         :                      pci_pme_wakeup(pme_dev->dev, NULL);
         :                      } else {
         :                      list_del(&pme_dev->list);
         :                      kfree(pme_dev);
    0.00 :   ffff800010533598:       mov     x0, x1
         :                      __write_once_size():
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
         :                      case 8: *(volatile __u64 *)p = *(__u64 *)res; break;
    0.00 :   ffff80001053359c:       str     x3, [x2]
         :                      list_del():
         :                      entry->prev = LIST_POISON2;
    0.00 :   ffff8000105335a0:       stp     x24, x23, [x1]
         :                      pci_pme_list_scan():
    0.00 :   ffff8000105335a4:       bl      ffff80001024fe88 <kfree>
    0.00 :   ffff8000105335a8:       mov     x1, x20
         :                      list_for_each_entry_safe(pme_dev, n, &pci_pme_list, list) {
    0.00 :   ffff8000105335ac:       cmp     x20, x21
    0.00 :   ffff8000105335b0:       ldr     x20, [x20]
    0.00 :   ffff8000105335b4:       b.ne    ffff800010533584 <pci_pme_list_scan+0x8c>  // b.any
    0.00 :   ffff8000105335b8:       ldr     x19, [x29, #16]
    0.00 :   ffff8000105335bc:       ldp     x23, x24, [x29, #48]
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000105335c0:       add     x2, x22, #0x478
    0.00 :   ffff8000105335c4:       mov     x0, x2
    0.00 :   ffff8000105335c8:       ldr     x1, [x0, #168]!
         :                      pci_pme_list_scan():
         :                      }
         :                      }
         :                      if (!list_empty(&pci_pme_list))
    0.00 :   ffff8000105335cc:       cmp     x1, x0
    0.00 :   ffff8000105335d0:       b.eq    ffff8000105335ec <pci_pme_list_scan+0xf4>  // b.none
         :                      queue_delayed_work(system_freezable_wq, &pci_pme_work,
    0.00 :   ffff8000105335d4:       adrp    x1, ffff80001189a000 <__per_cpu_offset+0x718>
         :                      queue_delayed_work():
         :                      */
         :                      static inline bool queue_delayed_work(struct workqueue_struct *wq,
         :                      struct delayed_work *dwork,
         :                      unsigned long delay)
         :                      {
         :                      return queue_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);
    0.00 :   ffff8000105335d8:       mov     x3, #0xfa                       // #250
    0.00 :   ffff8000105335dc:       add     x2, x2, #0xb8
    0.00 :   ffff8000105335e0:       mov     w0, #0x100                      // #256
    0.00 :   ffff8000105335e4:       ldr     x1, [x1, #464]
    0.00 :   ffff8000105335e8:       bl      ffff800010104848 <queue_delayed_work_on>
         :                      pci_pme_list_scan():
         :                      msecs_to_jiffies(PME_TIMEOUT));
         :                      mutex_unlock(&pci_pme_list_mutex);
    0.00 :   ffff8000105335ec:       add     x0, x22, #0x478
    0.00 :   ffff8000105335f0:       add     x0, x0, #0x88
    0.00 :   ffff8000105335f4:       bl      ffff800010caec10 <mutex_unlock>
         :                      }
    0.00 :   ffff8000105335f8:       ldp     x20, x21, [sp, #24]
    0.00 :   ffff8000105335fc:       ldr     x22, [sp, #40]
  100.00 :   ffff800010533600:       ldp     x29, x30, [sp], #64
    0.00 :   ffff800010533604:       ret
         :                      pci_wakeup_event():
         :                      void pci_bridge_wait_for_secondary_bus(struct pci_dev *dev);
         :
         :                      static inline void pci_wakeup_event(struct pci_dev *dev)
         :                      {
         :                      /* Wait 100 ms before the system can be put into a sleep state. */
         :                      pm_wakeup_event(&dev->dev, 100);
    0.00 :   ffff800010533608:       add     x19, x19, #0xb0
         :                      pm_wakeup_event():
         :                      return pm_wakeup_ws_event(ws, msec, false);
         :                      }
         :
         :                      static inline void pm_wakeup_event(struct device *dev, unsigned int msec)
         :                      {
         :                      return pm_wakeup_dev_event(dev, msec, false);
    0.00 :   ffff80001053360c:       mov     w2, #0x0                        // #0
    0.00 :   ffff800010533610:       mov     x0, x19
    0.00 :   ffff800010533614:       mov     w1, #0x64                       // #100
    0.00 :   ffff800010533618:       bl      ffff80001072f4a0 <pm_wakeup_dev_event>
         :                      pm_request_resume():
         :                      return __pm_runtime_idle(dev, RPM_ASYNC);
         :                      }
         :
         :                      static inline int pm_request_resume(struct device *dev)
         :                      {
         :                      return __pm_runtime_resume(dev, RPM_ASYNC);
    0.00 :   ffff80001053361c:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010533620:       mov     x0, x19
    0.00 :   ffff800010533624:       bl      ffff8000107295d0 <__pm_runtime_resume>
    0.00 :   ffff800010533628:       b       ffff800010533574 <pci_pme_list_scan+0x7c>
 Percent |	Source code & Disassembly of vmlinux for cycles (1 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101fd338 <should_failslab>:
         :                      should_failslab():
         :                      int should_failslab(struct kmem_cache *s, gfp_t gfpflags)
         :                      {
         :                      if (__should_failslab(s, gfpflags))
         :                      return -ENOMEM;
         :                      return 0;
         :                      }
  100.00 :   ffff8000101fd338:       mov     w0, #0x0                        // #0
    0.00 :   ffff8000101fd33c:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (1 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010463ef0 <blk_mq_queue_tag_busy_iter>:
         :                      blk_mq_queue_tag_busy_iter():
         :                      * called for all requests on all queues that share that tag set and not only
         :                      * for requests associated with @q.
         :                      */
         :                      void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
         :                      void *priv)
         :                      {
    0.00 :   ffff800010463ef0:       stp     x29, x30, [sp, #-160]!
    0.00 :   ffff800010463ef4:       mov     x29, sp
    0.00 :   ffff800010463ef8:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010463efc:       stp     x21, x22, [sp, #32]
    0.00 :   ffff800010463f00:       stp     x23, x24, [sp, #48]
    0.00 :   ffff800010463f04:       stp     x25, x26, [sp, #64]
    0.00 :   ffff800010463f08:       mov     x25, x0
    0.00 :   ffff800010463f0c:       stp     x27, x28, [sp, #80]
    0.00 :   ffff800010463f10:       adrp    x0, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010463f14:       add     x0, x0, #0x8c8
    0.00 :   ffff800010463f18:       stp     x2, x1, [x29, #96]
    0.00 :   ffff800010463f1c:       ldr     x1, [x0]
    0.00 :   ffff800010463f20:       str     x1, [x29, #152]
    0.00 :   ffff800010463f24:       mov     x1, #0x0                        // #0
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff800010463f28:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010463f2c:       ldr     x0, [x25, #1480]
         :                      __ref_is_percpu():
         :                      * Theoretically, the following could test just ATOMIC; however,
         :                      * then we'd have to mask off DEAD separately as DEAD may be
         :                      * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
         :                      * implies ATOMIC anyway.  Test them together.
         :                      */
         :                      if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff800010463f30:       tst     x0, #0x3
    0.00 :   ffff800010463f34:       b.ne    ffff8000104641b0 <blk_mq_queue_tag_busy_iter+0x2c0>  // b.any
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800010463f38:       mrs     x1, sp_el0
         :                      __read_once_size():
    0.00 :   ffff800010463f3c:       ldr     w2, [x1, #16]
         :                      __preempt_count_add():
         :                      }
         :
         :                      static inline void __preempt_count_add(int val)
         :                      {
         :                      u32 pc = READ_ONCE(current_thread_info()->preempt.count);
         :                      pc += val;
    0.00 :   ffff800010463f40:       add     w2, w2, #0x1
         :                      __write_once_size():
         :                      static __always_inline void __write_once_size(volatile void *p, void *res, int size)
         :                      {
         :                      switch (size) {
         :                      case 1: *(volatile __u8 *)p = *(__u8 *)res; break;
         :                      case 2: *(volatile __u16 *)p = *(__u16 *)res; break;
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010463f44:       str     w2, [x1, #16]
         :                      __percpu_add_case_64():
         :
         :                      PERCPU_RW_OPS(8)
         :                      PERCPU_RW_OPS(16)
         :                      PERCPU_RW_OPS(32)
         :                      PERCPU_RW_OPS(64)
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff800010463f48:       mov     x3, #0x1                        // #1
         :                      __my_cpu_offset():
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800010463f4c:       mrs     x2, tpidr_el1
         :                      __percpu_add_case_64():
         :                      PERCPU_OP(add, add, stadd)
    0.00 :   ffff800010463f50:       add     x0, x0, x2
    0.00 :   ffff800010463f54:       ldxr    x5, [x0]
    0.00 :   ffff800010463f58:       add     x5, x5, x3
    0.00 :   ffff800010463f5c:       stxr    w4, x5, [x0]
    0.00 :   ffff800010463f60:       cbnz    w4, ffff800010463f54 <blk_mq_queue_tag_busy_iter+0x64>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010463f64:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      {
         :                      struct thread_info *ti = current_thread_info();
         :                      u64 pc = READ_ONCE(ti->preempt_count);
         :
         :                      /* Update only the count field, leaving need_resched unchanged */
         :                      WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800010463f68:       sub     x0, x0, #0x1
         :                      __write_once_size():
         :                      case 4: *(volatile __u32 *)p = *(__u32 *)res; break;
    0.00 :   ffff800010463f6c:       str     w0, [x1, #16]
         :                      __preempt_count_dec_and_test():
         :                      * need of a reschedule. Otherwise, we need to reload the
         :                      * preempt_count in case the need_resched flag was cleared by an
         :                      * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         :                      * pair.
         :                      */
         :                      return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800010463f70:       cbnz    x0, ffff8000104641a4 <blk_mq_queue_tag_busy_iter+0x2b4>
         :                      percpu_ref_tryget():
         :                      bool ret;
         :
         :                      rcu_read_lock();
         :
         :                      if (__ref_is_percpu(ref, &percpu_count)) {
         :                      this_cpu_inc(*percpu_count);
    0.00 :   ffff800010463f74:       bl      ffff800010cad668 <preempt_schedule_notrace>
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.00 :   ffff800010463f78:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      blk_mq_queue_tag_busy_iter():
         :                      * below.
         :                      */
         :                      if (!percpu_ref_tryget(&q->q_usage_counter))
         :                      return;
         :
         :                      queue_for_each_hw_ctx(q, hctx, i) {
    0.00 :   ffff800010463f7c:       mov     w21, #0x0                       // #0
    0.00 :   ffff800010463f80:       ldr     w0, [x25, #80]
    0.00 :   ffff800010463f84:       cbnz    w0, ffff800010463f98 <blk_mq_queue_tag_busy_iter+0xa8>
    0.00 :   ffff800010463f88:       b       ffff800010464048 <blk_mq_queue_tag_busy_iter+0x158>
    0.00 :   ffff800010463f8c:       add     w21, w21, #0x1
    0.00 :   ffff800010463f90:       cmp     w0, w21
    0.00 :   ffff800010463f94:       b.ls    ffff800010464048 <blk_mq_queue_tag_busy_iter+0x158>  // b.plast
    0.00 :   ffff800010463f98:       ldr     x1, [x25, #72]
    0.00 :   ffff800010463f9c:       ldr     x23, [x1, w21, sxtw #3]
         :                      blk_mq_hw_queue_mapped():
         :                      return test_bit(BLK_MQ_S_STOPPED, &hctx->state);
         :                      }
         :
         :                      static inline bool blk_mq_hw_queue_mapped(struct blk_mq_hw_ctx *hctx)
         :                      {
         :                      return hctx->nr_ctx && hctx->tags;
    0.00 :   ffff800010463fa0:       ldrh    w1, [x23, #270]
         :                      blk_mq_queue_tag_busy_iter():
         :                      struct blk_mq_tags *tags = hctx->tags;
    0.00 :   ffff800010463fa4:       ldr     x22, [x23, #336]
         :                      blk_mq_hw_queue_mapped():
    0.00 :   ffff800010463fa8:       cmp     w1, #0x0
    0.00 :   ffff800010463fac:       ccmp    x22, #0x0, #0x4, ne  // ne = any
    0.00 :   ffff800010463fb0:       b.eq    ffff800010463f8c <blk_mq_queue_tag_busy_iter+0x9c>  // b.none
         :                      blk_mq_queue_tag_busy_iter():
         :                      * hardware queue, there's nothing to check
         :                      */
         :                      if (!blk_mq_hw_queue_mapped(hctx))
         :                      continue;
         :
         :                      if (tags->nr_reserved_tags)
    0.00 :   ffff800010463fb4:       ldr     w0, [x22, #4]
    0.00 :   ffff800010463fb8:       cbnz    w0, ffff8000104640d4 <blk_mq_queue_tag_busy_iter+0x1e4>
         :                      bt_for_each():
         :                      struct bt_iter_data iter_data = {
    0.00 :   ffff800010463fbc:       ldr     x0, [x29, #104]
         :                      sbitmap_for_each_set(&bt->sb, bt_iter, &iter_data);
    0.00 :   ffff800010463fc0:       add     x27, x22, #0x10
         :                      struct bt_iter_data iter_data = {
    0.00 :   ffff800010463fc4:       stp     x23, x0, [x29, #120]
         :                      __sbitmap_for_each_set():
         :                      unsigned int nr;
         :                      unsigned int scanned = 0;
         :
         :                      if (start >= sb->depth)
         :                      start = 0;
         :                      index = SB_NR_TO_INDEX(sb, start);
    0.00 :   ffff800010463fc8:       mov     w20, #0x0                       // #0
         :                      bt_for_each():
    0.00 :   ffff800010463fcc:       ldr     x0, [x29, #96]
         :                      __sbitmap_for_each_set():
         :                      nr = SB_NR_TO_BIT(sb, start);
         :
         :                      while (scanned < sb->depth) {
         :                      unsigned long word;
         :                      unsigned int depth = min_t(unsigned int,
    0.00 :   ffff800010463fd0:       mov     w24, #0xc0                      // #192
         :                      bt_for_each():
    0.00 :   ffff800010463fd4:       str     x0, [x29, #136]
         :                      __sbitmap_for_each_set():
         :                      unsigned int scanned = 0;
    0.00 :   ffff800010463fd8:       mov     w23, #0x0                       // #0
         :                      bt_for_each():
    0.00 :   ffff800010463fdc:       strb    wzr, [x29, #144]
         :                      __sbitmap_for_each_set():
         :                      while (scanned < sb->depth) {
    0.00 :   ffff800010463fe0:       ldr     w19, [x22, #16]
    0.00 :   ffff800010463fe4:       cbz     w19, ffff800010464038 <blk_mq_queue_tag_busy_iter+0x148>
         :                      unsigned int depth = min_t(unsigned int,
    0.00 :   ffff800010463fe8:       umull   x0, w20, w24
    0.00 :   ffff800010463fec:       ldr     x1, [x27, #16]
    0.00 :   ffff800010463ff0:       sub     w19, w19, w23
    0.00 :   ffff800010463ff4:       add     x2, x1, x0
  100.00 :   ffff800010463ff8:       ldr     x1, [x1, x0]
         :                      sb->map[index].depth - nr,
         :                      sb->depth - scanned);
         :
         :                      scanned += depth;
         :                      word = sb->map[index].word & ~sb->map[index].cleared;
    0.00 :   ffff800010463ffc:       ldr     x4, [x2, #64]
    0.00 :   ffff800010464000:       ldr     x0, [x2, #128]
         :                      unsigned int depth = min_t(unsigned int,
    0.00 :   ffff800010464004:       cmp     w19, w1
    0.00 :   ffff800010464008:       csel    w19, w19, w1, cc  // cc = lo, ul, last
         :                      word = sb->map[index].word & ~sb->map[index].cleared;
    0.00 :   ffff80001046400c:       bic     x0, x4, x0
    0.00 :   ffff800010464010:       str     x0, [x29, #112]
         :                      scanned += depth;
    0.00 :   ffff800010464014:       add     w23, w23, w19
         :                      if (!word)
    0.00 :   ffff800010464018:       cbnz    x0, ffff800010464084 <blk_mq_queue_tag_busy_iter+0x194>
         :
         :                      nr++;
         :                      }
         :                      next:
         :                      nr = 0;
         :                      if (++index >= sb->map_nr)
    0.00 :   ffff80001046401c:       ldr     w0, [x27, #8]
    0.00 :   ffff800010464020:       add     w20, w20, #0x1
         :                      while (scanned < sb->depth) {
    0.00 :   ffff800010464024:       ldr     w19, [x27]
         :                      index = 0;
    0.00 :   ffff800010464028:       cmp     w20, w0
    0.00 :   ffff80001046402c:       csel    w20, w20, wzr, cc  // cc = lo, ul, last
         :                      while (scanned < sb->depth) {
    0.00 :   ffff800010464030:       cmp     w19, w23
    0.00 :   ffff800010464034:       b.hi    ffff800010463fe8 <blk_mq_queue_tag_busy_iter+0xf8>  // b.pmore
    0.00 :   ffff800010464038:       ldr     w0, [x25, #80]
         :                      blk_mq_queue_tag_busy_iter():
         :                      queue_for_each_hw_ctx(q, hctx, i) {
    0.00 :   ffff80001046403c:       add     w21, w21, #0x1
    0.00 :   ffff800010464040:       cmp     w0, w21
    0.00 :   ffff800010464044:       b.hi    ffff800010463f98 <blk_mq_queue_tag_busy_iter+0xa8>  // b.pmore
         :                      bt_for_each(hctx, &tags->breserved_tags, fn, priv, true);
         :                      bt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);
         :                      }
         :                      blk_queue_exit(q);
    0.00 :   ffff800010464048:       mov     x0, x25
    0.00 :   ffff80001046404c:       bl      ffff800010455110 <blk_queue_exit>
         :                      }
    0.00 :   ffff800010464050:       adrp    x0, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010464054:       add     x0, x0, #0x8c8
    0.00 :   ffff800010464058:       ldr     x1, [x29, #152]
    0.00 :   ffff80001046405c:       ldr     x0, [x0]
    0.00 :   ffff800010464060:       eor     x0, x1, x0
    0.00 :   ffff800010464064:       cbnz    x0, ffff8000104641fc <blk_mq_queue_tag_busy_iter+0x30c>
    0.00 :   ffff800010464068:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001046406c:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010464070:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff800010464074:       ldp     x25, x26, [sp, #64]
    0.00 :   ffff800010464078:       ldp     x27, x28, [sp, #80]
    0.00 :   ffff80001046407c:       ldp     x29, x30, [sp], #160
    0.00 :   ffff800010464080:       ret
    0.00 :   ffff800010464084:       mov     w22, w19
         :                      __sbitmap_for_each_set():
         :                      if (!word)
    0.00 :   ffff800010464088:       mov     x2, #0x0                        // #0
    0.00 :   ffff80001046408c:       b       ffff800010464094 <blk_mq_queue_tag_busy_iter+0x1a4>
         :                      nr++;
    0.00 :   ffff800010464090:       add     w2, w26, #0x1
         :                      nr = find_next_bit(&word, depth, nr);
    0.00 :   ffff800010464094:       mov     x1, x22
    0.00 :   ffff800010464098:       add     x0, x29, #0x70
    0.00 :   ffff80001046409c:       bl      ffff800010487db8 <find_next_bit>
    0.00 :   ffff8000104640a0:       mov     x26, x0
         :                      if (nr >= depth)
    0.00 :   ffff8000104640a4:       cmp     w19, w0
    0.00 :   ffff8000104640a8:       b.ls    ffff80001046401c <blk_mq_queue_tag_busy_iter+0x12c>  // b.plast
         :                      if (!fn(sb, (index << sb->shift) + nr, data))
    0.00 :   ffff8000104640ac:       ldr     w1, [x27, #4]
    0.00 :   ffff8000104640b0:       add     x2, x29, #0x78
    0.00 :   ffff8000104640b4:       mov     x0, x27
    0.00 :   ffff8000104640b8:       lsl     w1, w20, w1
    0.00 :   ffff8000104640bc:       add     w1, w1, w26
    0.00 :   ffff8000104640c0:       bl      ffff800010463598 <bt_iter>
    0.00 :   ffff8000104640c4:       tst     w0, #0xff
    0.00 :   ffff8000104640c8:       b.ne    ffff800010464090 <blk_mq_queue_tag_busy_iter+0x1a0>  // b.any
    0.00 :   ffff8000104640cc:       ldr     w0, [x25, #80]
    0.00 :   ffff8000104640d0:       b       ffff80001046403c <blk_mq_queue_tag_busy_iter+0x14c>
         :                      bt_for_each():
         :                      struct bt_iter_data iter_data = {
    0.00 :   ffff8000104640d4:       ldr     x0, [x29, #104]
         :                      sbitmap_for_each_set(&bt->sb, bt_iter, &iter_data);
    0.00 :   ffff8000104640d8:       add     x27, x22, #0x50
         :                      struct bt_iter_data iter_data = {
    0.00 :   ffff8000104640dc:       stp     x23, x0, [x29, #120]
         :                      __sbitmap_for_each_set():
         :                      unsigned int scanned = 0;
    0.00 :   ffff8000104640e0:       mov     w24, #0x0                       // #0
         :                      bt_for_each():
    0.00 :   ffff8000104640e4:       ldr     x0, [x29, #96]
         :                      __sbitmap_for_each_set():
         :                      index = SB_NR_TO_INDEX(sb, start);
    0.00 :   ffff8000104640e8:       mov     w20, #0x0                       // #0
         :                      bt_for_each():
    0.00 :   ffff8000104640ec:       str     x0, [x29, #136]
    0.00 :   ffff8000104640f0:       mov     w0, #0x1                        // #1
    0.00 :   ffff8000104640f4:       strb    w0, [x29, #144]
         :                      __sbitmap_for_each_set():
         :                      while (scanned < sb->depth) {
    0.00 :   ffff8000104640f8:       ldr     w19, [x22, #80]
    0.00 :   ffff8000104640fc:       cbz     w19, ffff800010463fbc <blk_mq_queue_tag_busy_iter+0xcc>
         :                      unsigned int depth = min_t(unsigned int,
    0.00 :   ffff800010464100:       mov     w0, #0xc0                       // #192
    0.00 :   ffff800010464104:       ldr     x1, [x27, #16]
    0.00 :   ffff800010464108:       sub     w19, w19, w24
    0.00 :   ffff80001046410c:       umull   x0, w20, w0
    0.00 :   ffff800010464110:       add     x2, x1, x0
    0.00 :   ffff800010464114:       ldr     x1, [x1, x0]
         :                      word = sb->map[index].word & ~sb->map[index].cleared;
    0.00 :   ffff800010464118:       ldr     x3, [x2, #64]
    0.00 :   ffff80001046411c:       ldr     x0, [x2, #128]
         :                      unsigned int depth = min_t(unsigned int,
    0.00 :   ffff800010464120:       cmp     w19, w1
    0.00 :   ffff800010464124:       csel    w19, w19, w1, cc  // cc = lo, ul, last
         :                      word = sb->map[index].word & ~sb->map[index].cleared;
    0.00 :   ffff800010464128:       bic     x0, x3, x0
    0.00 :   ffff80001046412c:       str     x0, [x29, #112]
         :                      scanned += depth;
    0.00 :   ffff800010464130:       add     w24, w24, w19
         :                      if (!word)
    0.00 :   ffff800010464134:       cbnz    x0, ffff800010464158 <blk_mq_queue_tag_busy_iter+0x268>
         :                      if (++index >= sb->map_nr)
    0.00 :   ffff800010464138:       ldr     w0, [x27, #8]
    0.00 :   ffff80001046413c:       add     w20, w20, #0x1
         :                      while (scanned < sb->depth) {
    0.00 :   ffff800010464140:       ldr     w19, [x27]
         :                      index = 0;
    0.00 :   ffff800010464144:       cmp     w20, w0
    0.00 :   ffff800010464148:       csel    w20, w20, wzr, cc  // cc = lo, ul, last
         :                      while (scanned < sb->depth) {
    0.00 :   ffff80001046414c:       cmp     w19, w24
    0.00 :   ffff800010464150:       b.hi    ffff800010464100 <blk_mq_queue_tag_busy_iter+0x210>  // b.pmore
    0.00 :   ffff800010464154:       b       ffff800010463fbc <blk_mq_queue_tag_busy_iter+0xcc>
    0.00 :   ffff800010464158:       mov     w26, w19
         :                      if (!word)
    0.00 :   ffff80001046415c:       mov     x2, #0x0                        // #0
    0.00 :   ffff800010464160:       b       ffff800010464188 <blk_mq_queue_tag_busy_iter+0x298>
         :                      if (!fn(sb, (index << sb->shift) + nr, data))
    0.00 :   ffff800010464164:       ldr     w1, [x27, #4]
    0.00 :   ffff800010464168:       add     x2, x29, #0x78
    0.00 :   ffff80001046416c:       mov     x0, x27
    0.00 :   ffff800010464170:       lsl     w1, w20, w1
    0.00 :   ffff800010464174:       add     w1, w1, w28
    0.00 :   ffff800010464178:       bl      ffff800010463598 <bt_iter>
    0.00 :   ffff80001046417c:       tst     w0, #0xff
    0.00 :   ffff800010464180:       b.eq    ffff800010463fbc <blk_mq_queue_tag_busy_iter+0xcc>  // b.none
         :                      nr++;
    0.00 :   ffff800010464184:       add     w2, w28, #0x1
         :                      nr = find_next_bit(&word, depth, nr);
    0.00 :   ffff800010464188:       mov     x1, x26
    0.00 :   ffff80001046418c:       add     x0, x29, #0x70
    0.00 :   ffff800010464190:       bl      ffff800010487db8 <find_next_bit>
    0.00 :   ffff800010464194:       mov     x28, x0
         :                      if (nr >= depth)
    0.00 :   ffff800010464198:       cmp     w19, w0
    0.00 :   ffff80001046419c:       b.hi    ffff800010464164 <blk_mq_queue_tag_busy_iter+0x274>  // b.pmore
    0.00 :   ffff8000104641a0:       b       ffff800010464138 <blk_mq_queue_tag_busy_iter+0x248>
         :                      __read_once_size():
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff8000104641a4:       ldr     x0, [x1, #16]
         :                      __preempt_count_dec_and_test():
    0.00 :   ffff8000104641a8:       cbz     x0, ffff800010463f74 <blk_mq_queue_tag_busy_iter+0x84>
    0.00 :   ffff8000104641ac:       b       ffff800010463f78 <blk_mq_queue_tag_busy_iter+0x88>
         :                      __read_once_size():
    0.00 :   ffff8000104641b0:       ldr     x3, [x25, #1472]
         :                      atomic64_fetch_add_unless():
         :                      atomic64_fetch_add_unless(atomic64_t *v, s64 a, s64 u)
         :                      {
         :                      s64 c = atomic64_read(v);
         :
         :                      do {
         :                      if (unlikely(c == u))
    0.00 :   ffff8000104641b4:       cbz     x3, ffff8000104641f4 <blk_mq_queue_tag_busy_iter+0x304>
    0.00 :   ffff8000104641b8:       add     x4, x25, #0x5c0
         :                      break;
         :                      } while (!atomic64_try_cmpxchg(v, &c, c + a));
    0.00 :   ffff8000104641bc:       add     x2, x3, #0x1
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff8000104641c0:       b       ffff8000104641e0 <blk_mq_queue_tag_busy_iter+0x2f0>
    0.00 :   ffff8000104641c4:       b       ffff8000104641e0 <blk_mq_queue_tag_busy_iter+0x2f0>
         :                      __lse__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,  l, "memory")
         :                      __CMPXCHG_CASE(x,  , rel_, 64,  l, "memory")
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, al, "memory")
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, al, "memory")
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, al, "memory")
         :                      __CMPXCHG_CASE(x,  ,  mb_, 64, al, "memory")
    0.00 :   ffff8000104641c8:       mov     x0, x4
    0.00 :   ffff8000104641cc:       mov     x1, x3
    0.00 :   ffff8000104641d0:       mov     x5, x1
    0.00 :   ffff8000104641d4:       casal   x5, x2, [x4]
    0.00 :   ffff8000104641d8:       mov     x0, x5
    0.00 :   ffff8000104641dc:       b       ffff8000104641e4 <blk_mq_queue_tag_busy_iter+0x2f4>
         :                      __ll_sc__cmpxchg_case_mb_64():
         :                      __CMPXCHG_CASE(w,  , rel_, 32,        ,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  , rel_, 64,        ,  , l, "memory", L)
         :                      __CMPXCHG_CASE(w, b,  mb_,  8, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w, h,  mb_, 16, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE(w,  ,  mb_, 32, dmb ish,  , l, "memory", K)
         :                      __CMPXCHG_CASE( ,  ,  mb_, 64, dmb ish,  , l, "memory", L)
    0.00 :   ffff8000104641e0:       b       ffff800010464550 <blk_mq_tag_update_depth+0x1d0>
         :                      atomic64_try_cmpxchg():
         :                      if (unlikely(r != o))
    0.00 :   ffff8000104641e4:       cmp     x0, x3
    0.00 :   ffff8000104641e8:       b.eq    ffff800010463f78 <blk_mq_queue_tag_busy_iter+0x88>  // b.none
    0.00 :   ffff8000104641ec:       mov     x3, x0
         :                      atomic64_fetch_add_unless():
         :                      if (unlikely(c == u))
    0.00 :   ffff8000104641f0:       cbnz    x0, ffff8000104641bc <blk_mq_queue_tag_busy_iter+0x2cc>
         :                      rcu_read_unlock():
    0.00 :   ffff8000104641f4:       bl      ffff80001015eee0 <__rcu_read_unlock>
    0.00 :   ffff8000104641f8:       b       ffff800010464050 <blk_mq_queue_tag_busy_iter+0x160>
         :                      blk_mq_queue_tag_busy_iter():
         :                      }
    0.00 :   ffff8000104641fc:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (1 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001010bf30 <kthread_data>:
         :                      to_kthread():
         :                      current->set_child_tid = (__force void __user *)kthread;
         :                      }
         :
         :                      static inline struct kthread *to_kthread(struct task_struct *k)
         :                      {
         :                      WARN_ON(!(k->flags & PF_KTHREAD));
    0.00 :   ffff80001010bf30:       ldr     w1, [x0, #44]
    0.00 :   ffff80001010bf34:       tbz     w1, #21, ffff80001010bf44 <kthread_data+0x14>
         :                      kthread_data():
         :                      * The caller is responsible for ensuring the validity of @task when
         :                      * calling this function.
         :                      */
         :                      void *kthread_data(struct task_struct *task)
         :                      {
         :                      return to_kthread(task)->data;
    0.00 :   ffff80001010bf38:       ldr     x0, [x0, #1344]
         :                      }
    0.00 :   ffff80001010bf3c:       ldr     x0, [x0, #16]
  100.00 :   ffff80001010bf40:       ret
         :                      to_kthread():
         :                      WARN_ON(!(k->flags & PF_KTHREAD));
    0.00 :   ffff80001010bf44:       brk     #0x800
         :                      kthread_data():
         :                      return to_kthread(task)->data;
    0.00 :   ffff80001010bf48:       ldr     x0, [x0, #1344]
         :                      }
    0.00 :   ffff80001010bf4c:       ldr     x0, [x0, #16]
    0.00 :   ffff80001010bf50:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (1 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010086930 <el1_sync_handler>:
         :                      el1_sync_handler():
         :                      do_debug_exception(far, esr, regs);
         :                      }
         :                      NOKPROBE_SYMBOL(el1_dbg);
         :
         :                      asmlinkage void notrace el1_sync_handler(struct pt_regs *regs)
         :                      {
    0.00 :   ffff800010086930:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff800010086934:       mov     x29, sp
         :                      unsigned long esr = read_sysreg(esr_el1);
    0.00 :   ffff800010086938:       mrs     x3, esr_el1
         :
         :                      switch (ESR_ELx_EC(esr)) {
    0.00 :   ffff80001008693c:       lsr     w1, w3, #26
    0.00 :   ffff800010086940:       cmp     x1, #0x25
    0.00 :   ffff800010086944:       b.eq    ffff8000100869e4 <el1_sync_handler+0xb4>  // b.none
    0.00 :   ffff800010086948:       b.hi    ffff800010086990 <el1_sync_handler+0x60>  // b.pmore
    0.00 :   ffff80001008694c:       cmp     x1, #0x18
    0.00 :   ffff800010086950:       b.eq    ffff8000100869cc <el1_sync_handler+0x9c>  // b.none
    0.00 :   ffff800010086954:       b.ls    ffff8000100869c8 <el1_sync_handler+0x98>  // b.plast
    0.00 :   ffff800010086958:       cmp     x1, #0x21
    0.00 :   ffff80001008695c:       b.eq    ffff8000100869e4 <el1_sync_handler+0xb4>  // b.none
    0.00 :   ffff800010086960:       cmp     x1, #0x22
    0.00 :   ffff800010086964:       b.ne    ffff800010086a18 <el1_sync_handler+0xe8>  // b.any
         :                      el1_pc():
         :                      unsigned long far = read_sysreg(far_el1);
    0.00 :   ffff800010086968:       mrs     x4, far_el1
         :                      local_daif_inherit():
         :                      * Called by synchronous exception handlers to restore the DAIF bits that were
         :                      * modified by taking an exception.
         :                      */
         :                      static inline void local_daif_inherit(struct pt_regs *regs)
         :                      {
         :                      unsigned long flags = regs->pstate & DAIF_MASK;
    0.00 :   ffff80001008696c:       ldr     x1, [x0, #264]
    0.00 :   ffff800010086970:       and     x1, x1, #0x3c0
         :                      /*
         :                      * We can't use local_daif_restore(regs->pstate) here as
         :                      * system_has_prio_mask_debugging() won't restore the I bit if it can
         :                      * use the pmr instead.
         :                      */
         :                      write_sysreg(flags, daif);
    0.00 :   ffff800010086974:       msr     daif, x1
         :                      el1_pc():
         :                      do_sp_pc_abort(far, esr, regs);
    0.00 :   ffff800010086978:       mov     x2, x0
    0.00 :   ffff80001008697c:       mov     w1, w3
    0.00 :   ffff800010086980:       mov     x0, x4
    0.00 :   ffff800010086984:       bl      ffff8000100a1ca8 <do_sp_pc_abort>
         :                      el1_sync_handler():
         :                      el1_dbg(regs, esr);
         :                      break;
         :                      default:
         :                      el1_inv(regs, esr);
         :                      };
         :                      }
    0.00 :   ffff800010086988:       ldp     x29, x30, [sp], #16
    0.00 :   ffff80001008698c:       ret
         :                      switch (ESR_ELx_EC(esr)) {
    0.00 :   ffff800010086990:       cmp     x1, #0x33
    0.00 :   ffff800010086994:       b.eq    ffff8000100869ac <el1_sync_handler+0x7c>  // b.none
    0.00 :   ffff800010086998:       b.ls    ffff800010086a10 <el1_sync_handler+0xe0>  // b.plast
    0.00 :   ffff80001008699c:       cmp     x1, #0x35
    0.00 :   ffff8000100869a0:       b.eq    ffff8000100869ac <el1_sync_handler+0x7c>  // b.none
    0.00 :   ffff8000100869a4:       cmp     x1, #0x3c
    0.00 :   ffff8000100869a8:       b.ne    ffff800010086a18 <el1_sync_handler+0xe8>  // b.any
         :                      el1_dbg():
         :                      unsigned long far = read_sysreg(far_el1);
    0.00 :   ffff8000100869ac:       mrs     x4, far_el1
         :                      do_debug_exception(far, esr, regs);
    0.00 :   ffff8000100869b0:       mov     x2, x0
    0.00 :   ffff8000100869b4:       mov     w1, w3
    0.00 :   ffff8000100869b8:       mov     x0, x4
    0.00 :   ffff8000100869bc:       bl      ffff8000100a1ce0 <do_debug_exception>
         :                      el1_sync_handler():
         :                      }
    0.00 :   ffff8000100869c0:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000100869c4:       ret
         :                      switch (ESR_ELx_EC(esr)) {
    0.00 :   ffff8000100869c8:       cbnz    x1, ffff800010086a18 <el1_sync_handler+0xe8>
         :                      local_daif_inherit():
         :                      unsigned long flags = regs->pstate & DAIF_MASK;
    0.00 :   ffff8000100869cc:       ldr     x1, [x0, #264]
    0.00 :   ffff8000100869d0:       and     x1, x1, #0x3c0
         :                      write_sysreg(flags, daif);
    0.00 :   ffff8000100869d4:       msr     daif, x1
         :                      el1_undef():
         :                      do_undefinstr(regs);
    0.00 :   ffff8000100869d8:       bl      ffff80001008e998 <do_undefinstr>
         :                      el1_sync_handler():
         :                      }
    0.00 :   ffff8000100869dc:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000100869e0:       ret
         :                      el1_abort():
         :                      unsigned long far = read_sysreg(far_el1);
    0.00 :   ffff8000100869e4:       mrs     x4, far_el1
         :                      local_daif_inherit():
         :                      unsigned long flags = regs->pstate & DAIF_MASK;
    0.00 :   ffff8000100869e8:       ldr     x1, [x0, #264]
    0.00 :   ffff8000100869ec:       and     x1, x1, #0x3c0
         :                      write_sysreg(flags, daif);
    0.00 :   ffff8000100869f0:       msr     daif, x1
         :                      sign_extend64():
         :                      * @index: 0 based bit index (0<=index<64) to sign bit
         :                      */
         :                      static inline __s64 sign_extend64(__u64 value, int index)
         :                      {
         :                      __u8 shift = 63 - index;
         :                      return (__s64)(value << shift) >> shift;
  100.00 :   ffff8000100869f4:       sbfx    x5, x4, #0, #56
         :                      el1_abort():
         :                      do_mem_abort(far, esr, regs);
    0.00 :   ffff8000100869f8:       mov     x2, x0
    0.00 :   ffff8000100869fc:       mov     w1, w3
    0.00 :   ffff800010086a00:       and     x0, x5, x4
    0.00 :   ffff800010086a04:       bl      ffff8000100a1bc0 <do_mem_abort>
         :                      el1_sync_handler():
         :                      }
    0.00 :   ffff800010086a08:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010086a0c:       ret
         :                      switch (ESR_ELx_EC(esr)) {
    0.00 :   ffff800010086a10:       cmp     x1, #0x31
    0.00 :   ffff800010086a14:       b.eq    ffff8000100869ac <el1_sync_handler+0x7c>  // b.none
         :                      local_daif_inherit():
         :                      unsigned long flags = regs->pstate & DAIF_MASK;
    0.00 :   ffff800010086a18:       ldr     x1, [x0, #264]
    0.00 :   ffff800010086a1c:       and     x1, x1, #0x3c0
         :                      write_sysreg(flags, daif);
    0.00 :   ffff800010086a20:       msr     daif, x1
         :                      el1_inv():
         :                      bad_mode(regs, 0, esr);
    0.00 :   ffff800010086a24:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010086a28:       mov     w2, w3
    0.00 :   ffff800010086a2c:       bl      ffff80001008ee70 <bad_mode>
         :                      el1_sync_handler():
         :                      }
    0.00 :   ffff800010086a30:       ldp     x29, x30, [sp], #16
    0.00 :   ffff800010086a34:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (1 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001011ab70 <arch_cpu_idle_exit>:
         :                      arch_cpu_idle_exit():
         :
         :                      return 1;
         :                      }
         :
         :                      /* Weak implementations for optional arch specific functions */
         :                      void __weak arch_cpu_idle_prepare(void) { }
  100.00 :   ffff80001011ab70:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (1 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001029b6e8 <file_remove_privs>:
         :                      file_remove_privs():
         :                      /*
         :                      * Remove special file priviledges (suid, capabilities) when file is written
         :                      * to or truncated.
         :                      */
         :                      int file_remove_privs(struct file *file)
         :                      {
  100.00 :   ffff80001029b6e8:       stp     x29, x30, [sp, #-144]!
    0.00 :   ffff80001029b6ec:       mov     x29, sp
    0.00 :   ffff80001029b6f0:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001029b6f4:       mov     x20, x0
    0.00 :   ffff80001029b6f8:       stp     x21, x22, [sp, #32]
    0.00 :   ffff80001029b6fc:       adrp    x19, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff80001029b700:       add     x0, x19, #0x8c8
         :                      file_dentry():
         :                      return f->f_inode;
         :                      }
         :
         :                      static inline struct dentry *file_dentry(const struct file *file)
         :                      {
         :                      return d_real(file->f_path.dentry, file_inode(file));
    0.00 :   ffff80001029b704:       ldp     x22, x21, [x20, #24]
         :                      file_remove_privs():
    0.00 :   ffff80001029b708:       ldr     x1, [x0]
    0.00 :   ffff80001029b70c:       str     x1, [x29, #136]
    0.00 :   ffff80001029b710:       mov     x1, #0x0                        // #0
         :                      d_real():
         :                      * See also: Documentation/filesystems/vfs.rst
         :                      */
         :                      static inline struct dentry *d_real(struct dentry *dentry,
         :                      const struct inode *inode)
         :                      {
         :                      if (unlikely(dentry->d_flags & DCACHE_OP_REAL))
    0.00 :   ffff80001029b714:       ldr     w0, [x22]
    0.00 :   ffff80001029b718:       tbnz    w0, #26, ffff80001029b7d4 <file_remove_privs+0xec>
         :                      file_remove_privs():
         :                      * Fast path for nothing security related.
         :                      * As well for non-regular files, e.g. blkdev inodes.
         :                      * For example, blkdev_write_iter() might get here
         :                      * trying to remove privs which it is not allowed to.
         :                      */
         :                      if (IS_NOSEC(inode) || !S_ISREG(inode->i_mode))
    0.00 :   ffff80001029b71c:       ldr     w0, [x21, #12]
    0.00 :   ffff80001029b720:       tbnz    w0, #12, ffff80001029b734 <file_remove_privs+0x4c>
    0.00 :   ffff80001029b724:       ldrh    w0, [x21]
    0.00 :   ffff80001029b728:       and     w1, w0, #0xf000
    0.00 :   ffff80001029b72c:       cmp     w1, #0x8, lsl #12
    0.00 :   ffff80001029b730:       b.eq    ffff80001029b75c <file_remove_privs+0x74>  // b.none
         :                      return 0;
    0.00 :   ffff80001029b734:       mov     w0, #0x0                        // #0
         :                      error = __remove_privs(dentry, kill);
         :                      if (!error)
         :                      inode_has_no_xattr(inode);
         :
         :                      return error;
         :                      }
    0.00 :   ffff80001029b738:       add     x19, x19, #0x8c8
    0.00 :   ffff80001029b73c:       ldr     x2, [x29, #136]
    0.00 :   ffff80001029b740:       ldr     x1, [x19]
    0.00 :   ffff80001029b744:       eor     x1, x2, x1
    0.00 :   ffff80001029b748:       cbnz    x1, ffff80001029b7f4 <file_remove_privs+0x10c>
    0.00 :   ffff80001029b74c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001029b750:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff80001029b754:       ldp     x29, x30, [sp], #144
    0.00 :   ffff80001029b758:       ret
         :                      dentry_needs_remove_privs():
         :                      if (IS_NOSEC(inode))
    0.00 :   ffff80001029b75c:       ldr     x1, [x22, #48]
    0.00 :   ffff80001029b760:       ldr     w1, [x1, #12]
    0.00 :   ffff80001029b764:       tbz     w1, #12, ffff80001029b79c <file_remove_privs+0xb4>
         :                      is_sxid():
         :                      #define OPEN_FMODE(flag) ((__force fmode_t)(((flag + 1) & O_ACCMODE) | \
         :                      (flag & __FMODE_NONOTIFY)))
         :
         :                      static inline bool is_sxid(umode_t mode)
         :                      {
         :                      return (mode & S_ISUID) || ((mode & S_ISGID) && (mode & S_IXGRP));
    0.00 :   ffff80001029b768:       tbnz    w0, #11, ffff80001029b734 <file_remove_privs+0x4c>
    0.00 :   ffff80001029b76c:       mov     w1, #0x408                      // #1032
    0.00 :   ffff80001029b770:       and     w0, w0, w1
    0.00 :   ffff80001029b774:       cmp     w0, w1
    0.00 :   ffff80001029b778:       b.eq    ffff80001029b734 <file_remove_privs+0x4c>  // b.none
         :                      inode_has_no_xattr():
         :                      return __check_sticky(dir, inode);
         :                      }
         :
         :                      static inline void inode_has_no_xattr(struct inode *inode)
         :                      {
         :                      if (!is_sxid(inode->i_mode) && (inode->i_sb->s_flags & SB_NOSEC))
    0.00 :   ffff80001029b77c:       ldr     x0, [x21, #40]
    0.00 :   ffff80001029b780:       ldr     x0, [x0, #80]
    0.00 :   ffff80001029b784:       tbz     w0, #28, ffff80001029b734 <file_remove_privs+0x4c>
         :                      inode->i_flags |= S_NOSEC;
    0.00 :   ffff80001029b788:       ldr     w1, [x21, #12]
    0.00 :   ffff80001029b78c:       mov     w0, #0x0                        // #0
    0.00 :   ffff80001029b790:       orr     w1, w1, #0x1000
    0.00 :   ffff80001029b794:       str     w1, [x21, #12]
    0.00 :   ffff80001029b798:       b       ffff80001029b738 <file_remove_privs+0x50>
         :                      dentry_needs_remove_privs():
    0.00 :   ffff80001029b79c:       mov     x0, x22
    0.00 :   ffff80001029b7a0:       bl      ffff80001029b6a8 <dentry_needs_remove_privs.part.27>
         :                      file_remove_privs():
         :                      if (kill < 0)
    0.00 :   ffff80001029b7a4:       cmp     w0, #0x0
    0.00 :   ffff80001029b7a8:       b.lt    ffff80001029b738 <file_remove_privs+0x50>  // b.tstop
         :                      if (kill)
    0.00 :   ffff80001029b7ac:       b.eq    ffff80001029b7cc <file_remove_privs+0xe4>  // b.none
         :                      __remove_privs():
         :                      newattrs.ia_valid = ATTR_FORCE | kill;
    0.00 :   ffff80001029b7b0:       add     x1, x29, #0x90
    0.00 :   ffff80001029b7b4:       orr     w3, w0, #0x200
         :                      return notify_change(dentry, &newattrs, NULL);
    0.00 :   ffff80001029b7b8:       mov     x2, #0x0                        // #0
    0.00 :   ffff80001029b7bc:       mov     x0, x22
         :                      newattrs.ia_valid = ATTR_FORCE | kill;
    0.00 :   ffff80001029b7c0:       str     w3, [x1, #-88]!
         :                      return notify_change(dentry, &newattrs, NULL);
    0.00 :   ffff80001029b7c4:       bl      ffff80001029e118 <notify_change>
         :                      file_remove_privs():
         :                      if (!error)
    0.00 :   ffff80001029b7c8:       cbnz    w0, ffff80001029b738 <file_remove_privs+0x50>
    0.00 :   ffff80001029b7cc:       ldrh    w0, [x21]
    0.00 :   ffff80001029b7d0:       b       ffff80001029b768 <file_remove_privs+0x80>
         :                      d_real():
         :                      return dentry->d_op->d_real(dentry, inode);
    0.00 :   ffff80001029b7d4:       ldr     x2, [x22, #96]
    0.00 :   ffff80001029b7d8:       mov     x1, x21
    0.00 :   ffff80001029b7dc:       mov     x0, x22
    0.00 :   ffff80001029b7e0:       ldr     x2, [x2, #96]
    0.00 :   ffff80001029b7e4:       blr     x2
    0.00 :   ffff80001029b7e8:       ldr     x21, [x20, #32]
    0.00 :   ffff80001029b7ec:       mov     x22, x0
    0.00 :   ffff80001029b7f0:       b       ffff80001029b71c <file_remove_privs+0x34>
         :                      file_remove_privs():
         :                      }
    0.00 :   ffff80001029b7f4:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (1 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010ca7fe8 <num_to_str>:
         :                      num_to_str():
         :                      * Returns the length of string.  On buffer overflow, returns 0.
         :                      *
         :                      * If speed is not important, use snprintf(). It's easy to read the code.
         :                      */
         :                      int num_to_str(char *buf, int size, unsigned long long num, unsigned int width)
         :                      {
    0.00 :   ffff800010ca7fe8:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff800010ca7fec:       adrp    x14, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010ca7ff0:       add     x4, x14, #0x8c8
    0.00 :   ffff800010ca7ff4:       mov     x13, x0
    0.00 :   ffff800010ca7ff8:       mov     x29, sp
    0.00 :   ffff800010ca7ffc:       str     x19, [sp, #16]
    0.00 :   ffff800010ca8000:       mov     w18, w3
         :                      /* put_dec requires 2-byte alignment of the buffer. */
         :                      char tmp[sizeof(num) * 3] __aligned(2);
         :                      int idx, len;
         :
         :                      /* put_dec() may work incorrectly for num = 0 (generate "", not "0") */
         :                      if (num <= 9) {
    0.00 :   ffff800010ca8004:       cmp     x2, #0x9
         :                      {
    0.00 :   ffff800010ca8008:       mov     w19, w1
    0.00 :   ffff800010ca800c:       ldr     x5, [x4]
    0.00 :   ffff800010ca8010:       str     x5, [x29, #56]
    0.00 :   ffff800010ca8014:       mov     x5, #0x0                        // #0
         :                      if (num <= 9) {
    0.00 :   ffff800010ca8018:       b.hi    ffff800010ca80b8 <num_to_str+0xd0>  // b.pmore
         :                      tmp[0] = '0' + num;
  100.00 :   ffff800010ca801c:       add     w2, w2, #0x30
         :                      len = 1;
    0.00 :   ffff800010ca8020:       mov     w1, #0x1                        // #1
         :                      tmp[0] = '0' + num;
    0.00 :   ffff800010ca8024:       strb    w2, [x29, #32]
         :                      } else {
         :                      len = put_dec(tmp, num) - tmp;
         :                      }
         :
         :                      if (len > size || width > size)
    0.00 :   ffff800010ca8028:       cmp     w1, w19
         :                      return 0;
    0.00 :   ffff800010ca802c:       mov     w0, #0x0                        // #0
         :                      if (len > size || width > size)
    0.00 :   ffff800010ca8030:       ccmp    w19, w18, #0x0, le
    0.00 :   ffff800010ca8034:       b.cc    ffff800010ca8070 <num_to_str+0x88>  // b.lo, b.ul, b.last
         :
         :                      if (width > len) {
    0.00 :   ffff800010ca8038:       cmp     w1, w18
    0.00 :   ffff800010ca803c:       b.cc    ffff800010ca8090 <num_to_str+0xa8>  // b.lo, b.ul, b.last
    0.00 :   ffff800010ca8040:       mov     w0, w1
         :                      width = width - len;
         :                      for (idx = 0; idx < width; idx++)
         :                      buf[idx] = ' ';
         :                      } else {
         :                      width = 0;
    0.00 :   ffff800010ca8044:       mov     w2, #0x0                        // #0
         :                      }
         :
         :                      for (idx = 0; idx < len; ++idx)
    0.00 :   ffff800010ca8048:       cmp     w1, #0x0
    0.00 :   ffff800010ca804c:       b.le    ffff800010ca8070 <num_to_str+0x88>
    0.00 :   ffff800010ca8050:       sub     w1, w1, #0x1
    0.00 :   ffff800010ca8054:       add     x15, x29, #0x20
         :                      buf[idx + width] = tmp[len - idx - 1];
    0.00 :   ffff800010ca8058:       ldrb    w3, [x15, w1, sxtw]
    0.00 :   ffff800010ca805c:       sub     w1, w1, #0x1
    0.00 :   ffff800010ca8060:       strb    w3, [x13, w2, uxtw]
         :                      for (idx = 0; idx < len; ++idx)
    0.00 :   ffff800010ca8064:       cmn     w1, #0x1
    0.00 :   ffff800010ca8068:       add     w2, w2, #0x1
    0.00 :   ffff800010ca806c:       b.ne    ffff800010ca8058 <num_to_str+0x70>  // b.any
         :
         :                      return len + width;
         :                      }
    0.00 :   ffff800010ca8070:       add     x14, x14, #0x8c8
    0.00 :   ffff800010ca8074:       ldr     x2, [x29, #56]
    0.00 :   ffff800010ca8078:       ldr     x1, [x14]
    0.00 :   ffff800010ca807c:       eor     x1, x2, x1
    0.00 :   ffff800010ca8080:       cbnz    x1, ffff800010ca80d8 <num_to_str+0xf0>
    0.00 :   ffff800010ca8084:       ldr     x19, [sp, #16]
    0.00 :   ffff800010ca8088:       ldp     x29, x30, [sp], #64
    0.00 :   ffff800010ca808c:       ret
         :                      for (idx = 0; idx < width; idx++)
    0.00 :   ffff800010ca8090:       subs    w2, w18, w1
    0.00 :   ffff800010ca8094:       b.eq    ffff800010ca80d0 <num_to_str+0xe8>  // b.none
         :                      buf[idx] = ' ';
    0.00 :   ffff800010ca8098:       mov     w3, #0x20                       // #32
    0.00 :   ffff800010ca809c:       nop
    0.00 :   ffff800010ca80a0:       strb    w3, [x13, w0, sxtw]
         :                      for (idx = 0; idx < width; idx++)
    0.00 :   ffff800010ca80a4:       add     w0, w0, #0x1
    0.00 :   ffff800010ca80a8:       cmp     w0, w2
    0.00 :   ffff800010ca80ac:       b.ne    ffff800010ca80a0 <num_to_str+0xb8>  // b.any
    0.00 :   ffff800010ca80b0:       mov     w0, w18
    0.00 :   ffff800010ca80b4:       b       ffff800010ca8048 <num_to_str+0x60>
         :                      len = put_dec(tmp, num) - tmp;
    0.00 :   ffff800010ca80b8:       add     x15, x29, #0x20
    0.00 :   ffff800010ca80bc:       mov     x1, x2
    0.00 :   ffff800010ca80c0:       mov     x0, x15
    0.00 :   ffff800010ca80c4:       bl      ffff800010ca3460 <put_dec>
    0.00 :   ffff800010ca80c8:       sub     w1, w0, w15
    0.00 :   ffff800010ca80cc:       b       ffff800010ca8028 <num_to_str+0x40>
         :                      for (idx = 0; idx < width; idx++)
    0.00 :   ffff800010ca80d0:       mov     w0, w1
    0.00 :   ffff800010ca80d4:       b       ffff800010ca8048 <num_to_str+0x60>
         :                      }
    0.00 :   ffff800010ca80d8:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (1 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff800010117db0 <sched_setaffinity>:
         :                      sched_setaffinity():
         :                      rcu_read_unlock();
         :                      return retval;
         :                      }
         :
         :                      long sched_setaffinity(pid_t pid, const struct cpumask *in_mask)
         :                      {
    0.00 :   ffff800010117db0:       stp     x29, x30, [sp, #-144]!
    0.00 :   ffff800010117db4:       mov     x29, sp
    0.00 :   ffff800010117db8:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800010117dbc:       mov     w19, w0
    0.00 :   ffff800010117dc0:       stp     x21, x22, [sp, #32]
    0.00 :   ffff800010117dc4:       adrp    x21, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010117dc8:       str     x23, [sp, #48]
    0.00 :   ffff800010117dcc:       add     x2, x21, #0x8c8
    0.00 :   ffff800010117dd0:       mov     x23, x1
    0.00 :   ffff800010117dd4:       ldr     x0, [x2]
    0.00 :   ffff800010117dd8:       str     x0, [x29, #136]
    0.00 :   ffff800010117ddc:       mov     x0, #0x0                        // #0
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff800010117de0:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      find_process_by_pid():
         :                      return pid ? find_task_by_vpid(pid) : current;
    0.00 :   ffff800010117de4:       cbnz    w19, ffff800010117ef8 <sched_setaffinity+0x148>
         :                      get_current():
         :                      */
         :                      static __always_inline struct task_struct *get_current(void)
         :                      {
         :                      unsigned long sp_el0;
         :
         :                      asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800010117de8:       mrs     x20, sp_el0
         :                      sched_setaffinity():
         :                      int retval;
         :
         :                      rcu_read_lock();
         :
         :                      p = find_process_by_pid(pid);
         :                      if (!p) {
    0.00 :   ffff800010117dec:       cbz     x20, ffff800010118014 <sched_setaffinity+0x264>
         :                      get_task_struct():
         :                      #define sched_exec()   {}
         :                      #endif
         :
         :                      static inline struct task_struct *get_task_struct(struct task_struct *t)
         :                      {
         :                      refcount_inc(&t->usage);
    0.00 :   ffff800010117df0:       add     x19, x20, #0x28
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff800010117df4:       b       ffff800010117e08 <sched_setaffinity+0x58>
    0.00 :   ffff800010117df8:       b       ffff800010117e08 <sched_setaffinity+0x58>
         :                      __lse_atomic_fetch_add_relaxed():
         :                      ATOMIC_FETCH_OP(        , al, op, asm_op, "memory")
         :
         :                      ATOMIC_FETCH_OPS(andnot, ldclr)
         :                      ATOMIC_FETCH_OPS(or, ldset)
         :                      ATOMIC_FETCH_OPS(xor, ldeor)
         :                      ATOMIC_FETCH_OPS(add, ldadd)
    0.00 :   ffff800010117dfc:       mov     w0, #0x1                        // #1
    0.00 :   ffff800010117e00:       ldadd   w0, w0, [x19]
    0.00 :   ffff800010117e04:       b       ffff800010117e0c <sched_setaffinity+0x5c>
         :                      __ll_sc_atomic_fetch_add_relaxed():
         :                      ATOMIC_FETCH_OP (        , dmb ish,  , l, "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
    0.00 :   ffff800010117e08:       b       ffff80001011981c <dump_cpu_task+0x23c>
         :                      refcount_add():
         :                      */
         :                      static inline void refcount_add(int i, refcount_t *r)
         :                      {
         :                      int old = atomic_fetch_add_relaxed(i, &r->refs);
         :
         :                      if (unlikely(!old))
    0.00 :   ffff800010117e0c:       cmp     w0, #0x0
    0.00 :   ffff800010117e10:       b.eq    ffff800010117fa4 <sched_setaffinity+0x1f4>  // b.none
         :                      refcount_warn_saturate(r, REFCOUNT_ADD_UAF);
         :                      else if (unlikely(old < 0 || old + i < 0))
    0.00 :   ffff800010117e14:       b.lt    ffff800010117f84 <sched_setaffinity+0x1d4>  // b.tstop
    0.00 :   ffff800010117e18:       cmn     w0, #0x1
    0.00 :   ffff800010117e1c:       b.mi    ffff800010117f84 <sched_setaffinity+0x1d4>  // b.first
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.00 :   ffff800010117e20:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      sched_setaffinity():
    0.00 :   ffff800010117e24:       mov     x22, #0xffffffffffffffea        // #-22
         :
         :                      /* Prevent p going away */
         :                      get_task_struct(p);
         :                      rcu_read_unlock();
         :
         :                      if (p->flags & PF_NO_SETAFFINITY) {
    0.00 :   ffff800010117e28:       ldr     w0, [x20, #44]
    0.00 :   ffff800010117e2c:       tbz     w0, #26, ffff800010117ea0 <sched_setaffinity+0xf0>
         :                      arch_static_branch_jump():
    0.00 :   ffff800010117e30:       b       ffff800010117e5c <sched_setaffinity+0xac>
    0.00 :   ffff800010117e34:       b       ffff800010117e5c <sched_setaffinity+0xac>
         :                      __lse_atomic_fetch_sub_release():
         :                      return i;                                                       \
         :                      }
         :
         :                      ATOMIC_FETCH_OP_SUB(_relaxed,   )
         :                      ATOMIC_FETCH_OP_SUB(_acquire,  a, "memory")
         :                      ATOMIC_FETCH_OP_SUB(_release,  l, "memory")
    0.00 :   ffff800010117e38:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010117e3c:       neg     w1, w1
    0.00 :   ffff800010117e40:       ldaddl  w1, w1, [x19]
         :                      refcount_sub_and_test():
         :                      */
         :                      static inline __must_check bool refcount_sub_and_test(int i, refcount_t *r)
         :                      {
         :                      int old = atomic_fetch_sub_release(i, &r->refs);
         :
         :                      if (old == i) {
    0.00 :   ffff800010117e44:       cmp     w1, #0x1
    0.00 :   ffff800010117e48:       b.ne    ffff800010117e6c <sched_setaffinity+0xbc>  // b.any
         :                      smp_acquire__after_ctrl_dep();
    0.00 :   ffff800010117e4c:       dmb     ishld
         :                      put_task_struct():
         :                      extern void __put_task_struct(struct task_struct *t);
         :
         :                      static inline void put_task_struct(struct task_struct *t)
         :                      {
         :                      if (refcount_dec_and_test(&t->usage))
         :                      __put_task_struct(t);
    0.00 :   ffff800010117e50:       mov     x0, x20
    0.00 :   ffff800010117e54:       bl      ffff8000100e1e18 <__put_task_struct>
    0.00 :   ffff800010117e58:       b       ffff800010117e74 <sched_setaffinity+0xc4>
         :                      __ll_sc_atomic_fetch_sub_release():
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff800010117e5c:       mov     w2, #0x1                        // #1
    0.00 :   ffff800010117e60:       b       ffff800010119834 <dump_cpu_task+0x254>
         :                      refcount_sub_and_test():
         :                      if (old == i) {
    0.00 :   ffff800010117e64:       cmp     w1, #0x1
    0.00 :   ffff800010117e68:       b.eq    ffff800010117e4c <sched_setaffinity+0x9c>  // b.none
         :                      return true;
         :                      }
         :
         :                      if (unlikely(old < 0 || old - i < 0))
    0.00 :   ffff800010117e6c:       cmp     w1, #0x0
    0.00 :   ffff800010117e70:       b.le    ffff800010117f94 <sched_setaffinity+0x1e4>
         :                      sched_setaffinity():
         :                      out_free_cpus_allowed:
         :                      free_cpumask_var(cpus_allowed);
         :                      out_put_task:
         :                      put_task_struct(p);
         :                      return retval;
         :                      }
    0.00 :   ffff800010117e74:       add     x21, x21, #0x8c8
    0.00 :   ffff800010117e78:       mov     x0, x22
    0.00 :   ffff800010117e7c:       ldr     x2, [x29, #136]
    0.00 :   ffff800010117e80:       ldr     x1, [x21]
    0.00 :   ffff800010117e84:       eor     x1, x2, x1
    0.00 :   ffff800010117e88:       cbnz    x1, ffff800010118028 <sched_setaffinity+0x278>
    0.00 :   ffff800010117e8c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010117e90:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800010117e94:       ldr     x23, [sp, #48]
    0.00 :   ffff800010117e98:       ldp     x29, x30, [sp], #144
    0.00 :   ffff800010117e9c:       ret
         :                      if (!check_same_owner(p)) {
    0.00 :   ffff800010117ea0:       mov     x0, x20
    0.00 :   ffff800010117ea4:       bl      ffff8000101124c8 <check_same_owner>
    0.00 :   ffff800010117ea8:       tst     w0, #0xff
    0.00 :   ffff800010117eac:       b.ne    ffff800010117ee0 <sched_setaffinity+0x130>  // b.any
         :                      rcu_read_lock():
         :                      __rcu_read_lock();
    0.00 :   ffff800010117eb0:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff800010117eb4:       ldr     x0, [x20, #1544]
         :                      sched_setaffinity():
         :                      if (!ns_capable(__task_cred(p)->user_ns, CAP_SYS_NICE)) {
    0.00 :   ffff800010117eb8:       mov     w1, #0x17                       // #23
    0.00 :   ffff800010117ebc:       ldr     x0, [x0, #136]
    0.00 :   ffff800010117ec0:       bl      ffff8000100f0700 <ns_capable>
    0.00 :   ffff800010117ec4:       tst     w0, #0xff
    0.00 :   ffff800010117ec8:       b.ne    ffff800010117edc <sched_setaffinity+0x12c>  // b.any
         :                      rcu_read_unlock():
         :                      __rcu_read_unlock();
    0.00 :   ffff800010117ecc:       bl      ffff80001015eee0 <__rcu_read_unlock>
    0.00 :   ffff800010117ed0:       mov     x22, #0xffffffffffffffff        // #-1
         :                      arch_static_branch_jump():
    0.00 :   ffff800010117ed4:       b       ffff800010117e5c <sched_setaffinity+0xac>
    0.00 :   ffff800010117ed8:       b       ffff800010117e34 <sched_setaffinity+0x84>
         :                      rcu_read_unlock():
    0.00 :   ffff800010117edc:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      sched_setaffinity():
         :                      retval = security_task_setscheduler(p);
    0.00 :   ffff800010117ee0:       mov     x0, x20
    0.00 :   ffff800010117ee4:       bl      ffff800010433450 <security_task_setscheduler>
         :                      if (retval)
    0.00 :   ffff800010117ee8:       cbz     w0, ffff800010117f08 <sched_setaffinity+0x158>
    0.00 :   ffff800010117eec:       sxtw    x22, w0
         :                      arch_static_branch_jump():
    0.00 :   ffff800010117ef0:       b       ffff800010117e5c <sched_setaffinity+0xac>
    0.00 :   ffff800010117ef4:       b       ffff800010117e34 <sched_setaffinity+0x84>
         :                      find_process_by_pid():
         :                      return pid ? find_task_by_vpid(pid) : current;
    0.00 :   ffff800010117ef8:       mov     w0, w19
    0.00 :   ffff800010117efc:       bl      ffff800010108b88 <find_task_by_vpid>
    0.00 :   ffff800010117f00:       mov     x20, x0
    0.00 :   ffff800010117f04:       b       ffff800010117dec <sched_setaffinity+0x3c>
         :                      sched_setaffinity():
         :                      cpuset_cpus_allowed(p, cpus_allowed);
    0.00 :   ffff800010117f08:       mov     x0, x20
    0.00 :   ffff800010117f0c:       add     x1, x29, #0x48
    0.00 :   ffff800010117f10:       bl      ffff80001019dab0 <cpuset_cpus_allowed>
         :                      bitmap_and():
         :                      static inline int bitmap_and(unsigned long *dst, const unsigned long *src1,
         :                      const unsigned long *src2, unsigned int nbits)
         :                      {
         :                      if (small_const_nbits(nbits))
         :                      return (*dst = *src1 & *src2 & BITMAP_LAST_WORD_MASK(nbits)) != 0;
         :                      return __bitmap_and(dst, src1, src2, nbits);
    0.00 :   ffff800010117f14:       mov     w3, #0x100                      // #256
    0.00 :   ffff800010117f18:       add     x2, x29, #0x48
    0.00 :   ffff800010117f1c:       mov     x1, x23
    0.00 :   ffff800010117f20:       add     x0, x29, #0x68
    0.00 :   ffff800010117f24:       bl      ffff80001047f4e0 <__bitmap_and>
         :                      sched_setaffinity():
         :                      if (task_has_dl_policy(p) && dl_bandwidth_enabled()) {
    0.00 :   ffff800010117f28:       ldr     w0, [x20, #728]
    0.00 :   ffff800010117f2c:       cmp     w0, #0x6
    0.00 :   ffff800010117f30:       b.ne    ffff800010117f68 <sched_setaffinity+0x1b8>  // b.any
    0.00 :   ffff800010117f34:       b       ffff800010117fc0 <sched_setaffinity+0x210>
         :                      cpuset_cpus_allowed(p, cpus_allowed);
  100.00 :   ffff800010117f38:       add     x1, x29, #0x48
    0.00 :   ffff800010117f3c:       mov     x0, x20
    0.00 :   ffff800010117f40:       bl      ffff80001019dab0 <cpuset_cpus_allowed>
         :                      bitmap_subset():
         :                      const unsigned long *src2, unsigned int nbits)
         :                      {
         :                      if (small_const_nbits(nbits))
         :                      return ! ((*src1 & ~(*src2)) & BITMAP_LAST_WORD_MASK(nbits));
         :                      else
         :                      return __bitmap_subset(src1, src2, nbits);
    0.00 :   ffff800010117f44:       mov     w2, #0x100                      // #256
    0.00 :   ffff800010117f48:       add     x1, x29, #0x48
    0.00 :   ffff800010117f4c:       add     x0, x29, #0x68
    0.00 :   ffff800010117f50:       bl      ffff80001047f718 <__bitmap_subset>
         :                      sched_setaffinity():
         :                      if (!cpumask_subset(new_mask, cpus_allowed)) {
    0.00 :   ffff800010117f54:       cbnz    w0, ffff800010117fb4 <sched_setaffinity+0x204>
         :                      bitmap_copy():
         :                      memcpy(dst, src, len);
    0.00 :   ffff800010117f58:       ldp     x2, x3, [x29, #72]
    0.00 :   ffff800010117f5c:       stp     x2, x3, [x29, #104]
    0.00 :   ffff800010117f60:       ldp     x0, x1, [x29, #88]
    0.00 :   ffff800010117f64:       stp     x0, x1, [x29, #120]
         :                      sched_setaffinity():
         :                      retval = __set_cpus_allowed_ptr(p, new_mask, true);
    0.00 :   ffff800010117f68:       mov     w2, #0x1                        // #1
    0.00 :   ffff800010117f6c:       add     x1, x29, #0x68
    0.00 :   ffff800010117f70:       mov     x0, x20
    0.00 :   ffff800010117f74:       bl      ffff800010115220 <__set_cpus_allowed_ptr>
         :                      if (!retval) {
    0.00 :   ffff800010117f78:       cbz     w0, ffff800010117f38 <sched_setaffinity+0x188>
    0.00 :   ffff800010117f7c:       sxtw    x22, w0
    0.00 :   ffff800010117f80:       b       ffff800010117ef0 <sched_setaffinity+0x140>
         :                      refcount_add():
         :                      refcount_warn_saturate(r, REFCOUNT_ADD_OVF);
    0.00 :   ffff800010117f84:       mov     w1, #0x1                        // #1
    0.00 :   ffff800010117f88:       mov     x0, x19
    0.00 :   ffff800010117f8c:       bl      ffff80001048ba28 <refcount_warn_saturate>
    0.00 :   ffff800010117f90:       b       ffff800010117e20 <sched_setaffinity+0x70>
         :                      refcount_sub_and_test():
         :                      refcount_warn_saturate(r, REFCOUNT_SUB_UAF);
    0.00 :   ffff800010117f94:       mov     w1, #0x3                        // #3
    0.00 :   ffff800010117f98:       mov     x0, x19
    0.00 :   ffff800010117f9c:       bl      ffff80001048ba28 <refcount_warn_saturate>
         :                      sched_setaffinity():
         :                      return retval;
    0.00 :   ffff800010117fa0:       b       ffff800010117e74 <sched_setaffinity+0xc4>
         :                      refcount_add():
         :                      refcount_warn_saturate(r, REFCOUNT_ADD_UAF);
    0.00 :   ffff800010117fa4:       mov     w1, #0x2                        // #2
    0.00 :   ffff800010117fa8:       mov     x0, x19
    0.00 :   ffff800010117fac:       bl      ffff80001048ba28 <refcount_warn_saturate>
    0.00 :   ffff800010117fb0:       b       ffff800010117e20 <sched_setaffinity+0x70>
         :                      sched_setaffinity():
         :                      out_free_new_mask:
    0.00 :   ffff800010117fb4:       mov     x22, #0x0                       // #0
         :                      arch_static_branch_jump():
    0.00 :   ffff800010117fb8:       b       ffff800010117e5c <sched_setaffinity+0xac>
    0.00 :   ffff800010117fbc:       b       ffff800010117e34 <sched_setaffinity+0x84>
         :                      dl_bandwidth_enabled():
         :                      u64                     dl_period;
         :                      };
         :
         :                      static inline int dl_bandwidth_enabled(void)
         :                      {
         :                      return sysctl_sched_rt_runtime >= 0;
    0.00 :   ffff800010117fc0:       adrp    x0, ffff8000118b0000 <user_table+0x188>
         :                      sched_setaffinity():
         :                      if (task_has_dl_policy(p) && dl_bandwidth_enabled()) {
    0.00 :   ffff800010117fc4:       ldr     w0, [x0, #688]
    0.00 :   ffff800010117fc8:       tbnz    w0, #31, ffff800010117f68 <sched_setaffinity+0x1b8>
         :                      rcu_read_lock():
         :                      __rcu_read_lock();
    0.00 :   ffff800010117fcc:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
    0.00 :   ffff800010117fd0:       ldr     w3, [x20, #68]
         :                      sched_setaffinity():
         :                      if (!cpumask_subset(task_rq(p)->rd->span, new_mask)) {
    0.00 :   ffff800010117fd4:       adrp    x1, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff800010117fd8:       add     x1, x1, #0x8e8
    0.00 :   ffff800010117fdc:       adrp    x0, ffff8000114d5000 <tegra_to+0x180>
    0.00 :   ffff800010117fe0:       add     x0, x0, #0xd80
         :                      bitmap_subset():
         :                      return __bitmap_subset(src1, src2, nbits);
    0.00 :   ffff800010117fe4:       mov     w2, #0x100                      // #256
         :                      sched_setaffinity():
    0.00 :   ffff800010117fe8:       ldr     x3, [x1, w3, uxtw #3]
         :                      bitmap_subset():
    0.00 :   ffff800010117fec:       add     x1, x29, #0x68
         :                      sched_setaffinity():
    0.00 :   ffff800010117ff0:       add     x0, x3, x0
         :                      cpumask_subset():
         :                      * Returns 1 if *@src1p is a subset of *@src2p, else returns 0
         :                      */
         :                      static inline int cpumask_subset(const struct cpumask *src1p,
         :                      const struct cpumask *src2p)
         :                      {
         :                      return bitmap_subset(cpumask_bits(src1p), cpumask_bits(src2p),
    0.00 :   ffff800010117ff4:       ldr     x0, [x0, #2464]
         :                      bitmap_subset():
    0.00 :   ffff800010117ff8:       add     x0, x0, #0x18
    0.00 :   ffff800010117ffc:       bl      ffff80001047f718 <__bitmap_subset>
         :                      sched_setaffinity():
    0.00 :   ffff800010118000:       cbnz    w0, ffff800010118020 <sched_setaffinity+0x270>
         :                      rcu_read_unlock():
         :                      __rcu_read_unlock();
    0.00 :   ffff800010118004:       bl      ffff80001015eee0 <__rcu_read_unlock>
    0.00 :   ffff800010118008:       mov     x22, #0xfffffffffffffff0        // #-16
         :                      arch_static_branch_jump():
    0.00 :   ffff80001011800c:       b       ffff800010117e5c <sched_setaffinity+0xac>
    0.00 :   ffff800010118010:       b       ffff800010117e34 <sched_setaffinity+0x84>
         :                      rcu_read_unlock():
    0.00 :   ffff800010118014:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      sched_setaffinity():
         :                      return -ESRCH;
    0.00 :   ffff800010118018:       mov     x22, #0xfffffffffffffffd        // #-3
    0.00 :   ffff80001011801c:       b       ffff800010117e74 <sched_setaffinity+0xc4>
         :                      rcu_read_unlock():
    0.00 :   ffff800010118020:       bl      ffff80001015eee0 <__rcu_read_unlock>
    0.00 :   ffff800010118024:       b       ffff800010117f68 <sched_setaffinity+0x1b8>
         :                      sched_setaffinity():
         :                      }
    0.00 :   ffff800010118028:       bl      ffff8000100e5630 <__stack_chk_fail>
 Percent |	Source code & Disassembly of vmlinux for cycles (1 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001030a2e8 <task_dump_owner>:
         :                      task_dump_owner():
         :                      */
         :                      const struct cred *cred;
         :                      kuid_t uid;
         :                      kgid_t gid;
         :
         :                      if (unlikely(task->flags & PF_KTHREAD)) {
    0.00 :   ffff80001030a2e8:       ldr     w4, [x0, #44]
    0.00 :   ffff80001030a2ec:       tbnz    w4, #21, ffff80001030a3e4 <task_dump_owner+0xfc>
         :                      {
    0.00 :   ffff80001030a2f0:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff80001030a2f4:       mov     x29, sp
    0.00 :   ffff80001030a2f8:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001030a2fc:       mov     x19, x0
    0.00 :   ffff80001030a300:       stp     x21, x22, [sp, #32]
    0.00 :   ffff80001030a304:       mov     x22, x2
    0.00 :   ffff80001030a308:       stp     x23, x24, [sp, #48]
    0.00 :   ffff80001030a30c:       and     w24, w1, #0xffff
    0.00 :   ffff80001030a310:       mov     x23, x3
         :                      rcu_read_lock():
         :                      * read-side critical sections may be preempted and they may also block, but
         :                      * only when acquiring spinlocks that are subject to priority inheritance.
         :                      */
         :                      static __always_inline void rcu_read_lock(void)
         :                      {
         :                      __rcu_read_lock();
    0.00 :   ffff80001030a314:       bl      ffff80001015a698 <__rcu_read_lock>
         :                      __read_once_size():
         :                      })
         :
         :                      static __always_inline
         :                      void __read_once_size(const volatile void *p, void *res, int size)
         :                      {
         :                      __READ_ONCE_SIZE;
    0.00 :   ffff80001030a318:       ldr     x0, [x19, #1544]
         :                      task_dump_owner():
         :
         :                      /* Default to the tasks effective ownership */
         :                      rcu_read_lock();
         :                      cred = __task_cred(task);
         :                      uid = cred->euid;
         :                      gid = cred->egid;
    0.00 :   ffff80001030a31c:       ldp     w21, w20, [x0, #20]
         :                      rcu_read_unlock():
         :                      static inline void rcu_read_unlock(void)
         :                      {
         :                      RCU_LOCKDEP_WARN(!rcu_is_watching(),
         :                      "rcu_read_unlock() used illegally while idle");
         :                      __release(RCU);
         :                      __rcu_read_unlock();
    0.00 :   ffff80001030a320:       bl      ffff80001015eee0 <__rcu_read_unlock>
         :                      task_dump_owner():
         :                      * /proc/pid/status is slow enough that procps and other packages
         :                      * kept stating /proc/pid.  To keep the rules in /proc simple I have
         :                      * made this apply to all per process world readable and executable
         :                      * directories.
         :                      */
         :                      if (mode != (S_IFDIR|S_IRUGO|S_IXUGO)) {
    0.00 :   ffff80001030a324:       mov     w0, #0x416d                     // #16749
    0.00 :   ffff80001030a328:       cmp     w24, w0
    0.00 :   ffff80001030a32c:       b.ne    ffff80001030a34c <task_dump_owner+0x64>  // b.any
         :                      uid = GLOBAL_ROOT_UID;
         :                      gid = GLOBAL_ROOT_GID;
         :                      }
         :                      task_unlock(task);
         :                      }
         :                      *ruid = uid;
    0.00 :   ffff80001030a330:       str     w21, [x22]
         :                      *rgid = gid;
    0.00 :   ffff80001030a334:       str     w20, [x23]
         :                      }
    0.00 :   ffff80001030a338:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001030a33c:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff80001030a340:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff80001030a344:       ldp     x29, x30, [sp], #64
    0.00 :   ffff80001030a348:       ret
         :                      spin_lock():
         :                      raw_spin_lock_init(&(_lock)->rlock);            \
         :                      } while (0)
         :
         :                      static __always_inline void spin_lock(spinlock_t *lock)
         :                      {
         :                      raw_spin_lock(&lock->rlock);
    0.00 :   ffff80001030a34c:       add     x24, x19, #0x6f0
    0.00 :   ffff80001030a350:       mov     x0, x24
    0.00 :   ffff80001030a354:       bl      ffff800010cb2b08 <_raw_spin_lock>
         :                      task_dump_owner():
         :                      mm = task->mm;
    0.00 :   ffff80001030a358:       ldr     x0, [x19, #952]
         :                      if (mm) {
    0.00 :   ffff80001030a35c:       cbz     x0, ffff80001030a3d0 <task_dump_owner+0xe8>
         :                      __get_dumpable():
         :                      * test against SUID_DUMP_USER rather than treating it as a boolean
         :                      * value.
         :                      */
         :                      static inline int __get_dumpable(unsigned long mm_flags)
         :                      {
         :                      return mm_flags & MMF_DUMPABLE_MASK;
    0.00 :   ffff80001030a360:       ldr     x1, [x0, #760]
  100.00 :   ffff80001030a364:       and     w1, w1, #0x3
         :                      task_dump_owner():
         :                      if (get_dumpable(mm) != SUID_DUMP_USER) {
    0.00 :   ffff80001030a368:       cmp     w1, #0x1
    0.00 :   ffff80001030a36c:       b.ne    ffff80001030a394 <task_dump_owner+0xac>  // b.any
         :                      spin_unlock():
         :                      raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
         :                      } while (0)
         :
         :                      static __always_inline void spin_unlock(spinlock_t *lock)
         :                      {
         :                      raw_spin_unlock(&lock->rlock);
    0.00 :   ffff80001030a370:       mov     x0, x24
    0.00 :   ffff80001030a374:       bl      ffff800010cb2408 <_raw_spin_unlock>
         :                      task_dump_owner():
         :                      *ruid = uid;
    0.00 :   ffff80001030a378:       str     w21, [x22]
         :                      *rgid = gid;
    0.00 :   ffff80001030a37c:       str     w20, [x23]
         :                      }
    0.00 :   ffff80001030a380:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80001030a384:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff80001030a388:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff80001030a38c:       ldp     x29, x30, [sp], #64
    0.00 :   ffff80001030a390:       ret
         :                      struct user_namespace *user_ns = mm->user_ns;
    0.00 :   ffff80001030a394:       ldr     x19, [x0, #800]
         :                      uid = make_kuid(user_ns, 0);
    0.00 :   ffff80001030a398:       mov     w1, #0x0                        // #0
    0.00 :   ffff80001030a39c:       mov     x0, x19
    0.00 :   ffff80001030a3a0:       bl      ffff80001019ec50 <make_kuid>
    0.00 :   ffff80001030a3a4:       mov     w21, w0
         :                      gid = make_kgid(user_ns, 0);
    0.00 :   ffff80001030a3a8:       mov     w1, #0x0                        // #0
         :                      uid = GLOBAL_ROOT_UID;
    0.00 :   ffff80001030a3ac:       cmn     w21, #0x1
         :                      gid = make_kgid(user_ns, 0);
    0.00 :   ffff80001030a3b0:       mov     x0, x19
         :                      uid = GLOBAL_ROOT_UID;
    0.00 :   ffff80001030a3b4:       csel    w21, w21, wzr, ne  // ne = any
         :                      gid = make_kgid(user_ns, 0);
    0.00 :   ffff80001030a3b8:       bl      ffff80001019ec68 <make_kgid>
         :                      gid = GLOBAL_ROOT_GID;
    0.00 :   ffff80001030a3bc:       cmn     w0, #0x1
    0.00 :   ffff80001030a3c0:       csel    w20, w0, wzr, ne  // ne = any
         :                      spin_unlock():
    0.00 :   ffff80001030a3c4:       mov     x0, x24
    0.00 :   ffff80001030a3c8:       bl      ffff800010cb2408 <_raw_spin_unlock>
    0.00 :   ffff80001030a3cc:       b       ffff80001030a378 <task_dump_owner+0x90>
         :                      task_dump_owner():
         :                      gid = GLOBAL_ROOT_GID;
    0.00 :   ffff80001030a3d0:       mov     w20, #0x0                       // #0
         :                      uid = GLOBAL_ROOT_UID;
    0.00 :   ffff80001030a3d4:       mov     w21, #0x0                       // #0
         :                      spin_unlock():
    0.00 :   ffff80001030a3d8:       mov     x0, x24
    0.00 :   ffff80001030a3dc:       bl      ffff800010cb2408 <_raw_spin_unlock>
    0.00 :   ffff80001030a3e0:       b       ffff80001030a378 <task_dump_owner+0x90>
         :                      task_dump_owner():
         :                      *ruid = GLOBAL_ROOT_UID;
    0.00 :   ffff80001030a3e4:       str     wzr, [x2]
         :                      *rgid = GLOBAL_ROOT_GID;
    0.00 :   ffff80001030a3e8:       str     wzr, [x3]
    0.00 :   ffff80001030a3ec:       ret
 Percent |	Source code & Disassembly of libc-2.27.so for cycles (1 samples, percent: local period)
-------------------------------------------------------------------------------------------------------
         :
         :
         :
         :           Disassembly of section .text:
         :
         :           00000000000de900 <__snprintf_chk@@GLIBC_2.17>:
  100.00 :   de900:  stp     x29, x30, [sp, #-272]!
    0.00 :   de904:  mov     x29, sp
    0.00 :   de908:  add     x8, x29, #0x110
    0.00 :   de90c:  add     x9, x29, #0xf0
    0.00 :   de910:  str     x19, [sp, #16]
    0.00 :   de914:  adrp    x19, 152000 <sys_sigabbrev@@GLIBC_2.17+0x278>
    0.00 :   de918:  stp     x8, x8, [x29, #72]
    0.00 :   de91c:  mov     w8, #0xffffffe8                 // #-24
    0.00 :   de920:  str     x9, [x29, #88]
    0.00 :   de924:  str     w8, [x29, #96]
    0.00 :   de928:  mov     w8, #0xffffff80                 // #-128
    0.00 :   de92c:  str     w8, [x29, #100]
    0.00 :   de930:  ldp     x10, x11, [x29, #72]
    0.00 :   de934:  stp     x5, x6, [x29, #248]
    0.00 :   de938:  ldp     x8, x9, [x29, #88]
    0.00 :   de93c:  str     x7, [x29, #264]
    0.00 :   de940:  ldr     x19, [x19, #3800]
    0.00 :   de944:  str     q0, [x29, #112]
    0.00 :   de948:  str     q1, [x29, #128]
    0.00 :   de94c:  str     q2, [x29, #144]
    0.00 :   de950:  str     q3, [x29, #160]
    0.00 :   de954:  str     q4, [x29, #176]
    0.00 :   de958:  str     q5, [x29, #192]
    0.00 :   de95c:  str     q6, [x29, #208]
    0.00 :   de960:  str     q7, [x29, #224]
    0.00 :   de964:  ldr     x5, [x19]
    0.00 :   de968:  str     x5, [x29, #104]
    0.00 :   de96c:  mov     x5, #0x0                        // #0
    0.00 :   de970:  add     x5, x29, #0x20
    0.00 :   de974:  stp     x10, x11, [x29, #32]
    0.00 :   de978:  stp     x8, x9, [x29, #48]
    0.00 :   de97c:  bl      de9a0 <__vsnprintf_chk@@GLIBC_2.17>
    0.00 :   de980:  ldr     x2, [x29, #104]
    0.00 :   de984:  ldr     x1, [x19]
    0.00 :   de988:  eor     x1, x2, x1
    0.00 :   de98c:  cbnz    x1, de99c <__snprintf_chk@@GLIBC_2.17+0x9c>
    0.00 :   de990:  ldr     x19, [sp, #16]
    0.00 :   de994:  ldp     x29, x30, [sp], #272
    0.00 :   de998:  ret
    0.00 :   de99c:  bl      e1708 <__stack_chk_fail@@GLIBC_2.17>
 Percent |	Source code & Disassembly of vmlinux for cycles (1 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff8000101319e0 <group_balance_cpu>:
         :                      group_balance_cpu():
         :                      * group. See build_balance_mask().
         :                      *
         :                      * Also see should_we_balance().
         :                      */
         :                      int group_balance_cpu(struct sched_group *sg)
         :                      {
    0.00 :   ffff8000101319e0:       stp     x29, x30, [sp, #-16]!
         :                      cpumask_first():
         :                      *
         :                      * Returns >= nr_cpu_ids if no cpus set.
         :                      */
         :                      static inline unsigned int cpumask_first(const struct cpumask *srcp)
         :                      {
         :                      return find_first_bit(cpumask_bits(srcp), nr_cpumask_bits);
    0.00 :   ffff8000101319e4:       mov     x2, #0x0                        // #0
    0.00 :   ffff8000101319e8:       mov     x1, #0x100                      // #256
         :                      group_balance_cpu():
    0.00 :   ffff8000101319ec:       mov     x29, sp
         :                      cpumask_first():
    0.00 :   ffff8000101319f0:       ldr     x0, [x0, #16]
    0.00 :   ffff8000101319f4:       add     x0, x0, #0x30
    0.00 :   ffff8000101319f8:       bl      ffff800010487db8 <find_next_bit>
         :                      group_balance_cpu():
         :                      return cpumask_first(group_balance_mask(sg));
         :                      }
    0.00 :   ffff8000101319fc:       ldp     x29, x30, [sp], #16
  100.00 :   ffff800010131a00:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (1 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001027b3d8 <file_free_rcu>:
         :                      file_free_rcu():
         :                      static struct kmem_cache *filp_cachep __read_mostly;
         :
         :                      static struct percpu_counter nr_files __cacheline_aligned_in_smp;
         :
         :                      static void file_free_rcu(struct rcu_head *head)
         :                      {
    0.00 :   ffff80001027b3d8:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff80001027b3dc:       mov     x1, x0
    0.00 :   ffff80001027b3e0:       mov     x29, sp
         :                      struct file *f = container_of(head, struct file, f_u.fu_rcuhead);
         :
         :                      put_cred(f->f_cred);
    0.00 :   ffff80001027b3e4:       ldr     x0, [x0, #144]
         :                      put_cred():
         :                      */
         :                      static inline void put_cred(const struct cred *_cred)
         :                      {
         :                      struct cred *cred = (struct cred *) _cred;
         :
         :                      if (cred) {
    0.00 :   ffff80001027b3e8:       cbz     x0, ffff80001027b408 <file_free_rcu+0x30>
         :                      arch_static_branch_jump():
         :                      }
         :
         :                      static __always_inline bool arch_static_branch_jump(struct static_key *key,
         :                      bool branch)
         :                      {
         :                      asm_volatile_goto(
    0.00 :   ffff80001027b3ec:       b       ffff80001027b41c <file_free_rcu+0x44>
    0.00 :   ffff80001027b3f0:       b       ffff80001027b41c <file_free_rcu+0x44>
         :                      __lse_atomic_sub_return():
         :                      }
         :
         :                      ATOMIC_OP_SUB_RETURN(_relaxed,   )
         :                      ATOMIC_OP_SUB_RETURN(_acquire,  a, "memory")
         :                      ATOMIC_OP_SUB_RETURN(_release,  l, "memory")
         :                      ATOMIC_OP_SUB_RETURN(        , al, "memory")
    0.00 :   ffff80001027b3f4:       mov     w2, #0x1                        // #1
    0.00 :   ffff80001027b3f8:       neg     w2, w2
    0.00 :   ffff80001027b3fc:       ldaddal w2, w3, [x0]
  100.00 :   ffff80001027b400:       add     w2, w2, w3
         :                      put_cred():
         :                      validate_creds(cred);
         :                      if (atomic_dec_and_test(&(cred)->usage))
    0.00 :   ffff80001027b404:       cbz     w2, ffff80001027b428 <file_free_rcu+0x50>
         :                      file_free_rcu():
         :                      kmem_cache_free(filp_cachep, f);
    0.00 :   ffff80001027b408:       adrp    x0, ffff80001189c000 <mm_slots_hash+0x1a28>
    0.00 :   ffff80001027b40c:       ldr     x0, [x0, #1816]
    0.00 :   ffff80001027b410:       bl      ffff800010250100 <kmem_cache_free>
         :                      }
    0.00 :   ffff80001027b414:       ldp     x29, x30, [sp], #32
    0.00 :   ffff80001027b418:       ret
         :                      __ll_sc_atomic_sub_return():
         :                      ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         :                      ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         :                      ATOMIC_OPS(add, add, I)
         :                      ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff80001027b41c:       mov     w3, #0x1                        // #1
    0.00 :   ffff80001027b420:       b       ffff80001027bcf4 <__fput_sync+0x54>
         :                      put_cred():
    0.00 :   ffff80001027b424:       cbnz    w2, ffff80001027b408 <file_free_rcu+0x30>
    0.00 :   ffff80001027b428:       str     x1, [x29, #24]
         :                      __put_cred(cred);
    0.00 :   ffff80001027b42c:       bl      ffff80001010e3c0 <__put_cred>
         :                      file_free_rcu():
         :                      kmem_cache_free(filp_cachep, f);
    0.00 :   ffff80001027b430:       adrp    x0, ffff80001189c000 <mm_slots_hash+0x1a28>
         :                      put_cred():
    0.00 :   ffff80001027b434:       ldr     x1, [x29, #24]
         :                      file_free_rcu():
    0.00 :   ffff80001027b438:       ldr     x0, [x0, #1816]
    0.00 :   ffff80001027b43c:       bl      ffff800010250100 <kmem_cache_free>
         :                      }
    0.00 :   ffff80001027b440:       ldp     x29, x30, [sp], #32
    0.00 :   ffff80001027b444:       ret
 Percent |	Source code & Disassembly of vmlinux for cycles (7 samples, percent: local period)
--------------------------------------------------------------------------------------------------
         :
         :
         :
         :                      Disassembly of section .text:
         :
         :                      ffff80001017ff88 <generic_exec_single>:
         :                      generic_exec_single():
         :                      * for execution on the given CPU. data must already have
         :                      * ->func, ->info, and ->flags set.
         :                      */
         :                      static int generic_exec_single(int cpu, call_single_data_t *csd,
         :                      smp_call_func_t func, void *info)
         :                      {
    0.00 :   ffff80001017ff88:       stp     x29, x30, [sp, #-32]!
         :                      if (cpu == smp_processor_id()) {
    0.00 :   ffff80001017ff8c:       adrp    x4, ffff8000114ca000 <bp_hardening_data>
    0.00 :   ffff80001017ff90:       add     x4, x4, #0x18
         :                      {
    0.00 :   ffff80001017ff94:       mov     x29, sp
    0.00 :   ffff80001017ff98:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80001017ff9c:       mov     w19, w0
         :                      __my_cpu_offset():
         :
         :                      /*
         :                      * We want to allow caching the value, so avoid using volatile and
         :                      * instead use a fake stack read to hazard against barrier().
         :                      */
         :                      asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff80001017ffa0:       mrs     x0, tpidr_el1
         :                      generic_exec_single():
         :                      if (cpu == smp_processor_id()) {
    0.00 :   ffff80001017ffa4:       ldr     w4, [x4, x0]
         :                      {
    0.00 :   ffff80001017ffa8:       mov     x0, x1
         :                      if (cpu == smp_processor_id()) {
    0.00 :   ffff80001017ffac:       cmp     w4, w19
    0.00 :   ffff80001017ffb0:       b.eq    ffff800010180040 <generic_exec_single+0xb8>  // b.none
         :                      local_irq_restore(flags);
         :                      return 0;
         :                      }
         :
         :
         :                      if ((unsigned)cpu >= nr_cpu_ids || !cpu_online(cpu)) {
    0.00 :   ffff80001017ffb4:       adrp    x4, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff80001017ffb8:       ldr     w4, [x4, #692]
    0.00 :   ffff80001017ffbc:       cmp     w19, w4
    0.00 :   ffff80001017ffc0:       b.cs    ffff800010180088 <generic_exec_single+0x100>  // b.hs, b.nlast
         :                      test_bit():
         :                      * @nr: bit number to test
         :                      * @addr: Address to start counting from
         :                      */
         :                      static inline int test_bit(int nr, const volatile unsigned long *addr)
         :                      {
         :                      return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff80001017ffc4:       add     w4, w19, #0x3f
    0.00 :   ffff80001017ffc8:       cmp     w19, #0x0
    0.00 :   ffff80001017ffcc:       csel    w4, w4, w19, lt  // lt = tstop
    0.00 :   ffff80001017ffd0:       adrp    x5, ffff80001189a000 <__per_cpu_offset+0x718>
    0.00 :   ffff80001017ffd4:       add     x5, x5, #0x120
    0.00 :   ffff80001017ffd8:       asr     w4, w4, #6
    0.00 :   ffff80001017ffdc:       sxtw    x4, w4
    0.00 :   ffff80001017ffe0:       ldr     x4, [x5, x4, lsl #3]
    0.00 :   ffff80001017ffe4:       lsr     x4, x4, x19
         :                      generic_exec_single():
    0.00 :   ffff80001017ffe8:       tbz     w4, #0, ffff800010180088 <generic_exec_single+0x100>
         :                      * in an architecture, sufficient synchronisation should be added
         :                      * to arch code to make it appear to obey cache coherency WRT
         :                      * locking and barrier primitives. Generic code isn't really
         :                      * equipped to do the right thing...
         :                      */
         :                      if (llist_add(&csd->llist, &per_cpu(call_single_queue, cpu)))
    0.00 :   ffff80001017ffec:       adrp    x4, ffff800011899000 <page_wait_table+0x1500>
    0.00 :   ffff80001017fff0:       add     x4, x4, #0x8e8
         :                      csd->info = info;
    0.00 :   ffff80001017fff4:       stp     x2, x3, [x1, #8]
         :                      arch_send_call_function_single_ipi(cpu);
         :
         :                      return 0;
    0.00 :   ffff80001017fff8:       mov     w20, #0x0                       // #0
         :                      if (llist_add(&csd->llist, &per_cpu(call_single_queue, cpu)))
    0.00 :   ffff80001017fffc:       adrp    x2, ffff8000114d6000 <runqueues+0x280>
    0.00 :   ffff800010180000:       add     x2, x2, #0xb40
    0.00 :   ffff800010180004:       ldr     x3, [x4, w19, sxtw #3]
         :                      llist_add():
         :                      *
         :                      * Returns true if the list was empty prior to adding this entry.
         :                      */
         :                      static inline bool llist_add(struct llist_node *new, struct llist_head *head)
         :                      {
         :                      return llist_add_batch(new, new, head);
    0.00 :   ffff800010180008:       add     x2, x2, x3
    0.00 :   ffff80001018000c:       bl      ffff800010487fa0 <llist_add_batch>
         :                      generic_exec_single():
    0.00 :   ffff800010180010:       tst     w0, #0xff
    0.00 :   ffff800010180014:       b.ne    ffff800010180028 <generic_exec_single+0xa0>  // b.any
         :                      }
    0.00 :   ffff800010180018:       mov     w0, w20
    0.00 :   ffff80001018001c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010180020:       ldp     x29, x30, [sp], #32
    0.00 :   ffff800010180024:       ret
         :                      arch_send_call_function_single_ipi(cpu);
    0.00 :   ffff800010180028:       mov     w0, w19
    0.00 :   ffff80001018002c:       bl      ffff800010095fb8 <arch_send_call_function_single_ipi>
         :                      }
    0.00 :   ffff800010180030:       mov     w0, w20
    0.00 :   ffff800010180034:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010180038:       ldp     x29, x30, [sp], #32
    0.00 :   ffff80001018003c:       ret
         :                      csd_unlock():
         :                      WARN_ON(!(csd->flags & CSD_FLAG_LOCK));
    0.00 :   ffff800010180040:       ldr     w1, [x1, #24]
    0.00 :   ffff800010180044:       tbz     w1, #0, ffff8000101800a4 <generic_exec_single+0x11c>
         :                      smp_store_release(&csd->flags, 0);
    0.00 :   ffff800010180048:       mov     w1, #0x0                        // #0
    0.00 :   ffff80001018004c:       add     x0, x0, #0x18
    0.00 :   ffff800010180050:       stlr    w1, [x0]
         :                      arch_local_save_flags():
         :                      */
         :                      static inline unsigned long arch_local_save_flags(void)
         :                      {
         :                      unsigned long flags;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010180054:       mrs     x19, daif
         :                      arch_irqs_disabled_flags():
         :
         :                      static inline int arch_irqs_disabled_flags(unsigned long flags)
         :                      {
         :                      int res;
         :
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010180058:       and     w0, w19, #0x80
         :                      arch_local_irq_save():
         :
         :                      /*
         :                      * There are too many states with IRQs disabled, just keep the current
         :                      * state if interrupts are already disabled/masked.
         :                      */
         :                      if (!arch_irqs_disabled_flags(flags))
    0.00 :   ffff80001018005c:       cbnz    w0, ffff800010180068 <generic_exec_single+0xe0>
         :                      arch_local_irq_disable():
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010180060:       mov     x0, #0x60                       // #96
    0.00 :   ffff800010180064:       msr     daifset, #0x2
         :                      generic_exec_single():
         :                      func(info);
    0.00 :   ffff800010180068:       mov     x0, x3
    0.00 :   ffff80001018006c:       blr     x2
         :                      arch_local_irq_restore():
         :                      /*
         :                      * restore saved IRQ state
         :                      */
         :                      static inline void arch_local_irq_restore(unsigned long flags)
         :                      {
         :                      asm volatile(ALTERNATIVE(
    0.00 :   ffff800010180070:       msr     daif, x19
         :                      generic_exec_single():
         :                      return 0;
  100.00 :   ffff800010180074:       mov     w20, #0x0                       // #0
         :                      }
    0.00 :   ffff800010180078:       mov     w0, w20
    0.00 :   ffff80001018007c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800010180080:       ldp     x29, x30, [sp], #32
    0.00 :   ffff800010180084:       ret
         :                      csd_unlock():
         :                      WARN_ON(!(csd->flags & CSD_FLAG_LOCK));
    0.00 :   ffff800010180088:       ldr     w1, [x0, #24]
    0.00 :   ffff80001018008c:       tbz     w1, #0, ffff8000101800ac <generic_exec_single+0x124>
         :                      smp_store_release(&csd->flags, 0);
    0.00 :   ffff800010180090:       mov     w1, #0x0                        // #0
    0.00 :   ffff800010180094:       add     x0, x0, #0x18
    0.00 :   ffff800010180098:       stlr    w1, [x0]
         :                      generic_exec_single():
         :                      return -ENXIO;
    0.00 :   ffff80001018009c:       mov     w20, #0xfffffffa                // #-6
    0.00 :   ffff8000101800a0:       b       ffff800010180018 <generic_exec_single+0x90>
         :                      csd_unlock():
         :                      WARN_ON(!(csd->flags & CSD_FLAG_LOCK));
    0.00 :   ffff8000101800a4:       brk     #0x800
    0.00 :   ffff8000101800a8:       b       ffff800010180048 <generic_exec_single+0xc0>
    0.00 :   ffff8000101800ac:       brk     #0x800
    0.00 :   ffff8000101800b0:       b       ffff800010180090 <generic_exec_single+0x108>
